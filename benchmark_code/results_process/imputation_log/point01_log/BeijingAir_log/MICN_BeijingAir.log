2024-06-04 02:49:23 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-04 02:49:23 [INFO]: Using the given device: cuda:0
2024-06-04 02:49:24 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_0/20240604_T024924
2024-06-04 02:49:24 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_0/20240604_T024924/tensorboard
2024-06-04 02:49:26 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 57,048,200
2024-06-04 02:49:39 [INFO]: Epoch 001 - training loss: 0.7655, validation loss: 0.2710
2024-06-04 02:49:43 [INFO]: Epoch 002 - training loss: 0.5917, validation loss: 0.2657
2024-06-04 02:49:48 [INFO]: Epoch 003 - training loss: 0.5774, validation loss: 0.2581
2024-06-04 02:49:53 [INFO]: Epoch 004 - training loss: 0.5583, validation loss: 0.2560
2024-06-04 02:49:57 [INFO]: Epoch 005 - training loss: 0.5443, validation loss: 0.2520
2024-06-04 02:50:02 [INFO]: Epoch 006 - training loss: 0.5346, validation loss: 0.2492
2024-06-04 02:50:07 [INFO]: Epoch 007 - training loss: 0.5262, validation loss: 0.2488
2024-06-04 02:50:12 [INFO]: Epoch 008 - training loss: 0.5188, validation loss: 0.2451
2024-06-04 02:50:17 [INFO]: Epoch 009 - training loss: 0.5130, validation loss: 0.2455
2024-06-04 02:50:22 [INFO]: Epoch 010 - training loss: 0.5054, validation loss: 0.2393
2024-06-04 02:50:27 [INFO]: Epoch 011 - training loss: 0.5039, validation loss: 0.2375
2024-06-04 02:50:31 [INFO]: Epoch 012 - training loss: 0.4939, validation loss: 0.2345
2024-06-04 02:50:36 [INFO]: Epoch 013 - training loss: 0.4873, validation loss: 0.2328
2024-06-04 02:50:41 [INFO]: Epoch 014 - training loss: 0.4811, validation loss: 0.2299
2024-06-04 02:50:46 [INFO]: Epoch 015 - training loss: 0.4762, validation loss: 0.2270
2024-06-04 02:50:51 [INFO]: Epoch 016 - training loss: 0.4729, validation loss: 0.2235
2024-06-04 02:50:55 [INFO]: Epoch 017 - training loss: 0.4656, validation loss: 0.2238
2024-06-04 02:51:00 [INFO]: Epoch 018 - training loss: 0.4659, validation loss: 0.2206
2024-06-04 02:51:04 [INFO]: Epoch 019 - training loss: 0.4600, validation loss: 0.2160
2024-06-04 02:51:09 [INFO]: Epoch 020 - training loss: 0.4553, validation loss: 0.2157
2024-06-04 02:51:13 [INFO]: Epoch 021 - training loss: 0.4537, validation loss: 0.2122
2024-06-04 02:51:18 [INFO]: Epoch 022 - training loss: 0.4508, validation loss: 0.2113
2024-06-04 02:51:22 [INFO]: Epoch 023 - training loss: 0.4456, validation loss: 0.2138
2024-06-04 02:51:27 [INFO]: Epoch 024 - training loss: 0.4422, validation loss: 0.2100
2024-06-04 02:51:32 [INFO]: Epoch 025 - training loss: 0.4371, validation loss: 0.2101
2024-06-04 02:51:36 [INFO]: Epoch 026 - training loss: 0.4325, validation loss: 0.2079
2024-06-04 02:51:42 [INFO]: Epoch 027 - training loss: 0.4323, validation loss: 0.2076
2024-06-04 02:51:46 [INFO]: Epoch 028 - training loss: 0.4311, validation loss: 0.2054
2024-06-04 02:51:51 [INFO]: Epoch 029 - training loss: 0.4285, validation loss: 0.2068
2024-06-04 02:51:55 [INFO]: Epoch 030 - training loss: 0.4229, validation loss: 0.2047
2024-06-04 02:52:00 [INFO]: Epoch 031 - training loss: 0.4207, validation loss: 0.2063
2024-06-04 02:52:04 [INFO]: Epoch 032 - training loss: 0.4192, validation loss: 0.2045
2024-06-04 02:52:09 [INFO]: Epoch 033 - training loss: 0.4165, validation loss: 0.2054
2024-06-04 02:52:13 [INFO]: Epoch 034 - training loss: 0.4142, validation loss: 0.2036
2024-06-04 02:52:18 [INFO]: Epoch 035 - training loss: 0.4116, validation loss: 0.2034
2024-06-04 02:52:23 [INFO]: Epoch 036 - training loss: 0.4111, validation loss: 0.2015
2024-06-04 02:52:28 [INFO]: Epoch 037 - training loss: 0.4099, validation loss: 0.2018
2024-06-04 02:52:32 [INFO]: Epoch 038 - training loss: 0.4061, validation loss: 0.2029
2024-06-04 02:52:37 [INFO]: Epoch 039 - training loss: 0.4053, validation loss: 0.2007
2024-06-04 02:52:42 [INFO]: Epoch 040 - training loss: 0.4043, validation loss: 0.2024
2024-06-04 02:52:47 [INFO]: Epoch 041 - training loss: 0.4034, validation loss: 0.2021
2024-06-04 02:52:51 [INFO]: Epoch 042 - training loss: 0.4023, validation loss: 0.1991
2024-06-04 02:52:56 [INFO]: Epoch 043 - training loss: 0.4010, validation loss: 0.2026
2024-06-04 02:53:01 [INFO]: Epoch 044 - training loss: 0.3988, validation loss: 0.2006
2024-06-04 02:53:05 [INFO]: Epoch 045 - training loss: 0.3983, validation loss: 0.2006
2024-06-04 02:53:10 [INFO]: Epoch 046 - training loss: 0.3956, validation loss: 0.2008
2024-06-04 02:53:15 [INFO]: Epoch 047 - training loss: 0.3966, validation loss: 0.1982
2024-06-04 02:53:19 [INFO]: Epoch 048 - training loss: 0.3962, validation loss: 0.1985
2024-06-04 02:53:24 [INFO]: Epoch 049 - training loss: 0.3934, validation loss: 0.2010
2024-06-04 02:53:28 [INFO]: Epoch 050 - training loss: 0.3934, validation loss: 0.1976
2024-06-04 02:53:32 [INFO]: Epoch 051 - training loss: 0.3908, validation loss: 0.1977
2024-06-04 02:53:37 [INFO]: Epoch 052 - training loss: 0.3897, validation loss: 0.1998
2024-06-04 02:53:42 [INFO]: Epoch 053 - training loss: 0.3890, validation loss: 0.1957
2024-06-04 02:53:46 [INFO]: Epoch 054 - training loss: 0.3879, validation loss: 0.2004
2024-06-04 02:53:51 [INFO]: Epoch 055 - training loss: 0.3870, validation loss: 0.1973
2024-06-04 02:53:56 [INFO]: Epoch 056 - training loss: 0.3849, validation loss: 0.1992
2024-06-04 02:54:01 [INFO]: Epoch 057 - training loss: 0.3827, validation loss: 0.1994
2024-06-04 02:54:05 [INFO]: Epoch 058 - training loss: 0.3835, validation loss: 0.1968
2024-06-04 02:54:10 [INFO]: Epoch 059 - training loss: 0.3824, validation loss: 0.1962
2024-06-04 02:54:15 [INFO]: Epoch 060 - training loss: 0.3810, validation loss: 0.1944
2024-06-04 02:54:20 [INFO]: Epoch 061 - training loss: 0.3803, validation loss: 0.1933
2024-06-04 02:54:24 [INFO]: Epoch 062 - training loss: 0.3795, validation loss: 0.1962
2024-06-04 02:54:29 [INFO]: Epoch 063 - training loss: 0.3779, validation loss: 0.1947
2024-06-04 02:54:34 [INFO]: Epoch 064 - training loss: 0.3793, validation loss: 0.1939
2024-06-04 02:54:39 [INFO]: Epoch 065 - training loss: 0.3769, validation loss: 0.1931
2024-06-04 02:54:44 [INFO]: Epoch 066 - training loss: 0.3744, validation loss: 0.1930
2024-06-04 02:54:48 [INFO]: Epoch 067 - training loss: 0.3753, validation loss: 0.1931
2024-06-04 02:54:53 [INFO]: Epoch 068 - training loss: 0.3730, validation loss: 0.1948
2024-06-04 02:54:58 [INFO]: Epoch 069 - training loss: 0.3745, validation loss: 0.1961
2024-06-04 02:55:03 [INFO]: Epoch 070 - training loss: 0.3731, validation loss: 0.1927
2024-06-04 02:55:07 [INFO]: Epoch 071 - training loss: 0.3712, validation loss: 0.1959
2024-06-04 02:55:12 [INFO]: Epoch 072 - training loss: 0.3696, validation loss: 0.1945
2024-06-04 02:55:16 [INFO]: Epoch 073 - training loss: 0.3722, validation loss: 0.1937
2024-06-04 02:55:21 [INFO]: Epoch 074 - training loss: 0.3704, validation loss: 0.1935
2024-06-04 02:55:25 [INFO]: Epoch 075 - training loss: 0.3699, validation loss: 0.1921
2024-06-04 02:55:30 [INFO]: Epoch 076 - training loss: 0.3687, validation loss: 0.1935
2024-06-04 02:55:35 [INFO]: Epoch 077 - training loss: 0.3693, validation loss: 0.1928
2024-06-04 02:55:39 [INFO]: Epoch 078 - training loss: 0.3681, validation loss: 0.1926
2024-06-04 02:55:44 [INFO]: Epoch 079 - training loss: 0.3674, validation loss: 0.1916
2024-06-04 02:55:49 [INFO]: Epoch 080 - training loss: 0.3654, validation loss: 0.1969
2024-06-04 02:55:54 [INFO]: Epoch 081 - training loss: 0.3647, validation loss: 0.1936
2024-06-04 02:55:58 [INFO]: Epoch 082 - training loss: 0.3647, validation loss: 0.1919
2024-06-04 02:56:03 [INFO]: Epoch 083 - training loss: 0.3647, validation loss: 0.1917
2024-06-04 02:56:07 [INFO]: Epoch 084 - training loss: 0.3646, validation loss: 0.1918
2024-06-04 02:56:12 [INFO]: Epoch 085 - training loss: 0.3642, validation loss: 0.1900
2024-06-04 02:56:17 [INFO]: Epoch 086 - training loss: 0.3640, validation loss: 0.1895
2024-06-04 02:56:21 [INFO]: Epoch 087 - training loss: 0.3625, validation loss: 0.1899
2024-06-04 02:56:26 [INFO]: Epoch 088 - training loss: 0.3623, validation loss: 0.1919
2024-06-04 02:56:31 [INFO]: Epoch 089 - training loss: 0.3618, validation loss: 0.1909
2024-06-04 02:56:36 [INFO]: Epoch 090 - training loss: 0.3604, validation loss: 0.1891
2024-06-04 02:56:40 [INFO]: Epoch 091 - training loss: 0.3586, validation loss: 0.1887
2024-06-04 02:56:45 [INFO]: Epoch 092 - training loss: 0.3593, validation loss: 0.1901
2024-06-04 02:56:50 [INFO]: Epoch 093 - training loss: 0.3595, validation loss: 0.1914
2024-06-04 02:56:54 [INFO]: Epoch 094 - training loss: 0.3588, validation loss: 0.1897
2024-06-04 02:56:59 [INFO]: Epoch 095 - training loss: 0.3578, validation loss: 0.1903
2024-06-04 02:57:04 [INFO]: Epoch 096 - training loss: 0.3579, validation loss: 0.1904
2024-06-04 02:57:09 [INFO]: Epoch 097 - training loss: 0.3573, validation loss: 0.1917
2024-06-04 02:57:14 [INFO]: Epoch 098 - training loss: 0.3548, validation loss: 0.1890
2024-06-04 02:57:18 [INFO]: Epoch 099 - training loss: 0.3549, validation loss: 0.1883
2024-06-04 02:57:23 [INFO]: Epoch 100 - training loss: 0.3556, validation loss: 0.1890
2024-06-04 02:57:23 [INFO]: Finished training. The best model is from epoch#99.
2024-06-04 02:57:24 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_0/20240604_T024924/MICN.pypots
2024-06-04 02:57:26 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_0/imputation.pkl
2024-06-04 02:57:26 [INFO]: Round0 - MICN on BeijingAir: MAE=0.2286, MSE=0.1770, MRE=0.3454
2024-06-04 02:57:26 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-04 02:57:26 [INFO]: Using the given device: cuda:0
2024-06-04 02:57:26 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_1/20240604_T025726
2024-06-04 02:57:26 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_1/20240604_T025726/tensorboard
2024-06-04 02:57:28 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 57,048,200
2024-06-04 02:57:32 [INFO]: Epoch 001 - training loss: 0.7657, validation loss: 0.2648
2024-06-04 02:57:37 [INFO]: Epoch 002 - training loss: 0.5975, validation loss: 0.2576
2024-06-04 02:57:42 [INFO]: Epoch 003 - training loss: 0.5779, validation loss: 0.2545
2024-06-04 02:57:46 [INFO]: Epoch 004 - training loss: 0.5586, validation loss: 0.2467
2024-06-04 02:57:51 [INFO]: Epoch 005 - training loss: 0.5375, validation loss: 0.2420
2024-06-04 02:57:56 [INFO]: Epoch 006 - training loss: 0.5293, validation loss: 0.2386
2024-06-04 02:58:00 [INFO]: Epoch 007 - training loss: 0.5193, validation loss: 0.2369
2024-06-04 02:58:05 [INFO]: Epoch 008 - training loss: 0.5058, validation loss: 0.2370
2024-06-04 02:58:09 [INFO]: Epoch 009 - training loss: 0.4994, validation loss: 0.2305
2024-06-04 02:58:14 [INFO]: Epoch 010 - training loss: 0.4919, validation loss: 0.2300
2024-06-04 02:58:19 [INFO]: Epoch 011 - training loss: 0.4854, validation loss: 0.2272
2024-06-04 02:58:24 [INFO]: Epoch 012 - training loss: 0.4776, validation loss: 0.2249
2024-06-04 02:58:29 [INFO]: Epoch 013 - training loss: 0.4737, validation loss: 0.2206
2024-06-04 02:58:34 [INFO]: Epoch 014 - training loss: 0.4671, validation loss: 0.2184
2024-06-04 02:58:39 [INFO]: Epoch 015 - training loss: 0.4626, validation loss: 0.2177
2024-06-04 02:58:43 [INFO]: Epoch 016 - training loss: 0.4569, validation loss: 0.2181
2024-06-04 02:58:48 [INFO]: Epoch 017 - training loss: 0.4538, validation loss: 0.2165
2024-06-04 02:58:53 [INFO]: Epoch 018 - training loss: 0.4491, validation loss: 0.2132
2024-06-04 02:58:58 [INFO]: Epoch 019 - training loss: 0.4453, validation loss: 0.2134
2024-06-04 02:59:03 [INFO]: Epoch 020 - training loss: 0.4443, validation loss: 0.2109
2024-06-04 02:59:07 [INFO]: Epoch 021 - training loss: 0.4414, validation loss: 0.2099
2024-06-04 02:59:12 [INFO]: Epoch 022 - training loss: 0.4350, validation loss: 0.2089
2024-06-04 02:59:17 [INFO]: Epoch 023 - training loss: 0.4355, validation loss: 0.2080
2024-06-04 02:59:21 [INFO]: Epoch 024 - training loss: 0.4321, validation loss: 0.2066
2024-06-04 02:59:26 [INFO]: Epoch 025 - training loss: 0.4270, validation loss: 0.2061
2024-06-04 02:59:31 [INFO]: Epoch 026 - training loss: 0.4238, validation loss: 0.2082
2024-06-04 02:59:36 [INFO]: Epoch 027 - training loss: 0.4216, validation loss: 0.2041
2024-06-04 02:59:40 [INFO]: Epoch 028 - training loss: 0.4177, validation loss: 0.2044
2024-06-04 02:59:46 [INFO]: Epoch 029 - training loss: 0.4149, validation loss: 0.2063
2024-06-04 02:59:50 [INFO]: Epoch 030 - training loss: 0.4129, validation loss: 0.2045
2024-06-04 02:59:55 [INFO]: Epoch 031 - training loss: 0.4115, validation loss: 0.2037
2024-06-04 02:59:59 [INFO]: Epoch 032 - training loss: 0.4104, validation loss: 0.2053
2024-06-04 03:00:04 [INFO]: Epoch 033 - training loss: 0.4086, validation loss: 0.2034
2024-06-04 03:00:09 [INFO]: Epoch 034 - training loss: 0.4098, validation loss: 0.2037
2024-06-04 03:00:13 [INFO]: Epoch 035 - training loss: 0.4062, validation loss: 0.2043
2024-06-04 03:00:18 [INFO]: Epoch 036 - training loss: 0.4037, validation loss: 0.2035
2024-06-04 03:00:23 [INFO]: Epoch 037 - training loss: 0.4016, validation loss: 0.2002
2024-06-04 03:00:27 [INFO]: Epoch 038 - training loss: 0.3988, validation loss: 0.2014
2024-06-04 03:00:32 [INFO]: Epoch 039 - training loss: 0.4007, validation loss: 0.2009
2024-06-04 03:00:36 [INFO]: Epoch 040 - training loss: 0.3976, validation loss: 0.2005
2024-06-04 03:00:41 [INFO]: Epoch 041 - training loss: 0.3967, validation loss: 0.1980
2024-06-04 03:00:46 [INFO]: Epoch 042 - training loss: 0.3961, validation loss: 0.1989
2024-06-04 03:00:50 [INFO]: Epoch 043 - training loss: 0.3951, validation loss: 0.1991
2024-06-04 03:00:55 [INFO]: Epoch 044 - training loss: 0.3924, validation loss: 0.1976
2024-06-04 03:01:00 [INFO]: Epoch 045 - training loss: 0.3913, validation loss: 0.1983
2024-06-04 03:01:05 [INFO]: Epoch 046 - training loss: 0.3922, validation loss: 0.1978
2024-06-04 03:01:09 [INFO]: Epoch 047 - training loss: 0.3890, validation loss: 0.1969
2024-06-04 03:01:14 [INFO]: Epoch 048 - training loss: 0.3896, validation loss: 0.1957
2024-06-04 03:01:20 [INFO]: Epoch 049 - training loss: 0.3875, validation loss: 0.1956
2024-06-04 03:01:25 [INFO]: Epoch 050 - training loss: 0.3872, validation loss: 0.1968
2024-06-04 03:01:29 [INFO]: Epoch 051 - training loss: 0.3849, validation loss: 0.1941
2024-06-04 03:01:33 [INFO]: Epoch 052 - training loss: 0.3851, validation loss: 0.1943
2024-06-04 03:01:38 [INFO]: Epoch 053 - training loss: 0.3819, validation loss: 0.1954
2024-06-04 03:01:41 [INFO]: Epoch 054 - training loss: 0.3822, validation loss: 0.1942
2024-06-04 03:01:45 [INFO]: Epoch 055 - training loss: 0.3818, validation loss: 0.1969
2024-06-04 03:01:49 [INFO]: Epoch 056 - training loss: 0.3807, validation loss: 0.1938
2024-06-04 03:01:54 [INFO]: Epoch 057 - training loss: 0.3804, validation loss: 0.1939
2024-06-04 03:01:59 [INFO]: Epoch 058 - training loss: 0.3781, validation loss: 0.1947
2024-06-04 03:02:03 [INFO]: Epoch 059 - training loss: 0.3786, validation loss: 0.1953
2024-06-04 03:02:08 [INFO]: Epoch 060 - training loss: 0.3768, validation loss: 0.1932
2024-06-04 03:02:13 [INFO]: Epoch 061 - training loss: 0.3776, validation loss: 0.1932
2024-06-04 03:02:17 [INFO]: Epoch 062 - training loss: 0.3754, validation loss: 0.1931
2024-06-04 03:02:22 [INFO]: Epoch 063 - training loss: 0.3747, validation loss: 0.1939
2024-06-04 03:02:27 [INFO]: Epoch 064 - training loss: 0.3750, validation loss: 0.1931
2024-06-04 03:02:32 [INFO]: Epoch 065 - training loss: 0.3733, validation loss: 0.1950
2024-06-04 03:02:37 [INFO]: Epoch 066 - training loss: 0.3734, validation loss: 0.1945
2024-06-04 03:02:41 [INFO]: Epoch 067 - training loss: 0.3716, validation loss: 0.1931
2024-06-04 03:02:46 [INFO]: Epoch 068 - training loss: 0.3713, validation loss: 0.1928
2024-06-04 03:02:50 [INFO]: Epoch 069 - training loss: 0.3716, validation loss: 0.1909
2024-06-04 03:02:55 [INFO]: Epoch 070 - training loss: 0.3704, validation loss: 0.1929
2024-06-04 03:02:59 [INFO]: Epoch 071 - training loss: 0.3698, validation loss: 0.1913
2024-06-04 03:03:04 [INFO]: Epoch 072 - training loss: 0.3700, validation loss: 0.1922
2024-06-04 03:03:09 [INFO]: Epoch 073 - training loss: 0.3691, validation loss: 0.1909
2024-06-04 03:03:14 [INFO]: Epoch 074 - training loss: 0.3671, validation loss: 0.1906
2024-06-04 03:03:19 [INFO]: Epoch 075 - training loss: 0.3661, validation loss: 0.1893
2024-06-04 03:03:24 [INFO]: Epoch 076 - training loss: 0.3656, validation loss: 0.1923
2024-06-04 03:03:28 [INFO]: Epoch 077 - training loss: 0.3650, validation loss: 0.1911
2024-06-04 03:03:33 [INFO]: Epoch 078 - training loss: 0.3647, validation loss: 0.1896
2024-06-04 03:03:38 [INFO]: Epoch 079 - training loss: 0.3637, validation loss: 0.1907
2024-06-04 03:03:43 [INFO]: Epoch 080 - training loss: 0.3629, validation loss: 0.1914
2024-06-04 03:03:48 [INFO]: Epoch 081 - training loss: 0.3631, validation loss: 0.1896
2024-06-04 03:03:52 [INFO]: Epoch 082 - training loss: 0.3629, validation loss: 0.1925
2024-06-04 03:03:57 [INFO]: Epoch 083 - training loss: 0.3620, validation loss: 0.1942
2024-06-04 03:04:03 [INFO]: Epoch 084 - training loss: 0.3618, validation loss: 0.1891
2024-06-04 03:04:07 [INFO]: Epoch 085 - training loss: 0.3620, validation loss: 0.1916
2024-06-04 03:04:12 [INFO]: Epoch 086 - training loss: 0.3607, validation loss: 0.1893
2024-06-04 03:04:17 [INFO]: Epoch 087 - training loss: 0.3594, validation loss: 0.1901
2024-06-04 03:04:22 [INFO]: Epoch 088 - training loss: 0.3579, validation loss: 0.1912
2024-06-04 03:04:27 [INFO]: Epoch 089 - training loss: 0.3565, validation loss: 0.1874
2024-06-04 03:04:31 [INFO]: Epoch 090 - training loss: 0.3581, validation loss: 0.1891
2024-06-04 03:04:36 [INFO]: Epoch 091 - training loss: 0.3570, validation loss: 0.1876
2024-06-04 03:04:41 [INFO]: Epoch 092 - training loss: 0.3573, validation loss: 0.1879
2024-06-04 03:04:45 [INFO]: Epoch 093 - training loss: 0.3565, validation loss: 0.1904
2024-06-04 03:04:50 [INFO]: Epoch 094 - training loss: 0.3568, validation loss: 0.1887
2024-06-04 03:04:55 [INFO]: Epoch 095 - training loss: 0.3558, validation loss: 0.1899
2024-06-04 03:04:59 [INFO]: Epoch 096 - training loss: 0.3536, validation loss: 0.1907
2024-06-04 03:05:04 [INFO]: Epoch 097 - training loss: 0.3541, validation loss: 0.1902
2024-06-04 03:05:09 [INFO]: Epoch 098 - training loss: 0.3531, validation loss: 0.1933
2024-06-04 03:05:14 [INFO]: Epoch 099 - training loss: 0.3536, validation loss: 0.1879
2024-06-04 03:05:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:05:14 [INFO]: Finished training. The best model is from epoch#89.
2024-06-04 03:05:15 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_1/20240604_T025726/MICN.pypots
2024-06-04 03:05:16 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_1/imputation.pkl
2024-06-04 03:05:16 [INFO]: Round1 - MICN on BeijingAir: MAE=0.2259, MSE=0.1747, MRE=0.3413
2024-06-04 03:05:16 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-04 03:05:16 [INFO]: Using the given device: cuda:0
2024-06-04 03:05:16 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_2/20240604_T030516
2024-06-04 03:05:16 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_2/20240604_T030516/tensorboard
2024-06-04 03:05:18 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 57,048,200
2024-06-04 03:05:23 [INFO]: Epoch 001 - training loss: 0.7596, validation loss: 0.2674
2024-06-04 03:05:28 [INFO]: Epoch 002 - training loss: 0.5830, validation loss: 0.2572
2024-06-04 03:05:33 [INFO]: Epoch 003 - training loss: 0.5677, validation loss: 0.2535
2024-06-04 03:05:38 [INFO]: Epoch 004 - training loss: 0.5509, validation loss: 0.2510
2024-06-04 03:05:42 [INFO]: Epoch 005 - training loss: 0.5391, validation loss: 0.2497
2024-06-04 03:05:47 [INFO]: Epoch 006 - training loss: 0.5248, validation loss: 0.2479
2024-06-04 03:05:51 [INFO]: Epoch 007 - training loss: 0.5154, validation loss: 0.2453
2024-06-04 03:05:56 [INFO]: Epoch 008 - training loss: 0.5095, validation loss: 0.2432
2024-06-04 03:06:00 [INFO]: Epoch 009 - training loss: 0.5038, validation loss: 0.2427
2024-06-04 03:06:04 [INFO]: Epoch 010 - training loss: 0.4982, validation loss: 0.2394
2024-06-04 03:06:09 [INFO]: Epoch 011 - training loss: 0.4891, validation loss: 0.2373
2024-06-04 03:06:13 [INFO]: Epoch 012 - training loss: 0.4842, validation loss: 0.2380
2024-06-04 03:06:17 [INFO]: Epoch 013 - training loss: 0.4817, validation loss: 0.2320
2024-06-04 03:06:22 [INFO]: Epoch 014 - training loss: 0.4755, validation loss: 0.2325
2024-06-04 03:06:26 [INFO]: Epoch 015 - training loss: 0.4728, validation loss: 0.2284
2024-06-04 03:06:31 [INFO]: Epoch 016 - training loss: 0.4680, validation loss: 0.2264
2024-06-04 03:06:35 [INFO]: Epoch 017 - training loss: 0.4636, validation loss: 0.2237
2024-06-04 03:06:39 [INFO]: Epoch 018 - training loss: 0.4594, validation loss: 0.2217
2024-06-04 03:06:43 [INFO]: Epoch 019 - training loss: 0.4552, validation loss: 0.2225
2024-06-04 03:06:47 [INFO]: Epoch 020 - training loss: 0.4516, validation loss: 0.2186
2024-06-04 03:06:52 [INFO]: Epoch 021 - training loss: 0.4472, validation loss: 0.2178
2024-06-04 03:06:56 [INFO]: Epoch 022 - training loss: 0.4439, validation loss: 0.2184
2024-06-04 03:07:01 [INFO]: Epoch 023 - training loss: 0.4430, validation loss: 0.2164
2024-06-04 03:07:05 [INFO]: Epoch 024 - training loss: 0.4374, validation loss: 0.2143
2024-06-04 03:07:10 [INFO]: Epoch 025 - training loss: 0.4343, validation loss: 0.2121
2024-06-04 03:07:14 [INFO]: Epoch 026 - training loss: 0.4321, validation loss: 0.2124
2024-06-04 03:07:18 [INFO]: Epoch 027 - training loss: 0.4292, validation loss: 0.2113
2024-06-04 03:07:22 [INFO]: Epoch 028 - training loss: 0.4273, validation loss: 0.2122
2024-06-04 03:07:26 [INFO]: Epoch 029 - training loss: 0.4251, validation loss: 0.2119
2024-06-04 03:07:30 [INFO]: Epoch 030 - training loss: 0.4225, validation loss: 0.2091
2024-06-04 03:07:34 [INFO]: Epoch 031 - training loss: 0.4212, validation loss: 0.2067
2024-06-04 03:07:39 [INFO]: Epoch 032 - training loss: 0.4187, validation loss: 0.2106
2024-06-04 03:07:43 [INFO]: Epoch 033 - training loss: 0.4178, validation loss: 0.2073
2024-06-04 03:07:47 [INFO]: Epoch 034 - training loss: 0.4172, validation loss: 0.2078
2024-06-04 03:07:51 [INFO]: Epoch 035 - training loss: 0.4138, validation loss: 0.2095
2024-06-04 03:07:56 [INFO]: Epoch 036 - training loss: 0.4125, validation loss: 0.2038
2024-06-04 03:08:00 [INFO]: Epoch 037 - training loss: 0.4100, validation loss: 0.2037
2024-06-04 03:08:04 [INFO]: Epoch 038 - training loss: 0.4088, validation loss: 0.2042
2024-06-04 03:08:08 [INFO]: Epoch 039 - training loss: 0.4081, validation loss: 0.2039
2024-06-04 03:08:13 [INFO]: Epoch 040 - training loss: 0.4056, validation loss: 0.2039
2024-06-04 03:08:17 [INFO]: Epoch 041 - training loss: 0.4028, validation loss: 0.2043
2024-06-04 03:08:21 [INFO]: Epoch 042 - training loss: 0.4029, validation loss: 0.2034
2024-06-04 03:08:26 [INFO]: Epoch 043 - training loss: 0.4010, validation loss: 0.2028
2024-06-04 03:08:30 [INFO]: Epoch 044 - training loss: 0.4003, validation loss: 0.2040
2024-06-04 03:08:34 [INFO]: Epoch 045 - training loss: 0.3992, validation loss: 0.2024
2024-06-04 03:08:38 [INFO]: Epoch 046 - training loss: 0.3965, validation loss: 0.2006
2024-06-04 03:08:43 [INFO]: Epoch 047 - training loss: 0.3952, validation loss: 0.2004
2024-06-04 03:08:47 [INFO]: Epoch 048 - training loss: 0.3950, validation loss: 0.2025
2024-06-04 03:08:51 [INFO]: Epoch 049 - training loss: 0.3929, validation loss: 0.2007
2024-06-04 03:08:56 [INFO]: Epoch 050 - training loss: 0.3938, validation loss: 0.2002
2024-06-04 03:09:00 [INFO]: Epoch 051 - training loss: 0.3915, validation loss: 0.2016
2024-06-04 03:09:04 [INFO]: Epoch 052 - training loss: 0.3903, validation loss: 0.1991
2024-06-04 03:09:08 [INFO]: Epoch 053 - training loss: 0.3892, validation loss: 0.1983
2024-06-04 03:09:13 [INFO]: Epoch 054 - training loss: 0.3889, validation loss: 0.2012
2024-06-04 03:09:17 [INFO]: Epoch 055 - training loss: 0.3882, validation loss: 0.1978
2024-06-04 03:09:22 [INFO]: Epoch 056 - training loss: 0.3869, validation loss: 0.1977
2024-06-04 03:09:26 [INFO]: Epoch 057 - training loss: 0.3847, validation loss: 0.1973
2024-06-04 03:09:30 [INFO]: Epoch 058 - training loss: 0.3846, validation loss: 0.1981
2024-06-04 03:09:34 [INFO]: Epoch 059 - training loss: 0.3847, validation loss: 0.1969
2024-06-04 03:09:38 [INFO]: Epoch 060 - training loss: 0.3824, validation loss: 0.1952
2024-06-04 03:09:43 [INFO]: Epoch 061 - training loss: 0.3839, validation loss: 0.1991
2024-06-04 03:09:47 [INFO]: Epoch 062 - training loss: 0.3812, validation loss: 0.1962
2024-06-04 03:09:52 [INFO]: Epoch 063 - training loss: 0.3807, validation loss: 0.1946
2024-06-04 03:09:56 [INFO]: Epoch 064 - training loss: 0.3803, validation loss: 0.1954
2024-06-04 03:10:00 [INFO]: Epoch 065 - training loss: 0.3789, validation loss: 0.1941
2024-06-04 03:10:04 [INFO]: Epoch 066 - training loss: 0.3797, validation loss: 0.1951
2024-06-04 03:10:08 [INFO]: Epoch 067 - training loss: 0.3781, validation loss: 0.1955
2024-06-04 03:10:12 [INFO]: Epoch 068 - training loss: 0.3770, validation loss: 0.1950
2024-06-04 03:10:16 [INFO]: Epoch 069 - training loss: 0.3767, validation loss: 0.1939
2024-06-04 03:10:20 [INFO]: Epoch 070 - training loss: 0.3768, validation loss: 0.1944
2024-06-04 03:10:24 [INFO]: Epoch 071 - training loss: 0.3762, validation loss: 0.1939
2024-06-04 03:10:29 [INFO]: Epoch 072 - training loss: 0.3749, validation loss: 0.1929
2024-06-04 03:10:33 [INFO]: Epoch 073 - training loss: 0.3726, validation loss: 0.1937
2024-06-04 03:10:37 [INFO]: Epoch 074 - training loss: 0.3728, validation loss: 0.1940
2024-06-04 03:10:42 [INFO]: Epoch 075 - training loss: 0.3712, validation loss: 0.1940
2024-06-04 03:10:46 [INFO]: Epoch 076 - training loss: 0.3727, validation loss: 0.1947
2024-06-04 03:10:50 [INFO]: Epoch 077 - training loss: 0.3716, validation loss: 0.1923
2024-06-04 03:10:54 [INFO]: Epoch 078 - training loss: 0.3704, validation loss: 0.1933
2024-06-04 03:10:59 [INFO]: Epoch 079 - training loss: 0.3704, validation loss: 0.1935
2024-06-04 03:11:03 [INFO]: Epoch 080 - training loss: 0.3683, validation loss: 0.1918
2024-06-04 03:11:07 [INFO]: Epoch 081 - training loss: 0.3671, validation loss: 0.1920
2024-06-04 03:11:11 [INFO]: Epoch 082 - training loss: 0.3672, validation loss: 0.1927
2024-06-04 03:11:16 [INFO]: Epoch 083 - training loss: 0.3679, validation loss: 0.1921
2024-06-04 03:11:20 [INFO]: Epoch 084 - training loss: 0.3674, validation loss: 0.1925
2024-06-04 03:11:24 [INFO]: Epoch 085 - training loss: 0.3660, validation loss: 0.1934
2024-06-04 03:11:28 [INFO]: Epoch 086 - training loss: 0.3646, validation loss: 0.1930
2024-06-04 03:11:32 [INFO]: Epoch 087 - training loss: 0.3649, validation loss: 0.1917
2024-06-04 03:11:37 [INFO]: Epoch 088 - training loss: 0.3651, validation loss: 0.1929
2024-06-04 03:11:41 [INFO]: Epoch 089 - training loss: 0.3636, validation loss: 0.1899
2024-06-04 03:11:45 [INFO]: Epoch 090 - training loss: 0.3637, validation loss: 0.1911
2024-06-04 03:11:49 [INFO]: Epoch 091 - training loss: 0.3622, validation loss: 0.1920
2024-06-04 03:11:53 [INFO]: Epoch 092 - training loss: 0.3621, validation loss: 0.1938
2024-06-04 03:11:57 [INFO]: Epoch 093 - training loss: 0.3629, validation loss: 0.1953
2024-06-04 03:12:01 [INFO]: Epoch 094 - training loss: 0.3606, validation loss: 0.1909
2024-06-04 03:12:06 [INFO]: Epoch 095 - training loss: 0.3603, validation loss: 0.1913
2024-06-04 03:12:10 [INFO]: Epoch 096 - training loss: 0.3593, validation loss: 0.1907
2024-06-04 03:12:14 [INFO]: Epoch 097 - training loss: 0.3602, validation loss: 0.1908
2024-06-04 03:12:18 [INFO]: Epoch 098 - training loss: 0.3596, validation loss: 0.1896
2024-06-04 03:12:23 [INFO]: Epoch 099 - training loss: 0.3597, validation loss: 0.1901
2024-06-04 03:12:27 [INFO]: Epoch 100 - training loss: 0.3571, validation loss: 0.1913
2024-06-04 03:12:27 [INFO]: Finished training. The best model is from epoch#98.
2024-06-04 03:12:28 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_2/20240604_T030516/MICN.pypots
2024-06-04 03:12:29 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_2/imputation.pkl
2024-06-04 03:12:29 [INFO]: Round2 - MICN on BeijingAir: MAE=0.2262, MSE=0.1749, MRE=0.3417
2024-06-04 03:12:29 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-04 03:12:29 [INFO]: Using the given device: cuda:0
2024-06-04 03:12:29 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_3/20240604_T031229
2024-06-04 03:12:29 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_3/20240604_T031229/tensorboard
2024-06-04 03:12:31 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 57,048,200
2024-06-04 03:12:36 [INFO]: Epoch 001 - training loss: 0.7602, validation loss: 0.2668
2024-06-04 03:12:40 [INFO]: Epoch 002 - training loss: 0.5914, validation loss: 0.2581
2024-06-04 03:12:45 [INFO]: Epoch 003 - training loss: 0.5766, validation loss: 0.2489
2024-06-04 03:12:49 [INFO]: Epoch 004 - training loss: 0.5545, validation loss: 0.2458
2024-06-04 03:12:53 [INFO]: Epoch 005 - training loss: 0.5451, validation loss: 0.2439
2024-06-04 03:12:58 [INFO]: Epoch 006 - training loss: 0.5264, validation loss: 0.2400
2024-06-04 03:13:02 [INFO]: Epoch 007 - training loss: 0.5196, validation loss: 0.2371
2024-06-04 03:13:06 [INFO]: Epoch 008 - training loss: 0.5130, validation loss: 0.2378
2024-06-04 03:13:10 [INFO]: Epoch 009 - training loss: 0.5064, validation loss: 0.2350
2024-06-04 03:13:14 [INFO]: Epoch 010 - training loss: 0.5004, validation loss: 0.2334
2024-06-04 03:13:18 [INFO]: Epoch 011 - training loss: 0.4923, validation loss: 0.2326
2024-06-04 03:13:22 [INFO]: Epoch 012 - training loss: 0.4860, validation loss: 0.2282
2024-06-04 03:13:25 [INFO]: Epoch 013 - training loss: 0.4803, validation loss: 0.2260
2024-06-04 03:13:28 [INFO]: Epoch 014 - training loss: 0.4767, validation loss: 0.2237
2024-06-04 03:13:32 [INFO]: Epoch 015 - training loss: 0.4716, validation loss: 0.2220
2024-06-04 03:13:36 [INFO]: Epoch 016 - training loss: 0.4655, validation loss: 0.2177
2024-06-04 03:13:40 [INFO]: Epoch 017 - training loss: 0.4595, validation loss: 0.2213
2024-06-04 03:13:45 [INFO]: Epoch 018 - training loss: 0.4580, validation loss: 0.2169
2024-06-04 03:13:49 [INFO]: Epoch 019 - training loss: 0.4517, validation loss: 0.2149
2024-06-04 03:13:53 [INFO]: Epoch 020 - training loss: 0.4494, validation loss: 0.2139
2024-06-04 03:13:57 [INFO]: Epoch 021 - training loss: 0.4466, validation loss: 0.2157
2024-06-04 03:14:02 [INFO]: Epoch 022 - training loss: 0.4425, validation loss: 0.2124
2024-06-04 03:14:06 [INFO]: Epoch 023 - training loss: 0.4402, validation loss: 0.2114
2024-06-04 03:14:10 [INFO]: Epoch 024 - training loss: 0.4344, validation loss: 0.2095
2024-06-04 03:14:15 [INFO]: Epoch 025 - training loss: 0.4293, validation loss: 0.2108
2024-06-04 03:14:19 [INFO]: Epoch 026 - training loss: 0.4289, validation loss: 0.2093
2024-06-04 03:14:23 [INFO]: Epoch 027 - training loss: 0.4249, validation loss: 0.2082
2024-06-04 03:14:27 [INFO]: Epoch 028 - training loss: 0.4228, validation loss: 0.2111
2024-06-04 03:14:32 [INFO]: Epoch 029 - training loss: 0.4217, validation loss: 0.2077
2024-06-04 03:14:36 [INFO]: Epoch 030 - training loss: 0.4188, validation loss: 0.2084
2024-06-04 03:14:40 [INFO]: Epoch 031 - training loss: 0.4182, validation loss: 0.2072
2024-06-04 03:14:44 [INFO]: Epoch 032 - training loss: 0.4168, validation loss: 0.2065
2024-06-04 03:14:49 [INFO]: Epoch 033 - training loss: 0.4146, validation loss: 0.2087
2024-06-04 03:14:53 [INFO]: Epoch 034 - training loss: 0.4123, validation loss: 0.2075
2024-06-04 03:14:58 [INFO]: Epoch 035 - training loss: 0.4121, validation loss: 0.2069
2024-06-04 03:15:02 [INFO]: Epoch 036 - training loss: 0.4058, validation loss: 0.2060
2024-06-04 03:15:06 [INFO]: Epoch 037 - training loss: 0.4073, validation loss: 0.2044
2024-06-04 03:15:10 [INFO]: Epoch 038 - training loss: 0.4062, validation loss: 0.2037
2024-06-04 03:15:14 [INFO]: Epoch 039 - training loss: 0.4030, validation loss: 0.2037
2024-06-04 03:15:18 [INFO]: Epoch 040 - training loss: 0.4006, validation loss: 0.2021
2024-06-04 03:15:22 [INFO]: Epoch 041 - training loss: 0.4006, validation loss: 0.2004
2024-06-04 03:15:26 [INFO]: Epoch 042 - training loss: 0.3980, validation loss: 0.1994
2024-06-04 03:15:29 [INFO]: Epoch 043 - training loss: 0.3973, validation loss: 0.2021
2024-06-04 03:15:33 [INFO]: Epoch 044 - training loss: 0.3958, validation loss: 0.2010
2024-06-04 03:15:36 [INFO]: Epoch 045 - training loss: 0.3940, validation loss: 0.2014
2024-06-04 03:15:40 [INFO]: Epoch 046 - training loss: 0.3940, validation loss: 0.2008
2024-06-04 03:15:43 [INFO]: Epoch 047 - training loss: 0.3934, validation loss: 0.1993
2024-06-04 03:15:47 [INFO]: Epoch 048 - training loss: 0.3909, validation loss: 0.1999
2024-06-04 03:15:50 [INFO]: Epoch 049 - training loss: 0.3915, validation loss: 0.2004
2024-06-04 03:15:54 [INFO]: Epoch 050 - training loss: 0.3897, validation loss: 0.2035
2024-06-04 03:15:58 [INFO]: Epoch 051 - training loss: 0.3886, validation loss: 0.1989
2024-06-04 03:16:01 [INFO]: Epoch 052 - training loss: 0.3880, validation loss: 0.1991
2024-06-04 03:16:04 [INFO]: Epoch 053 - training loss: 0.3866, validation loss: 0.2003
2024-06-04 03:16:08 [INFO]: Epoch 054 - training loss: 0.3864, validation loss: 0.1966
2024-06-04 03:16:11 [INFO]: Epoch 055 - training loss: 0.3859, validation loss: 0.1969
2024-06-04 03:16:15 [INFO]: Epoch 056 - training loss: 0.3838, validation loss: 0.1978
2024-06-04 03:16:18 [INFO]: Epoch 057 - training loss: 0.3835, validation loss: 0.1972
2024-06-04 03:16:22 [INFO]: Epoch 058 - training loss: 0.3825, validation loss: 0.1979
2024-06-04 03:16:25 [INFO]: Epoch 059 - training loss: 0.3819, validation loss: 0.1973
2024-06-04 03:16:29 [INFO]: Epoch 060 - training loss: 0.3810, validation loss: 0.1953
2024-06-04 03:16:33 [INFO]: Epoch 061 - training loss: 0.3783, validation loss: 0.1960
2024-06-04 03:16:36 [INFO]: Epoch 062 - training loss: 0.3776, validation loss: 0.1965
2024-06-04 03:16:39 [INFO]: Epoch 063 - training loss: 0.3774, validation loss: 0.1959
2024-06-04 03:16:43 [INFO]: Epoch 064 - training loss: 0.3766, validation loss: 0.1950
2024-06-04 03:16:46 [INFO]: Epoch 065 - training loss: 0.3763, validation loss: 0.1938
2024-06-04 03:16:50 [INFO]: Epoch 066 - training loss: 0.3749, validation loss: 0.1949
2024-06-04 03:16:53 [INFO]: Epoch 067 - training loss: 0.3730, validation loss: 0.1931
2024-06-04 03:16:57 [INFO]: Epoch 068 - training loss: 0.3747, validation loss: 0.1929
2024-06-04 03:17:00 [INFO]: Epoch 069 - training loss: 0.3727, validation loss: 0.1937
2024-06-04 03:17:04 [INFO]: Epoch 070 - training loss: 0.3731, validation loss: 0.1942
2024-06-04 03:17:07 [INFO]: Epoch 071 - training loss: 0.3722, validation loss: 0.1937
2024-06-04 03:17:11 [INFO]: Epoch 072 - training loss: 0.3718, validation loss: 0.1940
2024-06-04 03:17:14 [INFO]: Epoch 073 - training loss: 0.3714, validation loss: 0.1922
2024-06-04 03:17:18 [INFO]: Epoch 074 - training loss: 0.3715, validation loss: 0.1947
2024-06-04 03:17:21 [INFO]: Epoch 075 - training loss: 0.3703, validation loss: 0.1943
2024-06-04 03:17:25 [INFO]: Epoch 076 - training loss: 0.3685, validation loss: 0.1935
2024-06-04 03:17:28 [INFO]: Epoch 077 - training loss: 0.3683, validation loss: 0.1937
2024-06-04 03:17:32 [INFO]: Epoch 078 - training loss: 0.3673, validation loss: 0.1952
2024-06-04 03:17:35 [INFO]: Epoch 079 - training loss: 0.3684, validation loss: 0.1950
2024-06-04 03:17:39 [INFO]: Epoch 080 - training loss: 0.3662, validation loss: 0.1939
2024-06-04 03:17:42 [INFO]: Epoch 081 - training loss: 0.3662, validation loss: 0.1928
2024-06-04 03:17:46 [INFO]: Epoch 082 - training loss: 0.3645, validation loss: 0.1926
2024-06-04 03:17:50 [INFO]: Epoch 083 - training loss: 0.3646, validation loss: 0.1925
2024-06-04 03:17:50 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:17:50 [INFO]: Finished training. The best model is from epoch#73.
2024-06-04 03:17:50 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_3/20240604_T031229/MICN.pypots
2024-06-04 03:17:51 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_3/imputation.pkl
2024-06-04 03:17:51 [INFO]: Round3 - MICN on BeijingAir: MAE=0.2279, MSE=0.1768, MRE=0.3444
2024-06-04 03:17:51 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-04 03:17:51 [INFO]: Using the given device: cuda:0
2024-06-04 03:17:51 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_4/20240604_T031751
2024-06-04 03:17:51 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_4/20240604_T031751/tensorboard
2024-06-04 03:17:53 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 57,048,200
2024-06-04 03:17:56 [INFO]: Epoch 001 - training loss: 0.7741, validation loss: 0.2667
2024-06-04 03:18:00 [INFO]: Epoch 002 - training loss: 0.5931, validation loss: 0.2579
2024-06-04 03:18:03 [INFO]: Epoch 003 - training loss: 0.5715, validation loss: 0.2532
2024-06-04 03:18:07 [INFO]: Epoch 004 - training loss: 0.5555, validation loss: 0.2522
2024-06-04 03:18:11 [INFO]: Epoch 005 - training loss: 0.5408, validation loss: 0.2493
2024-06-04 03:18:14 [INFO]: Epoch 006 - training loss: 0.5297, validation loss: 0.2442
2024-06-04 03:18:18 [INFO]: Epoch 007 - training loss: 0.5160, validation loss: 0.2427
2024-06-04 03:18:21 [INFO]: Epoch 008 - training loss: 0.5095, validation loss: 0.2426
2024-06-04 03:18:24 [INFO]: Epoch 009 - training loss: 0.5018, validation loss: 0.2393
2024-06-04 03:18:28 [INFO]: Epoch 010 - training loss: 0.4962, validation loss: 0.2338
2024-06-04 03:18:31 [INFO]: Epoch 011 - training loss: 0.4872, validation loss: 0.2354
2024-06-04 03:18:35 [INFO]: Epoch 012 - training loss: 0.4805, validation loss: 0.2308
2024-06-04 03:18:38 [INFO]: Epoch 013 - training loss: 0.4784, validation loss: 0.2301
2024-06-04 03:18:42 [INFO]: Epoch 014 - training loss: 0.4750, validation loss: 0.2296
2024-06-04 03:18:45 [INFO]: Epoch 015 - training loss: 0.4652, validation loss: 0.2262
2024-06-04 03:18:49 [INFO]: Epoch 016 - training loss: 0.4598, validation loss: 0.2238
2024-06-04 03:18:52 [INFO]: Epoch 017 - training loss: 0.4561, validation loss: 0.2228
2024-06-04 03:18:56 [INFO]: Epoch 018 - training loss: 0.4535, validation loss: 0.2248
2024-06-04 03:18:59 [INFO]: Epoch 019 - training loss: 0.4534, validation loss: 0.2213
2024-06-04 03:19:03 [INFO]: Epoch 020 - training loss: 0.4447, validation loss: 0.2193
2024-06-04 03:19:07 [INFO]: Epoch 021 - training loss: 0.4423, validation loss: 0.2171
2024-06-04 03:19:10 [INFO]: Epoch 022 - training loss: 0.4389, validation loss: 0.2192
2024-06-04 03:19:14 [INFO]: Epoch 023 - training loss: 0.4397, validation loss: 0.2169
2024-06-04 03:19:17 [INFO]: Epoch 024 - training loss: 0.4327, validation loss: 0.2154
2024-06-04 03:19:21 [INFO]: Epoch 025 - training loss: 0.4337, validation loss: 0.2156
2024-06-04 03:19:24 [INFO]: Epoch 026 - training loss: 0.4305, validation loss: 0.2117
2024-06-04 03:19:28 [INFO]: Epoch 027 - training loss: 0.4272, validation loss: 0.2115
2024-06-04 03:19:32 [INFO]: Epoch 028 - training loss: 0.4255, validation loss: 0.2108
2024-06-04 03:19:35 [INFO]: Epoch 029 - training loss: 0.4230, validation loss: 0.2116
2024-06-04 03:19:39 [INFO]: Epoch 030 - training loss: 0.4215, validation loss: 0.2083
2024-06-04 03:19:43 [INFO]: Epoch 031 - training loss: 0.4174, validation loss: 0.2098
2024-06-04 03:19:46 [INFO]: Epoch 032 - training loss: 0.4158, validation loss: 0.2098
2024-06-04 03:19:50 [INFO]: Epoch 033 - training loss: 0.4166, validation loss: 0.2073
2024-06-04 03:19:53 [INFO]: Epoch 034 - training loss: 0.4152, validation loss: 0.2063
2024-06-04 03:19:57 [INFO]: Epoch 035 - training loss: 0.4114, validation loss: 0.2067
2024-06-04 03:20:00 [INFO]: Epoch 036 - training loss: 0.4117, validation loss: 0.2060
2024-06-04 03:20:04 [INFO]: Epoch 037 - training loss: 0.4067, validation loss: 0.2089
2024-06-04 03:20:07 [INFO]: Epoch 038 - training loss: 0.4085, validation loss: 0.2062
2024-06-04 03:20:11 [INFO]: Epoch 039 - training loss: 0.4069, validation loss: 0.2034
2024-06-04 03:20:14 [INFO]: Epoch 040 - training loss: 0.4042, validation loss: 0.2039
2024-06-04 03:20:18 [INFO]: Epoch 041 - training loss: 0.4021, validation loss: 0.2042
2024-06-04 03:20:21 [INFO]: Epoch 042 - training loss: 0.3986, validation loss: 0.2026
2024-06-04 03:20:25 [INFO]: Epoch 043 - training loss: 0.4007, validation loss: 0.2029
2024-06-04 03:20:28 [INFO]: Epoch 044 - training loss: 0.3980, validation loss: 0.2006
2024-06-04 03:20:32 [INFO]: Epoch 045 - training loss: 0.3965, validation loss: 0.2009
2024-06-04 03:20:35 [INFO]: Epoch 046 - training loss: 0.3966, validation loss: 0.2000
2024-06-04 03:20:39 [INFO]: Epoch 047 - training loss: 0.3942, validation loss: 0.2008
2024-06-04 03:20:42 [INFO]: Epoch 048 - training loss: 0.3945, validation loss: 0.2007
2024-06-04 03:20:46 [INFO]: Epoch 049 - training loss: 0.3920, validation loss: 0.2008
2024-06-04 03:20:50 [INFO]: Epoch 050 - training loss: 0.3910, validation loss: 0.1989
2024-06-04 03:20:53 [INFO]: Epoch 051 - training loss: 0.3910, validation loss: 0.1982
2024-06-04 03:20:57 [INFO]: Epoch 052 - training loss: 0.3869, validation loss: 0.1996
2024-06-04 03:21:00 [INFO]: Epoch 053 - training loss: 0.3881, validation loss: 0.1978
2024-06-04 03:21:04 [INFO]: Epoch 054 - training loss: 0.3886, validation loss: 0.1977
2024-06-04 03:21:07 [INFO]: Epoch 055 - training loss: 0.3862, validation loss: 0.2007
2024-06-04 03:21:11 [INFO]: Epoch 056 - training loss: 0.3877, validation loss: 0.1946
2024-06-04 03:21:14 [INFO]: Epoch 057 - training loss: 0.3851, validation loss: 0.1959
2024-06-04 03:21:18 [INFO]: Epoch 058 - training loss: 0.3829, validation loss: 0.1968
2024-06-04 03:21:22 [INFO]: Epoch 059 - training loss: 0.3829, validation loss: 0.1963
2024-06-04 03:21:25 [INFO]: Epoch 060 - training loss: 0.3835, validation loss: 0.1970
2024-06-04 03:21:29 [INFO]: Epoch 061 - training loss: 0.3829, validation loss: 0.2020
2024-06-04 03:21:32 [INFO]: Epoch 062 - training loss: 0.3811, validation loss: 0.1955
2024-06-04 03:21:36 [INFO]: Epoch 063 - training loss: 0.3795, validation loss: 0.1953
2024-06-04 03:21:39 [INFO]: Epoch 064 - training loss: 0.3798, validation loss: 0.1938
2024-06-04 03:21:43 [INFO]: Epoch 065 - training loss: 0.3785, validation loss: 0.1933
2024-06-04 03:21:46 [INFO]: Epoch 066 - training loss: 0.3772, validation loss: 0.1966
2024-06-04 03:21:50 [INFO]: Epoch 067 - training loss: 0.3751, validation loss: 0.1951
2024-06-04 03:21:53 [INFO]: Epoch 068 - training loss: 0.3765, validation loss: 0.1960
2024-06-04 03:21:56 [INFO]: Epoch 069 - training loss: 0.3744, validation loss: 0.1916
2024-06-04 03:22:00 [INFO]: Epoch 070 - training loss: 0.3748, validation loss: 0.1912
2024-06-04 03:22:04 [INFO]: Epoch 071 - training loss: 0.3738, validation loss: 0.1936
2024-06-04 03:22:07 [INFO]: Epoch 072 - training loss: 0.3740, validation loss: 0.1924
2024-06-04 03:22:10 [INFO]: Epoch 073 - training loss: 0.3740, validation loss: 0.1932
2024-06-04 03:22:14 [INFO]: Epoch 074 - training loss: 0.3715, validation loss: 0.1947
2024-06-04 03:22:18 [INFO]: Epoch 075 - training loss: 0.3719, validation loss: 0.1959
2024-06-04 03:22:21 [INFO]: Epoch 076 - training loss: 0.3704, validation loss: 0.1926
2024-06-04 03:22:25 [INFO]: Epoch 077 - training loss: 0.3694, validation loss: 0.1933
2024-06-04 03:22:29 [INFO]: Epoch 078 - training loss: 0.3697, validation loss: 0.1910
2024-06-04 03:22:32 [INFO]: Epoch 079 - training loss: 0.3686, validation loss: 0.1915
2024-06-04 03:22:35 [INFO]: Epoch 080 - training loss: 0.3692, validation loss: 0.1918
2024-06-04 03:22:39 [INFO]: Epoch 081 - training loss: 0.3682, validation loss: 0.1917
2024-06-04 03:22:42 [INFO]: Epoch 082 - training loss: 0.3665, validation loss: 0.1925
2024-06-04 03:22:46 [INFO]: Epoch 083 - training loss: 0.3671, validation loss: 0.1905
2024-06-04 03:22:49 [INFO]: Epoch 084 - training loss: 0.3653, validation loss: 0.1912
2024-06-04 03:22:53 [INFO]: Epoch 085 - training loss: 0.3668, validation loss: 0.1905
2024-06-04 03:22:56 [INFO]: Epoch 086 - training loss: 0.3655, validation loss: 0.1923
2024-06-04 03:23:00 [INFO]: Epoch 087 - training loss: 0.3641, validation loss: 0.1909
2024-06-04 03:23:03 [INFO]: Epoch 088 - training loss: 0.3641, validation loss: 0.1906
2024-06-04 03:23:06 [INFO]: Epoch 089 - training loss: 0.3637, validation loss: 0.1913
2024-06-04 03:23:09 [INFO]: Epoch 090 - training loss: 0.3635, validation loss: 0.1910
2024-06-04 03:23:11 [INFO]: Epoch 091 - training loss: 0.3623, validation loss: 0.1912
2024-06-04 03:23:14 [INFO]: Epoch 092 - training loss: 0.3629, validation loss: 0.1907
2024-06-04 03:23:17 [INFO]: Epoch 093 - training loss: 0.3614, validation loss: 0.1887
2024-06-04 03:23:19 [INFO]: Epoch 094 - training loss: 0.3607, validation loss: 0.1907
2024-06-04 03:23:22 [INFO]: Epoch 095 - training loss: 0.3607, validation loss: 0.1905
2024-06-04 03:23:25 [INFO]: Epoch 096 - training loss: 0.3597, validation loss: 0.1907
2024-06-04 03:23:27 [INFO]: Epoch 097 - training loss: 0.3606, validation loss: 0.1911
2024-06-04 03:23:30 [INFO]: Epoch 098 - training loss: 0.3574, validation loss: 0.1895
2024-06-04 03:23:33 [INFO]: Epoch 099 - training loss: 0.3587, validation loss: 0.1911
2024-06-04 03:23:35 [INFO]: Epoch 100 - training loss: 0.3578, validation loss: 0.1909
2024-06-04 03:23:35 [INFO]: Finished training. The best model is from epoch#93.
2024-06-04 03:23:36 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_4/20240604_T031751/MICN.pypots
2024-06-04 03:23:37 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/MICN_BeijingAir/round_4/imputation.pkl
2024-06-04 03:23:37 [INFO]: Round4 - MICN on BeijingAir: MAE=0.2266, MSE=0.1750, MRE=0.3423
2024-06-04 03:23:37 [INFO]: Done! Final results:
Averaged MICN (57,048,200 params) on BeijingAir: MAE=0.2030 ± 0.001069954828600257, MSE=0.1504 ± 0.0011490962822286491, MRE=0.2700 ± 0.001423114814772485, average inference time=0.23