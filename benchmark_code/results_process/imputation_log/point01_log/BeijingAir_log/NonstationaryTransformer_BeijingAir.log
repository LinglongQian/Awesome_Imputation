2024-06-04 02:49:23 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-04 02:49:23 [INFO]: Using the given device: cuda:0
2024-06-04 02:49:24 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240604_T024924
2024-06-04 02:49:24 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240604_T024924/tensorboard
2024-06-04 02:49:26 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-04 02:49:40 [INFO]: Epoch 001 - training loss: 0.6403, validation loss: 0.3020
2024-06-04 02:49:45 [INFO]: Epoch 002 - training loss: 0.5496, validation loss: 0.2704
2024-06-04 02:49:51 [INFO]: Epoch 003 - training loss: 0.5332, validation loss: 0.2601
2024-06-04 02:49:56 [INFO]: Epoch 004 - training loss: 0.5228, validation loss: 0.2661
2024-06-04 02:50:03 [INFO]: Epoch 005 - training loss: 0.5154, validation loss: 0.2561
2024-06-04 02:50:08 [INFO]: Epoch 006 - training loss: 0.5072, validation loss: 0.2501
2024-06-04 02:50:14 [INFO]: Epoch 007 - training loss: 0.5001, validation loss: 0.2483
2024-06-04 02:50:19 [INFO]: Epoch 008 - training loss: 0.4944, validation loss: 0.2528
2024-06-04 02:50:25 [INFO]: Epoch 009 - training loss: 0.4903, validation loss: 0.2369
2024-06-04 02:50:31 [INFO]: Epoch 010 - training loss: 0.4867, validation loss: 0.2376
2024-06-04 02:50:37 [INFO]: Epoch 011 - training loss: 0.4831, validation loss: 0.2390
2024-06-04 02:50:43 [INFO]: Epoch 012 - training loss: 0.4790, validation loss: 0.2368
2024-06-04 02:50:49 [INFO]: Epoch 013 - training loss: 0.4745, validation loss: 0.2360
2024-06-04 02:50:55 [INFO]: Epoch 014 - training loss: 0.4728, validation loss: 0.2365
2024-06-04 02:51:01 [INFO]: Epoch 015 - training loss: 0.4713, validation loss: 0.2382
2024-06-04 02:51:06 [INFO]: Epoch 016 - training loss: 0.4686, validation loss: 0.2380
2024-06-04 02:51:12 [INFO]: Epoch 017 - training loss: 0.4652, validation loss: 0.2407
2024-06-04 02:51:18 [INFO]: Epoch 018 - training loss: 0.4620, validation loss: 0.2302
2024-06-04 02:51:24 [INFO]: Epoch 019 - training loss: 0.4594, validation loss: 0.2286
2024-06-04 02:51:30 [INFO]: Epoch 020 - training loss: 0.4560, validation loss: 0.2381
2024-06-04 02:51:35 [INFO]: Epoch 021 - training loss: 0.4566, validation loss: 0.2249
2024-06-04 02:51:41 [INFO]: Epoch 022 - training loss: 0.4534, validation loss: 0.2253
2024-06-04 02:51:46 [INFO]: Epoch 023 - training loss: 0.4500, validation loss: 0.2269
2024-06-04 02:51:52 [INFO]: Epoch 024 - training loss: 0.4494, validation loss: 0.2423
2024-06-04 02:51:57 [INFO]: Epoch 025 - training loss: 0.4483, validation loss: 0.2301
2024-06-04 02:52:03 [INFO]: Epoch 026 - training loss: 0.4475, validation loss: 0.2294
2024-06-04 02:52:09 [INFO]: Epoch 027 - training loss: 0.4451, validation loss: 0.2299
2024-06-04 02:52:15 [INFO]: Epoch 028 - training loss: 0.4447, validation loss: 0.2261
2024-06-04 02:52:20 [INFO]: Epoch 029 - training loss: 0.4443, validation loss: 0.2251
2024-06-04 02:52:26 [INFO]: Epoch 030 - training loss: 0.4399, validation loss: 0.2268
2024-06-04 02:52:31 [INFO]: Epoch 031 - training loss: 0.4409, validation loss: 0.2222
2024-06-04 02:52:37 [INFO]: Epoch 032 - training loss: 0.4387, validation loss: 0.2268
2024-06-04 02:52:43 [INFO]: Epoch 033 - training loss: 0.4374, validation loss: 0.2282
2024-06-04 02:52:49 [INFO]: Epoch 034 - training loss: 0.4363, validation loss: 0.2285
2024-06-04 02:52:55 [INFO]: Epoch 035 - training loss: 0.4345, validation loss: 0.2237
2024-06-04 02:53:01 [INFO]: Epoch 036 - training loss: 0.4338, validation loss: 0.2242
2024-06-04 02:53:07 [INFO]: Epoch 037 - training loss: 0.4341, validation loss: 0.2198
2024-06-04 02:53:12 [INFO]: Epoch 038 - training loss: 0.4327, validation loss: 0.2228
2024-06-04 02:53:18 [INFO]: Epoch 039 - training loss: 0.4320, validation loss: 0.2294
2024-06-04 02:53:24 [INFO]: Epoch 040 - training loss: 0.4320, validation loss: 0.2256
2024-06-04 02:53:30 [INFO]: Epoch 041 - training loss: 0.4294, validation loss: 0.2316
2024-06-04 02:53:35 [INFO]: Epoch 042 - training loss: 0.4280, validation loss: 0.2268
2024-06-04 02:53:41 [INFO]: Epoch 043 - training loss: 0.4268, validation loss: 0.2274
2024-06-04 02:53:47 [INFO]: Epoch 044 - training loss: 0.4268, validation loss: 0.2243
2024-06-04 02:53:53 [INFO]: Epoch 045 - training loss: 0.4273, validation loss: 0.2243
2024-06-04 02:53:58 [INFO]: Epoch 046 - training loss: 0.4252, validation loss: 0.2263
2024-06-04 02:54:04 [INFO]: Epoch 047 - training loss: 0.4256, validation loss: 0.2276
2024-06-04 02:54:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 02:54:04 [INFO]: Finished training. The best model is from epoch#37.
2024-06-04 02:54:04 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240604_T024924/NonstationaryTransformer.pypots
2024-06-04 02:54:06 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/imputation.pkl
2024-06-04 02:54:06 [INFO]: Round0 - NonstationaryTransformer on BeijingAir: MAE=0.2732, MSE=0.3489, MRE=0.4128
2024-06-04 02:54:06 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-04 02:54:06 [INFO]: Using the given device: cuda:0
2024-06-04 02:54:06 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240604_T025406
2024-06-04 02:54:06 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240604_T025406/tensorboard
2024-06-04 02:54:07 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-04 02:54:12 [INFO]: Epoch 001 - training loss: 0.6334, validation loss: 0.3038
2024-06-04 02:54:18 [INFO]: Epoch 002 - training loss: 0.5520, validation loss: 0.2786
2024-06-04 02:54:23 [INFO]: Epoch 003 - training loss: 0.5346, validation loss: 0.2675
2024-06-04 02:54:29 [INFO]: Epoch 004 - training loss: 0.5216, validation loss: 0.2636
2024-06-04 02:54:35 [INFO]: Epoch 005 - training loss: 0.5147, validation loss: 0.2518
2024-06-04 02:54:41 [INFO]: Epoch 006 - training loss: 0.5049, validation loss: 0.2431
2024-06-04 02:54:47 [INFO]: Epoch 007 - training loss: 0.4983, validation loss: 0.2537
2024-06-04 02:54:53 [INFO]: Epoch 008 - training loss: 0.4948, validation loss: 0.2435
2024-06-04 02:54:58 [INFO]: Epoch 009 - training loss: 0.4899, validation loss: 0.2436
2024-06-04 02:55:04 [INFO]: Epoch 010 - training loss: 0.4868, validation loss: 0.2409
2024-06-04 02:55:09 [INFO]: Epoch 011 - training loss: 0.4828, validation loss: 0.2453
2024-06-04 02:55:16 [INFO]: Epoch 012 - training loss: 0.4763, validation loss: 0.2309
2024-06-04 02:55:21 [INFO]: Epoch 013 - training loss: 0.4750, validation loss: 0.2395
2024-06-04 02:55:27 [INFO]: Epoch 014 - training loss: 0.4709, validation loss: 0.2384
2024-06-04 02:55:33 [INFO]: Epoch 015 - training loss: 0.4683, validation loss: 0.2350
2024-06-04 02:55:39 [INFO]: Epoch 016 - training loss: 0.4678, validation loss: 0.2301
2024-06-04 02:55:44 [INFO]: Epoch 017 - training loss: 0.4631, validation loss: 0.2351
2024-06-04 02:55:50 [INFO]: Epoch 018 - training loss: 0.4614, validation loss: 0.2240
2024-06-04 02:55:56 [INFO]: Epoch 019 - training loss: 0.4579, validation loss: 0.2344
2024-06-04 02:56:02 [INFO]: Epoch 020 - training loss: 0.4582, validation loss: 0.2271
2024-06-04 02:56:08 [INFO]: Epoch 021 - training loss: 0.4523, validation loss: 0.2304
2024-06-04 02:56:13 [INFO]: Epoch 022 - training loss: 0.4528, validation loss: 0.2306
2024-06-04 02:56:19 [INFO]: Epoch 023 - training loss: 0.4515, validation loss: 0.2297
2024-06-04 02:56:25 [INFO]: Epoch 024 - training loss: 0.4494, validation loss: 0.2227
2024-06-04 02:56:31 [INFO]: Epoch 025 - training loss: 0.4457, validation loss: 0.2242
2024-06-04 02:56:36 [INFO]: Epoch 026 - training loss: 0.4454, validation loss: 0.2238
2024-06-04 02:56:42 [INFO]: Epoch 027 - training loss: 0.4449, validation loss: 0.2189
2024-06-04 02:56:48 [INFO]: Epoch 028 - training loss: 0.4440, validation loss: 0.2214
2024-06-04 02:56:54 [INFO]: Epoch 029 - training loss: 0.4418, validation loss: 0.2200
2024-06-04 02:56:59 [INFO]: Epoch 030 - training loss: 0.4412, validation loss: 0.2170
2024-06-04 02:57:05 [INFO]: Epoch 031 - training loss: 0.4390, validation loss: 0.2204
2024-06-04 02:57:10 [INFO]: Epoch 032 - training loss: 0.4386, validation loss: 0.2225
2024-06-04 02:57:16 [INFO]: Epoch 033 - training loss: 0.4367, validation loss: 0.2176
2024-06-04 02:57:22 [INFO]: Epoch 034 - training loss: 0.4352, validation loss: 0.2231
2024-06-04 02:57:27 [INFO]: Epoch 035 - training loss: 0.4355, validation loss: 0.2205
2024-06-04 02:57:33 [INFO]: Epoch 036 - training loss: 0.4356, validation loss: 0.2276
2024-06-04 02:57:39 [INFO]: Epoch 037 - training loss: 0.4317, validation loss: 0.2204
2024-06-04 02:57:45 [INFO]: Epoch 038 - training loss: 0.4324, validation loss: 0.2166
2024-06-04 02:57:51 [INFO]: Epoch 039 - training loss: 0.4318, validation loss: 0.2214
2024-06-04 02:57:56 [INFO]: Epoch 040 - training loss: 0.4310, validation loss: 0.2259
2024-06-04 02:58:02 [INFO]: Epoch 041 - training loss: 0.4302, validation loss: 0.2162
2024-06-04 02:58:07 [INFO]: Epoch 042 - training loss: 0.4283, validation loss: 0.2193
2024-06-04 02:58:13 [INFO]: Epoch 043 - training loss: 0.4278, validation loss: 0.2203
2024-06-04 02:58:18 [INFO]: Epoch 044 - training loss: 0.4277, validation loss: 0.2272
2024-06-04 02:58:24 [INFO]: Epoch 045 - training loss: 0.4269, validation loss: 0.2166
2024-06-04 02:58:30 [INFO]: Epoch 046 - training loss: 0.4258, validation loss: 0.2199
2024-06-04 02:58:35 [INFO]: Epoch 047 - training loss: 0.4247, validation loss: 0.2229
2024-06-04 02:58:41 [INFO]: Epoch 048 - training loss: 0.4253, validation loss: 0.2204
2024-06-04 02:58:47 [INFO]: Epoch 049 - training loss: 0.4224, validation loss: 0.2158
2024-06-04 02:58:53 [INFO]: Epoch 050 - training loss: 0.4231, validation loss: 0.2156
2024-06-04 02:58:59 [INFO]: Epoch 051 - training loss: 0.4226, validation loss: 0.2198
2024-06-04 02:59:05 [INFO]: Epoch 052 - training loss: 0.4225, validation loss: 0.2173
2024-06-04 02:59:10 [INFO]: Epoch 053 - training loss: 0.4227, validation loss: 0.2157
2024-06-04 02:59:16 [INFO]: Epoch 054 - training loss: 0.4206, validation loss: 0.2192
2024-06-04 02:59:22 [INFO]: Epoch 055 - training loss: 0.4200, validation loss: 0.2248
2024-06-04 02:59:27 [INFO]: Epoch 056 - training loss: 0.4201, validation loss: 0.2132
2024-06-04 02:59:33 [INFO]: Epoch 057 - training loss: 0.4191, validation loss: 0.2174
2024-06-04 02:59:38 [INFO]: Epoch 058 - training loss: 0.4179, validation loss: 0.2201
2024-06-04 02:59:44 [INFO]: Epoch 059 - training loss: 0.4182, validation loss: 0.2168
2024-06-04 02:59:50 [INFO]: Epoch 060 - training loss: 0.4181, validation loss: 0.2144
2024-06-04 02:59:55 [INFO]: Epoch 061 - training loss: 0.4170, validation loss: 0.2165
2024-06-04 03:00:01 [INFO]: Epoch 062 - training loss: 0.4162, validation loss: 0.2209
2024-06-04 03:00:07 [INFO]: Epoch 063 - training loss: 0.4149, validation loss: 0.2153
2024-06-04 03:00:13 [INFO]: Epoch 064 - training loss: 0.4147, validation loss: 0.2204
2024-06-04 03:00:19 [INFO]: Epoch 065 - training loss: 0.4154, validation loss: 0.2177
2024-06-04 03:00:24 [INFO]: Epoch 066 - training loss: 0.4125, validation loss: 0.2279
2024-06-04 03:00:24 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:00:24 [INFO]: Finished training. The best model is from epoch#56.
2024-06-04 03:00:24 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240604_T025406/NonstationaryTransformer.pypots
2024-06-04 03:00:26 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/imputation.pkl
2024-06-04 03:00:26 [INFO]: Round1 - NonstationaryTransformer on BeijingAir: MAE=0.2748, MSE=0.3793, MRE=0.4151
2024-06-04 03:00:26 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-04 03:00:26 [INFO]: Using the given device: cuda:0
2024-06-04 03:00:26 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240604_T030026
2024-06-04 03:00:26 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240604_T030026/tensorboard
2024-06-04 03:00:27 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-04 03:00:33 [INFO]: Epoch 001 - training loss: 0.6342, validation loss: 0.2969
2024-06-04 03:00:39 [INFO]: Epoch 002 - training loss: 0.5484, validation loss: 0.2750
2024-06-04 03:00:44 [INFO]: Epoch 003 - training loss: 0.5328, validation loss: 0.2610
2024-06-04 03:00:50 [INFO]: Epoch 004 - training loss: 0.5250, validation loss: 0.2553
2024-06-04 03:00:55 [INFO]: Epoch 005 - training loss: 0.5140, validation loss: 0.2526
2024-06-04 03:01:00 [INFO]: Epoch 006 - training loss: 0.5076, validation loss: 0.2531
2024-06-04 03:01:05 [INFO]: Epoch 007 - training loss: 0.5025, validation loss: 0.2455
2024-06-04 03:01:10 [INFO]: Epoch 008 - training loss: 0.4971, validation loss: 0.2499
2024-06-04 03:01:15 [INFO]: Epoch 009 - training loss: 0.4921, validation loss: 0.2462
2024-06-04 03:01:20 [INFO]: Epoch 010 - training loss: 0.4872, validation loss: 0.2458
2024-06-04 03:01:25 [INFO]: Epoch 011 - training loss: 0.4828, validation loss: 0.2448
2024-06-04 03:01:30 [INFO]: Epoch 012 - training loss: 0.4802, validation loss: 0.2439
2024-06-04 03:01:35 [INFO]: Epoch 013 - training loss: 0.4762, validation loss: 0.2401
2024-06-04 03:01:40 [INFO]: Epoch 014 - training loss: 0.4728, validation loss: 0.2437
2024-06-04 03:01:44 [INFO]: Epoch 015 - training loss: 0.4708, validation loss: 0.2414
2024-06-04 03:01:49 [INFO]: Epoch 016 - training loss: 0.4702, validation loss: 0.2377
2024-06-04 03:01:55 [INFO]: Epoch 017 - training loss: 0.4656, validation loss: 0.2332
2024-06-04 03:01:59 [INFO]: Epoch 018 - training loss: 0.4631, validation loss: 0.2270
2024-06-04 03:02:05 [INFO]: Epoch 019 - training loss: 0.4608, validation loss: 0.2288
2024-06-04 03:02:10 [INFO]: Epoch 020 - training loss: 0.4583, validation loss: 0.2275
2024-06-04 03:02:15 [INFO]: Epoch 021 - training loss: 0.4560, validation loss: 0.2282
2024-06-04 03:02:20 [INFO]: Epoch 022 - training loss: 0.4548, validation loss: 0.2287
2024-06-04 03:02:25 [INFO]: Epoch 023 - training loss: 0.4509, validation loss: 0.2260
2024-06-04 03:02:30 [INFO]: Epoch 024 - training loss: 0.4524, validation loss: 0.2267
2024-06-04 03:02:35 [INFO]: Epoch 025 - training loss: 0.4482, validation loss: 0.2275
2024-06-04 03:02:40 [INFO]: Epoch 026 - training loss: 0.4466, validation loss: 0.2229
2024-06-04 03:02:45 [INFO]: Epoch 027 - training loss: 0.4442, validation loss: 0.2270
2024-06-04 03:02:50 [INFO]: Epoch 028 - training loss: 0.4442, validation loss: 0.2263
2024-06-04 03:02:55 [INFO]: Epoch 029 - training loss: 0.4432, validation loss: 0.2220
2024-06-04 03:03:00 [INFO]: Epoch 030 - training loss: 0.4396, validation loss: 0.2270
2024-06-04 03:03:05 [INFO]: Epoch 031 - training loss: 0.4404, validation loss: 0.2199
2024-06-04 03:03:10 [INFO]: Epoch 032 - training loss: 0.4382, validation loss: 0.2266
2024-06-04 03:03:15 [INFO]: Epoch 033 - training loss: 0.4374, validation loss: 0.2266
2024-06-04 03:03:19 [INFO]: Epoch 034 - training loss: 0.4385, validation loss: 0.2212
2024-06-04 03:03:24 [INFO]: Epoch 035 - training loss: 0.4354, validation loss: 0.2234
2024-06-04 03:03:30 [INFO]: Epoch 036 - training loss: 0.4335, validation loss: 0.2298
2024-06-04 03:03:35 [INFO]: Epoch 037 - training loss: 0.4331, validation loss: 0.2255
2024-06-04 03:03:40 [INFO]: Epoch 038 - training loss: 0.4326, validation loss: 0.2214
2024-06-04 03:03:45 [INFO]: Epoch 039 - training loss: 0.4321, validation loss: 0.2240
2024-06-04 03:03:50 [INFO]: Epoch 040 - training loss: 0.4318, validation loss: 0.2227
2024-06-04 03:03:55 [INFO]: Epoch 041 - training loss: 0.4297, validation loss: 0.2209
2024-06-04 03:03:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:03:55 [INFO]: Finished training. The best model is from epoch#31.
2024-06-04 03:03:55 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240604_T030026/NonstationaryTransformer.pypots
2024-06-04 03:03:56 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/imputation.pkl
2024-06-04 03:03:56 [INFO]: Round2 - NonstationaryTransformer on BeijingAir: MAE=0.2722, MSE=0.3369, MRE=0.4113
2024-06-04 03:03:56 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-04 03:03:56 [INFO]: Using the given device: cuda:0
2024-06-04 03:03:56 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240604_T030356
2024-06-04 03:03:56 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240604_T030356/tensorboard
2024-06-04 03:03:57 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-04 03:04:02 [INFO]: Epoch 001 - training loss: 0.6338, validation loss: 0.3083
2024-06-04 03:04:07 [INFO]: Epoch 002 - training loss: 0.5478, validation loss: 0.2830
2024-06-04 03:04:12 [INFO]: Epoch 003 - training loss: 0.5317, validation loss: 0.2808
2024-06-04 03:04:17 [INFO]: Epoch 004 - training loss: 0.5230, validation loss: 0.2786
2024-06-04 03:04:22 [INFO]: Epoch 005 - training loss: 0.5149, validation loss: 0.2839
2024-06-04 03:04:27 [INFO]: Epoch 006 - training loss: 0.5064, validation loss: 0.2657
2024-06-04 03:04:32 [INFO]: Epoch 007 - training loss: 0.4997, validation loss: 0.2581
2024-06-04 03:04:37 [INFO]: Epoch 008 - training loss: 0.4945, validation loss: 0.2533
2024-06-04 03:04:42 [INFO]: Epoch 009 - training loss: 0.4911, validation loss: 0.2722
2024-06-04 03:04:47 [INFO]: Epoch 010 - training loss: 0.4866, validation loss: 0.2514
2024-06-04 03:04:52 [INFO]: Epoch 011 - training loss: 0.4816, validation loss: 0.2470
2024-06-04 03:04:57 [INFO]: Epoch 012 - training loss: 0.4787, validation loss: 0.2421
2024-06-04 03:05:02 [INFO]: Epoch 013 - training loss: 0.4762, validation loss: 0.2471
2024-06-04 03:05:07 [INFO]: Epoch 014 - training loss: 0.4741, validation loss: 0.2429
2024-06-04 03:05:12 [INFO]: Epoch 015 - training loss: 0.4717, validation loss: 0.2330
2024-06-04 03:05:17 [INFO]: Epoch 016 - training loss: 0.4663, validation loss: 0.2426
2024-06-04 03:05:22 [INFO]: Epoch 017 - training loss: 0.4638, validation loss: 0.2447
2024-06-04 03:05:27 [INFO]: Epoch 018 - training loss: 0.4612, validation loss: 0.2363
2024-06-04 03:05:32 [INFO]: Epoch 019 - training loss: 0.4580, validation loss: 0.2348
2024-06-04 03:05:37 [INFO]: Epoch 020 - training loss: 0.4579, validation loss: 0.2408
2024-06-04 03:05:42 [INFO]: Epoch 021 - training loss: 0.4566, validation loss: 0.2359
2024-06-04 03:05:47 [INFO]: Epoch 022 - training loss: 0.4531, validation loss: 0.2277
2024-06-04 03:05:52 [INFO]: Epoch 023 - training loss: 0.4508, validation loss: 0.2300
2024-06-04 03:05:57 [INFO]: Epoch 024 - training loss: 0.4497, validation loss: 0.2347
2024-06-04 03:06:02 [INFO]: Epoch 025 - training loss: 0.4471, validation loss: 0.2276
2024-06-04 03:06:07 [INFO]: Epoch 026 - training loss: 0.4455, validation loss: 0.2340
2024-06-04 03:06:11 [INFO]: Epoch 027 - training loss: 0.4461, validation loss: 0.2363
2024-06-04 03:06:16 [INFO]: Epoch 028 - training loss: 0.4425, validation loss: 0.2342
2024-06-04 03:06:21 [INFO]: Epoch 029 - training loss: 0.4413, validation loss: 0.2346
2024-06-04 03:06:26 [INFO]: Epoch 030 - training loss: 0.4417, validation loss: 0.2319
2024-06-04 03:06:31 [INFO]: Epoch 031 - training loss: 0.4391, validation loss: 0.2241
2024-06-04 03:06:36 [INFO]: Epoch 032 - training loss: 0.4386, validation loss: 0.2272
2024-06-04 03:06:41 [INFO]: Epoch 033 - training loss: 0.4380, validation loss: 0.2363
2024-06-04 03:06:47 [INFO]: Epoch 034 - training loss: 0.4363, validation loss: 0.2253
2024-06-04 03:06:51 [INFO]: Epoch 035 - training loss: 0.4358, validation loss: 0.2276
2024-06-04 03:06:56 [INFO]: Epoch 036 - training loss: 0.4336, validation loss: 0.2243
2024-06-04 03:07:01 [INFO]: Epoch 037 - training loss: 0.4332, validation loss: 0.2250
2024-06-04 03:07:06 [INFO]: Epoch 038 - training loss: 0.4314, validation loss: 0.2222
2024-06-04 03:07:11 [INFO]: Epoch 039 - training loss: 0.4302, validation loss: 0.2215
2024-06-04 03:07:16 [INFO]: Epoch 040 - training loss: 0.4296, validation loss: 0.2268
2024-06-04 03:07:21 [INFO]: Epoch 041 - training loss: 0.4296, validation loss: 0.2260
2024-06-04 03:07:27 [INFO]: Epoch 042 - training loss: 0.4296, validation loss: 0.2233
2024-06-04 03:07:32 [INFO]: Epoch 043 - training loss: 0.4279, validation loss: 0.2235
2024-06-04 03:07:37 [INFO]: Epoch 044 - training loss: 0.4274, validation loss: 0.2205
2024-06-04 03:07:42 [INFO]: Epoch 045 - training loss: 0.4258, validation loss: 0.2212
2024-06-04 03:07:46 [INFO]: Epoch 046 - training loss: 0.4248, validation loss: 0.2307
2024-06-04 03:07:51 [INFO]: Epoch 047 - training loss: 0.4270, validation loss: 0.2187
2024-06-04 03:07:56 [INFO]: Epoch 048 - training loss: 0.4239, validation loss: 0.2158
2024-06-04 03:08:02 [INFO]: Epoch 049 - training loss: 0.4235, validation loss: 0.2172
2024-06-04 03:08:07 [INFO]: Epoch 050 - training loss: 0.4216, validation loss: 0.2201
2024-06-04 03:08:12 [INFO]: Epoch 051 - training loss: 0.4231, validation loss: 0.2272
2024-06-04 03:08:17 [INFO]: Epoch 052 - training loss: 0.4229, validation loss: 0.2213
2024-06-04 03:08:21 [INFO]: Epoch 053 - training loss: 0.4195, validation loss: 0.2190
2024-06-04 03:08:26 [INFO]: Epoch 054 - training loss: 0.4214, validation loss: 0.2187
2024-06-04 03:08:31 [INFO]: Epoch 055 - training loss: 0.4198, validation loss: 0.2175
2024-06-04 03:08:35 [INFO]: Epoch 056 - training loss: 0.4197, validation loss: 0.2237
2024-06-04 03:08:40 [INFO]: Epoch 057 - training loss: 0.4176, validation loss: 0.2178
2024-06-04 03:08:45 [INFO]: Epoch 058 - training loss: 0.4194, validation loss: 0.2191
2024-06-04 03:08:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:08:45 [INFO]: Finished training. The best model is from epoch#48.
2024-06-04 03:08:45 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240604_T030356/NonstationaryTransformer.pypots
2024-06-04 03:08:46 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/imputation.pkl
2024-06-04 03:08:46 [INFO]: Round3 - NonstationaryTransformer on BeijingAir: MAE=0.2692, MSE=0.3461, MRE=0.4067
2024-06-04 03:08:46 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-04 03:08:46 [INFO]: Using the given device: cuda:0
2024-06-04 03:08:46 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240604_T030846
2024-06-04 03:08:46 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240604_T030846/tensorboard
2024-06-04 03:08:47 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-04 03:08:51 [INFO]: Epoch 001 - training loss: 0.6352, validation loss: 0.3047
2024-06-04 03:08:56 [INFO]: Epoch 002 - training loss: 0.5492, validation loss: 0.2792
2024-06-04 03:09:00 [INFO]: Epoch 003 - training loss: 0.5329, validation loss: 0.2657
2024-06-04 03:09:05 [INFO]: Epoch 004 - training loss: 0.5239, validation loss: 0.2624
2024-06-04 03:09:10 [INFO]: Epoch 005 - training loss: 0.5129, validation loss: 0.2566
2024-06-04 03:09:14 [INFO]: Epoch 006 - training loss: 0.5066, validation loss: 0.2545
2024-06-04 03:09:19 [INFO]: Epoch 007 - training loss: 0.5009, validation loss: 0.2414
2024-06-04 03:09:23 [INFO]: Epoch 008 - training loss: 0.4962, validation loss: 0.2434
2024-06-04 03:09:28 [INFO]: Epoch 009 - training loss: 0.4897, validation loss: 0.2474
2024-06-04 03:09:32 [INFO]: Epoch 010 - training loss: 0.4851, validation loss: 0.2337
2024-06-04 03:09:37 [INFO]: Epoch 011 - training loss: 0.4821, validation loss: 0.2328
2024-06-04 03:09:41 [INFO]: Epoch 012 - training loss: 0.4782, validation loss: 0.2464
2024-06-04 03:09:46 [INFO]: Epoch 013 - training loss: 0.4763, validation loss: 0.2333
2024-06-04 03:09:50 [INFO]: Epoch 014 - training loss: 0.4708, validation loss: 0.2301
2024-06-04 03:09:55 [INFO]: Epoch 015 - training loss: 0.4693, validation loss: 0.2307
2024-06-04 03:10:00 [INFO]: Epoch 016 - training loss: 0.4671, validation loss: 0.2244
2024-06-04 03:10:04 [INFO]: Epoch 017 - training loss: 0.4641, validation loss: 0.2240
2024-06-04 03:10:09 [INFO]: Epoch 018 - training loss: 0.4612, validation loss: 0.2228
2024-06-04 03:10:13 [INFO]: Epoch 019 - training loss: 0.4587, validation loss: 0.2246
2024-06-04 03:10:18 [INFO]: Epoch 020 - training loss: 0.4575, validation loss: 0.2348
2024-06-04 03:10:22 [INFO]: Epoch 021 - training loss: 0.4566, validation loss: 0.2260
2024-06-04 03:10:27 [INFO]: Epoch 022 - training loss: 0.4525, validation loss: 0.2235
2024-06-04 03:10:31 [INFO]: Epoch 023 - training loss: 0.4509, validation loss: 0.2202
2024-06-04 03:10:36 [INFO]: Epoch 024 - training loss: 0.4481, validation loss: 0.2254
2024-06-04 03:10:40 [INFO]: Epoch 025 - training loss: 0.4454, validation loss: 0.2209
2024-06-04 03:10:45 [INFO]: Epoch 026 - training loss: 0.4459, validation loss: 0.2177
2024-06-04 03:10:49 [INFO]: Epoch 027 - training loss: 0.4448, validation loss: 0.2232
2024-06-04 03:10:54 [INFO]: Epoch 028 - training loss: 0.4442, validation loss: 0.2183
2024-06-04 03:10:59 [INFO]: Epoch 029 - training loss: 0.4421, validation loss: 0.2213
2024-06-04 03:11:04 [INFO]: Epoch 030 - training loss: 0.4409, validation loss: 0.2186
2024-06-04 03:11:08 [INFO]: Epoch 031 - training loss: 0.4375, validation loss: 0.2209
2024-06-04 03:11:13 [INFO]: Epoch 032 - training loss: 0.4380, validation loss: 0.2271
2024-06-04 03:11:18 [INFO]: Epoch 033 - training loss: 0.4354, validation loss: 0.2180
2024-06-04 03:11:22 [INFO]: Epoch 034 - training loss: 0.4365, validation loss: 0.2202
2024-06-04 03:11:27 [INFO]: Epoch 035 - training loss: 0.4362, validation loss: 0.2170
2024-06-04 03:11:32 [INFO]: Epoch 036 - training loss: 0.4333, validation loss: 0.2203
2024-06-04 03:11:37 [INFO]: Epoch 037 - training loss: 0.4341, validation loss: 0.2173
2024-06-04 03:11:41 [INFO]: Epoch 038 - training loss: 0.4320, validation loss: 0.2174
2024-06-04 03:11:46 [INFO]: Epoch 039 - training loss: 0.4312, validation loss: 0.2154
2024-06-04 03:11:50 [INFO]: Epoch 040 - training loss: 0.4303, validation loss: 0.2183
2024-06-04 03:11:55 [INFO]: Epoch 041 - training loss: 0.4291, validation loss: 0.2153
2024-06-04 03:11:59 [INFO]: Epoch 042 - training loss: 0.4291, validation loss: 0.2211
2024-06-04 03:12:04 [INFO]: Epoch 043 - training loss: 0.4282, validation loss: 0.2158
2024-06-04 03:12:09 [INFO]: Epoch 044 - training loss: 0.4285, validation loss: 0.2176
2024-06-04 03:12:14 [INFO]: Epoch 045 - training loss: 0.4271, validation loss: 0.2206
2024-06-04 03:12:18 [INFO]: Epoch 046 - training loss: 0.4264, validation loss: 0.2182
2024-06-04 03:12:23 [INFO]: Epoch 047 - training loss: 0.4250, validation loss: 0.2157
2024-06-04 03:12:27 [INFO]: Epoch 048 - training loss: 0.4240, validation loss: 0.2168
2024-06-04 03:12:32 [INFO]: Epoch 049 - training loss: 0.4233, validation loss: 0.2232
2024-06-04 03:12:36 [INFO]: Epoch 050 - training loss: 0.4228, validation loss: 0.2162
2024-06-04 03:12:41 [INFO]: Epoch 051 - training loss: 0.4226, validation loss: 0.2177
2024-06-04 03:12:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:12:41 [INFO]: Finished training. The best model is from epoch#41.
2024-06-04 03:12:41 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240604_T030846/NonstationaryTransformer.pypots
2024-06-04 03:12:43 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/imputation.pkl
2024-06-04 03:12:43 [INFO]: Round4 - NonstationaryTransformer on BeijingAir: MAE=0.2756, MSE=0.3405, MRE=0.4164
2024-06-04 03:12:43 [INFO]: Done! Final results:
Averaged NonstationaryTransformer (6,978,068 params) on BeijingAir: MAE=0.2090 ± 0.0015412071398034463, MSE=0.2416 ± 0.014469175415935379, MRE=0.2780 ± 0.0020499133745269823, average inference time=0.38