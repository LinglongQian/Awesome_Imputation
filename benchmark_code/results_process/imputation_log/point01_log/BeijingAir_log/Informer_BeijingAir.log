2024-06-04 02:49:23 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-04 02:49:23 [INFO]: Using the given device: cuda:0
2024-06-04 02:49:24 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_0/20240604_T024924
2024-06-04 02:49:24 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_0/20240604_T024924/tensorboard
2024-06-04 02:49:25 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-04 02:49:44 [INFO]: Epoch 001 - training loss: 0.8689, validation loss: 0.3267
2024-06-04 02:49:52 [INFO]: Epoch 002 - training loss: 0.5454, validation loss: 0.2663
2024-06-04 02:50:01 [INFO]: Epoch 003 - training loss: 0.4751, validation loss: 0.2388
2024-06-04 02:50:10 [INFO]: Epoch 004 - training loss: 0.4397, validation loss: 0.2223
2024-06-04 02:50:18 [INFO]: Epoch 005 - training loss: 0.4218, validation loss: 0.2076
2024-06-04 02:50:27 [INFO]: Epoch 006 - training loss: 0.4051, validation loss: 0.1970
2024-06-04 02:50:36 [INFO]: Epoch 007 - training loss: 0.3924, validation loss: 0.1893
2024-06-04 02:50:45 [INFO]: Epoch 008 - training loss: 0.3805, validation loss: 0.1849
2024-06-04 02:50:53 [INFO]: Epoch 009 - training loss: 0.3773, validation loss: 0.1803
2024-06-04 02:51:02 [INFO]: Epoch 010 - training loss: 0.3667, validation loss: 0.1775
2024-06-04 02:51:10 [INFO]: Epoch 011 - training loss: 0.3585, validation loss: 0.1743
2024-06-04 02:51:19 [INFO]: Epoch 012 - training loss: 0.3576, validation loss: 0.1722
2024-06-04 02:51:27 [INFO]: Epoch 013 - training loss: 0.3540, validation loss: 0.1719
2024-06-04 02:51:35 [INFO]: Epoch 014 - training loss: 0.3484, validation loss: 0.1705
2024-06-04 02:51:44 [INFO]: Epoch 015 - training loss: 0.3464, validation loss: 0.1734
2024-06-04 02:51:52 [INFO]: Epoch 016 - training loss: 0.3479, validation loss: 0.1678
2024-06-04 02:52:01 [INFO]: Epoch 017 - training loss: 0.3406, validation loss: 0.1689
2024-06-04 02:52:09 [INFO]: Epoch 018 - training loss: 0.3371, validation loss: 0.1682
2024-06-04 02:52:18 [INFO]: Epoch 019 - training loss: 0.3388, validation loss: 0.1681
2024-06-04 02:52:26 [INFO]: Epoch 020 - training loss: 0.3340, validation loss: 0.1666
2024-06-04 02:52:34 [INFO]: Epoch 021 - training loss: 0.3298, validation loss: 0.1663
2024-06-04 02:52:43 [INFO]: Epoch 022 - training loss: 0.3247, validation loss: 0.1620
2024-06-04 02:52:52 [INFO]: Epoch 023 - training loss: 0.3219, validation loss: 0.1637
2024-06-04 02:53:00 [INFO]: Epoch 024 - training loss: 0.3167, validation loss: 0.1647
2024-06-04 02:53:09 [INFO]: Epoch 025 - training loss: 0.3184, validation loss: 0.1661
2024-06-04 02:53:18 [INFO]: Epoch 026 - training loss: 0.3180, validation loss: 0.1632
2024-06-04 02:53:26 [INFO]: Epoch 027 - training loss: 0.3144, validation loss: 0.1652
2024-06-04 02:53:35 [INFO]: Epoch 028 - training loss: 0.3109, validation loss: 0.1645
2024-06-04 02:53:43 [INFO]: Epoch 029 - training loss: 0.3079, validation loss: 0.1629
2024-06-04 02:53:52 [INFO]: Epoch 030 - training loss: 0.3068, validation loss: 0.1633
2024-06-04 02:54:00 [INFO]: Epoch 031 - training loss: 0.3062, validation loss: 0.1598
2024-06-04 02:54:08 [INFO]: Epoch 032 - training loss: 0.3025, validation loss: 0.1601
2024-06-04 02:54:17 [INFO]: Epoch 033 - training loss: 0.3041, validation loss: 0.1619
2024-06-04 02:54:26 [INFO]: Epoch 034 - training loss: 0.3191, validation loss: 0.1635
2024-06-04 02:54:35 [INFO]: Epoch 035 - training loss: 0.3048, validation loss: 0.1570
2024-06-04 02:54:43 [INFO]: Epoch 036 - training loss: 0.2974, validation loss: 0.1558
2024-06-04 02:54:51 [INFO]: Epoch 037 - training loss: 0.2960, validation loss: 0.1598
2024-06-04 02:55:00 [INFO]: Epoch 038 - training loss: 0.2931, validation loss: 0.1569
2024-06-04 02:55:08 [INFO]: Epoch 039 - training loss: 0.2911, validation loss: 0.1553
2024-06-04 02:55:17 [INFO]: Epoch 040 - training loss: 0.2857, validation loss: 0.1567
2024-06-04 02:55:25 [INFO]: Epoch 041 - training loss: 0.2863, validation loss: 0.1539
2024-06-04 02:55:34 [INFO]: Epoch 042 - training loss: 0.2921, validation loss: 0.1566
2024-06-04 02:55:42 [INFO]: Epoch 043 - training loss: 0.2876, validation loss: 0.1562
2024-06-04 02:55:51 [INFO]: Epoch 044 - training loss: 0.2828, validation loss: 0.1569
2024-06-04 02:56:00 [INFO]: Epoch 045 - training loss: 0.2836, validation loss: 0.1534
2024-06-04 02:56:09 [INFO]: Epoch 046 - training loss: 0.2859, validation loss: 0.1530
2024-06-04 02:56:17 [INFO]: Epoch 047 - training loss: 0.2844, validation loss: 0.1527
2024-06-04 02:56:26 [INFO]: Epoch 048 - training loss: 0.2797, validation loss: 0.1513
2024-06-04 02:56:34 [INFO]: Epoch 049 - training loss: 0.2760, validation loss: 0.1519
2024-06-04 02:56:42 [INFO]: Epoch 050 - training loss: 0.2746, validation loss: 0.1497
2024-06-04 02:56:51 [INFO]: Epoch 051 - training loss: 0.2756, validation loss: 0.1501
2024-06-04 02:57:00 [INFO]: Epoch 052 - training loss: 0.2710, validation loss: 0.1501
2024-06-04 02:57:08 [INFO]: Epoch 053 - training loss: 0.2697, validation loss: 0.1493
2024-06-04 02:57:17 [INFO]: Epoch 054 - training loss: 0.2675, validation loss: 0.1457
2024-06-04 02:57:25 [INFO]: Epoch 055 - training loss: 0.2718, validation loss: 0.1501
2024-06-04 02:57:34 [INFO]: Epoch 056 - training loss: 0.2693, validation loss: 0.1487
2024-06-04 02:57:42 [INFO]: Epoch 057 - training loss: 0.2670, validation loss: 0.1478
2024-06-04 02:57:51 [INFO]: Epoch 058 - training loss: 0.2643, validation loss: 0.1468
2024-06-04 02:57:59 [INFO]: Epoch 059 - training loss: 0.2623, validation loss: 0.1473
2024-06-04 02:58:07 [INFO]: Epoch 060 - training loss: 0.2635, validation loss: 0.1475
2024-06-04 02:58:16 [INFO]: Epoch 061 - training loss: 0.2643, validation loss: 0.1451
2024-06-04 02:58:25 [INFO]: Epoch 062 - training loss: 0.2629, validation loss: 0.1442
2024-06-04 02:58:33 [INFO]: Epoch 063 - training loss: 0.2624, validation loss: 0.1453
2024-06-04 02:58:42 [INFO]: Epoch 064 - training loss: 0.2638, validation loss: 0.1441
2024-06-04 02:58:50 [INFO]: Epoch 065 - training loss: 0.2615, validation loss: 0.1434
2024-06-04 02:58:58 [INFO]: Epoch 066 - training loss: 0.2612, validation loss: 0.1467
2024-06-04 02:59:06 [INFO]: Epoch 067 - training loss: 0.2600, validation loss: 0.1453
2024-06-04 02:59:15 [INFO]: Epoch 068 - training loss: 0.2546, validation loss: 0.1442
2024-06-04 02:59:24 [INFO]: Epoch 069 - training loss: 0.2561, validation loss: 0.1416
2024-06-04 02:59:32 [INFO]: Epoch 070 - training loss: 0.2564, validation loss: 0.1421
2024-06-04 02:59:41 [INFO]: Epoch 071 - training loss: 0.2560, validation loss: 0.1439
2024-06-04 02:59:50 [INFO]: Epoch 072 - training loss: 0.2634, validation loss: 0.1416
2024-06-04 02:59:59 [INFO]: Epoch 073 - training loss: 0.2568, validation loss: 0.1443
2024-06-04 03:00:07 [INFO]: Epoch 074 - training loss: 0.2553, validation loss: 0.1413
2024-06-04 03:00:15 [INFO]: Epoch 075 - training loss: 0.2508, validation loss: 0.1383
2024-06-04 03:00:24 [INFO]: Epoch 076 - training loss: 0.2497, validation loss: 0.1412
2024-06-04 03:00:33 [INFO]: Epoch 077 - training loss: 0.2513, validation loss: 0.1403
2024-06-04 03:00:42 [INFO]: Epoch 078 - training loss: 0.2501, validation loss: 0.1379
2024-06-04 03:00:50 [INFO]: Epoch 079 - training loss: 0.2468, validation loss: 0.1402
2024-06-04 03:00:58 [INFO]: Epoch 080 - training loss: 0.2461, validation loss: 0.1386
2024-06-04 03:01:05 [INFO]: Epoch 081 - training loss: 0.2501, validation loss: 0.1395
2024-06-04 03:01:13 [INFO]: Epoch 082 - training loss: 0.2456, validation loss: 0.1375
2024-06-04 03:01:20 [INFO]: Epoch 083 - training loss: 0.2486, validation loss: 0.1387
2024-06-04 03:01:28 [INFO]: Epoch 084 - training loss: 0.2478, validation loss: 0.1369
2024-06-04 03:01:35 [INFO]: Epoch 085 - training loss: 0.2449, validation loss: 0.1370
2024-06-04 03:01:42 [INFO]: Epoch 086 - training loss: 0.2465, validation loss: 0.1368
2024-06-04 03:01:50 [INFO]: Epoch 087 - training loss: 0.2461, validation loss: 0.1368
2024-06-04 03:01:57 [INFO]: Epoch 088 - training loss: 0.2402, validation loss: 0.1360
2024-06-04 03:02:05 [INFO]: Epoch 089 - training loss: 0.2419, validation loss: 0.1373
2024-06-04 03:02:13 [INFO]: Epoch 090 - training loss: 0.2429, validation loss: 0.1352
2024-06-04 03:02:20 [INFO]: Epoch 091 - training loss: 0.2420, validation loss: 0.1350
2024-06-04 03:02:28 [INFO]: Epoch 092 - training loss: 0.2436, validation loss: 0.1365
2024-06-04 03:02:36 [INFO]: Epoch 093 - training loss: 0.2400, validation loss: 0.1356
2024-06-04 03:02:43 [INFO]: Epoch 094 - training loss: 0.2384, validation loss: 0.1355
2024-06-04 03:02:51 [INFO]: Epoch 095 - training loss: 0.2398, validation loss: 0.1388
2024-06-04 03:02:58 [INFO]: Epoch 096 - training loss: 0.2417, validation loss: 0.1319
2024-06-04 03:03:06 [INFO]: Epoch 097 - training loss: 0.2415, validation loss: 0.1328
2024-06-04 03:03:14 [INFO]: Epoch 098 - training loss: 0.2387, validation loss: 0.1336
2024-06-04 03:03:21 [INFO]: Epoch 099 - training loss: 0.2400, validation loss: 0.1342
2024-06-04 03:03:29 [INFO]: Epoch 100 - training loss: 0.2406, validation loss: 0.1322
2024-06-04 03:03:29 [INFO]: Finished training. The best model is from epoch#96.
2024-06-04 03:03:29 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_0/20240604_T024924/Informer.pypots
2024-06-04 03:03:34 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_0/imputation.pkl
2024-06-04 03:03:34 [INFO]: Round0 - Informer on BeijingAir: MAE=0.1984, MSE=0.1931, MRE=0.2997
2024-06-04 03:03:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-04 03:03:34 [INFO]: Using the given device: cuda:0
2024-06-04 03:03:34 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_1/20240604_T030334
2024-06-04 03:03:34 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_1/20240604_T030334/tensorboard
2024-06-04 03:03:35 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-04 03:03:42 [INFO]: Epoch 001 - training loss: 0.8836, validation loss: 0.3432
2024-06-04 03:03:50 [INFO]: Epoch 002 - training loss: 0.5529, validation loss: 0.2735
2024-06-04 03:03:57 [INFO]: Epoch 003 - training loss: 0.4777, validation loss: 0.2457
2024-06-04 03:04:04 [INFO]: Epoch 004 - training loss: 0.4397, validation loss: 0.2278
2024-06-04 03:04:12 [INFO]: Epoch 005 - training loss: 0.4206, validation loss: 0.2171
2024-06-04 03:04:19 [INFO]: Epoch 006 - training loss: 0.4113, validation loss: 0.1993
2024-06-04 03:04:26 [INFO]: Epoch 007 - training loss: 0.3942, validation loss: 0.1903
2024-06-04 03:04:34 [INFO]: Epoch 008 - training loss: 0.3793, validation loss: 0.1872
2024-06-04 03:04:41 [INFO]: Epoch 009 - training loss: 0.3765, validation loss: 0.1796
2024-06-04 03:04:49 [INFO]: Epoch 010 - training loss: 0.3703, validation loss: 0.1814
2024-06-04 03:04:56 [INFO]: Epoch 011 - training loss: 0.3666, validation loss: 0.1747
2024-06-04 03:05:04 [INFO]: Epoch 012 - training loss: 0.3602, validation loss: 0.1726
2024-06-04 03:05:11 [INFO]: Epoch 013 - training loss: 0.3510, validation loss: 0.1741
2024-06-04 03:05:19 [INFO]: Epoch 014 - training loss: 0.3511, validation loss: 0.1743
2024-06-04 03:05:26 [INFO]: Epoch 015 - training loss: 0.3484, validation loss: 0.1683
2024-06-04 03:05:33 [INFO]: Epoch 016 - training loss: 0.3451, validation loss: 0.1678
2024-06-04 03:05:41 [INFO]: Epoch 017 - training loss: 0.3395, validation loss: 0.1714
2024-06-04 03:05:48 [INFO]: Epoch 018 - training loss: 0.3375, validation loss: 0.1674
2024-06-04 03:05:56 [INFO]: Epoch 019 - training loss: 0.3327, validation loss: 0.1671
2024-06-04 03:06:03 [INFO]: Epoch 020 - training loss: 0.3320, validation loss: 0.1646
2024-06-04 03:06:10 [INFO]: Epoch 021 - training loss: 0.3282, validation loss: 0.1666
2024-06-04 03:06:17 [INFO]: Epoch 022 - training loss: 0.3267, validation loss: 0.1648
2024-06-04 03:06:24 [INFO]: Epoch 023 - training loss: 0.3287, validation loss: 0.1688
2024-06-04 03:06:31 [INFO]: Epoch 024 - training loss: 0.3213, validation loss: 0.1689
2024-06-04 03:06:38 [INFO]: Epoch 025 - training loss: 0.3207, validation loss: 0.1642
2024-06-04 03:06:46 [INFO]: Epoch 026 - training loss: 0.3148, validation loss: 0.1635
2024-06-04 03:06:53 [INFO]: Epoch 027 - training loss: 0.3120, validation loss: 0.1674
2024-06-04 03:07:00 [INFO]: Epoch 028 - training loss: 0.3109, validation loss: 0.1643
2024-06-04 03:07:08 [INFO]: Epoch 029 - training loss: 0.3130, validation loss: 0.1650
2024-06-04 03:07:15 [INFO]: Epoch 030 - training loss: 0.3087, validation loss: 0.1651
2024-06-04 03:07:23 [INFO]: Epoch 031 - training loss: 0.3031, validation loss: 0.1630
2024-06-04 03:07:30 [INFO]: Epoch 032 - training loss: 0.3086, validation loss: 0.1587
2024-06-04 03:07:38 [INFO]: Epoch 033 - training loss: 0.3089, validation loss: 0.1616
2024-06-04 03:07:46 [INFO]: Epoch 034 - training loss: 0.3010, validation loss: 0.1601
2024-06-04 03:07:54 [INFO]: Epoch 035 - training loss: 0.2958, validation loss: 0.1593
2024-06-04 03:08:01 [INFO]: Epoch 036 - training loss: 0.3035, validation loss: 0.1610
2024-06-04 03:08:08 [INFO]: Epoch 037 - training loss: 0.2997, validation loss: 0.1595
2024-06-04 03:08:16 [INFO]: Epoch 038 - training loss: 0.2961, validation loss: 0.1585
2024-06-04 03:08:22 [INFO]: Epoch 039 - training loss: 0.2956, validation loss: 0.1564
2024-06-04 03:08:29 [INFO]: Epoch 040 - training loss: 0.2958, validation loss: 0.1561
2024-06-04 03:08:36 [INFO]: Epoch 041 - training loss: 0.2915, validation loss: 0.1550
2024-06-04 03:08:43 [INFO]: Epoch 042 - training loss: 0.2877, validation loss: 0.1561
2024-06-04 03:08:50 [INFO]: Epoch 043 - training loss: 0.2898, validation loss: 0.1563
2024-06-04 03:08:57 [INFO]: Epoch 044 - training loss: 0.2863, validation loss: 0.1555
2024-06-04 03:09:04 [INFO]: Epoch 045 - training loss: 0.2850, validation loss: 0.1542
2024-06-04 03:09:11 [INFO]: Epoch 046 - training loss: 0.2794, validation loss: 0.1552
2024-06-04 03:09:18 [INFO]: Epoch 047 - training loss: 0.2796, validation loss: 0.1561
2024-06-04 03:09:25 [INFO]: Epoch 048 - training loss: 0.2768, validation loss: 0.1531
2024-06-04 03:09:32 [INFO]: Epoch 049 - training loss: 0.2750, validation loss: 0.1529
2024-06-04 03:09:40 [INFO]: Epoch 050 - training loss: 0.2750, validation loss: 0.1510
2024-06-04 03:09:46 [INFO]: Epoch 051 - training loss: 0.2727, validation loss: 0.1491
2024-06-04 03:09:53 [INFO]: Epoch 052 - training loss: 0.2726, validation loss: 0.1488
2024-06-04 03:10:00 [INFO]: Epoch 053 - training loss: 0.2722, validation loss: 0.1477
2024-06-04 03:10:07 [INFO]: Epoch 054 - training loss: 0.2672, validation loss: 0.1499
2024-06-04 03:10:14 [INFO]: Epoch 055 - training loss: 0.2691, validation loss: 0.1507
2024-06-04 03:10:21 [INFO]: Epoch 056 - training loss: 0.2676, validation loss: 0.1490
2024-06-04 03:10:29 [INFO]: Epoch 057 - training loss: 0.2662, validation loss: 0.1476
2024-06-04 03:10:35 [INFO]: Epoch 058 - training loss: 0.2698, validation loss: 0.1479
2024-06-04 03:10:42 [INFO]: Epoch 059 - training loss: 0.2724, validation loss: 0.1518
2024-06-04 03:10:49 [INFO]: Epoch 060 - training loss: 0.2916, validation loss: 0.1514
2024-06-04 03:10:56 [INFO]: Epoch 061 - training loss: 0.2685, validation loss: 0.1463
2024-06-04 03:11:03 [INFO]: Epoch 062 - training loss: 0.2633, validation loss: 0.1496
2024-06-04 03:11:10 [INFO]: Epoch 063 - training loss: 0.2625, validation loss: 0.1454
2024-06-04 03:11:17 [INFO]: Epoch 064 - training loss: 0.2629, validation loss: 0.1465
2024-06-04 03:11:24 [INFO]: Epoch 065 - training loss: 0.2628, validation loss: 0.1464
2024-06-04 03:11:31 [INFO]: Epoch 066 - training loss: 0.2615, validation loss: 0.1453
2024-06-04 03:11:38 [INFO]: Epoch 067 - training loss: 0.2579, validation loss: 0.1478
2024-06-04 03:11:45 [INFO]: Epoch 068 - training loss: 0.2576, validation loss: 0.1454
2024-06-04 03:11:51 [INFO]: Epoch 069 - training loss: 0.2566, validation loss: 0.1456
2024-06-04 03:11:58 [INFO]: Epoch 070 - training loss: 0.2581, validation loss: 0.1432
2024-06-04 03:12:05 [INFO]: Epoch 071 - training loss: 0.2546, validation loss: 0.1438
2024-06-04 03:12:12 [INFO]: Epoch 072 - training loss: 0.2554, validation loss: 0.1416
2024-06-04 03:12:19 [INFO]: Epoch 073 - training loss: 0.2548, validation loss: 0.1408
2024-06-04 03:12:26 [INFO]: Epoch 074 - training loss: 0.2513, validation loss: 0.1422
2024-06-04 03:12:33 [INFO]: Epoch 075 - training loss: 0.2503, validation loss: 0.1422
2024-06-04 03:12:40 [INFO]: Epoch 076 - training loss: 0.2545, validation loss: 0.1446
2024-06-04 03:12:46 [INFO]: Epoch 077 - training loss: 0.2569, validation loss: 0.1415
2024-06-04 03:12:52 [INFO]: Epoch 078 - training loss: 0.2538, validation loss: 0.1417
2024-06-04 03:12:58 [INFO]: Epoch 079 - training loss: 0.2522, validation loss: 0.1388
2024-06-04 03:13:05 [INFO]: Epoch 080 - training loss: 0.2500, validation loss: 0.1395
2024-06-04 03:13:11 [INFO]: Epoch 081 - training loss: 0.2489, validation loss: 0.1394
2024-06-04 03:13:17 [INFO]: Epoch 082 - training loss: 0.2462, validation loss: 0.1390
2024-06-04 03:13:24 [INFO]: Epoch 083 - training loss: 0.2489, validation loss: 0.1397
2024-06-04 03:13:30 [INFO]: Epoch 084 - training loss: 0.2550, validation loss: 0.1411
2024-06-04 03:13:36 [INFO]: Epoch 085 - training loss: 0.2506, validation loss: 0.1395
2024-06-04 03:13:42 [INFO]: Epoch 086 - training loss: 0.2479, validation loss: 0.1382
2024-06-04 03:13:48 [INFO]: Epoch 087 - training loss: 0.2476, validation loss: 0.1375
2024-06-04 03:13:55 [INFO]: Epoch 088 - training loss: 0.2470, validation loss: 0.1392
2024-06-04 03:14:01 [INFO]: Epoch 089 - training loss: 0.2475, validation loss: 0.1401
2024-06-04 03:14:06 [INFO]: Epoch 090 - training loss: 0.2455, validation loss: 0.1383
2024-06-04 03:14:13 [INFO]: Epoch 091 - training loss: 0.2431, validation loss: 0.1379
2024-06-04 03:14:19 [INFO]: Epoch 092 - training loss: 0.2404, validation loss: 0.1373
2024-06-04 03:14:25 [INFO]: Epoch 093 - training loss: 0.2405, validation loss: 0.1355
2024-06-04 03:14:32 [INFO]: Epoch 094 - training loss: 0.2390, validation loss: 0.1351
2024-06-04 03:14:38 [INFO]: Epoch 095 - training loss: 0.2416, validation loss: 0.1353
2024-06-04 03:14:44 [INFO]: Epoch 096 - training loss: 0.2409, validation loss: 0.1365
2024-06-04 03:14:50 [INFO]: Epoch 097 - training loss: 0.2409, validation loss: 0.1348
2024-06-04 03:14:56 [INFO]: Epoch 098 - training loss: 0.2386, validation loss: 0.1337
2024-06-04 03:15:02 [INFO]: Epoch 099 - training loss: 0.2464, validation loss: 0.1338
2024-06-04 03:15:08 [INFO]: Epoch 100 - training loss: 0.2437, validation loss: 0.1363
2024-06-04 03:15:08 [INFO]: Finished training. The best model is from epoch#98.
2024-06-04 03:15:08 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_1/20240604_T030334/Informer.pypots
2024-06-04 03:15:11 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_1/imputation.pkl
2024-06-04 03:15:11 [INFO]: Round1 - Informer on BeijingAir: MAE=0.2044, MSE=0.1996, MRE=0.3088
2024-06-04 03:15:11 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-04 03:15:11 [INFO]: Using the given device: cuda:0
2024-06-04 03:15:11 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_2/20240604_T031511
2024-06-04 03:15:11 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_2/20240604_T031511/tensorboard
2024-06-04 03:15:11 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-04 03:15:17 [INFO]: Epoch 001 - training loss: 0.8767, validation loss: 0.3346
2024-06-04 03:15:24 [INFO]: Epoch 002 - training loss: 0.5475, validation loss: 0.2758
2024-06-04 03:15:30 [INFO]: Epoch 003 - training loss: 0.4736, validation loss: 0.2390
2024-06-04 03:15:36 [INFO]: Epoch 004 - training loss: 0.4389, validation loss: 0.2270
2024-06-04 03:15:42 [INFO]: Epoch 005 - training loss: 0.4203, validation loss: 0.2123
2024-06-04 03:15:48 [INFO]: Epoch 006 - training loss: 0.4074, validation loss: 0.2020
2024-06-04 03:15:54 [INFO]: Epoch 007 - training loss: 0.3963, validation loss: 0.1961
2024-06-04 03:16:00 [INFO]: Epoch 008 - training loss: 0.3881, validation loss: 0.1884
2024-06-04 03:16:07 [INFO]: Epoch 009 - training loss: 0.3830, validation loss: 0.1870
2024-06-04 03:16:13 [INFO]: Epoch 010 - training loss: 0.3680, validation loss: 0.1799
2024-06-04 03:16:19 [INFO]: Epoch 011 - training loss: 0.3601, validation loss: 0.1784
2024-06-04 03:16:25 [INFO]: Epoch 012 - training loss: 0.3586, validation loss: 0.1783
2024-06-04 03:16:31 [INFO]: Epoch 013 - training loss: 0.3578, validation loss: 0.1739
2024-06-04 03:16:37 [INFO]: Epoch 014 - training loss: 0.3496, validation loss: 0.1725
2024-06-04 03:16:43 [INFO]: Epoch 015 - training loss: 0.3469, validation loss: 0.1717
2024-06-04 03:16:49 [INFO]: Epoch 016 - training loss: 0.3395, validation loss: 0.1682
2024-06-04 03:16:56 [INFO]: Epoch 017 - training loss: 0.3376, validation loss: 0.1738
2024-06-04 03:17:02 [INFO]: Epoch 018 - training loss: 0.3392, validation loss: 0.1694
2024-06-04 03:17:08 [INFO]: Epoch 019 - training loss: 0.3366, validation loss: 0.1727
2024-06-04 03:17:14 [INFO]: Epoch 020 - training loss: 0.3346, validation loss: 0.1701
2024-06-04 03:17:21 [INFO]: Epoch 021 - training loss: 0.3300, validation loss: 0.1672
2024-06-04 03:17:27 [INFO]: Epoch 022 - training loss: 0.3239, validation loss: 0.1711
2024-06-04 03:17:33 [INFO]: Epoch 023 - training loss: 0.3285, validation loss: 0.1666
2024-06-04 03:17:40 [INFO]: Epoch 024 - training loss: 0.3237, validation loss: 0.1671
2024-06-04 03:17:46 [INFO]: Epoch 025 - training loss: 0.3162, validation loss: 0.1645
2024-06-04 03:17:52 [INFO]: Epoch 026 - training loss: 0.3133, validation loss: 0.1628
2024-06-04 03:17:59 [INFO]: Epoch 027 - training loss: 0.3144, validation loss: 0.1665
2024-06-04 03:18:05 [INFO]: Epoch 028 - training loss: 0.3090, validation loss: 0.1632
2024-06-04 03:18:11 [INFO]: Epoch 029 - training loss: 0.3094, validation loss: 0.1639
2024-06-04 03:18:17 [INFO]: Epoch 030 - training loss: 0.3108, validation loss: 0.1621
2024-06-04 03:18:24 [INFO]: Epoch 031 - training loss: 0.3071, validation loss: 0.1613
2024-06-04 03:18:30 [INFO]: Epoch 032 - training loss: 0.3032, validation loss: 0.1628
2024-06-04 03:18:36 [INFO]: Epoch 033 - training loss: 0.3036, validation loss: 0.1607
2024-06-04 03:18:42 [INFO]: Epoch 034 - training loss: 0.3019, validation loss: 0.1596
2024-06-04 03:18:48 [INFO]: Epoch 035 - training loss: 0.3008, validation loss: 0.1589
2024-06-04 03:18:54 [INFO]: Epoch 036 - training loss: 0.2951, validation loss: 0.1578
2024-06-04 03:19:00 [INFO]: Epoch 037 - training loss: 0.2944, validation loss: 0.1581
2024-06-04 03:19:06 [INFO]: Epoch 038 - training loss: 0.3034, validation loss: 0.1608
2024-06-04 03:19:12 [INFO]: Epoch 039 - training loss: 0.2969, validation loss: 0.1554
2024-06-04 03:19:18 [INFO]: Epoch 040 - training loss: 0.2908, validation loss: 0.1570
2024-06-04 03:19:25 [INFO]: Epoch 041 - training loss: 0.2867, validation loss: 0.1549
2024-06-04 03:19:31 [INFO]: Epoch 042 - training loss: 0.2867, validation loss: 0.1569
2024-06-04 03:19:37 [INFO]: Epoch 043 - training loss: 0.2841, validation loss: 0.1548
2024-06-04 03:19:43 [INFO]: Epoch 044 - training loss: 0.2887, validation loss: 0.1556
2024-06-04 03:19:49 [INFO]: Epoch 045 - training loss: 0.2858, validation loss: 0.1532
2024-06-04 03:19:55 [INFO]: Epoch 046 - training loss: 0.2824, validation loss: 0.1521
2024-06-04 03:20:02 [INFO]: Epoch 047 - training loss: 0.2764, validation loss: 0.1533
2024-06-04 03:20:08 [INFO]: Epoch 048 - training loss: 0.2747, validation loss: 0.1531
2024-06-04 03:20:14 [INFO]: Epoch 049 - training loss: 0.2745, validation loss: 0.1507
2024-06-04 03:20:20 [INFO]: Epoch 050 - training loss: 0.2764, validation loss: 0.1508
2024-06-04 03:20:27 [INFO]: Epoch 051 - training loss: 0.2700, validation loss: 0.1489
2024-06-04 03:20:33 [INFO]: Epoch 052 - training loss: 0.2719, validation loss: 0.1501
2024-06-04 03:20:39 [INFO]: Epoch 053 - training loss: 0.2691, validation loss: 0.1482
2024-06-04 03:20:45 [INFO]: Epoch 054 - training loss: 0.2707, validation loss: 0.1513
2024-06-04 03:20:51 [INFO]: Epoch 055 - training loss: 0.2728, validation loss: 0.1492
2024-06-04 03:20:57 [INFO]: Epoch 056 - training loss: 0.2759, validation loss: 0.1497
2024-06-04 03:21:04 [INFO]: Epoch 057 - training loss: 0.2672, validation loss: 0.1495
2024-06-04 03:21:09 [INFO]: Epoch 058 - training loss: 0.2662, validation loss: 0.1479
2024-06-04 03:21:16 [INFO]: Epoch 059 - training loss: 0.2675, validation loss: 0.1503
2024-06-04 03:21:22 [INFO]: Epoch 060 - training loss: 0.2673, validation loss: 0.1493
2024-06-04 03:21:28 [INFO]: Epoch 061 - training loss: 0.2649, validation loss: 0.1481
2024-06-04 03:21:35 [INFO]: Epoch 062 - training loss: 0.2604, validation loss: 0.1462
2024-06-04 03:21:41 [INFO]: Epoch 063 - training loss: 0.2587, validation loss: 0.1464
2024-06-04 03:21:47 [INFO]: Epoch 064 - training loss: 0.2594, validation loss: 0.1463
2024-06-04 03:21:53 [INFO]: Epoch 065 - training loss: 0.2569, validation loss: 0.1459
2024-06-04 03:21:59 [INFO]: Epoch 066 - training loss: 0.2622, validation loss: 0.1478
2024-06-04 03:22:05 [INFO]: Epoch 067 - training loss: 0.2604, validation loss: 0.1452
2024-06-04 03:22:11 [INFO]: Epoch 068 - training loss: 0.2576, validation loss: 0.1441
2024-06-04 03:22:17 [INFO]: Epoch 069 - training loss: 0.2579, validation loss: 0.1428
2024-06-04 03:22:23 [INFO]: Epoch 070 - training loss: 0.2621, validation loss: 0.1431
2024-06-04 03:22:29 [INFO]: Epoch 071 - training loss: 0.2593, validation loss: 0.1433
2024-06-04 03:22:35 [INFO]: Epoch 072 - training loss: 0.2556, validation loss: 0.1445
2024-06-04 03:22:41 [INFO]: Epoch 073 - training loss: 0.2531, validation loss: 0.1428
2024-06-04 03:22:47 [INFO]: Epoch 074 - training loss: 0.2530, validation loss: 0.1428
2024-06-04 03:22:53 [INFO]: Epoch 075 - training loss: 0.2512, validation loss: 0.1410
2024-06-04 03:23:00 [INFO]: Epoch 076 - training loss: 0.2490, validation loss: 0.1413
2024-06-04 03:23:05 [INFO]: Epoch 077 - training loss: 0.2471, validation loss: 0.1404
2024-06-04 03:23:12 [INFO]: Epoch 078 - training loss: 0.2482, validation loss: 0.1405
2024-06-04 03:23:18 [INFO]: Epoch 079 - training loss: 0.2479, validation loss: 0.1429
2024-06-04 03:23:24 [INFO]: Epoch 080 - training loss: 0.2464, validation loss: 0.1391
2024-06-04 03:23:30 [INFO]: Epoch 081 - training loss: 0.2456, validation loss: 0.1376
2024-06-04 03:23:37 [INFO]: Epoch 082 - training loss: 0.2471, validation loss: 0.1390
2024-06-04 03:23:43 [INFO]: Epoch 083 - training loss: 0.2468, validation loss: 0.1362
2024-06-04 03:23:49 [INFO]: Epoch 084 - training loss: 0.2434, validation loss: 0.1395
2024-06-04 03:23:56 [INFO]: Epoch 085 - training loss: 0.2419, validation loss: 0.1404
2024-06-04 03:24:01 [INFO]: Epoch 086 - training loss: 0.2408, validation loss: 0.1376
2024-06-04 03:24:08 [INFO]: Epoch 087 - training loss: 0.2448, validation loss: 0.1380
2024-06-04 03:24:14 [INFO]: Epoch 088 - training loss: 0.2445, validation loss: 0.1378
2024-06-04 03:24:20 [INFO]: Epoch 089 - training loss: 0.2440, validation loss: 0.1371
2024-06-04 03:24:26 [INFO]: Epoch 090 - training loss: 0.2429, validation loss: 0.1353
2024-06-04 03:24:33 [INFO]: Epoch 091 - training loss: 0.2426, validation loss: 0.1369
2024-06-04 03:24:39 [INFO]: Epoch 092 - training loss: 0.2399, validation loss: 0.1345
2024-06-04 03:24:45 [INFO]: Epoch 093 - training loss: 0.2397, validation loss: 0.1355
2024-06-04 03:24:52 [INFO]: Epoch 094 - training loss: 0.2372, validation loss: 0.1369
2024-06-04 03:24:58 [INFO]: Epoch 095 - training loss: 0.2422, validation loss: 0.1349
2024-06-04 03:25:04 [INFO]: Epoch 096 - training loss: 0.2416, validation loss: 0.1353
2024-06-04 03:25:11 [INFO]: Epoch 097 - training loss: 0.2378, validation loss: 0.1340
2024-06-04 03:25:17 [INFO]: Epoch 098 - training loss: 0.2360, validation loss: 0.1329
2024-06-04 03:25:23 [INFO]: Epoch 099 - training loss: 0.2387, validation loss: 0.1339
2024-06-04 03:25:30 [INFO]: Epoch 100 - training loss: 0.2348, validation loss: 0.1344
2024-06-04 03:25:30 [INFO]: Finished training. The best model is from epoch#98.
2024-06-04 03:25:30 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_2/20240604_T031511/Informer.pypots
2024-06-04 03:25:34 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_2/imputation.pkl
2024-06-04 03:25:34 [INFO]: Round2 - Informer on BeijingAir: MAE=0.1977, MSE=0.1868, MRE=0.2988
2024-06-04 03:25:34 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-04 03:25:34 [INFO]: Using the given device: cuda:0
2024-06-04 03:25:34 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_3/20240604_T032534
2024-06-04 03:25:34 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_3/20240604_T032534/tensorboard
2024-06-04 03:25:34 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-04 03:25:40 [INFO]: Epoch 001 - training loss: 0.8580, validation loss: 0.3405
2024-06-04 03:25:46 [INFO]: Epoch 002 - training loss: 0.5506, validation loss: 0.2689
2024-06-04 03:25:51 [INFO]: Epoch 003 - training loss: 0.4790, validation loss: 0.2460
2024-06-04 03:25:56 [INFO]: Epoch 004 - training loss: 0.4464, validation loss: 0.2241
2024-06-04 03:26:01 [INFO]: Epoch 005 - training loss: 0.4187, validation loss: 0.2096
2024-06-04 03:26:06 [INFO]: Epoch 006 - training loss: 0.4089, validation loss: 0.2006
2024-06-04 03:26:11 [INFO]: Epoch 007 - training loss: 0.3909, validation loss: 0.1959
2024-06-04 03:26:16 [INFO]: Epoch 008 - training loss: 0.3856, validation loss: 0.1882
2024-06-04 03:26:21 [INFO]: Epoch 009 - training loss: 0.3725, validation loss: 0.1835
2024-06-04 03:26:26 [INFO]: Epoch 010 - training loss: 0.3705, validation loss: 0.1773
2024-06-04 03:26:31 [INFO]: Epoch 011 - training loss: 0.3658, validation loss: 0.1764
2024-06-04 03:26:35 [INFO]: Epoch 012 - training loss: 0.3544, validation loss: 0.1735
2024-06-04 03:26:41 [INFO]: Epoch 013 - training loss: 0.3572, validation loss: 0.1727
2024-06-04 03:26:46 [INFO]: Epoch 014 - training loss: 0.3547, validation loss: 0.1732
2024-06-04 03:26:51 [INFO]: Epoch 015 - training loss: 0.3466, validation loss: 0.1695
2024-06-04 03:26:55 [INFO]: Epoch 016 - training loss: 0.3469, validation loss: 0.1696
2024-06-04 03:27:00 [INFO]: Epoch 017 - training loss: 0.3406, validation loss: 0.1657
2024-06-04 03:27:05 [INFO]: Epoch 018 - training loss: 0.3414, validation loss: 0.1708
2024-06-04 03:27:10 [INFO]: Epoch 019 - training loss: 0.3388, validation loss: 0.1651
2024-06-04 03:27:15 [INFO]: Epoch 020 - training loss: 0.3328, validation loss: 0.1695
2024-06-04 03:27:20 [INFO]: Epoch 021 - training loss: 0.3384, validation loss: 0.1674
2024-06-04 03:27:25 [INFO]: Epoch 022 - training loss: 0.3267, validation loss: 0.1645
2024-06-04 03:27:30 [INFO]: Epoch 023 - training loss: 0.3239, validation loss: 0.1680
2024-06-04 03:27:34 [INFO]: Epoch 024 - training loss: 0.3213, validation loss: 0.1654
2024-06-04 03:27:39 [INFO]: Epoch 025 - training loss: 0.3193, validation loss: 0.1656
2024-06-04 03:27:44 [INFO]: Epoch 026 - training loss: 0.3152, validation loss: 0.1648
2024-06-04 03:27:49 [INFO]: Epoch 027 - training loss: 0.3147, validation loss: 0.1663
2024-06-04 03:27:54 [INFO]: Epoch 028 - training loss: 0.3122, validation loss: 0.1668
2024-06-04 03:27:59 [INFO]: Epoch 029 - training loss: 0.3151, validation loss: 0.1641
2024-06-04 03:28:04 [INFO]: Epoch 030 - training loss: 0.3088, validation loss: 0.1650
2024-06-04 03:28:09 [INFO]: Epoch 031 - training loss: 0.3075, validation loss: 0.1625
2024-06-04 03:28:14 [INFO]: Epoch 032 - training loss: 0.3039, validation loss: 0.1625
2024-06-04 03:28:19 [INFO]: Epoch 033 - training loss: 0.3026, validation loss: 0.1608
2024-06-04 03:28:24 [INFO]: Epoch 034 - training loss: 0.3018, validation loss: 0.1597
2024-06-04 03:28:28 [INFO]: Epoch 035 - training loss: 0.2990, validation loss: 0.1591
2024-06-04 03:28:33 [INFO]: Epoch 036 - training loss: 0.2970, validation loss: 0.1582
2024-06-04 03:28:38 [INFO]: Epoch 037 - training loss: 0.2947, validation loss: 0.1577
2024-06-04 03:28:43 [INFO]: Epoch 038 - training loss: 0.2942, validation loss: 0.1597
2024-06-04 03:28:48 [INFO]: Epoch 039 - training loss: 0.2916, validation loss: 0.1575
2024-06-04 03:28:53 [INFO]: Epoch 040 - training loss: 0.2905, validation loss: 0.1549
2024-06-04 03:28:58 [INFO]: Epoch 041 - training loss: 0.2893, validation loss: 0.1537
2024-06-04 03:29:03 [INFO]: Epoch 042 - training loss: 0.2837, validation loss: 0.1564
2024-06-04 03:29:08 [INFO]: Epoch 043 - training loss: 0.2838, validation loss: 0.1532
2024-06-04 03:29:13 [INFO]: Epoch 044 - training loss: 0.2892, validation loss: 0.1553
2024-06-04 03:29:18 [INFO]: Epoch 045 - training loss: 0.2849, validation loss: 0.1561
2024-06-04 03:29:22 [INFO]: Epoch 046 - training loss: 0.2842, validation loss: 0.1523
2024-06-04 03:29:27 [INFO]: Epoch 047 - training loss: 0.2812, validation loss: 0.1519
2024-06-04 03:29:32 [INFO]: Epoch 048 - training loss: 0.2759, validation loss: 0.1534
2024-06-04 03:29:37 [INFO]: Epoch 049 - training loss: 0.2755, validation loss: 0.1497
2024-06-04 03:29:42 [INFO]: Epoch 050 - training loss: 0.2759, validation loss: 0.1509
2024-06-04 03:29:47 [INFO]: Epoch 051 - training loss: 0.2761, validation loss: 0.1512
2024-06-04 03:29:52 [INFO]: Epoch 052 - training loss: 0.2786, validation loss: 0.1537
2024-06-04 03:29:57 [INFO]: Epoch 053 - training loss: 0.2801, validation loss: 0.1481
2024-06-04 03:30:02 [INFO]: Epoch 054 - training loss: 0.2720, validation loss: 0.1536
2024-06-04 03:30:07 [INFO]: Epoch 055 - training loss: 0.2708, validation loss: 0.1491
2024-06-04 03:30:12 [INFO]: Epoch 056 - training loss: 0.2698, validation loss: 0.1495
2024-06-04 03:30:17 [INFO]: Epoch 057 - training loss: 0.2681, validation loss: 0.1504
2024-06-04 03:30:22 [INFO]: Epoch 058 - training loss: 0.2664, validation loss: 0.1502
2024-06-04 03:30:27 [INFO]: Epoch 059 - training loss: 0.2630, validation loss: 0.1457
2024-06-04 03:30:32 [INFO]: Epoch 060 - training loss: 0.2629, validation loss: 0.1459
2024-06-04 03:30:37 [INFO]: Epoch 061 - training loss: 0.2617, validation loss: 0.1473
2024-06-04 03:30:42 [INFO]: Epoch 062 - training loss: 0.2607, validation loss: 0.1500
2024-06-04 03:30:46 [INFO]: Epoch 063 - training loss: 0.2634, validation loss: 0.1472
2024-06-04 03:30:52 [INFO]: Epoch 064 - training loss: 0.2656, validation loss: 0.1471
2024-06-04 03:30:56 [INFO]: Epoch 065 - training loss: 0.2641, validation loss: 0.1491
2024-06-04 03:31:01 [INFO]: Epoch 066 - training loss: 0.2673, validation loss: 0.1445
2024-06-04 03:31:06 [INFO]: Epoch 067 - training loss: 0.2603, validation loss: 0.1451
2024-06-04 03:31:11 [INFO]: Epoch 068 - training loss: 0.2581, validation loss: 0.1445
2024-06-04 03:31:16 [INFO]: Epoch 069 - training loss: 0.2625, validation loss: 0.1443
2024-06-04 03:31:20 [INFO]: Epoch 070 - training loss: 0.2542, validation loss: 0.1432
2024-06-04 03:31:25 [INFO]: Epoch 071 - training loss: 0.2524, validation loss: 0.1436
2024-06-04 03:31:30 [INFO]: Epoch 072 - training loss: 0.2516, validation loss: 0.1425
2024-06-04 03:31:35 [INFO]: Epoch 073 - training loss: 0.2565, validation loss: 0.1400
2024-06-04 03:31:40 [INFO]: Epoch 074 - training loss: 0.2517, validation loss: 0.1422
2024-06-04 03:31:44 [INFO]: Epoch 075 - training loss: 0.2523, validation loss: 0.1423
2024-06-04 03:31:49 [INFO]: Epoch 076 - training loss: 0.2526, validation loss: 0.1409
2024-06-04 03:31:54 [INFO]: Epoch 077 - training loss: 0.2520, validation loss: 0.1393
2024-06-04 03:31:58 [INFO]: Epoch 078 - training loss: 0.2516, validation loss: 0.1396
2024-06-04 03:32:01 [INFO]: Epoch 079 - training loss: 0.2529, validation loss: 0.1409
2024-06-04 03:32:04 [INFO]: Epoch 080 - training loss: 0.2490, validation loss: 0.1407
2024-06-04 03:32:07 [INFO]: Epoch 081 - training loss: 0.2482, validation loss: 0.1383
2024-06-04 03:32:10 [INFO]: Epoch 082 - training loss: 0.2439, validation loss: 0.1379
2024-06-04 03:32:13 [INFO]: Epoch 083 - training loss: 0.2458, validation loss: 0.1395
2024-06-04 03:32:17 [INFO]: Epoch 084 - training loss: 0.2451, validation loss: 0.1377
2024-06-04 03:32:19 [INFO]: Epoch 085 - training loss: 0.2467, validation loss: 0.1371
2024-06-04 03:32:23 [INFO]: Epoch 086 - training loss: 0.2466, validation loss: 0.1389
2024-06-04 03:32:26 [INFO]: Epoch 087 - training loss: 0.2461, validation loss: 0.1397
2024-06-04 03:32:29 [INFO]: Epoch 088 - training loss: 0.2455, validation loss: 0.1365
2024-06-04 03:32:32 [INFO]: Epoch 089 - training loss: 0.2437, validation loss: 0.1380
2024-06-04 03:32:35 [INFO]: Epoch 090 - training loss: 0.2452, validation loss: 0.1371
2024-06-04 03:32:38 [INFO]: Epoch 091 - training loss: 0.2440, validation loss: 0.1353
2024-06-04 03:32:41 [INFO]: Epoch 092 - training loss: 0.2391, validation loss: 0.1352
2024-06-04 03:32:44 [INFO]: Epoch 093 - training loss: 0.2398, validation loss: 0.1343
2024-06-04 03:32:48 [INFO]: Epoch 094 - training loss: 0.2411, validation loss: 0.1338
2024-06-04 03:32:50 [INFO]: Epoch 095 - training loss: 0.2408, validation loss: 0.1333
2024-06-04 03:32:53 [INFO]: Epoch 096 - training loss: 0.2372, validation loss: 0.1323
2024-06-04 03:32:56 [INFO]: Epoch 097 - training loss: 0.2374, validation loss: 0.1340
2024-06-04 03:33:00 [INFO]: Epoch 098 - training loss: 0.2373, validation loss: 0.1329
2024-06-04 03:33:03 [INFO]: Epoch 099 - training loss: 0.2395, validation loss: 0.1332
2024-06-04 03:33:06 [INFO]: Epoch 100 - training loss: 0.2401, validation loss: 0.1324
2024-06-04 03:33:06 [INFO]: Finished training. The best model is from epoch#96.
2024-06-04 03:33:06 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_3/20240604_T032534/Informer.pypots
2024-06-04 03:33:08 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_3/imputation.pkl
2024-06-04 03:33:08 [INFO]: Round3 - Informer on BeijingAir: MAE=0.1990, MSE=0.1903, MRE=0.3006
2024-06-04 03:33:08 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-04 03:33:08 [INFO]: Using the given device: cuda:0
2024-06-04 03:33:08 [INFO]: Model files will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_4/20240604_T033308
2024-06-04 03:33:08 [INFO]: Tensorboard file will be saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_4/20240604_T033308/tensorboard
2024-06-04 03:33:08 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-04 03:33:11 [INFO]: Epoch 001 - training loss: 0.8833, validation loss: 0.3353
2024-06-04 03:33:14 [INFO]: Epoch 002 - training loss: 0.5531, validation loss: 0.2748
2024-06-04 03:33:17 [INFO]: Epoch 003 - training loss: 0.4772, validation loss: 0.2425
2024-06-04 03:33:21 [INFO]: Epoch 004 - training loss: 0.4441, validation loss: 0.2275
2024-06-04 03:33:23 [INFO]: Epoch 005 - training loss: 0.4202, validation loss: 0.2159
2024-06-04 03:33:27 [INFO]: Epoch 006 - training loss: 0.4049, validation loss: 0.2044
2024-06-04 03:33:30 [INFO]: Epoch 007 - training loss: 0.3931, validation loss: 0.1956
2024-06-04 03:33:33 [INFO]: Epoch 008 - training loss: 0.3849, validation loss: 0.1897
2024-06-04 03:33:36 [INFO]: Epoch 009 - training loss: 0.3823, validation loss: 0.1801
2024-06-04 03:33:39 [INFO]: Epoch 010 - training loss: 0.3687, validation loss: 0.1783
2024-06-04 03:33:42 [INFO]: Epoch 011 - training loss: 0.3628, validation loss: 0.1772
2024-06-04 03:33:45 [INFO]: Epoch 012 - training loss: 0.3573, validation loss: 0.1742
2024-06-04 03:33:48 [INFO]: Epoch 013 - training loss: 0.3555, validation loss: 0.1750
2024-06-04 03:33:51 [INFO]: Epoch 014 - training loss: 0.3509, validation loss: 0.1732
2024-06-04 03:33:54 [INFO]: Epoch 015 - training loss: 0.3518, validation loss: 0.1716
2024-06-04 03:33:57 [INFO]: Epoch 016 - training loss: 0.3449, validation loss: 0.1725
2024-06-04 03:34:01 [INFO]: Epoch 017 - training loss: 0.3402, validation loss: 0.1690
2024-06-04 03:34:04 [INFO]: Epoch 018 - training loss: 0.3338, validation loss: 0.1716
2024-06-04 03:34:07 [INFO]: Epoch 019 - training loss: 0.3386, validation loss: 0.1697
2024-06-04 03:34:10 [INFO]: Epoch 020 - training loss: 0.3374, validation loss: 0.1698
2024-06-04 03:34:13 [INFO]: Epoch 021 - training loss: 0.3324, validation loss: 0.1706
2024-06-04 03:34:16 [INFO]: Epoch 022 - training loss: 0.3270, validation loss: 0.1734
2024-06-04 03:34:19 [INFO]: Epoch 023 - training loss: 0.3222, validation loss: 0.1686
2024-06-04 03:34:22 [INFO]: Epoch 024 - training loss: 0.3222, validation loss: 0.1708
2024-06-04 03:34:25 [INFO]: Epoch 025 - training loss: 0.3174, validation loss: 0.1683
2024-06-04 03:34:28 [INFO]: Epoch 026 - training loss: 0.3141, validation loss: 0.1677
2024-06-04 03:34:31 [INFO]: Epoch 027 - training loss: 0.3181, validation loss: 0.1658
2024-06-04 03:34:35 [INFO]: Epoch 028 - training loss: 0.3153, validation loss: 0.1641
2024-06-04 03:34:38 [INFO]: Epoch 029 - training loss: 0.3095, validation loss: 0.1649
2024-06-04 03:34:41 [INFO]: Epoch 030 - training loss: 0.3040, validation loss: 0.1643
2024-06-04 03:34:44 [INFO]: Epoch 031 - training loss: 0.3044, validation loss: 0.1629
2024-06-04 03:34:47 [INFO]: Epoch 032 - training loss: 0.3041, validation loss: 0.1641
2024-06-04 03:34:51 [INFO]: Epoch 033 - training loss: 0.3041, validation loss: 0.1650
2024-06-04 03:34:54 [INFO]: Epoch 034 - training loss: 0.3005, validation loss: 0.1651
2024-06-04 03:34:57 [INFO]: Epoch 035 - training loss: 0.3031, validation loss: 0.1620
2024-06-04 03:34:59 [INFO]: Epoch 036 - training loss: 0.3012, validation loss: 0.1622
2024-06-04 03:35:02 [INFO]: Epoch 037 - training loss: 0.2986, validation loss: 0.1590
2024-06-04 03:35:06 [INFO]: Epoch 038 - training loss: 0.2914, validation loss: 0.1603
2024-06-04 03:35:09 [INFO]: Epoch 039 - training loss: 0.2915, validation loss: 0.1590
2024-06-04 03:35:12 [INFO]: Epoch 040 - training loss: 0.2884, validation loss: 0.1563
2024-06-04 03:35:15 [INFO]: Epoch 041 - training loss: 0.2902, validation loss: 0.1565
2024-06-04 03:35:18 [INFO]: Epoch 042 - training loss: 0.2890, validation loss: 0.1573
2024-06-04 03:35:21 [INFO]: Epoch 043 - training loss: 0.2835, validation loss: 0.1563
2024-06-04 03:35:24 [INFO]: Epoch 044 - training loss: 0.2836, validation loss: 0.1563
2024-06-04 03:35:27 [INFO]: Epoch 045 - training loss: 0.2886, validation loss: 0.1541
2024-06-04 03:35:30 [INFO]: Epoch 046 - training loss: 0.2830, validation loss: 0.1549
2024-06-04 03:35:34 [INFO]: Epoch 047 - training loss: 0.2781, validation loss: 0.1515
2024-06-04 03:35:37 [INFO]: Epoch 048 - training loss: 0.2742, validation loss: 0.1536
2024-06-04 03:35:40 [INFO]: Epoch 049 - training loss: 0.2745, validation loss: 0.1510
2024-06-04 03:35:43 [INFO]: Epoch 050 - training loss: 0.2784, validation loss: 0.1526
2024-06-04 03:35:46 [INFO]: Epoch 051 - training loss: 0.2793, validation loss: 0.1518
2024-06-04 03:35:49 [INFO]: Epoch 052 - training loss: 0.2744, validation loss: 0.1503
2024-06-04 03:35:52 [INFO]: Epoch 053 - training loss: 0.2704, validation loss: 0.1514
2024-06-04 03:35:55 [INFO]: Epoch 054 - training loss: 0.2714, validation loss: 0.1502
2024-06-04 03:35:59 [INFO]: Epoch 055 - training loss: 0.2688, validation loss: 0.1508
2024-06-04 03:36:02 [INFO]: Epoch 056 - training loss: 0.2681, validation loss: 0.1482
2024-06-04 03:36:05 [INFO]: Epoch 057 - training loss: 0.2648, validation loss: 0.1491
2024-06-04 03:36:08 [INFO]: Epoch 058 - training loss: 0.2659, validation loss: 0.1471
2024-06-04 03:36:11 [INFO]: Epoch 059 - training loss: 0.2640, validation loss: 0.1477
2024-06-04 03:36:15 [INFO]: Epoch 060 - training loss: 0.2628, validation loss: 0.1477
2024-06-04 03:36:18 [INFO]: Epoch 061 - training loss: 0.2618, validation loss: 0.1474
2024-06-04 03:36:21 [INFO]: Epoch 062 - training loss: 0.2599, validation loss: 0.1464
2024-06-04 03:36:24 [INFO]: Epoch 063 - training loss: 0.2596, validation loss: 0.1451
2024-06-04 03:36:28 [INFO]: Epoch 064 - training loss: 0.2635, validation loss: 0.1462
2024-06-04 03:36:31 [INFO]: Epoch 065 - training loss: 0.2606, validation loss: 0.1441
2024-06-04 03:36:34 [INFO]: Epoch 066 - training loss: 0.2616, validation loss: 0.1470
2024-06-04 03:36:37 [INFO]: Epoch 067 - training loss: 0.2573, validation loss: 0.1450
2024-06-04 03:36:40 [INFO]: Epoch 068 - training loss: 0.2533, validation loss: 0.1445
2024-06-04 03:36:43 [INFO]: Epoch 069 - training loss: 0.2553, validation loss: 0.1452
2024-06-04 03:36:46 [INFO]: Epoch 070 - training loss: 0.2572, validation loss: 0.1423
2024-06-04 03:36:49 [INFO]: Epoch 071 - training loss: 0.2589, validation loss: 0.1427
2024-06-04 03:36:52 [INFO]: Epoch 072 - training loss: 0.2516, validation loss: 0.1405
2024-06-04 03:36:56 [INFO]: Epoch 073 - training loss: 0.2509, validation loss: 0.1414
2024-06-04 03:36:59 [INFO]: Epoch 074 - training loss: 0.2502, validation loss: 0.1392
2024-06-04 03:37:02 [INFO]: Epoch 075 - training loss: 0.2489, validation loss: 0.1412
2024-06-04 03:37:05 [INFO]: Epoch 076 - training loss: 0.2526, validation loss: 0.1404
2024-06-04 03:37:08 [INFO]: Epoch 077 - training loss: 0.2510, validation loss: 0.1411
2024-06-04 03:37:11 [INFO]: Epoch 078 - training loss: 0.2508, validation loss: 0.1408
2024-06-04 03:37:14 [INFO]: Epoch 079 - training loss: 0.2493, validation loss: 0.1381
2024-06-04 03:37:17 [INFO]: Epoch 080 - training loss: 0.2496, validation loss: 0.1371
2024-06-04 03:37:20 [INFO]: Epoch 081 - training loss: 0.2489, validation loss: 0.1370
2024-06-04 03:37:24 [INFO]: Epoch 082 - training loss: 0.2471, validation loss: 0.1382
2024-06-04 03:37:27 [INFO]: Epoch 083 - training loss: 0.2477, validation loss: 0.1388
2024-06-04 03:37:30 [INFO]: Epoch 084 - training loss: 0.2481, validation loss: 0.1372
2024-06-04 03:37:33 [INFO]: Epoch 085 - training loss: 0.2470, validation loss: 0.1358
2024-06-04 03:37:36 [INFO]: Epoch 086 - training loss: 0.2423, validation loss: 0.1364
2024-06-04 03:37:39 [INFO]: Epoch 087 - training loss: 0.2440, validation loss: 0.1344
2024-06-04 03:37:42 [INFO]: Epoch 088 - training loss: 0.2428, validation loss: 0.1369
2024-06-04 03:37:45 [INFO]: Epoch 089 - training loss: 0.2462, validation loss: 0.1351
2024-06-04 03:37:48 [INFO]: Epoch 090 - training loss: 0.2491, validation loss: 0.1386
2024-06-04 03:37:51 [INFO]: Epoch 091 - training loss: 0.2428, validation loss: 0.1361
2024-06-04 03:37:54 [INFO]: Epoch 092 - training loss: 0.2399, validation loss: 0.1340
2024-06-04 03:37:58 [INFO]: Epoch 093 - training loss: 0.2429, validation loss: 0.1331
2024-06-04 03:38:01 [INFO]: Epoch 094 - training loss: 0.2389, validation loss: 0.1341
2024-06-04 03:38:04 [INFO]: Epoch 095 - training loss: 0.2384, validation loss: 0.1325
2024-06-04 03:38:07 [INFO]: Epoch 096 - training loss: 0.2397, validation loss: 0.1354
2024-06-04 03:38:10 [INFO]: Epoch 097 - training loss: 0.2377, validation loss: 0.1328
2024-06-04 03:38:13 [INFO]: Epoch 098 - training loss: 0.2411, validation loss: 0.1353
2024-06-04 03:38:16 [INFO]: Epoch 099 - training loss: 0.2371, validation loss: 0.1325
2024-06-04 03:38:20 [INFO]: Epoch 100 - training loss: 0.2352, validation loss: 0.1321
2024-06-04 03:38:20 [INFO]: Finished training. The best model is from epoch#100.
2024-06-04 03:38:20 [INFO]: Saved the model to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_4/20240604_T033308/Informer.pypots
2024-06-04 03:38:21 [INFO]: Successfully saved to new_results_point_rate01/BeijingAir/Informer_BeijingAir/round_4/imputation.pkl
2024-06-04 03:38:21 [INFO]: Round4 - Informer on BeijingAir: MAE=0.1959, MSE=0.1904, MRE=0.2961
2024-06-04 03:38:21 [INFO]: Done! Final results:
Averaged Informer (6,706,308 params) on BeijingAir: MAE=0.1485 ± 0.0022972847893715583, MSE=0.1224 ± 0.001965918654191124, MRE=0.1975 ± 0.003055549570988068, average inference time=0.70