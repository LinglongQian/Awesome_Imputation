2024-06-02 19:42:31 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:42:31 [INFO]: Using the given device: cuda:0
2024-06-02 19:42:31 [INFO]: Model files will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_0/20240602_T194231
2024-06-02 19:42:31 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_0/20240602_T194231/tensorboard
2024-06-02 19:42:31 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=4, d_k=128
2024-06-02 19:42:31 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (4) * d_k (128)
2024-06-02 19:42:32 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 4,419,410
2024-06-02 19:43:13 [INFO]: Epoch 001 - training loss: 1.2521, validation loss: 3.2923
2024-06-02 19:44:01 [INFO]: Epoch 002 - training loss: 0.8435, validation loss: 2.9741
2024-06-02 19:44:49 [INFO]: Epoch 003 - training loss: 0.7051, validation loss: 2.8132
2024-06-02 19:45:36 [INFO]: Epoch 004 - training loss: 0.6510, validation loss: 2.6565
2024-06-02 19:46:24 [INFO]: Epoch 005 - training loss: 0.6201, validation loss: 2.5357
2024-06-02 19:47:11 [INFO]: Epoch 006 - training loss: 0.5948, validation loss: 2.4328
2024-06-02 19:47:59 [INFO]: Epoch 007 - training loss: 0.5751, validation loss: 2.3490
2024-06-02 19:48:47 [INFO]: Epoch 008 - training loss: 0.5575, validation loss: 2.2691
2024-06-02 19:49:35 [INFO]: Epoch 009 - training loss: 0.5441, validation loss: 2.2043
2024-06-02 19:50:22 [INFO]: Epoch 010 - training loss: 0.5319, validation loss: 2.1454
2024-06-02 19:51:02 [INFO]: Epoch 011 - training loss: 0.5208, validation loss: 2.0870
2024-06-02 19:51:42 [INFO]: Epoch 012 - training loss: 0.5113, validation loss: 2.0419
2024-06-02 19:52:22 [INFO]: Epoch 013 - training loss: 0.5039, validation loss: 1.9895
2024-06-02 19:53:01 [INFO]: Epoch 014 - training loss: 0.4951, validation loss: 1.9421
2024-06-02 19:53:41 [INFO]: Epoch 015 - training loss: 0.4889, validation loss: 1.9068
2024-06-02 19:54:21 [INFO]: Epoch 016 - training loss: 0.4827, validation loss: 1.8632
2024-06-02 19:55:00 [INFO]: Epoch 017 - training loss: 0.4768, validation loss: 1.8301
2024-06-02 19:55:40 [INFO]: Epoch 018 - training loss: 0.4727, validation loss: 1.7862
2024-06-02 19:56:20 [INFO]: Epoch 019 - training loss: 0.4664, validation loss: 1.7573
2024-06-02 19:57:00 [INFO]: Epoch 020 - training loss: 0.4619, validation loss: 1.7213
2024-06-02 19:57:39 [INFO]: Epoch 021 - training loss: 0.4570, validation loss: 1.6936
2024-06-02 19:58:19 [INFO]: Epoch 022 - training loss: 0.4526, validation loss: 1.6614
2024-06-02 19:58:58 [INFO]: Epoch 023 - training loss: 0.4495, validation loss: 1.6366
2024-06-02 19:59:38 [INFO]: Epoch 024 - training loss: 0.4454, validation loss: 1.6104
2024-06-02 20:00:18 [INFO]: Epoch 025 - training loss: 0.4416, validation loss: 1.5834
2024-06-02 20:00:58 [INFO]: Epoch 026 - training loss: 0.4381, validation loss: 1.5556
2024-06-02 20:01:37 [INFO]: Epoch 027 - training loss: 0.4353, validation loss: 1.5344
2024-06-02 20:02:17 [INFO]: Epoch 028 - training loss: 0.4324, validation loss: 1.5176
2024-06-02 20:02:57 [INFO]: Epoch 029 - training loss: 0.4308, validation loss: 1.4967
2024-06-02 20:03:37 [INFO]: Epoch 030 - training loss: 0.4276, validation loss: 1.4666
2024-06-02 20:04:16 [INFO]: Epoch 031 - training loss: 0.4252, validation loss: 1.4500
2024-06-02 20:04:56 [INFO]: Epoch 032 - training loss: 0.4226, validation loss: 1.4235
2024-06-02 20:05:35 [INFO]: Epoch 033 - training loss: 0.4202, validation loss: 1.4116
2024-06-02 20:06:15 [INFO]: Epoch 034 - training loss: 0.4178, validation loss: 1.3896
2024-06-02 20:06:55 [INFO]: Epoch 035 - training loss: 0.4154, validation loss: 1.3706
2024-06-02 20:07:34 [INFO]: Epoch 036 - training loss: 0.4142, validation loss: 1.3556
2024-06-02 20:08:14 [INFO]: Epoch 037 - training loss: 0.4120, validation loss: 1.3335
2024-06-02 20:08:54 [INFO]: Epoch 038 - training loss: 0.4103, validation loss: 1.3197
2024-06-02 20:09:34 [INFO]: Epoch 039 - training loss: 0.4085, validation loss: 1.3073
2024-06-02 20:10:13 [INFO]: Epoch 040 - training loss: 0.4068, validation loss: 1.2920
2024-06-02 20:10:53 [INFO]: Epoch 041 - training loss: 0.4063, validation loss: 1.2818
2024-06-02 20:11:33 [INFO]: Epoch 042 - training loss: 0.4050, validation loss: 1.2643
2024-06-02 20:12:12 [INFO]: Epoch 043 - training loss: 0.4025, validation loss: 1.2627
2024-06-02 20:12:52 [INFO]: Epoch 044 - training loss: 0.4017, validation loss: 1.2394
2024-06-02 20:13:32 [INFO]: Epoch 045 - training loss: 0.4002, validation loss: 1.2247
2024-06-02 20:14:11 [INFO]: Epoch 046 - training loss: 0.3989, validation loss: 1.2135
2024-06-02 20:14:51 [INFO]: Epoch 047 - training loss: 0.3977, validation loss: 1.1969
2024-06-02 20:15:31 [INFO]: Epoch 048 - training loss: 0.3968, validation loss: 1.1868
2024-06-02 20:16:11 [INFO]: Epoch 049 - training loss: 0.3951, validation loss: 1.1772
2024-06-02 20:16:50 [INFO]: Epoch 050 - training loss: 0.3944, validation loss: 1.1736
2024-06-02 20:17:30 [INFO]: Epoch 051 - training loss: 0.3936, validation loss: 1.1562
2024-06-02 20:18:10 [INFO]: Epoch 052 - training loss: 0.3931, validation loss: 1.1463
2024-06-02 20:18:50 [INFO]: Epoch 053 - training loss: 0.3929, validation loss: 1.1469
2024-06-02 20:19:29 [INFO]: Epoch 054 - training loss: 0.3912, validation loss: 1.1356
2024-06-02 20:20:09 [INFO]: Epoch 055 - training loss: 0.3912, validation loss: 1.1327
2024-06-02 20:20:49 [INFO]: Epoch 056 - training loss: 0.3896, validation loss: 1.1161
2024-06-02 20:21:29 [INFO]: Epoch 057 - training loss: 0.3883, validation loss: 1.1247
2024-06-02 20:22:08 [INFO]: Epoch 058 - training loss: 0.3879, validation loss: 1.1102
2024-06-02 20:22:48 [INFO]: Epoch 059 - training loss: 0.3880, validation loss: 1.1032
2024-06-02 20:23:28 [INFO]: Epoch 060 - training loss: 0.3860, validation loss: 1.0978
2024-06-02 20:24:08 [INFO]: Epoch 061 - training loss: 0.3853, validation loss: 1.0802
2024-06-02 20:24:48 [INFO]: Epoch 062 - training loss: 0.3849, validation loss: 1.0808
2024-06-02 20:25:27 [INFO]: Epoch 063 - training loss: 0.3841, validation loss: 1.0769
2024-06-02 20:26:07 [INFO]: Epoch 064 - training loss: 0.3837, validation loss: 1.0716
2024-06-02 20:26:47 [INFO]: Epoch 065 - training loss: 0.3829, validation loss: 1.0644
2024-06-02 20:27:26 [INFO]: Epoch 066 - training loss: 0.3823, validation loss: 1.0626
2024-06-02 20:28:06 [INFO]: Epoch 067 - training loss: 0.3827, validation loss: 1.0682
2024-06-02 20:28:46 [INFO]: Epoch 068 - training loss: 0.3818, validation loss: 1.0537
2024-06-02 20:29:26 [INFO]: Epoch 069 - training loss: 0.3808, validation loss: 1.0523
2024-06-02 20:30:06 [INFO]: Epoch 070 - training loss: 0.3803, validation loss: 1.0524
2024-06-02 20:30:46 [INFO]: Epoch 071 - training loss: 0.3795, validation loss: 1.0411
2024-06-02 20:31:25 [INFO]: Epoch 072 - training loss: 0.3792, validation loss: 1.0405
2024-06-02 20:32:05 [INFO]: Epoch 073 - training loss: 0.3790, validation loss: 1.0310
2024-06-02 20:32:45 [INFO]: Epoch 074 - training loss: 0.3788, validation loss: 1.0342
2024-06-02 20:33:24 [INFO]: Epoch 075 - training loss: 0.3782, validation loss: 1.0239
2024-06-02 20:34:04 [INFO]: Epoch 076 - training loss: 0.3769, validation loss: 1.0182
2024-06-02 20:34:44 [INFO]: Epoch 077 - training loss: 0.3770, validation loss: 1.0211
2024-06-02 20:35:24 [INFO]: Epoch 078 - training loss: 0.3763, validation loss: 1.0254
2024-06-02 20:36:03 [INFO]: Epoch 079 - training loss: 0.3757, validation loss: 1.0153
2024-06-02 20:36:43 [INFO]: Epoch 080 - training loss: 0.3758, validation loss: 1.0107
2024-06-02 20:37:21 [INFO]: Epoch 081 - training loss: 0.3749, validation loss: 1.0172
2024-06-02 20:37:55 [INFO]: Epoch 082 - training loss: 0.3749, validation loss: 1.0135
2024-06-02 20:38:35 [INFO]: Epoch 083 - training loss: 0.3744, validation loss: 1.0086
2024-06-02 20:39:14 [INFO]: Epoch 084 - training loss: 0.3739, validation loss: 0.9980
2024-06-02 20:39:54 [INFO]: Epoch 085 - training loss: 0.3741, validation loss: 1.0116
2024-06-02 20:40:34 [INFO]: Epoch 086 - training loss: 0.3739, validation loss: 0.9901
2024-06-02 20:41:14 [INFO]: Epoch 087 - training loss: 0.3735, validation loss: 0.9995
2024-06-02 20:41:53 [INFO]: Epoch 088 - training loss: 0.3733, validation loss: 0.9956
2024-06-02 20:42:33 [INFO]: Epoch 089 - training loss: 0.3720, validation loss: 0.9892
2024-06-02 20:43:12 [INFO]: Epoch 090 - training loss: 0.3715, validation loss: 0.9780
2024-06-02 20:43:52 [INFO]: Epoch 091 - training loss: 0.3718, validation loss: 0.9807
2024-06-02 20:44:31 [INFO]: Epoch 092 - training loss: 0.3716, validation loss: 0.9871
2024-06-02 20:45:11 [INFO]: Epoch 093 - training loss: 0.3713, validation loss: 0.9838
2024-06-02 20:45:51 [INFO]: Epoch 094 - training loss: 0.3710, validation loss: 0.9753
2024-06-02 20:46:31 [INFO]: Epoch 095 - training loss: 0.3708, validation loss: 0.9866
2024-06-02 20:47:10 [INFO]: Epoch 096 - training loss: 0.3702, validation loss: 0.9786
2024-06-02 20:47:50 [INFO]: Epoch 097 - training loss: 0.3698, validation loss: 0.9640
2024-06-02 20:48:30 [INFO]: Epoch 098 - training loss: 0.3697, validation loss: 0.9755
2024-06-02 20:49:09 [INFO]: Epoch 099 - training loss: 0.3693, validation loss: 0.9757
2024-06-02 20:49:49 [INFO]: Epoch 100 - training loss: 0.3691, validation loss: 0.9777
2024-06-02 20:49:49 [INFO]: Finished training. The best model is from epoch#97.
2024-06-02 20:49:49 [INFO]: Saved the model to results_point_rate01/Electricity/PatchTST_Electricity/round_0/20240602_T194231/PatchTST.pypots
2024-06-02 20:49:54 [INFO]: Successfully saved to results_point_rate01/Electricity/PatchTST_Electricity/round_0/imputation.pkl
2024-06-02 20:49:54 [INFO]: Round0 - PatchTST on Electricity: MAE=0.6177, MSE=0.7583, MRE=0.3304
2024-06-02 20:49:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 20:49:54 [INFO]: Using the given device: cuda:0
2024-06-02 20:49:54 [INFO]: Model files will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_1/20240602_T204954
2024-06-02 20:49:54 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_1/20240602_T204954/tensorboard
2024-06-02 20:49:54 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=4, d_k=128
2024-06-02 20:49:54 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (4) * d_k (128)
2024-06-02 20:49:54 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 4,419,410
2024-06-02 20:50:34 [INFO]: Epoch 001 - training loss: 1.3016, validation loss: 3.5153
2024-06-02 20:51:13 [INFO]: Epoch 002 - training loss: 0.9353, validation loss: 3.2331
2024-06-02 20:51:53 [INFO]: Epoch 003 - training loss: 0.7568, validation loss: 2.9754
2024-06-02 20:52:33 [INFO]: Epoch 004 - training loss: 0.6774, validation loss: 2.7702
2024-06-02 20:53:12 [INFO]: Epoch 005 - training loss: 0.6341, validation loss: 2.6376
2024-06-02 20:53:52 [INFO]: Epoch 006 - training loss: 0.6028, validation loss: 2.5103
2024-06-02 20:54:32 [INFO]: Epoch 007 - training loss: 0.5806, validation loss: 2.4099
2024-06-02 20:55:11 [INFO]: Epoch 008 - training loss: 0.5629, validation loss: 2.3266
2024-06-02 20:55:51 [INFO]: Epoch 009 - training loss: 0.5482, validation loss: 2.2617
2024-06-02 20:56:31 [INFO]: Epoch 010 - training loss: 0.5383, validation loss: 2.1844
2024-06-02 20:57:10 [INFO]: Epoch 011 - training loss: 0.5263, validation loss: 2.1274
2024-06-02 20:57:50 [INFO]: Epoch 012 - training loss: 0.5159, validation loss: 2.0891
2024-06-02 20:58:29 [INFO]: Epoch 013 - training loss: 0.5069, validation loss: 2.0439
2024-06-02 20:59:09 [INFO]: Epoch 014 - training loss: 0.4995, validation loss: 1.9996
2024-06-02 20:59:49 [INFO]: Epoch 015 - training loss: 0.4923, validation loss: 1.9639
2024-06-02 21:00:28 [INFO]: Epoch 016 - training loss: 0.4862, validation loss: 1.9263
2024-06-02 21:01:08 [INFO]: Epoch 017 - training loss: 0.4795, validation loss: 1.8845
2024-06-02 21:01:47 [INFO]: Epoch 018 - training loss: 0.4738, validation loss: 1.8469
2024-06-02 21:02:27 [INFO]: Epoch 019 - training loss: 0.4680, validation loss: 1.8082
2024-06-02 21:03:06 [INFO]: Epoch 020 - training loss: 0.4644, validation loss: 1.7758
2024-06-02 21:03:46 [INFO]: Epoch 021 - training loss: 0.4594, validation loss: 1.7494
2024-06-02 21:04:25 [INFO]: Epoch 022 - training loss: 0.4540, validation loss: 1.7218
2024-06-02 21:05:05 [INFO]: Epoch 023 - training loss: 0.4498, validation loss: 1.6877
2024-06-02 21:05:45 [INFO]: Epoch 024 - training loss: 0.4467, validation loss: 1.6597
2024-06-02 21:06:24 [INFO]: Epoch 025 - training loss: 0.4439, validation loss: 1.6388
2024-06-02 21:07:04 [INFO]: Epoch 026 - training loss: 0.4399, validation loss: 1.6081
2024-06-02 21:07:43 [INFO]: Epoch 027 - training loss: 0.4364, validation loss: 1.5837
2024-06-02 21:08:23 [INFO]: Epoch 028 - training loss: 0.4330, validation loss: 1.5602
2024-06-02 21:09:03 [INFO]: Epoch 029 - training loss: 0.4301, validation loss: 1.5365
2024-06-02 21:09:43 [INFO]: Epoch 030 - training loss: 0.4277, validation loss: 1.5129
2024-06-02 21:10:23 [INFO]: Epoch 031 - training loss: 0.4253, validation loss: 1.4927
2024-06-02 21:11:02 [INFO]: Epoch 032 - training loss: 0.4228, validation loss: 1.4693
2024-06-02 21:11:42 [INFO]: Epoch 033 - training loss: 0.4210, validation loss: 1.4539
2024-06-02 21:12:22 [INFO]: Epoch 034 - training loss: 0.4188, validation loss: 1.4380
2024-06-02 21:13:01 [INFO]: Epoch 035 - training loss: 0.4165, validation loss: 1.4121
2024-06-02 21:13:41 [INFO]: Epoch 036 - training loss: 0.4156, validation loss: 1.3946
2024-06-02 21:14:21 [INFO]: Epoch 037 - training loss: 0.4127, validation loss: 1.3755
2024-06-02 21:15:00 [INFO]: Epoch 038 - training loss: 0.4113, validation loss: 1.3567
2024-06-02 21:15:40 [INFO]: Epoch 039 - training loss: 0.4100, validation loss: 1.3370
2024-06-02 21:16:20 [INFO]: Epoch 040 - training loss: 0.4078, validation loss: 1.3153
2024-06-02 21:16:59 [INFO]: Epoch 041 - training loss: 0.4060, validation loss: 1.3061
2024-06-02 21:17:39 [INFO]: Epoch 042 - training loss: 0.4045, validation loss: 1.2896
2024-06-02 21:18:18 [INFO]: Epoch 043 - training loss: 0.4034, validation loss: 1.2688
2024-06-02 21:18:58 [INFO]: Epoch 044 - training loss: 0.4048, validation loss: 1.2713
2024-06-02 21:19:37 [INFO]: Epoch 045 - training loss: 0.4006, validation loss: 1.2538
2024-06-02 21:20:17 [INFO]: Epoch 046 - training loss: 0.3997, validation loss: 1.2284
2024-06-02 21:20:57 [INFO]: Epoch 047 - training loss: 0.3980, validation loss: 1.2218
2024-06-02 21:21:36 [INFO]: Epoch 048 - training loss: 0.3974, validation loss: 1.2083
2024-06-02 21:22:16 [INFO]: Epoch 049 - training loss: 0.3963, validation loss: 1.1964
2024-06-02 21:22:56 [INFO]: Epoch 050 - training loss: 0.3950, validation loss: 1.1855
2024-06-02 21:23:35 [INFO]: Epoch 051 - training loss: 0.3944, validation loss: 1.1774
2024-06-02 21:24:15 [INFO]: Epoch 052 - training loss: 0.3930, validation loss: 1.1624
2024-06-02 21:24:54 [INFO]: Epoch 053 - training loss: 0.3922, validation loss: 1.1542
2024-06-02 21:25:34 [INFO]: Epoch 054 - training loss: 0.3914, validation loss: 1.1456
2024-06-02 21:26:13 [INFO]: Epoch 055 - training loss: 0.3901, validation loss: 1.1383
2024-06-02 21:26:53 [INFO]: Epoch 056 - training loss: 0.3896, validation loss: 1.1290
2024-06-02 21:27:33 [INFO]: Epoch 057 - training loss: 0.3889, validation loss: 1.1168
2024-06-02 21:28:12 [INFO]: Epoch 058 - training loss: 0.3885, validation loss: 1.1120
2024-06-02 21:28:52 [INFO]: Epoch 059 - training loss: 0.3884, validation loss: 1.1125
2024-06-02 21:29:31 [INFO]: Epoch 060 - training loss: 0.3873, validation loss: 1.0979
2024-06-02 21:30:11 [INFO]: Epoch 061 - training loss: 0.3859, validation loss: 1.0886
2024-06-02 21:30:51 [INFO]: Epoch 062 - training loss: 0.3855, validation loss: 1.0863
2024-06-02 21:31:23 [INFO]: Epoch 063 - training loss: 0.3849, validation loss: 1.0765
2024-06-02 21:32:03 [INFO]: Epoch 064 - training loss: 0.3844, validation loss: 1.0650
2024-06-02 21:32:42 [INFO]: Epoch 065 - training loss: 0.3842, validation loss: 1.0678
2024-06-02 21:33:22 [INFO]: Epoch 066 - training loss: 0.3837, validation loss: 1.0660
2024-06-02 21:34:01 [INFO]: Epoch 067 - training loss: 0.3827, validation loss: 1.0546
2024-06-02 21:34:41 [INFO]: Epoch 068 - training loss: 0.3820, validation loss: 1.0442
2024-06-02 21:35:21 [INFO]: Epoch 069 - training loss: 0.3822, validation loss: 1.0508
2024-06-02 21:36:00 [INFO]: Epoch 070 - training loss: 0.3812, validation loss: 1.0390
2024-06-02 21:36:40 [INFO]: Epoch 071 - training loss: 0.3803, validation loss: 1.0373
2024-06-02 21:37:19 [INFO]: Epoch 072 - training loss: 0.3797, validation loss: 1.0346
2024-06-02 21:37:59 [INFO]: Epoch 073 - training loss: 0.3797, validation loss: 1.0368
2024-06-02 21:38:38 [INFO]: Epoch 074 - training loss: 0.3785, validation loss: 1.0174
2024-06-02 21:39:18 [INFO]: Epoch 075 - training loss: 0.3783, validation loss: 1.0170
2024-06-02 21:39:57 [INFO]: Epoch 076 - training loss: 0.3779, validation loss: 1.0180
2024-06-02 21:40:37 [INFO]: Epoch 077 - training loss: 0.3773, validation loss: 1.0075
2024-06-02 21:41:17 [INFO]: Epoch 078 - training loss: 0.3770, validation loss: 1.0036
2024-06-02 21:41:56 [INFO]: Epoch 079 - training loss: 0.3771, validation loss: 1.0046
2024-06-02 21:42:36 [INFO]: Epoch 080 - training loss: 0.3765, validation loss: 0.9936
2024-06-02 21:43:15 [INFO]: Epoch 081 - training loss: 0.3762, validation loss: 0.9938
2024-06-02 21:43:55 [INFO]: Epoch 082 - training loss: 0.3758, validation loss: 0.9919
2024-06-02 21:44:35 [INFO]: Epoch 083 - training loss: 0.3752, validation loss: 0.9762
2024-06-02 21:45:14 [INFO]: Epoch 084 - training loss: 0.3748, validation loss: 0.9824
2024-06-02 21:45:54 [INFO]: Epoch 085 - training loss: 0.3745, validation loss: 0.9816
2024-06-02 21:46:34 [INFO]: Epoch 086 - training loss: 0.3739, validation loss: 0.9756
2024-06-02 21:47:13 [INFO]: Epoch 087 - training loss: 0.3737, validation loss: 0.9806
2024-06-02 21:47:53 [INFO]: Epoch 088 - training loss: 0.3730, validation loss: 0.9763
2024-06-02 21:48:33 [INFO]: Epoch 089 - training loss: 0.3729, validation loss: 0.9689
2024-06-02 21:49:12 [INFO]: Epoch 090 - training loss: 0.3723, validation loss: 0.9652
2024-06-02 21:49:52 [INFO]: Epoch 091 - training loss: 0.3724, validation loss: 0.9632
2024-06-02 21:50:32 [INFO]: Epoch 092 - training loss: 0.3718, validation loss: 0.9633
2024-06-02 21:51:11 [INFO]: Epoch 093 - training loss: 0.3721, validation loss: 0.9658
2024-06-02 21:51:51 [INFO]: Epoch 094 - training loss: 0.3712, validation loss: 0.9505
2024-06-02 21:52:31 [INFO]: Epoch 095 - training loss: 0.3712, validation loss: 0.9537
2024-06-02 21:53:10 [INFO]: Epoch 096 - training loss: 0.3709, validation loss: 0.9545
2024-06-02 21:53:50 [INFO]: Epoch 097 - training loss: 0.3703, validation loss: 0.9478
2024-06-02 21:54:29 [INFO]: Epoch 098 - training loss: 0.3702, validation loss: 0.9416
2024-06-02 21:55:09 [INFO]: Epoch 099 - training loss: 0.3700, validation loss: 0.9398
2024-06-02 21:55:48 [INFO]: Epoch 100 - training loss: 0.3695, validation loss: 0.9436
2024-06-02 21:55:48 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 21:55:49 [INFO]: Saved the model to results_point_rate01/Electricity/PatchTST_Electricity/round_1/20240602_T204954/PatchTST.pypots
2024-06-02 21:55:53 [INFO]: Successfully saved to results_point_rate01/Electricity/PatchTST_Electricity/round_1/imputation.pkl
2024-06-02 21:55:53 [INFO]: Round1 - PatchTST on Electricity: MAE=0.5566, MSE=0.5934, MRE=0.2977
2024-06-02 21:55:53 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 21:55:53 [INFO]: Using the given device: cuda:0
2024-06-02 21:55:53 [INFO]: Model files will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_2/20240602_T215553
2024-06-02 21:55:53 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_2/20240602_T215553/tensorboard
2024-06-02 21:55:53 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=4, d_k=128
2024-06-02 21:55:53 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (4) * d_k (128)
2024-06-02 21:55:53 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 4,419,410
2024-06-02 21:56:33 [INFO]: Epoch 001 - training loss: 1.2620, validation loss: 3.3625
2024-06-02 21:57:12 [INFO]: Epoch 002 - training loss: 0.8647, validation loss: 3.0240
2024-06-02 21:57:52 [INFO]: Epoch 003 - training loss: 0.7108, validation loss: 2.8299
2024-06-02 21:58:32 [INFO]: Epoch 004 - training loss: 0.6534, validation loss: 2.6866
2024-06-02 21:59:11 [INFO]: Epoch 005 - training loss: 0.6208, validation loss: 2.5700
2024-06-02 21:59:51 [INFO]: Epoch 006 - training loss: 0.5938, validation loss: 2.4635
2024-06-02 22:00:31 [INFO]: Epoch 007 - training loss: 0.5733, validation loss: 2.3844
2024-06-02 22:01:10 [INFO]: Epoch 008 - training loss: 0.5562, validation loss: 2.2967
2024-06-02 22:01:50 [INFO]: Epoch 009 - training loss: 0.5402, validation loss: 2.2279
2024-06-02 22:02:29 [INFO]: Epoch 010 - training loss: 0.5279, validation loss: 2.1721
2024-06-02 22:03:09 [INFO]: Epoch 011 - training loss: 0.5179, validation loss: 2.1016
2024-06-02 22:03:49 [INFO]: Epoch 012 - training loss: 0.5084, validation loss: 2.0533
2024-06-02 22:04:28 [INFO]: Epoch 013 - training loss: 0.4993, validation loss: 2.0081
2024-06-02 22:05:08 [INFO]: Epoch 014 - training loss: 0.4926, validation loss: 1.9542
2024-06-02 22:05:48 [INFO]: Epoch 015 - training loss: 0.4863, validation loss: 1.9143
2024-06-02 22:06:27 [INFO]: Epoch 016 - training loss: 0.4786, validation loss: 1.8702
2024-06-02 22:07:07 [INFO]: Epoch 017 - training loss: 0.4732, validation loss: 1.8339
2024-06-02 22:07:46 [INFO]: Epoch 018 - training loss: 0.4682, validation loss: 1.8019
2024-06-02 22:08:26 [INFO]: Epoch 019 - training loss: 0.4628, validation loss: 1.7625
2024-06-02 22:09:06 [INFO]: Epoch 020 - training loss: 0.4581, validation loss: 1.7150
2024-06-02 22:09:45 [INFO]: Epoch 021 - training loss: 0.4539, validation loss: 1.6863
2024-06-02 22:10:25 [INFO]: Epoch 022 - training loss: 0.4499, validation loss: 1.6513
2024-06-02 22:11:05 [INFO]: Epoch 023 - training loss: 0.4458, validation loss: 1.6251
2024-06-02 22:11:45 [INFO]: Epoch 024 - training loss: 0.4421, validation loss: 1.5907
2024-06-02 22:12:24 [INFO]: Epoch 025 - training loss: 0.4382, validation loss: 1.5598
2024-06-02 22:13:04 [INFO]: Epoch 026 - training loss: 0.4366, validation loss: 1.5388
2024-06-02 22:13:44 [INFO]: Epoch 027 - training loss: 0.4334, validation loss: 1.5131
2024-06-02 22:14:23 [INFO]: Epoch 028 - training loss: 0.4289, validation loss: 1.4816
2024-06-02 22:15:03 [INFO]: Epoch 029 - training loss: 0.4263, validation loss: 1.4587
2024-06-02 22:15:42 [INFO]: Epoch 030 - training loss: 0.4241, validation loss: 1.4317
2024-06-02 22:16:22 [INFO]: Epoch 031 - training loss: 0.4217, validation loss: 1.4043
2024-06-02 22:17:01 [INFO]: Epoch 032 - training loss: 0.4196, validation loss: 1.3951
2024-06-02 22:17:41 [INFO]: Epoch 033 - training loss: 0.4175, validation loss: 1.3635
2024-06-02 22:18:21 [INFO]: Epoch 034 - training loss: 0.4150, validation loss: 1.3452
2024-06-02 22:19:00 [INFO]: Epoch 035 - training loss: 0.4132, validation loss: 1.3254
2024-06-02 22:19:40 [INFO]: Epoch 036 - training loss: 0.4115, validation loss: 1.3055
2024-06-02 22:20:19 [INFO]: Epoch 037 - training loss: 0.4099, validation loss: 1.2817
2024-06-02 22:20:59 [INFO]: Epoch 038 - training loss: 0.4084, validation loss: 1.2682
2024-06-02 22:21:39 [INFO]: Epoch 039 - training loss: 0.4061, validation loss: 1.2412
2024-06-02 22:22:18 [INFO]: Epoch 040 - training loss: 0.4051, validation loss: 1.2256
2024-06-02 22:22:58 [INFO]: Epoch 041 - training loss: 0.4035, validation loss: 1.2093
2024-06-02 22:23:38 [INFO]: Epoch 042 - training loss: 0.4016, validation loss: 1.1934
2024-06-02 22:24:17 [INFO]: Epoch 043 - training loss: 0.4014, validation loss: 1.1783
2024-06-02 22:24:55 [INFO]: Epoch 044 - training loss: 0.3993, validation loss: 1.1654
2024-06-02 22:25:29 [INFO]: Epoch 045 - training loss: 0.3985, validation loss: 1.1478
2024-06-02 22:26:09 [INFO]: Epoch 046 - training loss: 0.3976, validation loss: 1.1363
2024-06-02 22:26:49 [INFO]: Epoch 047 - training loss: 0.3965, validation loss: 1.1275
2024-06-02 22:27:28 [INFO]: Epoch 048 - training loss: 0.3950, validation loss: 1.1159
2024-06-02 22:28:08 [INFO]: Epoch 049 - training loss: 0.3943, validation loss: 1.0973
2024-06-02 22:28:48 [INFO]: Epoch 050 - training loss: 0.3932, validation loss: 1.0862
2024-06-02 22:29:27 [INFO]: Epoch 051 - training loss: 0.3924, validation loss: 1.0702
2024-06-02 22:30:07 [INFO]: Epoch 052 - training loss: 0.3923, validation loss: 1.0622
2024-06-02 22:30:46 [INFO]: Epoch 053 - training loss: 0.3912, validation loss: 1.0608
2024-06-02 22:31:26 [INFO]: Epoch 054 - training loss: 0.3894, validation loss: 1.0453
2024-06-02 22:32:06 [INFO]: Epoch 055 - training loss: 0.3886, validation loss: 1.0332
2024-06-02 22:32:46 [INFO]: Epoch 056 - training loss: 0.3882, validation loss: 1.0333
2024-06-02 22:33:25 [INFO]: Epoch 057 - training loss: 0.3875, validation loss: 1.0141
2024-06-02 22:34:05 [INFO]: Epoch 058 - training loss: 0.3862, validation loss: 1.0069
2024-06-02 22:34:45 [INFO]: Epoch 059 - training loss: 0.3863, validation loss: 0.9928
2024-06-02 22:35:24 [INFO]: Epoch 060 - training loss: 0.3861, validation loss: 0.9937
2024-06-02 22:36:04 [INFO]: Epoch 061 - training loss: 0.3847, validation loss: 0.9925
2024-06-02 22:36:44 [INFO]: Epoch 062 - training loss: 0.3847, validation loss: 0.9897
2024-06-02 22:37:24 [INFO]: Epoch 063 - training loss: 0.3833, validation loss: 0.9734
2024-06-02 22:38:04 [INFO]: Epoch 064 - training loss: 0.3832, validation loss: 0.9685
2024-06-02 22:38:43 [INFO]: Epoch 065 - training loss: 0.3827, validation loss: 0.9584
2024-06-02 22:39:23 [INFO]: Epoch 066 - training loss: 0.3826, validation loss: 0.9569
2024-06-02 22:40:03 [INFO]: Epoch 067 - training loss: 0.3821, validation loss: 0.9424
2024-06-02 22:40:42 [INFO]: Epoch 068 - training loss: 0.3803, validation loss: 0.9385
2024-06-02 22:41:21 [INFO]: Epoch 069 - training loss: 0.3800, validation loss: 0.9381
2024-06-02 22:42:01 [INFO]: Epoch 070 - training loss: 0.3791, validation loss: 0.9410
2024-06-02 22:42:41 [INFO]: Epoch 071 - training loss: 0.3791, validation loss: 0.9212
2024-06-02 22:43:21 [INFO]: Epoch 072 - training loss: 0.3786, validation loss: 0.9179
2024-06-02 22:44:00 [INFO]: Epoch 073 - training loss: 0.3778, validation loss: 0.9165
2024-06-02 22:44:40 [INFO]: Epoch 074 - training loss: 0.3777, validation loss: 0.9144
2024-06-02 22:45:20 [INFO]: Epoch 075 - training loss: 0.3783, validation loss: 0.9093
2024-06-02 22:46:00 [INFO]: Epoch 076 - training loss: 0.3775, validation loss: 0.9075
2024-06-02 22:46:39 [INFO]: Epoch 077 - training loss: 0.3768, validation loss: 0.9054
2024-06-02 22:47:19 [INFO]: Epoch 078 - training loss: 0.3757, validation loss: 0.8938
2024-06-02 22:47:59 [INFO]: Epoch 079 - training loss: 0.3756, validation loss: 0.8889
2024-06-02 22:48:38 [INFO]: Epoch 080 - training loss: 0.3754, validation loss: 0.8887
2024-06-02 22:49:18 [INFO]: Epoch 081 - training loss: 0.3755, validation loss: 0.8846
2024-06-02 22:49:58 [INFO]: Epoch 082 - training loss: 0.3739, validation loss: 0.8765
2024-06-02 22:50:37 [INFO]: Epoch 083 - training loss: 0.3740, validation loss: 0.8722
2024-06-02 22:51:17 [INFO]: Epoch 084 - training loss: 0.3741, validation loss: 0.8729
2024-06-02 22:51:57 [INFO]: Epoch 085 - training loss: 0.3734, validation loss: 0.8771
2024-06-02 22:52:37 [INFO]: Epoch 086 - training loss: 0.3732, validation loss: 0.8770
2024-06-02 22:53:17 [INFO]: Epoch 087 - training loss: 0.3727, validation loss: 0.8588
2024-06-02 22:53:56 [INFO]: Epoch 088 - training loss: 0.3719, validation loss: 0.8581
2024-06-02 22:54:36 [INFO]: Epoch 089 - training loss: 0.3722, validation loss: 0.8482
2024-06-02 22:55:16 [INFO]: Epoch 090 - training loss: 0.3717, validation loss: 0.8521
2024-06-02 22:55:55 [INFO]: Epoch 091 - training loss: 0.3710, validation loss: 0.8508
2024-06-02 22:56:35 [INFO]: Epoch 092 - training loss: 0.3711, validation loss: 0.8389
2024-06-02 22:57:14 [INFO]: Epoch 093 - training loss: 0.3712, validation loss: 0.8482
2024-06-02 22:57:54 [INFO]: Epoch 094 - training loss: 0.3707, validation loss: 0.8438
2024-06-02 22:58:34 [INFO]: Epoch 095 - training loss: 0.3701, validation loss: 0.8331
2024-06-02 22:59:14 [INFO]: Epoch 096 - training loss: 0.3700, validation loss: 0.8314
2024-06-02 22:59:53 [INFO]: Epoch 097 - training loss: 0.3689, validation loss: 0.8305
2024-06-02 23:00:33 [INFO]: Epoch 098 - training loss: 0.3694, validation loss: 0.8224
2024-06-02 23:01:12 [INFO]: Epoch 099 - training loss: 0.3707, validation loss: 0.8209
2024-06-02 23:01:52 [INFO]: Epoch 100 - training loss: 0.3692, validation loss: 0.8227
2024-06-02 23:01:52 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 23:01:52 [INFO]: Saved the model to results_point_rate01/Electricity/PatchTST_Electricity/round_2/20240602_T215553/PatchTST.pypots
2024-06-02 23:01:57 [INFO]: Successfully saved to results_point_rate01/Electricity/PatchTST_Electricity/round_2/imputation.pkl
2024-06-02 23:01:57 [INFO]: Round2 - PatchTST on Electricity: MAE=0.5100, MSE=0.5211, MRE=0.2728
2024-06-02 23:01:57 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 23:01:57 [INFO]: Using the given device: cuda:0
2024-06-02 23:01:57 [INFO]: Model files will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_3/20240602_T230157
2024-06-02 23:01:57 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_3/20240602_T230157/tensorboard
2024-06-02 23:01:57 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=4, d_k=128
2024-06-02 23:01:57 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (4) * d_k (128)
2024-06-02 23:01:57 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 4,419,410
2024-06-02 23:02:37 [INFO]: Epoch 001 - training loss: 1.2760, validation loss: 3.4378
2024-06-02 23:03:17 [INFO]: Epoch 002 - training loss: 0.8807, validation loss: 3.1471
2024-06-02 23:03:57 [INFO]: Epoch 003 - training loss: 0.7290, validation loss: 2.9486
2024-06-02 23:04:36 [INFO]: Epoch 004 - training loss: 0.6653, validation loss: 2.7846
2024-06-02 23:05:16 [INFO]: Epoch 005 - training loss: 0.6277, validation loss: 2.6447
2024-06-02 23:05:55 [INFO]: Epoch 006 - training loss: 0.6003, validation loss: 2.5378
2024-06-02 23:06:35 [INFO]: Epoch 007 - training loss: 0.5782, validation loss: 2.4477
2024-06-02 23:07:15 [INFO]: Epoch 008 - training loss: 0.5609, validation loss: 2.3609
2024-06-02 23:07:55 [INFO]: Epoch 009 - training loss: 0.5464, validation loss: 2.2955
2024-06-02 23:08:34 [INFO]: Epoch 010 - training loss: 0.5342, validation loss: 2.2302
2024-06-02 23:09:14 [INFO]: Epoch 011 - training loss: 0.5236, validation loss: 2.1780
2024-06-02 23:09:53 [INFO]: Epoch 012 - training loss: 0.5130, validation loss: 2.1243
2024-06-02 23:10:33 [INFO]: Epoch 013 - training loss: 0.5049, validation loss: 2.0665
2024-06-02 23:11:13 [INFO]: Epoch 014 - training loss: 0.4966, validation loss: 2.0162
2024-06-02 23:11:52 [INFO]: Epoch 015 - training loss: 0.4898, validation loss: 1.9802
2024-06-02 23:12:32 [INFO]: Epoch 016 - training loss: 0.4833, validation loss: 1.9347
2024-06-02 23:13:11 [INFO]: Epoch 017 - training loss: 0.4777, validation loss: 1.8945
2024-06-02 23:13:51 [INFO]: Epoch 018 - training loss: 0.4711, validation loss: 1.8519
2024-06-02 23:14:31 [INFO]: Epoch 019 - training loss: 0.4658, validation loss: 1.8064
2024-06-02 23:15:10 [INFO]: Epoch 020 - training loss: 0.4616, validation loss: 1.7778
2024-06-02 23:15:50 [INFO]: Epoch 021 - training loss: 0.4572, validation loss: 1.7498
2024-06-02 23:16:30 [INFO]: Epoch 022 - training loss: 0.4533, validation loss: 1.7096
2024-06-02 23:17:09 [INFO]: Epoch 023 - training loss: 0.4498, validation loss: 1.6842
2024-06-02 23:17:49 [INFO]: Epoch 024 - training loss: 0.4465, validation loss: 1.6506
2024-06-02 23:18:29 [INFO]: Epoch 025 - training loss: 0.4416, validation loss: 1.6232
2024-06-02 23:19:08 [INFO]: Epoch 026 - training loss: 0.4393, validation loss: 1.5976
2024-06-02 23:19:48 [INFO]: Epoch 027 - training loss: 0.4354, validation loss: 1.5621
2024-06-02 23:20:28 [INFO]: Epoch 028 - training loss: 0.4331, validation loss: 1.5426
2024-06-02 23:21:07 [INFO]: Epoch 029 - training loss: 0.4299, validation loss: 1.5072
2024-06-02 23:21:47 [INFO]: Epoch 030 - training loss: 0.4272, validation loss: 1.4942
2024-06-02 23:22:27 [INFO]: Epoch 031 - training loss: 0.4255, validation loss: 1.4684
2024-06-02 23:23:07 [INFO]: Epoch 032 - training loss: 0.4233, validation loss: 1.4461
2024-06-02 23:23:41 [INFO]: Epoch 033 - training loss: 0.4204, validation loss: 1.4220
2024-06-02 23:24:18 [INFO]: Epoch 034 - training loss: 0.4181, validation loss: 1.3976
2024-06-02 23:24:58 [INFO]: Epoch 035 - training loss: 0.4155, validation loss: 1.3800
2024-06-02 23:25:38 [INFO]: Epoch 036 - training loss: 0.4137, validation loss: 1.3594
2024-06-02 23:26:17 [INFO]: Epoch 037 - training loss: 0.4126, validation loss: 1.3440
2024-06-02 23:26:57 [INFO]: Epoch 038 - training loss: 0.4104, validation loss: 1.3192
2024-06-02 23:27:36 [INFO]: Epoch 039 - training loss: 0.4087, validation loss: 1.3032
2024-06-02 23:28:16 [INFO]: Epoch 040 - training loss: 0.4073, validation loss: 1.2907
2024-06-02 23:28:56 [INFO]: Epoch 041 - training loss: 0.4054, validation loss: 1.2701
2024-06-02 23:29:35 [INFO]: Epoch 042 - training loss: 0.4042, validation loss: 1.2584
2024-06-02 23:30:15 [INFO]: Epoch 043 - training loss: 0.4034, validation loss: 1.2427
2024-06-02 23:30:55 [INFO]: Epoch 044 - training loss: 0.4014, validation loss: 1.2282
2024-06-02 23:31:34 [INFO]: Epoch 045 - training loss: 0.4001, validation loss: 1.2213
2024-06-02 23:32:14 [INFO]: Epoch 046 - training loss: 0.4002, validation loss: 1.2168
2024-06-02 23:32:54 [INFO]: Epoch 047 - training loss: 0.3982, validation loss: 1.1930
2024-06-02 23:33:33 [INFO]: Epoch 048 - training loss: 0.3976, validation loss: 1.1852
2024-06-02 23:34:13 [INFO]: Epoch 049 - training loss: 0.3958, validation loss: 1.1715
2024-06-02 23:34:52 [INFO]: Epoch 050 - training loss: 0.3950, validation loss: 1.1632
2024-06-02 23:35:32 [INFO]: Epoch 051 - training loss: 0.3950, validation loss: 1.1568
2024-06-02 23:36:12 [INFO]: Epoch 052 - training loss: 0.3935, validation loss: 1.1400
2024-06-02 23:36:52 [INFO]: Epoch 053 - training loss: 0.3919, validation loss: 1.1272
2024-06-02 23:37:31 [INFO]: Epoch 054 - training loss: 0.3914, validation loss: 1.1174
2024-06-02 23:38:11 [INFO]: Epoch 055 - training loss: 0.3911, validation loss: 1.1057
2024-06-02 23:38:50 [INFO]: Epoch 056 - training loss: 0.3901, validation loss: 1.1038
2024-06-02 23:39:30 [INFO]: Epoch 057 - training loss: 0.3889, validation loss: 1.1043
2024-06-02 23:40:10 [INFO]: Epoch 058 - training loss: 0.3883, validation loss: 1.1000
2024-06-02 23:40:50 [INFO]: Epoch 059 - training loss: 0.3877, validation loss: 1.0796
2024-06-02 23:41:29 [INFO]: Epoch 060 - training loss: 0.3870, validation loss: 1.0807
2024-06-02 23:42:09 [INFO]: Epoch 061 - training loss: 0.3864, validation loss: 1.0667
2024-06-02 23:42:49 [INFO]: Epoch 062 - training loss: 0.3855, validation loss: 1.0559
2024-06-02 23:43:28 [INFO]: Epoch 063 - training loss: 0.3852, validation loss: 1.0525
2024-06-02 23:44:08 [INFO]: Epoch 064 - training loss: 0.3863, validation loss: 1.0575
2024-06-02 23:44:47 [INFO]: Epoch 065 - training loss: 0.3845, validation loss: 1.0564
2024-06-02 23:45:27 [INFO]: Epoch 066 - training loss: 0.3828, validation loss: 1.0430
2024-06-02 23:46:06 [INFO]: Epoch 067 - training loss: 0.3823, validation loss: 1.0229
2024-06-02 23:46:46 [INFO]: Epoch 068 - training loss: 0.3816, validation loss: 1.0229
2024-06-02 23:47:25 [INFO]: Epoch 069 - training loss: 0.3809, validation loss: 1.0150
2024-06-02 23:48:05 [INFO]: Epoch 070 - training loss: 0.3802, validation loss: 1.0126
2024-06-02 23:48:45 [INFO]: Epoch 071 - training loss: 0.3802, validation loss: 1.0062
2024-06-02 23:49:25 [INFO]: Epoch 072 - training loss: 0.3801, validation loss: 1.0026
2024-06-02 23:50:04 [INFO]: Epoch 073 - training loss: 0.3790, validation loss: 0.9957
2024-06-02 23:50:44 [INFO]: Epoch 074 - training loss: 0.3785, validation loss: 0.9940
2024-06-02 23:51:24 [INFO]: Epoch 075 - training loss: 0.3788, validation loss: 0.9820
2024-06-02 23:52:03 [INFO]: Epoch 076 - training loss: 0.3781, validation loss: 0.9872
2024-06-02 23:52:43 [INFO]: Epoch 077 - training loss: 0.3775, validation loss: 0.9798
2024-06-02 23:53:23 [INFO]: Epoch 078 - training loss: 0.3773, validation loss: 0.9794
2024-06-02 23:54:02 [INFO]: Epoch 079 - training loss: 0.3770, validation loss: 0.9836
2024-06-02 23:54:42 [INFO]: Epoch 080 - training loss: 0.3760, validation loss: 0.9796
2024-06-02 23:55:22 [INFO]: Epoch 081 - training loss: 0.3756, validation loss: 0.9680
2024-06-02 23:56:01 [INFO]: Epoch 082 - training loss: 0.3756, validation loss: 0.9567
2024-06-02 23:56:41 [INFO]: Epoch 083 - training loss: 0.3756, validation loss: 0.9643
2024-06-02 23:57:21 [INFO]: Epoch 084 - training loss: 0.3748, validation loss: 0.9573
2024-06-02 23:58:01 [INFO]: Epoch 085 - training loss: 0.3745, validation loss: 0.9493
2024-06-02 23:58:40 [INFO]: Epoch 086 - training loss: 0.3739, validation loss: 0.9495
2024-06-02 23:59:20 [INFO]: Epoch 087 - training loss: 0.3736, validation loss: 0.9445
2024-06-03 00:00:00 [INFO]: Epoch 088 - training loss: 0.3735, validation loss: 0.9389
2024-06-03 00:00:40 [INFO]: Epoch 089 - training loss: 0.3727, validation loss: 0.9422
2024-06-03 00:01:19 [INFO]: Epoch 090 - training loss: 0.3729, validation loss: 0.9342
2024-06-03 00:01:59 [INFO]: Epoch 091 - training loss: 0.3730, validation loss: 0.9334
2024-06-03 00:02:39 [INFO]: Epoch 092 - training loss: 0.3718, validation loss: 0.9236
2024-06-03 00:03:18 [INFO]: Epoch 093 - training loss: 0.3713, validation loss: 0.9186
2024-06-03 00:03:58 [INFO]: Epoch 094 - training loss: 0.3717, validation loss: 0.9236
2024-06-03 00:04:38 [INFO]: Epoch 095 - training loss: 0.3716, validation loss: 0.9223
2024-06-03 00:05:18 [INFO]: Epoch 096 - training loss: 0.3714, validation loss: 0.9351
2024-06-03 00:05:58 [INFO]: Epoch 097 - training loss: 0.3712, validation loss: 0.9247
2024-06-03 00:06:37 [INFO]: Epoch 098 - training loss: 0.3707, validation loss: 0.9273
2024-06-03 00:07:17 [INFO]: Epoch 099 - training loss: 0.3701, validation loss: 0.9171
2024-06-03 00:07:57 [INFO]: Epoch 100 - training loss: 0.3694, validation loss: 0.9116
2024-06-03 00:07:57 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 00:07:57 [INFO]: Saved the model to results_point_rate01/Electricity/PatchTST_Electricity/round_3/20240602_T230157/PatchTST.pypots
2024-06-03 00:08:01 [INFO]: Successfully saved to results_point_rate01/Electricity/PatchTST_Electricity/round_3/imputation.pkl
2024-06-03 00:08:01 [INFO]: Round3 - PatchTST on Electricity: MAE=0.5502, MSE=0.6218, MRE=0.2943
2024-06-03 00:08:01 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 00:08:01 [INFO]: Using the given device: cuda:0
2024-06-03 00:08:01 [INFO]: Model files will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_4/20240603_T000801
2024-06-03 00:08:01 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/PatchTST_Electricity/round_4/20240603_T000801/tensorboard
2024-06-03 00:08:01 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=4, d_k=128
2024-06-03 00:08:01 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (4) * d_k (128)
2024-06-03 00:08:02 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 4,419,410
2024-06-03 00:08:41 [INFO]: Epoch 001 - training loss: 1.2402, validation loss: 3.3498
2024-06-03 00:09:21 [INFO]: Epoch 002 - training loss: 0.8475, validation loss: 3.0775
2024-06-03 00:10:00 [INFO]: Epoch 003 - training loss: 0.7105, validation loss: 2.8940
2024-06-03 00:10:40 [INFO]: Epoch 004 - training loss: 0.6583, validation loss: 2.7330
2024-06-03 00:11:19 [INFO]: Epoch 005 - training loss: 0.6211, validation loss: 2.6108
2024-06-03 00:11:59 [INFO]: Epoch 006 - training loss: 0.5945, validation loss: 2.5024
2024-06-03 00:12:38 [INFO]: Epoch 007 - training loss: 0.5725, validation loss: 2.4063
2024-06-03 00:13:18 [INFO]: Epoch 008 - training loss: 0.5561, validation loss: 2.3206
2024-06-03 00:13:58 [INFO]: Epoch 009 - training loss: 0.5414, validation loss: 2.2511
2024-06-03 00:14:37 [INFO]: Epoch 010 - training loss: 0.5304, validation loss: 2.1856
2024-06-03 00:15:17 [INFO]: Epoch 011 - training loss: 0.5206, validation loss: 2.1429
2024-06-03 00:15:57 [INFO]: Epoch 012 - training loss: 0.5106, validation loss: 2.0885
2024-06-03 00:16:36 [INFO]: Epoch 013 - training loss: 0.5026, validation loss: 2.0418
2024-06-03 00:17:16 [INFO]: Epoch 014 - training loss: 0.4953, validation loss: 2.0101
2024-06-03 00:17:55 [INFO]: Epoch 015 - training loss: 0.4884, validation loss: 1.9599
2024-06-03 00:18:35 [INFO]: Epoch 016 - training loss: 0.4820, validation loss: 1.9267
2024-06-03 00:19:15 [INFO]: Epoch 017 - training loss: 0.4758, validation loss: 1.8910
2024-06-03 00:19:54 [INFO]: Epoch 018 - training loss: 0.4704, validation loss: 1.8548
2024-06-03 00:20:34 [INFO]: Epoch 019 - training loss: 0.4652, validation loss: 1.8187
2024-06-03 00:21:13 [INFO]: Epoch 020 - training loss: 0.4606, validation loss: 1.7921
2024-06-03 00:21:53 [INFO]: Epoch 021 - training loss: 0.4554, validation loss: 1.7617
2024-06-03 00:22:32 [INFO]: Epoch 022 - training loss: 0.4515, validation loss: 1.7366
2024-06-03 00:23:12 [INFO]: Epoch 023 - training loss: 0.4494, validation loss: 1.7066
2024-06-03 00:23:52 [INFO]: Epoch 024 - training loss: 0.4442, validation loss: 1.6722
2024-06-03 00:24:31 [INFO]: Epoch 025 - training loss: 0.4402, validation loss: 1.6455
2024-06-03 00:25:11 [INFO]: Epoch 026 - training loss: 0.4385, validation loss: 1.6230
2024-06-03 00:25:51 [INFO]: Epoch 027 - training loss: 0.4339, validation loss: 1.5970
2024-06-03 00:26:30 [INFO]: Epoch 028 - training loss: 0.4318, validation loss: 1.5640
2024-06-03 00:27:09 [INFO]: Epoch 029 - training loss: 0.4302, validation loss: 1.5511
2024-06-03 00:27:39 [INFO]: Epoch 030 - training loss: 0.4265, validation loss: 1.5265
2024-06-03 00:28:00 [INFO]: Epoch 031 - training loss: 0.4231, validation loss: 1.5009
2024-06-03 00:28:21 [INFO]: Epoch 032 - training loss: 0.4202, validation loss: 1.4811
2024-06-03 00:28:42 [INFO]: Epoch 033 - training loss: 0.4192, validation loss: 1.4591
2024-06-03 00:29:03 [INFO]: Epoch 034 - training loss: 0.4170, validation loss: 1.4357
2024-06-03 00:29:25 [INFO]: Epoch 035 - training loss: 0.4150, validation loss: 1.4129
2024-06-03 00:29:46 [INFO]: Epoch 036 - training loss: 0.4137, validation loss: 1.3969
2024-06-03 00:30:07 [INFO]: Epoch 037 - training loss: 0.4115, validation loss: 1.3765
2024-06-03 00:30:28 [INFO]: Epoch 038 - training loss: 0.4098, validation loss: 1.3579
2024-06-03 00:30:49 [INFO]: Epoch 039 - training loss: 0.4072, validation loss: 1.3396
2024-06-03 00:31:10 [INFO]: Epoch 040 - training loss: 0.4064, validation loss: 1.3259
2024-06-03 00:31:31 [INFO]: Epoch 041 - training loss: 0.4044, validation loss: 1.2998
2024-06-03 00:31:52 [INFO]: Epoch 042 - training loss: 0.4035, validation loss: 1.2928
2024-06-03 00:32:14 [INFO]: Epoch 043 - training loss: 0.4027, validation loss: 1.2841
2024-06-03 00:32:35 [INFO]: Epoch 044 - training loss: 0.4007, validation loss: 1.2606
2024-06-03 00:32:56 [INFO]: Epoch 045 - training loss: 0.4000, validation loss: 1.2597
2024-06-03 00:33:17 [INFO]: Epoch 046 - training loss: 0.3981, validation loss: 1.2351
2024-06-03 00:33:38 [INFO]: Epoch 047 - training loss: 0.3972, validation loss: 1.2188
2024-06-03 00:33:59 [INFO]: Epoch 048 - training loss: 0.3977, validation loss: 1.2165
2024-06-03 00:34:21 [INFO]: Epoch 049 - training loss: 0.3950, validation loss: 1.2012
2024-06-03 00:34:42 [INFO]: Epoch 050 - training loss: 0.3937, validation loss: 1.1861
2024-06-03 00:35:03 [INFO]: Epoch 051 - training loss: 0.3928, validation loss: 1.1736
2024-06-03 00:35:24 [INFO]: Epoch 052 - training loss: 0.3921, validation loss: 1.1610
2024-06-03 00:35:45 [INFO]: Epoch 053 - training loss: 0.3916, validation loss: 1.1546
2024-06-03 00:36:06 [INFO]: Epoch 054 - training loss: 0.3907, validation loss: 1.1501
2024-06-03 00:36:28 [INFO]: Epoch 055 - training loss: 0.3898, validation loss: 1.1303
2024-06-03 00:36:49 [INFO]: Epoch 056 - training loss: 0.3901, validation loss: 1.1293
2024-06-03 00:37:10 [INFO]: Epoch 057 - training loss: 0.3887, validation loss: 1.1259
2024-06-03 00:37:31 [INFO]: Epoch 058 - training loss: 0.3873, validation loss: 1.1116
2024-06-03 00:37:52 [INFO]: Epoch 059 - training loss: 0.3873, validation loss: 1.1136
2024-06-03 00:38:13 [INFO]: Epoch 060 - training loss: 0.3859, validation loss: 1.0931
2024-06-03 00:38:35 [INFO]: Epoch 061 - training loss: 0.3857, validation loss: 1.0825
2024-06-03 00:38:56 [INFO]: Epoch 062 - training loss: 0.3846, validation loss: 1.0827
2024-06-03 00:39:17 [INFO]: Epoch 063 - training loss: 0.3841, validation loss: 1.0725
2024-06-03 00:39:38 [INFO]: Epoch 064 - training loss: 0.3832, validation loss: 1.0644
2024-06-03 00:39:59 [INFO]: Epoch 065 - training loss: 0.3828, validation loss: 1.0514
2024-06-03 00:40:20 [INFO]: Epoch 066 - training loss: 0.3826, validation loss: 1.0589
2024-06-03 00:40:42 [INFO]: Epoch 067 - training loss: 0.3820, validation loss: 1.0603
2024-06-03 00:41:03 [INFO]: Epoch 068 - training loss: 0.3818, validation loss: 1.0506
2024-06-03 00:41:24 [INFO]: Epoch 069 - training loss: 0.3809, validation loss: 1.0390
2024-06-03 00:41:45 [INFO]: Epoch 070 - training loss: 0.3801, validation loss: 1.0355
2024-06-03 00:42:06 [INFO]: Epoch 071 - training loss: 0.3795, validation loss: 1.0263
2024-06-03 00:42:27 [INFO]: Epoch 072 - training loss: 0.3794, validation loss: 1.0359
2024-06-03 00:42:48 [INFO]: Epoch 073 - training loss: 0.3794, validation loss: 1.0232
2024-06-03 00:43:10 [INFO]: Epoch 074 - training loss: 0.3779, validation loss: 1.0115
2024-06-03 00:43:31 [INFO]: Epoch 075 - training loss: 0.3781, validation loss: 1.0237
2024-06-03 00:43:52 [INFO]: Epoch 076 - training loss: 0.3783, validation loss: 0.9993
2024-06-03 00:44:13 [INFO]: Epoch 077 - training loss: 0.3782, validation loss: 1.0149
2024-06-03 00:44:34 [INFO]: Epoch 078 - training loss: 0.3777, validation loss: 0.9882
2024-06-03 00:44:52 [INFO]: Epoch 079 - training loss: 0.3770, validation loss: 1.0119
2024-06-03 00:45:09 [INFO]: Epoch 080 - training loss: 0.3754, validation loss: 0.9955
2024-06-03 00:45:26 [INFO]: Epoch 081 - training loss: 0.3748, validation loss: 0.9905
2024-06-03 00:45:44 [INFO]: Epoch 082 - training loss: 0.3746, validation loss: 0.9766
2024-06-03 00:46:01 [INFO]: Epoch 083 - training loss: 0.3744, validation loss: 0.9773
2024-06-03 00:46:18 [INFO]: Epoch 084 - training loss: 0.3739, validation loss: 0.9756
2024-06-03 00:46:35 [INFO]: Epoch 085 - training loss: 0.3738, validation loss: 0.9832
2024-06-03 00:46:53 [INFO]: Epoch 086 - training loss: 0.3735, validation loss: 0.9750
2024-06-03 00:47:10 [INFO]: Epoch 087 - training loss: 0.3732, validation loss: 0.9727
2024-06-03 00:47:27 [INFO]: Epoch 088 - training loss: 0.3725, validation loss: 0.9582
2024-06-03 00:47:45 [INFO]: Epoch 089 - training loss: 0.3726, validation loss: 0.9559
2024-06-03 00:48:02 [INFO]: Epoch 090 - training loss: 0.3723, validation loss: 0.9486
2024-06-03 00:48:19 [INFO]: Epoch 091 - training loss: 0.3717, validation loss: 0.9388
2024-06-03 00:48:36 [INFO]: Epoch 092 - training loss: 0.3716, validation loss: 0.9558
2024-06-03 00:48:54 [INFO]: Epoch 093 - training loss: 0.3708, validation loss: 0.9481
2024-06-03 00:49:11 [INFO]: Epoch 094 - training loss: 0.3705, validation loss: 0.9436
2024-06-03 00:49:28 [INFO]: Epoch 095 - training loss: 0.3704, validation loss: 0.9422
2024-06-03 00:49:45 [INFO]: Epoch 096 - training loss: 0.3698, validation loss: 0.9351
2024-06-03 00:50:03 [INFO]: Epoch 097 - training loss: 0.3696, validation loss: 0.9327
2024-06-03 00:50:20 [INFO]: Epoch 098 - training loss: 0.3697, validation loss: 0.9239
2024-06-03 00:50:37 [INFO]: Epoch 099 - training loss: 0.3698, validation loss: 0.9237
2024-06-03 00:50:55 [INFO]: Epoch 100 - training loss: 0.3691, validation loss: 0.9167
2024-06-03 00:50:55 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 00:50:55 [INFO]: Saved the model to results_point_rate01/Electricity/PatchTST_Electricity/round_4/20240603_T000801/PatchTST.pypots
2024-06-03 00:50:57 [INFO]: Successfully saved to results_point_rate01/Electricity/PatchTST_Electricity/round_4/imputation.pkl
2024-06-03 00:50:57 [INFO]: Round4 - PatchTST on Electricity: MAE=0.5145, MSE=0.5342, MRE=0.2752
2024-06-03 00:50:57 [INFO]: Done! Final results:
Averaged PatchTST (4,419,410 params) on Electricity: MAE=0.5498 ± 0.03868456897681435, MSE=0.6057 ± 0.08481367267482667, MRE=0.2941 ± 0.02069444285527291, average inference time=3.78
