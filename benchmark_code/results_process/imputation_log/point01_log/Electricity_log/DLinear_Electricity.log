2024-06-02 18:18:29 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 18:18:29 [INFO]: Using the given device: cuda:0
2024-06-02 18:18:29 [INFO]: Model files will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_0/20240602_T181829
2024-06-02 18:18:29 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_0/20240602_T181829/tensorboard
2024-06-02 18:18:30 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-02 18:18:34 [INFO]: Epoch 001 - training loss: 1.0701, validation loss: 2.9081
2024-06-02 18:18:37 [INFO]: Epoch 002 - training loss: 0.6834, validation loss: 2.6047
2024-06-02 18:18:39 [INFO]: Epoch 003 - training loss: 0.5949, validation loss: 2.4213
2024-06-02 18:18:41 [INFO]: Epoch 004 - training loss: 0.5544, validation loss: 2.2627
2024-06-02 18:18:44 [INFO]: Epoch 005 - training loss: 0.5271, validation loss: 2.1507
2024-06-02 18:18:46 [INFO]: Epoch 006 - training loss: 0.5061, validation loss: 2.0506
2024-06-02 18:18:49 [INFO]: Epoch 007 - training loss: 0.4897, validation loss: 1.9317
2024-06-02 18:18:52 [INFO]: Epoch 008 - training loss: 0.4748, validation loss: 1.8257
2024-06-02 18:18:54 [INFO]: Epoch 009 - training loss: 0.4622, validation loss: 1.7281
2024-06-02 18:18:56 [INFO]: Epoch 010 - training loss: 0.4519, validation loss: 1.6068
2024-06-02 18:18:59 [INFO]: Epoch 011 - training loss: 0.4437, validation loss: 1.5173
2024-06-02 18:19:02 [INFO]: Epoch 012 - training loss: 0.4356, validation loss: 1.4309
2024-06-02 18:19:04 [INFO]: Epoch 013 - training loss: 0.4302, validation loss: 1.3693
2024-06-02 18:19:07 [INFO]: Epoch 014 - training loss: 0.4235, validation loss: 1.2415
2024-06-02 18:19:09 [INFO]: Epoch 015 - training loss: 0.4165, validation loss: 1.1689
2024-06-02 18:19:12 [INFO]: Epoch 016 - training loss: 0.4119, validation loss: 1.0812
2024-06-02 18:19:14 [INFO]: Epoch 017 - training loss: 0.4134, validation loss: 1.0131
2024-06-02 18:19:17 [INFO]: Epoch 018 - training loss: 0.4073, validation loss: 0.9670
2024-06-02 18:19:19 [INFO]: Epoch 019 - training loss: 0.4019, validation loss: 0.8994
2024-06-02 18:19:21 [INFO]: Epoch 020 - training loss: 0.3998, validation loss: 0.8493
2024-06-02 18:19:24 [INFO]: Epoch 021 - training loss: 0.3987, validation loss: 0.7933
2024-06-02 18:19:26 [INFO]: Epoch 022 - training loss: 0.3959, validation loss: 0.7623
2024-06-02 18:19:29 [INFO]: Epoch 023 - training loss: 0.3941, validation loss: 0.7203
2024-06-02 18:19:31 [INFO]: Epoch 024 - training loss: 0.3930, validation loss: 0.6883
2024-06-02 18:19:34 [INFO]: Epoch 025 - training loss: 0.3923, validation loss: 0.6885
2024-06-02 18:19:37 [INFO]: Epoch 026 - training loss: 0.3893, validation loss: 0.6470
2024-06-02 18:19:39 [INFO]: Epoch 027 - training loss: 0.3868, validation loss: 0.6408
2024-06-02 18:19:42 [INFO]: Epoch 028 - training loss: 0.3851, validation loss: 0.6142
2024-06-02 18:19:44 [INFO]: Epoch 029 - training loss: 0.3849, validation loss: 0.5983
2024-06-02 18:19:47 [INFO]: Epoch 030 - training loss: 0.3839, validation loss: 0.5856
2024-06-02 18:19:49 [INFO]: Epoch 031 - training loss: 0.3836, validation loss: 0.5751
2024-06-02 18:19:52 [INFO]: Epoch 032 - training loss: 0.3827, validation loss: 0.5772
2024-06-02 18:19:54 [INFO]: Epoch 033 - training loss: 0.3822, validation loss: 0.5676
2024-06-02 18:19:57 [INFO]: Epoch 034 - training loss: 0.3828, validation loss: 0.5525
2024-06-02 18:19:59 [INFO]: Epoch 035 - training loss: 0.3789, validation loss: 0.5650
2024-06-02 18:20:02 [INFO]: Epoch 036 - training loss: 0.3782, validation loss: 0.5396
2024-06-02 18:20:04 [INFO]: Epoch 037 - training loss: 0.3762, validation loss: 0.5342
2024-06-02 18:20:07 [INFO]: Epoch 038 - training loss: 0.3760, validation loss: 0.4995
2024-06-02 18:20:09 [INFO]: Epoch 039 - training loss: 0.3746, validation loss: 0.5166
2024-06-02 18:20:11 [INFO]: Epoch 040 - training loss: 0.3749, validation loss: 0.5166
2024-06-02 18:20:14 [INFO]: Epoch 041 - training loss: 0.3726, validation loss: 0.5051
2024-06-02 18:20:16 [INFO]: Epoch 042 - training loss: 0.3713, validation loss: 0.4823
2024-06-02 18:20:19 [INFO]: Epoch 043 - training loss: 0.3706, validation loss: 0.4989
2024-06-02 18:20:21 [INFO]: Epoch 044 - training loss: 0.3686, validation loss: 0.4900
2024-06-02 18:20:24 [INFO]: Epoch 045 - training loss: 0.3670, validation loss: 0.4843
2024-06-02 18:20:27 [INFO]: Epoch 046 - training loss: 0.3661, validation loss: 0.4727
2024-06-02 18:20:29 [INFO]: Epoch 047 - training loss: 0.3646, validation loss: 0.4722
2024-06-02 18:20:31 [INFO]: Epoch 048 - training loss: 0.3633, validation loss: 0.4454
2024-06-02 18:20:34 [INFO]: Epoch 049 - training loss: 0.3625, validation loss: 0.4533
2024-06-02 18:20:37 [INFO]: Epoch 050 - training loss: 0.3622, validation loss: 0.4385
2024-06-02 18:20:39 [INFO]: Epoch 051 - training loss: 0.3609, validation loss: 0.4410
2024-06-02 18:20:42 [INFO]: Epoch 052 - training loss: 0.3605, validation loss: 0.4471
2024-06-02 18:20:44 [INFO]: Epoch 053 - training loss: 0.3599, validation loss: 0.4415
2024-06-02 18:20:47 [INFO]: Epoch 054 - training loss: 0.3593, validation loss: 0.4348
2024-06-02 18:20:49 [INFO]: Epoch 055 - training loss: 0.3594, validation loss: 0.4428
2024-06-02 18:20:51 [INFO]: Epoch 056 - training loss: 0.3579, validation loss: 0.4425
2024-06-02 18:20:54 [INFO]: Epoch 057 - training loss: 0.3580, validation loss: 0.4475
2024-06-02 18:20:57 [INFO]: Epoch 058 - training loss: 0.3586, validation loss: 0.4315
2024-06-02 18:20:59 [INFO]: Epoch 059 - training loss: 0.3571, validation loss: 0.4410
2024-06-02 18:21:02 [INFO]: Epoch 060 - training loss: 0.3552, validation loss: 0.4368
2024-06-02 18:21:04 [INFO]: Epoch 061 - training loss: 0.3552, validation loss: 0.4368
2024-06-02 18:21:07 [INFO]: Epoch 062 - training loss: 0.3550, validation loss: 0.4215
2024-06-02 18:21:10 [INFO]: Epoch 063 - training loss: 0.3555, validation loss: 0.4253
2024-06-02 18:21:12 [INFO]: Epoch 064 - training loss: 0.3535, validation loss: 0.4304
2024-06-02 18:21:14 [INFO]: Epoch 065 - training loss: 0.3541, validation loss: 0.4121
2024-06-02 18:21:17 [INFO]: Epoch 066 - training loss: 0.3533, validation loss: 0.4300
2024-06-02 18:21:20 [INFO]: Epoch 067 - training loss: 0.3528, validation loss: 0.4163
2024-06-02 18:21:22 [INFO]: Epoch 068 - training loss: 0.3532, validation loss: 0.4286
2024-06-02 18:21:25 [INFO]: Epoch 069 - training loss: 0.3543, validation loss: 0.4093
2024-06-02 18:21:27 [INFO]: Epoch 070 - training loss: 0.3532, validation loss: 0.4243
2024-06-02 18:21:30 [INFO]: Epoch 071 - training loss: 0.3521, validation loss: 0.4156
2024-06-02 18:21:32 [INFO]: Epoch 072 - training loss: 0.3516, validation loss: 0.4327
2024-06-02 18:21:34 [INFO]: Epoch 073 - training loss: 0.3515, validation loss: 0.4064
2024-06-02 18:21:36 [INFO]: Epoch 074 - training loss: 0.3513, validation loss: 0.4201
2024-06-02 18:21:39 [INFO]: Epoch 075 - training loss: 0.3512, validation loss: 0.4187
2024-06-02 18:21:41 [INFO]: Epoch 076 - training loss: 0.3505, validation loss: 0.4046
2024-06-02 18:21:44 [INFO]: Epoch 077 - training loss: 0.3515, validation loss: 0.4213
2024-06-02 18:21:46 [INFO]: Epoch 078 - training loss: 0.3506, validation loss: 0.4184
2024-06-02 18:21:48 [INFO]: Epoch 079 - training loss: 0.3508, validation loss: 0.4090
2024-06-02 18:21:51 [INFO]: Epoch 080 - training loss: 0.3498, validation loss: 0.4012
2024-06-02 18:21:53 [INFO]: Epoch 081 - training loss: 0.3506, validation loss: 0.4149
2024-06-02 18:21:56 [INFO]: Epoch 082 - training loss: 0.3505, validation loss: 0.4121
2024-06-02 18:21:58 [INFO]: Epoch 083 - training loss: 0.3494, validation loss: 0.4079
2024-06-02 18:22:00 [INFO]: Epoch 084 - training loss: 0.3493, validation loss: 0.4115
2024-06-02 18:22:03 [INFO]: Epoch 085 - training loss: 0.3491, validation loss: 0.3962
2024-06-02 18:22:05 [INFO]: Epoch 086 - training loss: 0.3487, validation loss: 0.4077
2024-06-02 18:22:07 [INFO]: Epoch 087 - training loss: 0.3489, validation loss: 0.4059
2024-06-02 18:22:10 [INFO]: Epoch 088 - training loss: 0.3493, validation loss: 0.4071
2024-06-02 18:22:12 [INFO]: Epoch 089 - training loss: 0.3486, validation loss: 0.4066
2024-06-02 18:22:15 [INFO]: Epoch 090 - training loss: 0.3477, validation loss: 0.4068
2024-06-02 18:22:17 [INFO]: Epoch 091 - training loss: 0.3479, validation loss: 0.4150
2024-06-02 18:22:19 [INFO]: Epoch 092 - training loss: 0.3476, validation loss: 0.4040
2024-06-02 18:22:22 [INFO]: Epoch 093 - training loss: 0.3477, validation loss: 0.4033
2024-06-02 18:22:24 [INFO]: Epoch 094 - training loss: 0.3480, validation loss: 0.4003
2024-06-02 18:22:26 [INFO]: Epoch 095 - training loss: 0.3479, validation loss: 0.4041
2024-06-02 18:22:26 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:22:26 [INFO]: Finished training. The best model is from epoch#85.
2024-06-02 18:22:26 [INFO]: Saved the model to results_point_rate01/Electricity/DLinear_Electricity/round_0/20240602_T181829/DLinear.pypots
2024-06-02 18:22:27 [INFO]: Successfully saved to results_point_rate01/Electricity/DLinear_Electricity/round_0/imputation.pkl
2024-06-02 18:22:27 [INFO]: Round0 - DLinear on Electricity: MAE=0.5106, MSE=0.4465, MRE=0.2731
2024-06-02 18:22:27 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 18:22:27 [INFO]: Using the given device: cuda:0
2024-06-02 18:22:27 [INFO]: Model files will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_1/20240602_T182227
2024-06-02 18:22:27 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_1/20240602_T182227/tensorboard
2024-06-02 18:22:27 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-02 18:22:29 [INFO]: Epoch 001 - training loss: 1.0773, validation loss: 2.9588
2024-06-02 18:22:32 [INFO]: Epoch 002 - training loss: 0.6725, validation loss: 2.6835
2024-06-02 18:22:34 [INFO]: Epoch 003 - training loss: 0.5960, validation loss: 2.4904
2024-06-02 18:22:37 [INFO]: Epoch 004 - training loss: 0.5503, validation loss: 2.3172
2024-06-02 18:22:39 [INFO]: Epoch 005 - training loss: 0.5218, validation loss: 2.1809
2024-06-02 18:22:42 [INFO]: Epoch 006 - training loss: 0.5007, validation loss: 2.0655
2024-06-02 18:22:44 [INFO]: Epoch 007 - training loss: 0.4847, validation loss: 1.9420
2024-06-02 18:22:47 [INFO]: Epoch 008 - training loss: 0.4710, validation loss: 1.8259
2024-06-02 18:22:50 [INFO]: Epoch 009 - training loss: 0.4582, validation loss: 1.7406
2024-06-02 18:22:52 [INFO]: Epoch 010 - training loss: 0.4499, validation loss: 1.6415
2024-06-02 18:22:54 [INFO]: Epoch 011 - training loss: 0.4389, validation loss: 1.5565
2024-06-02 18:22:57 [INFO]: Epoch 012 - training loss: 0.4334, validation loss: 1.4749
2024-06-02 18:22:59 [INFO]: Epoch 013 - training loss: 0.4269, validation loss: 1.3800
2024-06-02 18:23:02 [INFO]: Epoch 014 - training loss: 0.4202, validation loss: 1.2963
2024-06-02 18:23:04 [INFO]: Epoch 015 - training loss: 0.4153, validation loss: 1.1998
2024-06-02 18:23:07 [INFO]: Epoch 016 - training loss: 0.4105, validation loss: 1.1267
2024-06-02 18:23:09 [INFO]: Epoch 017 - training loss: 0.4064, validation loss: 1.0408
2024-06-02 18:23:11 [INFO]: Epoch 018 - training loss: 0.4067, validation loss: 1.0008
2024-06-02 18:23:14 [INFO]: Epoch 019 - training loss: 0.4031, validation loss: 0.9314
2024-06-02 18:23:16 [INFO]: Epoch 020 - training loss: 0.4000, validation loss: 0.8857
2024-06-02 18:23:19 [INFO]: Epoch 021 - training loss: 0.3961, validation loss: 0.8221
2024-06-02 18:23:21 [INFO]: Epoch 022 - training loss: 0.3950, validation loss: 0.7781
2024-06-02 18:23:24 [INFO]: Epoch 023 - training loss: 0.3923, validation loss: 0.7352
2024-06-02 18:23:26 [INFO]: Epoch 024 - training loss: 0.3903, validation loss: 0.6927
2024-06-02 18:23:29 [INFO]: Epoch 025 - training loss: 0.3899, validation loss: 0.6866
2024-06-02 18:23:31 [INFO]: Epoch 026 - training loss: 0.3895, validation loss: 0.6425
2024-06-02 18:23:34 [INFO]: Epoch 027 - training loss: 0.3880, validation loss: 0.6323
2024-06-02 18:23:36 [INFO]: Epoch 028 - training loss: 0.3861, validation loss: 0.6063
2024-06-02 18:23:39 [INFO]: Epoch 029 - training loss: 0.3844, validation loss: 0.5969
2024-06-02 18:23:41 [INFO]: Epoch 030 - training loss: 0.3833, validation loss: 0.5983
2024-06-02 18:23:44 [INFO]: Epoch 031 - training loss: 0.3818, validation loss: 0.5758
2024-06-02 18:23:46 [INFO]: Epoch 032 - training loss: 0.3813, validation loss: 0.5555
2024-06-02 18:23:49 [INFO]: Epoch 033 - training loss: 0.3814, validation loss: 0.5538
2024-06-02 18:23:51 [INFO]: Epoch 034 - training loss: 0.3821, validation loss: 0.5416
2024-06-02 18:23:54 [INFO]: Epoch 035 - training loss: 0.3798, validation loss: 0.5354
2024-06-02 18:23:56 [INFO]: Epoch 036 - training loss: 0.3780, validation loss: 0.5226
2024-06-02 18:23:59 [INFO]: Epoch 037 - training loss: 0.3786, validation loss: 0.5176
2024-06-02 18:24:01 [INFO]: Epoch 038 - training loss: 0.3771, validation loss: 0.5208
2024-06-02 18:24:03 [INFO]: Epoch 039 - training loss: 0.3771, validation loss: 0.4991
2024-06-02 18:24:06 [INFO]: Epoch 040 - training loss: 0.3759, validation loss: 0.4866
2024-06-02 18:24:08 [INFO]: Epoch 041 - training loss: 0.3754, validation loss: 0.4981
2024-06-02 18:24:11 [INFO]: Epoch 042 - training loss: 0.3731, validation loss: 0.4826
2024-06-02 18:24:13 [INFO]: Epoch 043 - training loss: 0.3708, validation loss: 0.4865
2024-06-02 18:24:15 [INFO]: Epoch 044 - training loss: 0.3701, validation loss: 0.4875
2024-06-02 18:24:18 [INFO]: Epoch 045 - training loss: 0.3676, validation loss: 0.4575
2024-06-02 18:24:20 [INFO]: Epoch 046 - training loss: 0.3659, validation loss: 0.4574
2024-06-02 18:24:23 [INFO]: Epoch 047 - training loss: 0.3663, validation loss: 0.4490
2024-06-02 18:24:25 [INFO]: Epoch 048 - training loss: 0.3635, validation loss: 0.4398
2024-06-02 18:24:28 [INFO]: Epoch 049 - training loss: 0.3620, validation loss: 0.4223
2024-06-02 18:24:31 [INFO]: Epoch 050 - training loss: 0.3605, validation loss: 0.4345
2024-06-02 18:24:33 [INFO]: Epoch 051 - training loss: 0.3600, validation loss: 0.4346
2024-06-02 18:24:36 [INFO]: Epoch 052 - training loss: 0.3600, validation loss: 0.4350
2024-06-02 18:24:38 [INFO]: Epoch 053 - training loss: 0.3578, validation loss: 0.4225
2024-06-02 18:24:41 [INFO]: Epoch 054 - training loss: 0.3586, validation loss: 0.4170
2024-06-02 18:24:44 [INFO]: Epoch 055 - training loss: 0.3579, validation loss: 0.4176
2024-06-02 18:24:46 [INFO]: Epoch 056 - training loss: 0.3570, validation loss: 0.4230
2024-06-02 18:24:48 [INFO]: Epoch 057 - training loss: 0.3560, validation loss: 0.4202
2024-06-02 18:24:51 [INFO]: Epoch 058 - training loss: 0.3569, validation loss: 0.4201
2024-06-02 18:24:53 [INFO]: Epoch 059 - training loss: 0.3563, validation loss: 0.4238
2024-06-02 18:24:56 [INFO]: Epoch 060 - training loss: 0.3546, validation loss: 0.4116
2024-06-02 18:24:58 [INFO]: Epoch 061 - training loss: 0.3554, validation loss: 0.4157
2024-06-02 18:25:00 [INFO]: Epoch 062 - training loss: 0.3547, validation loss: 0.4260
2024-06-02 18:25:03 [INFO]: Epoch 063 - training loss: 0.3538, validation loss: 0.4208
2024-06-02 18:25:05 [INFO]: Epoch 064 - training loss: 0.3532, validation loss: 0.4117
2024-06-02 18:25:08 [INFO]: Epoch 065 - training loss: 0.3525, validation loss: 0.4139
2024-06-02 18:25:10 [INFO]: Epoch 066 - training loss: 0.3524, validation loss: 0.4046
2024-06-02 18:25:13 [INFO]: Epoch 067 - training loss: 0.3532, validation loss: 0.4034
2024-06-02 18:25:15 [INFO]: Epoch 068 - training loss: 0.3522, validation loss: 0.4031
2024-06-02 18:25:18 [INFO]: Epoch 069 - training loss: 0.3517, validation loss: 0.4097
2024-06-02 18:25:20 [INFO]: Epoch 070 - training loss: 0.3520, validation loss: 0.4073
2024-06-02 18:25:23 [INFO]: Epoch 071 - training loss: 0.3518, validation loss: 0.4098
2024-06-02 18:25:25 [INFO]: Epoch 072 - training loss: 0.3511, validation loss: 0.3933
2024-06-02 18:25:28 [INFO]: Epoch 073 - training loss: 0.3519, validation loss: 0.4032
2024-06-02 18:25:30 [INFO]: Epoch 074 - training loss: 0.3511, validation loss: 0.4069
2024-06-02 18:25:33 [INFO]: Epoch 075 - training loss: 0.3503, validation loss: 0.3987
2024-06-02 18:25:35 [INFO]: Epoch 076 - training loss: 0.3495, validation loss: 0.3950
2024-06-02 18:25:37 [INFO]: Epoch 077 - training loss: 0.3494, validation loss: 0.4037
2024-06-02 18:25:40 [INFO]: Epoch 078 - training loss: 0.3504, validation loss: 0.4024
2024-06-02 18:25:42 [INFO]: Epoch 079 - training loss: 0.3501, validation loss: 0.4058
2024-06-02 18:25:45 [INFO]: Epoch 080 - training loss: 0.3499, validation loss: 0.4032
2024-06-02 18:25:47 [INFO]: Epoch 081 - training loss: 0.3482, validation loss: 0.3926
2024-06-02 18:25:49 [INFO]: Epoch 082 - training loss: 0.3495, validation loss: 0.4073
2024-06-02 18:25:52 [INFO]: Epoch 083 - training loss: 0.3493, validation loss: 0.3974
2024-06-02 18:25:55 [INFO]: Epoch 084 - training loss: 0.3486, validation loss: 0.4066
2024-06-02 18:25:57 [INFO]: Epoch 085 - training loss: 0.3495, validation loss: 0.4091
2024-06-02 18:25:59 [INFO]: Epoch 086 - training loss: 0.3480, validation loss: 0.4034
2024-06-02 18:26:02 [INFO]: Epoch 087 - training loss: 0.3483, validation loss: 0.4028
2024-06-02 18:26:04 [INFO]: Epoch 088 - training loss: 0.3481, validation loss: 0.3907
2024-06-02 18:26:07 [INFO]: Epoch 089 - training loss: 0.3478, validation loss: 0.3902
2024-06-02 18:26:09 [INFO]: Epoch 090 - training loss: 0.3473, validation loss: 0.3935
2024-06-02 18:26:12 [INFO]: Epoch 091 - training loss: 0.3478, validation loss: 0.3938
2024-06-02 18:26:15 [INFO]: Epoch 092 - training loss: 0.3474, validation loss: 0.4024
2024-06-02 18:26:17 [INFO]: Epoch 093 - training loss: 0.3484, validation loss: 0.4014
2024-06-02 18:26:19 [INFO]: Epoch 094 - training loss: 0.3481, validation loss: 0.4025
2024-06-02 18:26:22 [INFO]: Epoch 095 - training loss: 0.3474, validation loss: 0.3936
2024-06-02 18:26:24 [INFO]: Epoch 096 - training loss: 0.3467, validation loss: 0.4041
2024-06-02 18:26:27 [INFO]: Epoch 097 - training loss: 0.3468, validation loss: 0.3949
2024-06-02 18:26:29 [INFO]: Epoch 098 - training loss: 0.3478, validation loss: 0.4001
2024-06-02 18:26:32 [INFO]: Epoch 099 - training loss: 0.3476, validation loss: 0.4013
2024-06-02 18:26:32 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:26:32 [INFO]: Finished training. The best model is from epoch#89.
2024-06-02 18:26:32 [INFO]: Saved the model to results_point_rate01/Electricity/DLinear_Electricity/round_1/20240602_T182227/DLinear.pypots
2024-06-02 18:26:32 [INFO]: Successfully saved to results_point_rate01/Electricity/DLinear_Electricity/round_1/imputation.pkl
2024-06-02 18:26:32 [INFO]: Round1 - DLinear on Electricity: MAE=0.5327, MSE=0.4829, MRE=0.2850
2024-06-02 18:26:32 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 18:26:32 [INFO]: Using the given device: cuda:0
2024-06-02 18:26:32 [INFO]: Model files will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_2/20240602_T182632
2024-06-02 18:26:32 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_2/20240602_T182632/tensorboard
2024-06-02 18:26:32 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-02 18:26:35 [INFO]: Epoch 001 - training loss: 1.0589, validation loss: 2.9144
2024-06-02 18:26:37 [INFO]: Epoch 002 - training loss: 0.6761, validation loss: 2.6886
2024-06-02 18:26:39 [INFO]: Epoch 003 - training loss: 0.5976, validation loss: 2.4929
2024-06-02 18:26:42 [INFO]: Epoch 004 - training loss: 0.5559, validation loss: 2.2934
2024-06-02 18:26:45 [INFO]: Epoch 005 - training loss: 0.5289, validation loss: 2.1686
2024-06-02 18:26:47 [INFO]: Epoch 006 - training loss: 0.5052, validation loss: 2.0463
2024-06-02 18:26:49 [INFO]: Epoch 007 - training loss: 0.4893, validation loss: 1.9517
2024-06-02 18:26:52 [INFO]: Epoch 008 - training loss: 0.4733, validation loss: 1.8294
2024-06-02 18:26:54 [INFO]: Epoch 009 - training loss: 0.4619, validation loss: 1.7436
2024-06-02 18:26:56 [INFO]: Epoch 010 - training loss: 0.4519, validation loss: 1.6347
2024-06-02 18:26:59 [INFO]: Epoch 011 - training loss: 0.4429, validation loss: 1.5403
2024-06-02 18:27:01 [INFO]: Epoch 012 - training loss: 0.4348, validation loss: 1.4508
2024-06-02 18:27:03 [INFO]: Epoch 013 - training loss: 0.4291, validation loss: 1.3823
2024-06-02 18:27:06 [INFO]: Epoch 014 - training loss: 0.4234, validation loss: 1.2652
2024-06-02 18:27:08 [INFO]: Epoch 015 - training loss: 0.4175, validation loss: 1.2029
2024-06-02 18:27:11 [INFO]: Epoch 016 - training loss: 0.4143, validation loss: 1.1294
2024-06-02 18:27:13 [INFO]: Epoch 017 - training loss: 0.4100, validation loss: 1.0624
2024-06-02 18:27:15 [INFO]: Epoch 018 - training loss: 0.4065, validation loss: 0.9764
2024-06-02 18:27:18 [INFO]: Epoch 019 - training loss: 0.4052, validation loss: 0.9295
2024-06-02 18:27:21 [INFO]: Epoch 020 - training loss: 0.4015, validation loss: 0.8884
2024-06-02 18:27:23 [INFO]: Epoch 021 - training loss: 0.3968, validation loss: 0.8301
2024-06-02 18:27:26 [INFO]: Epoch 022 - training loss: 0.3942, validation loss: 0.7785
2024-06-02 18:27:28 [INFO]: Epoch 023 - training loss: 0.3918, validation loss: 0.7396
2024-06-02 18:27:31 [INFO]: Epoch 024 - training loss: 0.3913, validation loss: 0.7196
2024-06-02 18:27:33 [INFO]: Epoch 025 - training loss: 0.3897, validation loss: 0.6872
2024-06-02 18:27:36 [INFO]: Epoch 026 - training loss: 0.3886, validation loss: 0.6650
2024-06-02 18:27:38 [INFO]: Epoch 027 - training loss: 0.3891, validation loss: 0.6327
2024-06-02 18:27:40 [INFO]: Epoch 028 - training loss: 0.3846, validation loss: 0.6046
2024-06-02 18:27:43 [INFO]: Epoch 029 - training loss: 0.3842, validation loss: 0.5858
2024-06-02 18:27:45 [INFO]: Epoch 030 - training loss: 0.3830, validation loss: 0.5811
2024-06-02 18:27:48 [INFO]: Epoch 031 - training loss: 0.3833, validation loss: 0.5711
2024-06-02 18:27:50 [INFO]: Epoch 032 - training loss: 0.3814, validation loss: 0.5744
2024-06-02 18:27:53 [INFO]: Epoch 033 - training loss: 0.3799, validation loss: 0.5401
2024-06-02 18:27:55 [INFO]: Epoch 034 - training loss: 0.3797, validation loss: 0.5331
2024-06-02 18:27:58 [INFO]: Epoch 035 - training loss: 0.3800, validation loss: 0.5374
2024-06-02 18:28:00 [INFO]: Epoch 036 - training loss: 0.3791, validation loss: 0.5233
2024-06-02 18:28:03 [INFO]: Epoch 037 - training loss: 0.3770, validation loss: 0.5092
2024-06-02 18:28:05 [INFO]: Epoch 038 - training loss: 0.3763, validation loss: 0.5099
2024-06-02 18:28:08 [INFO]: Epoch 039 - training loss: 0.3758, validation loss: 0.4990
2024-06-02 18:28:10 [INFO]: Epoch 040 - training loss: 0.3747, validation loss: 0.4848
2024-06-02 18:28:13 [INFO]: Epoch 041 - training loss: 0.3762, validation loss: 0.4948
2024-06-02 18:28:15 [INFO]: Epoch 042 - training loss: 0.3745, validation loss: 0.4834
2024-06-02 18:28:18 [INFO]: Epoch 043 - training loss: 0.3736, validation loss: 0.4881
2024-06-02 18:28:20 [INFO]: Epoch 044 - training loss: 0.3731, validation loss: 0.4758
2024-06-02 18:28:22 [INFO]: Epoch 045 - training loss: 0.3722, validation loss: 0.4697
2024-06-02 18:28:25 [INFO]: Epoch 046 - training loss: 0.3716, validation loss: 0.4774
2024-06-02 18:28:28 [INFO]: Epoch 047 - training loss: 0.3705, validation loss: 0.4697
2024-06-02 18:28:30 [INFO]: Epoch 048 - training loss: 0.3708, validation loss: 0.4554
2024-06-02 18:28:33 [INFO]: Epoch 049 - training loss: 0.3697, validation loss: 0.4593
2024-06-02 18:28:35 [INFO]: Epoch 050 - training loss: 0.3688, validation loss: 0.4474
2024-06-02 18:28:38 [INFO]: Epoch 051 - training loss: 0.3680, validation loss: 0.4495
2024-06-02 18:28:40 [INFO]: Epoch 052 - training loss: 0.3665, validation loss: 0.4415
2024-06-02 18:28:42 [INFO]: Epoch 053 - training loss: 0.3655, validation loss: 0.4419
2024-06-02 18:28:45 [INFO]: Epoch 054 - training loss: 0.3637, validation loss: 0.4302
2024-06-02 18:28:47 [INFO]: Epoch 055 - training loss: 0.3618, validation loss: 0.4280
2024-06-02 18:28:50 [INFO]: Epoch 056 - training loss: 0.3626, validation loss: 0.4293
2024-06-02 18:28:52 [INFO]: Epoch 057 - training loss: 0.3617, validation loss: 0.4214
2024-06-02 18:28:55 [INFO]: Epoch 058 - training loss: 0.3600, validation loss: 0.4358
2024-06-02 18:28:58 [INFO]: Epoch 059 - training loss: 0.3590, validation loss: 0.4200
2024-06-02 18:29:00 [INFO]: Epoch 060 - training loss: 0.3583, validation loss: 0.4189
2024-06-02 18:29:02 [INFO]: Epoch 061 - training loss: 0.3574, validation loss: 0.4209
2024-06-02 18:29:05 [INFO]: Epoch 062 - training loss: 0.3569, validation loss: 0.4064
2024-06-02 18:29:07 [INFO]: Epoch 063 - training loss: 0.3568, validation loss: 0.4178
2024-06-02 18:29:10 [INFO]: Epoch 064 - training loss: 0.3563, validation loss: 0.4155
2024-06-02 18:29:12 [INFO]: Epoch 065 - training loss: 0.3558, validation loss: 0.4194
2024-06-02 18:29:14 [INFO]: Epoch 066 - training loss: 0.3554, validation loss: 0.4060
2024-06-02 18:29:17 [INFO]: Epoch 067 - training loss: 0.3545, validation loss: 0.4102
2024-06-02 18:29:19 [INFO]: Epoch 068 - training loss: 0.3543, validation loss: 0.4156
2024-06-02 18:29:22 [INFO]: Epoch 069 - training loss: 0.3540, validation loss: 0.4093
2024-06-02 18:29:24 [INFO]: Epoch 070 - training loss: 0.3527, validation loss: 0.4039
2024-06-02 18:29:27 [INFO]: Epoch 071 - training loss: 0.3536, validation loss: 0.4208
2024-06-02 18:29:29 [INFO]: Epoch 072 - training loss: 0.3532, validation loss: 0.3958
2024-06-02 18:29:32 [INFO]: Epoch 073 - training loss: 0.3524, validation loss: 0.3962
2024-06-02 18:29:35 [INFO]: Epoch 074 - training loss: 0.3522, validation loss: 0.4022
2024-06-02 18:29:37 [INFO]: Epoch 075 - training loss: 0.3541, validation loss: 0.4002
2024-06-02 18:29:39 [INFO]: Epoch 076 - training loss: 0.3510, validation loss: 0.3960
2024-06-02 18:29:42 [INFO]: Epoch 077 - training loss: 0.3519, validation loss: 0.3953
2024-06-02 18:29:44 [INFO]: Epoch 078 - training loss: 0.3513, validation loss: 0.3979
2024-06-02 18:29:47 [INFO]: Epoch 079 - training loss: 0.3513, validation loss: 0.3884
2024-06-02 18:29:49 [INFO]: Epoch 080 - training loss: 0.3506, validation loss: 0.3918
2024-06-02 18:29:52 [INFO]: Epoch 081 - training loss: 0.3504, validation loss: 0.4057
2024-06-02 18:29:54 [INFO]: Epoch 082 - training loss: 0.3504, validation loss: 0.3986
2024-06-02 18:29:57 [INFO]: Epoch 083 - training loss: 0.3500, validation loss: 0.3931
2024-06-02 18:29:59 [INFO]: Epoch 084 - training loss: 0.3499, validation loss: 0.3928
2024-06-02 18:30:02 [INFO]: Epoch 085 - training loss: 0.3496, validation loss: 0.4050
2024-06-02 18:30:04 [INFO]: Epoch 086 - training loss: 0.3488, validation loss: 0.3834
2024-06-02 18:30:07 [INFO]: Epoch 087 - training loss: 0.3496, validation loss: 0.3888
2024-06-02 18:30:09 [INFO]: Epoch 088 - training loss: 0.3494, validation loss: 0.3893
2024-06-02 18:30:12 [INFO]: Epoch 089 - training loss: 0.3505, validation loss: 0.3847
2024-06-02 18:30:14 [INFO]: Epoch 090 - training loss: 0.3499, validation loss: 0.3876
2024-06-02 18:30:17 [INFO]: Epoch 091 - training loss: 0.3484, validation loss: 0.3919
2024-06-02 18:30:19 [INFO]: Epoch 092 - training loss: 0.3485, validation loss: 0.3851
2024-06-02 18:30:22 [INFO]: Epoch 093 - training loss: 0.3488, validation loss: 0.3868
2024-06-02 18:30:24 [INFO]: Epoch 094 - training loss: 0.3484, validation loss: 0.3927
2024-06-02 18:30:27 [INFO]: Epoch 095 - training loss: 0.3479, validation loss: 0.3966
2024-06-02 18:30:30 [INFO]: Epoch 096 - training loss: 0.3486, validation loss: 0.3972
2024-06-02 18:30:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:30:30 [INFO]: Finished training. The best model is from epoch#86.
2024-06-02 18:30:30 [INFO]: Saved the model to results_point_rate01/Electricity/DLinear_Electricity/round_2/20240602_T182632/DLinear.pypots
2024-06-02 18:30:30 [INFO]: Successfully saved to results_point_rate01/Electricity/DLinear_Electricity/round_2/imputation.pkl
2024-06-02 18:30:30 [INFO]: Round2 - DLinear on Electricity: MAE=0.5133, MSE=0.4413, MRE=0.2746
2024-06-02 18:30:30 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 18:30:30 [INFO]: Using the given device: cuda:0
2024-06-02 18:30:30 [INFO]: Model files will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_3/20240602_T183030
2024-06-02 18:30:30 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_3/20240602_T183030/tensorboard
2024-06-02 18:30:30 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-02 18:30:33 [INFO]: Epoch 001 - training loss: 1.0419, validation loss: 2.8635
2024-06-02 18:30:35 [INFO]: Epoch 002 - training loss: 0.6722, validation loss: 2.6102
2024-06-02 18:30:38 [INFO]: Epoch 003 - training loss: 0.5927, validation loss: 2.4209
2024-06-02 18:30:40 [INFO]: Epoch 004 - training loss: 0.5480, validation loss: 2.2537
2024-06-02 18:30:43 [INFO]: Epoch 005 - training loss: 0.5200, validation loss: 2.1641
2024-06-02 18:30:45 [INFO]: Epoch 006 - training loss: 0.5000, validation loss: 2.0370
2024-06-02 18:30:48 [INFO]: Epoch 007 - training loss: 0.4831, validation loss: 1.9125
2024-06-02 18:30:50 [INFO]: Epoch 008 - training loss: 0.4708, validation loss: 1.8172
2024-06-02 18:30:53 [INFO]: Epoch 009 - training loss: 0.4579, validation loss: 1.7190
2024-06-02 18:30:55 [INFO]: Epoch 010 - training loss: 0.4458, validation loss: 1.6384
2024-06-02 18:30:57 [INFO]: Epoch 011 - training loss: 0.4375, validation loss: 1.5200
2024-06-02 18:31:00 [INFO]: Epoch 012 - training loss: 0.4321, validation loss: 1.4440
2024-06-02 18:31:02 [INFO]: Epoch 013 - training loss: 0.4231, validation loss: 1.3316
2024-06-02 18:31:05 [INFO]: Epoch 014 - training loss: 0.4182, validation loss: 1.2620
2024-06-02 18:31:07 [INFO]: Epoch 015 - training loss: 0.4143, validation loss: 1.1762
2024-06-02 18:31:10 [INFO]: Epoch 016 - training loss: 0.4110, validation loss: 1.0899
2024-06-02 18:31:12 [INFO]: Epoch 017 - training loss: 0.4052, validation loss: 0.9938
2024-06-02 18:31:15 [INFO]: Epoch 018 - training loss: 0.4019, validation loss: 0.9475
2024-06-02 18:31:17 [INFO]: Epoch 019 - training loss: 0.3988, validation loss: 0.8997
2024-06-02 18:31:20 [INFO]: Epoch 020 - training loss: 0.3963, validation loss: 0.8416
2024-06-02 18:31:22 [INFO]: Epoch 021 - training loss: 0.3943, validation loss: 0.8002
2024-06-02 18:31:25 [INFO]: Epoch 022 - training loss: 0.3927, validation loss: 0.7580
2024-06-02 18:31:27 [INFO]: Epoch 023 - training loss: 0.3904, validation loss: 0.7101
2024-06-02 18:31:29 [INFO]: Epoch 024 - training loss: 0.3885, validation loss: 0.6879
2024-06-02 18:31:32 [INFO]: Epoch 025 - training loss: 0.3878, validation loss: 0.6481
2024-06-02 18:31:34 [INFO]: Epoch 026 - training loss: 0.3872, validation loss: 0.6390
2024-06-02 18:31:37 [INFO]: Epoch 027 - training loss: 0.3852, validation loss: 0.6162
2024-06-02 18:31:39 [INFO]: Epoch 028 - training loss: 0.3844, validation loss: 0.5923
2024-06-02 18:31:42 [INFO]: Epoch 029 - training loss: 0.3825, validation loss: 0.5701
2024-06-02 18:31:44 [INFO]: Epoch 030 - training loss: 0.3815, validation loss: 0.5626
2024-06-02 18:31:47 [INFO]: Epoch 031 - training loss: 0.3808, validation loss: 0.5444
2024-06-02 18:31:49 [INFO]: Epoch 032 - training loss: 0.3784, validation loss: 0.5288
2024-06-02 18:31:52 [INFO]: Epoch 033 - training loss: 0.3808, validation loss: 0.5567
2024-06-02 18:31:54 [INFO]: Epoch 034 - training loss: 0.3779, validation loss: 0.5307
2024-06-02 18:31:57 [INFO]: Epoch 035 - training loss: 0.3762, validation loss: 0.5158
2024-06-02 18:31:59 [INFO]: Epoch 036 - training loss: 0.3763, validation loss: 0.5101
2024-06-02 18:32:02 [INFO]: Epoch 037 - training loss: 0.3754, validation loss: 0.5080
2024-06-02 18:32:04 [INFO]: Epoch 038 - training loss: 0.3747, validation loss: 0.5101
2024-06-02 18:32:07 [INFO]: Epoch 039 - training loss: 0.3736, validation loss: 0.4856
2024-06-02 18:32:09 [INFO]: Epoch 040 - training loss: 0.3717, validation loss: 0.4918
2024-06-02 18:32:11 [INFO]: Epoch 041 - training loss: 0.3709, validation loss: 0.4759
2024-06-02 18:32:14 [INFO]: Epoch 042 - training loss: 0.3712, validation loss: 0.4804
2024-06-02 18:32:16 [INFO]: Epoch 043 - training loss: 0.3713, validation loss: 0.4848
2024-06-02 18:32:19 [INFO]: Epoch 044 - training loss: 0.3709, validation loss: 0.4817
2024-06-02 18:32:21 [INFO]: Epoch 045 - training loss: 0.3679, validation loss: 0.4734
2024-06-02 18:32:23 [INFO]: Epoch 046 - training loss: 0.3668, validation loss: 0.4592
2024-06-02 18:32:25 [INFO]: Epoch 047 - training loss: 0.3673, validation loss: 0.4713
2024-06-02 18:32:28 [INFO]: Epoch 048 - training loss: 0.3653, validation loss: 0.4568
2024-06-02 18:32:30 [INFO]: Epoch 049 - training loss: 0.3653, validation loss: 0.4565
2024-06-02 18:32:33 [INFO]: Epoch 050 - training loss: 0.3630, validation loss: 0.4526
2024-06-02 18:32:35 [INFO]: Epoch 051 - training loss: 0.3633, validation loss: 0.4393
2024-06-02 18:32:37 [INFO]: Epoch 052 - training loss: 0.3607, validation loss: 0.4434
2024-06-02 18:32:40 [INFO]: Epoch 053 - training loss: 0.3604, validation loss: 0.4476
2024-06-02 18:32:42 [INFO]: Epoch 054 - training loss: 0.3592, validation loss: 0.4335
2024-06-02 18:32:45 [INFO]: Epoch 055 - training loss: 0.3586, validation loss: 0.4326
2024-06-02 18:32:47 [INFO]: Epoch 056 - training loss: 0.3573, validation loss: 0.4345
2024-06-02 18:32:50 [INFO]: Epoch 057 - training loss: 0.3579, validation loss: 0.4331
2024-06-02 18:32:52 [INFO]: Epoch 058 - training loss: 0.3562, validation loss: 0.4288
2024-06-02 18:32:54 [INFO]: Epoch 059 - training loss: 0.3564, validation loss: 0.4109
2024-06-02 18:32:57 [INFO]: Epoch 060 - training loss: 0.3566, validation loss: 0.4206
2024-06-02 18:32:59 [INFO]: Epoch 061 - training loss: 0.3559, validation loss: 0.4209
2024-06-02 18:33:02 [INFO]: Epoch 062 - training loss: 0.3560, validation loss: 0.4271
2024-06-02 18:33:04 [INFO]: Epoch 063 - training loss: 0.3562, validation loss: 0.4353
2024-06-02 18:33:07 [INFO]: Epoch 064 - training loss: 0.3549, validation loss: 0.4239
2024-06-02 18:33:09 [INFO]: Epoch 065 - training loss: 0.3547, validation loss: 0.4148
2024-06-02 18:33:12 [INFO]: Epoch 066 - training loss: 0.3534, validation loss: 0.4209
2024-06-02 18:33:14 [INFO]: Epoch 067 - training loss: 0.3517, validation loss: 0.4209
2024-06-02 18:33:17 [INFO]: Epoch 068 - training loss: 0.3522, validation loss: 0.4071
2024-06-02 18:33:19 [INFO]: Epoch 069 - training loss: 0.3520, validation loss: 0.4060
2024-06-02 18:33:22 [INFO]: Epoch 070 - training loss: 0.3528, validation loss: 0.4096
2024-06-02 18:33:24 [INFO]: Epoch 071 - training loss: 0.3516, validation loss: 0.4060
2024-06-02 18:33:27 [INFO]: Epoch 072 - training loss: 0.3518, validation loss: 0.4177
2024-06-02 18:33:29 [INFO]: Epoch 073 - training loss: 0.3511, validation loss: 0.4062
2024-06-02 18:33:32 [INFO]: Epoch 074 - training loss: 0.3517, validation loss: 0.3933
2024-06-02 18:33:34 [INFO]: Epoch 075 - training loss: 0.3509, validation loss: 0.3930
2024-06-02 18:33:37 [INFO]: Epoch 076 - training loss: 0.3510, validation loss: 0.4109
2024-06-02 18:33:39 [INFO]: Epoch 077 - training loss: 0.3514, validation loss: 0.4194
2024-06-02 18:33:42 [INFO]: Epoch 078 - training loss: 0.3497, validation loss: 0.3970
2024-06-02 18:33:44 [INFO]: Epoch 079 - training loss: 0.3494, validation loss: 0.4065
2024-06-02 18:33:46 [INFO]: Epoch 080 - training loss: 0.3495, validation loss: 0.4117
2024-06-02 18:33:49 [INFO]: Epoch 081 - training loss: 0.3490, validation loss: 0.4050
2024-06-02 18:33:51 [INFO]: Epoch 082 - training loss: 0.3491, validation loss: 0.4089
2024-06-02 18:33:53 [INFO]: Epoch 083 - training loss: 0.3494, validation loss: 0.4046
2024-06-02 18:33:56 [INFO]: Epoch 084 - training loss: 0.3491, validation loss: 0.4015
2024-06-02 18:33:59 [INFO]: Epoch 085 - training loss: 0.3489, validation loss: 0.3905
2024-06-02 18:34:01 [INFO]: Epoch 086 - training loss: 0.3498, validation loss: 0.3989
2024-06-02 18:34:03 [INFO]: Epoch 087 - training loss: 0.3494, validation loss: 0.4024
2024-06-02 18:34:06 [INFO]: Epoch 088 - training loss: 0.3489, validation loss: 0.4016
2024-06-02 18:34:08 [INFO]: Epoch 089 - training loss: 0.3491, validation loss: 0.3939
2024-06-02 18:34:10 [INFO]: Epoch 090 - training loss: 0.3481, validation loss: 0.4026
2024-06-02 18:34:13 [INFO]: Epoch 091 - training loss: 0.3484, validation loss: 0.3960
2024-06-02 18:34:15 [INFO]: Epoch 092 - training loss: 0.3484, validation loss: 0.4056
2024-06-02 18:34:17 [INFO]: Epoch 093 - training loss: 0.3500, validation loss: 0.4012
2024-06-02 18:34:20 [INFO]: Epoch 094 - training loss: 0.3477, validation loss: 0.3952
2024-06-02 18:34:22 [INFO]: Epoch 095 - training loss: 0.3472, validation loss: 0.3976
2024-06-02 18:34:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:34:22 [INFO]: Finished training. The best model is from epoch#85.
2024-06-02 18:34:22 [INFO]: Saved the model to results_point_rate01/Electricity/DLinear_Electricity/round_3/20240602_T183030/DLinear.pypots
2024-06-02 18:34:23 [INFO]: Successfully saved to results_point_rate01/Electricity/DLinear_Electricity/round_3/imputation.pkl
2024-06-02 18:34:23 [INFO]: Round3 - DLinear on Electricity: MAE=0.5190, MSE=0.4563, MRE=0.2776
2024-06-02 18:34:23 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 18:34:23 [INFO]: Using the given device: cuda:0
2024-06-02 18:34:23 [INFO]: Model files will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_4/20240602_T183423
2024-06-02 18:34:23 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/DLinear_Electricity/round_4/20240602_T183423/tensorboard
2024-06-02 18:34:23 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-02 18:34:26 [INFO]: Epoch 001 - training loss: 1.0536, validation loss: 2.8948
2024-06-02 18:34:28 [INFO]: Epoch 002 - training loss: 0.6726, validation loss: 2.6415
2024-06-02 18:34:31 [INFO]: Epoch 003 - training loss: 0.5959, validation loss: 2.4669
2024-06-02 18:34:33 [INFO]: Epoch 004 - training loss: 0.5521, validation loss: 2.3016
2024-06-02 18:34:36 [INFO]: Epoch 005 - training loss: 0.5234, validation loss: 2.1901
2024-06-02 18:34:38 [INFO]: Epoch 006 - training loss: 0.5021, validation loss: 2.0617
2024-06-02 18:34:41 [INFO]: Epoch 007 - training loss: 0.4852, validation loss: 1.9511
2024-06-02 18:34:43 [INFO]: Epoch 008 - training loss: 0.4735, validation loss: 1.8483
2024-06-02 18:34:46 [INFO]: Epoch 009 - training loss: 0.4588, validation loss: 1.7564
2024-06-02 18:34:48 [INFO]: Epoch 010 - training loss: 0.4477, validation loss: 1.6607
2024-06-02 18:34:50 [INFO]: Epoch 011 - training loss: 0.4419, validation loss: 1.5753
2024-06-02 18:34:53 [INFO]: Epoch 012 - training loss: 0.4315, validation loss: 1.4577
2024-06-02 18:34:55 [INFO]: Epoch 013 - training loss: 0.4259, validation loss: 1.3819
2024-06-02 18:34:58 [INFO]: Epoch 014 - training loss: 0.4210, validation loss: 1.2848
2024-06-02 18:35:00 [INFO]: Epoch 015 - training loss: 0.4154, validation loss: 1.1825
2024-06-02 18:35:03 [INFO]: Epoch 016 - training loss: 0.4113, validation loss: 1.1130
2024-06-02 18:35:05 [INFO]: Epoch 017 - training loss: 0.4084, validation loss: 1.0400
2024-06-02 18:35:08 [INFO]: Epoch 018 - training loss: 0.4055, validation loss: 0.9534
2024-06-02 18:35:10 [INFO]: Epoch 019 - training loss: 0.4031, validation loss: 0.9240
2024-06-02 18:35:12 [INFO]: Epoch 020 - training loss: 0.3970, validation loss: 0.8544
2024-06-02 18:35:15 [INFO]: Epoch 021 - training loss: 0.3963, validation loss: 0.7975
2024-06-02 18:35:17 [INFO]: Epoch 022 - training loss: 0.3945, validation loss: 0.7661
2024-06-02 18:35:20 [INFO]: Epoch 023 - training loss: 0.3945, validation loss: 0.7218
2024-06-02 18:35:22 [INFO]: Epoch 024 - training loss: 0.3893, validation loss: 0.6917
2024-06-02 18:35:25 [INFO]: Epoch 025 - training loss: 0.3876, validation loss: 0.6466
2024-06-02 18:35:27 [INFO]: Epoch 026 - training loss: 0.3867, validation loss: 0.6373
2024-06-02 18:35:30 [INFO]: Epoch 027 - training loss: 0.3846, validation loss: 0.5998
2024-06-02 18:35:32 [INFO]: Epoch 028 - training loss: 0.3835, validation loss: 0.5815
2024-06-02 18:35:34 [INFO]: Epoch 029 - training loss: 0.3826, validation loss: 0.5739
2024-06-02 18:35:37 [INFO]: Epoch 030 - training loss: 0.3826, validation loss: 0.5699
2024-06-02 18:35:39 [INFO]: Epoch 031 - training loss: 0.3815, validation loss: 0.5623
2024-06-02 18:35:42 [INFO]: Epoch 032 - training loss: 0.3810, validation loss: 0.5561
2024-06-02 18:35:44 [INFO]: Epoch 033 - training loss: 0.3795, validation loss: 0.5353
2024-06-02 18:35:47 [INFO]: Epoch 034 - training loss: 0.3787, validation loss: 0.5399
2024-06-02 18:35:49 [INFO]: Epoch 035 - training loss: 0.3791, validation loss: 0.5052
2024-06-02 18:35:52 [INFO]: Epoch 036 - training loss: 0.3773, validation loss: 0.5115
2024-06-02 18:35:54 [INFO]: Epoch 037 - training loss: 0.3751, validation loss: 0.5063
2024-06-02 18:35:57 [INFO]: Epoch 038 - training loss: 0.3737, validation loss: 0.4997
2024-06-02 18:35:59 [INFO]: Epoch 039 - training loss: 0.3730, validation loss: 0.4942
2024-06-02 18:36:01 [INFO]: Epoch 040 - training loss: 0.3732, validation loss: 0.4766
2024-06-02 18:36:04 [INFO]: Epoch 041 - training loss: 0.3717, validation loss: 0.4814
2024-06-02 18:36:06 [INFO]: Epoch 042 - training loss: 0.3714, validation loss: 0.4736
2024-06-02 18:36:09 [INFO]: Epoch 043 - training loss: 0.3702, validation loss: 0.4617
2024-06-02 18:36:11 [INFO]: Epoch 044 - training loss: 0.3692, validation loss: 0.4581
2024-06-02 18:36:14 [INFO]: Epoch 045 - training loss: 0.3677, validation loss: 0.4494
2024-06-02 18:36:16 [INFO]: Epoch 046 - training loss: 0.3676, validation loss: 0.4616
2024-06-02 18:36:18 [INFO]: Epoch 047 - training loss: 0.3665, validation loss: 0.4520
2024-06-02 18:36:21 [INFO]: Epoch 048 - training loss: 0.3659, validation loss: 0.4425
2024-06-02 18:36:23 [INFO]: Epoch 049 - training loss: 0.3663, validation loss: 0.4501
2024-06-02 18:36:26 [INFO]: Epoch 050 - training loss: 0.3637, validation loss: 0.4388
2024-06-02 18:36:28 [INFO]: Epoch 051 - training loss: 0.3616, validation loss: 0.4372
2024-06-02 18:36:31 [INFO]: Epoch 052 - training loss: 0.3599, validation loss: 0.4396
2024-06-02 18:36:33 [INFO]: Epoch 053 - training loss: 0.3592, validation loss: 0.4224
2024-06-02 18:36:36 [INFO]: Epoch 054 - training loss: 0.3580, validation loss: 0.4169
2024-06-02 18:36:38 [INFO]: Epoch 055 - training loss: 0.3577, validation loss: 0.4219
2024-06-02 18:36:40 [INFO]: Epoch 056 - training loss: 0.3570, validation loss: 0.4070
2024-06-02 18:36:43 [INFO]: Epoch 057 - training loss: 0.3563, validation loss: 0.4205
2024-06-02 18:36:45 [INFO]: Epoch 058 - training loss: 0.3551, validation loss: 0.4139
2024-06-02 18:36:48 [INFO]: Epoch 059 - training loss: 0.3559, validation loss: 0.4190
2024-06-02 18:36:50 [INFO]: Epoch 060 - training loss: 0.3557, validation loss: 0.4240
2024-06-02 18:36:52 [INFO]: Epoch 061 - training loss: 0.3552, validation loss: 0.4006
2024-06-02 18:36:55 [INFO]: Epoch 062 - training loss: 0.3552, validation loss: 0.4073
2024-06-02 18:36:57 [INFO]: Epoch 063 - training loss: 0.3540, validation loss: 0.4113
2024-06-02 18:37:00 [INFO]: Epoch 064 - training loss: 0.3538, validation loss: 0.4075
2024-06-02 18:37:02 [INFO]: Epoch 065 - training loss: 0.3532, validation loss: 0.4128
2024-06-02 18:37:04 [INFO]: Epoch 066 - training loss: 0.3540, validation loss: 0.4057
2024-06-02 18:37:07 [INFO]: Epoch 067 - training loss: 0.3522, validation loss: 0.4140
2024-06-02 18:37:09 [INFO]: Epoch 068 - training loss: 0.3528, validation loss: 0.4055
2024-06-02 18:37:11 [INFO]: Epoch 069 - training loss: 0.3520, validation loss: 0.4079
2024-06-02 18:37:14 [INFO]: Epoch 070 - training loss: 0.3519, validation loss: 0.4025
2024-06-02 18:37:16 [INFO]: Epoch 071 - training loss: 0.3517, validation loss: 0.4024
2024-06-02 18:37:16 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:37:16 [INFO]: Finished training. The best model is from epoch#61.
2024-06-02 18:37:16 [INFO]: Saved the model to results_point_rate01/Electricity/DLinear_Electricity/round_4/20240602_T183423/DLinear.pypots
2024-06-02 18:37:17 [INFO]: Successfully saved to results_point_rate01/Electricity/DLinear_Electricity/round_4/imputation.pkl
2024-06-02 18:37:17 [INFO]: Round4 - DLinear on Electricity: MAE=0.5182, MSE=0.4477, MRE=0.2772
2024-06-02 18:37:17 [INFO]: Done! Final results:
Averaged DLinear (2,294,692 params) on Electricity: MAE=0.5188 ± 0.007638080926915798, MSE=0.4550 ± 0.014772217684760058, MRE=0.2775 ± 0.0040860175890998236, average inference time=0.19
