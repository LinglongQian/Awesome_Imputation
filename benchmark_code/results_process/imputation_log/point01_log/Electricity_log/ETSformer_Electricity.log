2024-06-02 18:18:29 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 18:18:29 [INFO]: Using the given device: cuda:0
2024-06-02 18:18:29 [INFO]: Model files will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_0/20240602_T181829
2024-06-02 18:18:29 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_0/20240602_T181829/tensorboard
2024-06-02 18:18:30 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 10,518,266
2024-06-02 18:18:39 [INFO]: Epoch 001 - training loss: 0.8574, validation loss: 1.4208
2024-06-02 18:18:44 [INFO]: Epoch 002 - training loss: 0.6266, validation loss: 1.3734
2024-06-02 18:18:50 [INFO]: Epoch 003 - training loss: 0.5862, validation loss: 1.3399
2024-06-02 18:18:56 [INFO]: Epoch 004 - training loss: 0.5673, validation loss: 1.3268
2024-06-02 18:19:01 [INFO]: Epoch 005 - training loss: 0.5522, validation loss: 1.2994
2024-06-02 18:19:06 [INFO]: Epoch 006 - training loss: 0.5404, validation loss: 1.2768
2024-06-02 18:19:12 [INFO]: Epoch 007 - training loss: 0.5298, validation loss: 1.2547
2024-06-02 18:19:18 [INFO]: Epoch 008 - training loss: 0.5205, validation loss: 1.2225
2024-06-02 18:19:24 [INFO]: Epoch 009 - training loss: 0.5115, validation loss: 1.1931
2024-06-02 18:19:30 [INFO]: Epoch 010 - training loss: 0.5045, validation loss: 1.1750
2024-06-02 18:19:36 [INFO]: Epoch 011 - training loss: 0.4977, validation loss: 1.1504
2024-06-02 18:19:41 [INFO]: Epoch 012 - training loss: 0.4908, validation loss: 1.1238
2024-06-02 18:19:47 [INFO]: Epoch 013 - training loss: 0.4843, validation loss: 1.1102
2024-06-02 18:19:53 [INFO]: Epoch 014 - training loss: 0.4779, validation loss: 1.0877
2024-06-02 18:19:58 [INFO]: Epoch 015 - training loss: 0.4736, validation loss: 1.0726
2024-06-02 18:20:04 [INFO]: Epoch 016 - training loss: 0.4683, validation loss: 1.0589
2024-06-02 18:20:09 [INFO]: Epoch 017 - training loss: 0.4639, validation loss: 1.0432
2024-06-02 18:20:15 [INFO]: Epoch 018 - training loss: 0.4601, validation loss: 1.0240
2024-06-02 18:20:20 [INFO]: Epoch 019 - training loss: 0.4557, validation loss: 1.0162
2024-06-02 18:20:26 [INFO]: Epoch 020 - training loss: 0.4529, validation loss: 1.0034
2024-06-02 18:20:32 [INFO]: Epoch 021 - training loss: 0.4515, validation loss: 0.9855
2024-06-02 18:20:38 [INFO]: Epoch 022 - training loss: 0.4468, validation loss: 0.9694
2024-06-02 18:20:43 [INFO]: Epoch 023 - training loss: 0.4436, validation loss: 0.9563
2024-06-02 18:20:49 [INFO]: Epoch 024 - training loss: 0.4398, validation loss: 0.9378
2024-06-02 18:20:54 [INFO]: Epoch 025 - training loss: 0.4377, validation loss: 0.9317
2024-06-02 18:21:00 [INFO]: Epoch 026 - training loss: 0.4357, validation loss: 0.9122
2024-06-02 18:21:05 [INFO]: Epoch 027 - training loss: 0.4329, validation loss: 0.8964
2024-06-02 18:21:11 [INFO]: Epoch 028 - training loss: 0.4309, validation loss: 0.8858
2024-06-02 18:21:17 [INFO]: Epoch 029 - training loss: 0.4288, validation loss: 0.8734
2024-06-02 18:21:22 [INFO]: Epoch 030 - training loss: 0.4273, validation loss: 0.8618
2024-06-02 18:21:28 [INFO]: Epoch 031 - training loss: 0.4256, validation loss: 0.8536
2024-06-02 18:21:34 [INFO]: Epoch 032 - training loss: 0.4246, validation loss: 0.8405
2024-06-02 18:21:40 [INFO]: Epoch 033 - training loss: 0.4224, validation loss: 0.8329
2024-06-02 18:21:45 [INFO]: Epoch 034 - training loss: 0.4211, validation loss: 0.8209
2024-06-02 18:21:51 [INFO]: Epoch 035 - training loss: 0.4193, validation loss: 0.8158
2024-06-02 18:21:57 [INFO]: Epoch 036 - training loss: 0.4177, validation loss: 0.8074
2024-06-02 18:22:03 [INFO]: Epoch 037 - training loss: 0.4170, validation loss: 0.7949
2024-06-02 18:22:08 [INFO]: Epoch 038 - training loss: 0.4160, validation loss: 0.7899
2024-06-02 18:22:14 [INFO]: Epoch 039 - training loss: 0.4148, validation loss: 0.7867
2024-06-02 18:22:19 [INFO]: Epoch 040 - training loss: 0.4141, validation loss: 0.7765
2024-06-02 18:22:25 [INFO]: Epoch 041 - training loss: 0.4128, validation loss: 0.7653
2024-06-02 18:22:31 [INFO]: Epoch 042 - training loss: 0.4118, validation loss: 0.7633
2024-06-02 18:22:36 [INFO]: Epoch 043 - training loss: 0.4108, validation loss: 0.7575
2024-06-02 18:22:42 [INFO]: Epoch 044 - training loss: 0.4107, validation loss: 0.7553
2024-06-02 18:22:47 [INFO]: Epoch 045 - training loss: 0.4094, validation loss: 0.7444
2024-06-02 18:22:53 [INFO]: Epoch 046 - training loss: 0.4084, validation loss: 0.7395
2024-06-02 18:22:58 [INFO]: Epoch 047 - training loss: 0.4080, validation loss: 0.7350
2024-06-02 18:23:04 [INFO]: Epoch 048 - training loss: 0.4069, validation loss: 0.7306
2024-06-02 18:23:10 [INFO]: Epoch 049 - training loss: 0.4060, validation loss: 0.7234
2024-06-02 18:23:16 [INFO]: Epoch 050 - training loss: 0.4059, validation loss: 0.7242
2024-06-02 18:23:22 [INFO]: Epoch 051 - training loss: 0.4050, validation loss: 0.7157
2024-06-02 18:23:27 [INFO]: Epoch 052 - training loss: 0.4042, validation loss: 0.7148
2024-06-02 18:23:33 [INFO]: Epoch 053 - training loss: 0.4036, validation loss: 0.7120
2024-06-02 18:23:39 [INFO]: Epoch 054 - training loss: 0.4031, validation loss: 0.7157
2024-06-02 18:23:44 [INFO]: Epoch 055 - training loss: 0.4028, validation loss: 0.7142
2024-06-02 18:23:50 [INFO]: Epoch 056 - training loss: 0.4022, validation loss: 0.7051
2024-06-02 18:23:56 [INFO]: Epoch 057 - training loss: 0.4011, validation loss: 0.7024
2024-06-02 18:24:01 [INFO]: Epoch 058 - training loss: 0.4009, validation loss: 0.6971
2024-06-02 18:24:07 [INFO]: Epoch 059 - training loss: 0.4002, validation loss: 0.6983
2024-06-02 18:24:13 [INFO]: Epoch 060 - training loss: 0.3995, validation loss: 0.6955
2024-06-02 18:24:18 [INFO]: Epoch 061 - training loss: 0.3992, validation loss: 0.6953
2024-06-02 18:24:24 [INFO]: Epoch 062 - training loss: 0.3989, validation loss: 0.6927
2024-06-02 18:24:30 [INFO]: Epoch 063 - training loss: 0.3985, validation loss: 0.6869
2024-06-02 18:24:35 [INFO]: Epoch 064 - training loss: 0.3983, validation loss: 0.6847
2024-06-02 18:24:41 [INFO]: Epoch 065 - training loss: 0.3972, validation loss: 0.6825
2024-06-02 18:24:47 [INFO]: Epoch 066 - training loss: 0.3974, validation loss: 0.6835
2024-06-02 18:24:52 [INFO]: Epoch 067 - training loss: 0.3979, validation loss: 0.6787
2024-06-02 18:24:57 [INFO]: Epoch 068 - training loss: 0.3964, validation loss: 0.6789
2024-06-02 18:25:03 [INFO]: Epoch 069 - training loss: 0.3957, validation loss: 0.6757
2024-06-02 18:25:09 [INFO]: Epoch 070 - training loss: 0.3955, validation loss: 0.6760
2024-06-02 18:25:15 [INFO]: Epoch 071 - training loss: 0.3959, validation loss: 0.6757
2024-06-02 18:25:20 [INFO]: Epoch 072 - training loss: 0.3946, validation loss: 0.6772
2024-06-02 18:25:26 [INFO]: Epoch 073 - training loss: 0.3940, validation loss: 0.6717
2024-06-02 18:25:31 [INFO]: Epoch 074 - training loss: 0.3937, validation loss: 0.6680
2024-06-02 18:25:37 [INFO]: Epoch 075 - training loss: 0.3940, validation loss: 0.6646
2024-06-02 18:25:42 [INFO]: Epoch 076 - training loss: 0.3932, validation loss: 0.6684
2024-06-02 18:25:48 [INFO]: Epoch 077 - training loss: 0.3930, validation loss: 0.6670
2024-06-02 18:25:54 [INFO]: Epoch 078 - training loss: 0.3934, validation loss: 0.6683
2024-06-02 18:25:59 [INFO]: Epoch 079 - training loss: 0.3927, validation loss: 0.6636
2024-06-02 18:26:05 [INFO]: Epoch 080 - training loss: 0.3926, validation loss: 0.6607
2024-06-02 18:26:11 [INFO]: Epoch 081 - training loss: 0.3928, validation loss: 0.6644
2024-06-02 18:26:16 [INFO]: Epoch 082 - training loss: 0.3926, validation loss: 0.6615
2024-06-02 18:26:22 [INFO]: Epoch 083 - training loss: 0.3919, validation loss: 0.6645
2024-06-02 18:26:28 [INFO]: Epoch 084 - training loss: 0.3913, validation loss: 0.6583
2024-06-02 18:26:34 [INFO]: Epoch 085 - training loss: 0.3916, validation loss: 0.6617
2024-06-02 18:26:40 [INFO]: Epoch 086 - training loss: 0.3909, validation loss: 0.6576
2024-06-02 18:26:45 [INFO]: Epoch 087 - training loss: 0.3905, validation loss: 0.6528
2024-06-02 18:26:51 [INFO]: Epoch 088 - training loss: 0.3909, validation loss: 0.6542
2024-06-02 18:26:57 [INFO]: Epoch 089 - training loss: 0.3903, validation loss: 0.6569
2024-06-02 18:27:03 [INFO]: Epoch 090 - training loss: 0.3896, validation loss: 0.6467
2024-06-02 18:27:08 [INFO]: Epoch 091 - training loss: 0.3893, validation loss: 0.6524
2024-06-02 18:27:14 [INFO]: Epoch 092 - training loss: 0.3896, validation loss: 0.6566
2024-06-02 18:27:20 [INFO]: Epoch 093 - training loss: 0.3894, validation loss: 0.6540
2024-06-02 18:27:25 [INFO]: Epoch 094 - training loss: 0.3890, validation loss: 0.6495
2024-06-02 18:27:31 [INFO]: Epoch 095 - training loss: 0.3890, validation loss: 0.6481
2024-06-02 18:27:37 [INFO]: Epoch 096 - training loss: 0.3879, validation loss: 0.6507
2024-06-02 18:27:42 [INFO]: Epoch 097 - training loss: 0.3883, validation loss: 0.6505
2024-06-02 18:27:48 [INFO]: Epoch 098 - training loss: 0.3880, validation loss: 0.6553
2024-06-02 18:27:54 [INFO]: Epoch 099 - training loss: 0.3879, validation loss: 0.6504
2024-06-02 18:28:00 [INFO]: Epoch 100 - training loss: 0.3881, validation loss: 0.6495
2024-06-02 18:28:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:28:00 [INFO]: Finished training. The best model is from epoch#90.
2024-06-02 18:28:00 [INFO]: Saved the model to results_point_rate01/Electricity/ETSformer_Electricity/round_0/20240602_T181829/ETSformer.pypots
2024-06-02 18:28:01 [INFO]: Successfully saved to results_point_rate01/Electricity/ETSformer_Electricity/round_0/imputation.pkl
2024-06-02 18:28:01 [INFO]: Round0 - ETSformer on Electricity: MAE=0.4155, MSE=0.3501, MRE=0.2223
2024-06-02 18:28:01 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 18:28:01 [INFO]: Using the given device: cuda:0
2024-06-02 18:28:01 [INFO]: Model files will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_1/20240602_T182801
2024-06-02 18:28:01 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_1/20240602_T182801/tensorboard
2024-06-02 18:28:01 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 10,518,266
2024-06-02 18:28:07 [INFO]: Epoch 001 - training loss: 0.8500, validation loss: 1.4228
2024-06-02 18:28:13 [INFO]: Epoch 002 - training loss: 0.6241, validation loss: 1.3757
2024-06-02 18:28:19 [INFO]: Epoch 003 - training loss: 0.5832, validation loss: 1.3424
2024-06-02 18:28:24 [INFO]: Epoch 004 - training loss: 0.5631, validation loss: 1.3201
2024-06-02 18:28:29 [INFO]: Epoch 005 - training loss: 0.5489, validation loss: 1.2865
2024-06-02 18:28:35 [INFO]: Epoch 006 - training loss: 0.5366, validation loss: 1.2602
2024-06-02 18:28:41 [INFO]: Epoch 007 - training loss: 0.5260, validation loss: 1.2243
2024-06-02 18:28:47 [INFO]: Epoch 008 - training loss: 0.5167, validation loss: 1.2019
2024-06-02 18:28:52 [INFO]: Epoch 009 - training loss: 0.5083, validation loss: 1.1678
2024-06-02 18:28:58 [INFO]: Epoch 010 - training loss: 0.5007, validation loss: 1.1455
2024-06-02 18:29:04 [INFO]: Epoch 011 - training loss: 0.4930, validation loss: 1.1238
2024-06-02 18:29:09 [INFO]: Epoch 012 - training loss: 0.4861, validation loss: 1.0921
2024-06-02 18:29:15 [INFO]: Epoch 013 - training loss: 0.4808, validation loss: 1.0752
2024-06-02 18:29:21 [INFO]: Epoch 014 - training loss: 0.4763, validation loss: 1.0519
2024-06-02 18:29:26 [INFO]: Epoch 015 - training loss: 0.4701, validation loss: 1.0357
2024-06-02 18:29:32 [INFO]: Epoch 016 - training loss: 0.4660, validation loss: 1.0110
2024-06-02 18:29:38 [INFO]: Epoch 017 - training loss: 0.4615, validation loss: 0.9952
2024-06-02 18:29:43 [INFO]: Epoch 018 - training loss: 0.4579, validation loss: 0.9711
2024-06-02 18:29:49 [INFO]: Epoch 019 - training loss: 0.4535, validation loss: 0.9558
2024-06-02 18:29:54 [INFO]: Epoch 020 - training loss: 0.4507, validation loss: 0.9422
2024-06-02 18:30:00 [INFO]: Epoch 021 - training loss: 0.4471, validation loss: 0.9239
2024-06-02 18:30:06 [INFO]: Epoch 022 - training loss: 0.4443, validation loss: 0.9108
2024-06-02 18:30:11 [INFO]: Epoch 023 - training loss: 0.4407, validation loss: 0.8934
2024-06-02 18:30:16 [INFO]: Epoch 024 - training loss: 0.4385, validation loss: 0.8769
2024-06-02 18:30:22 [INFO]: Epoch 025 - training loss: 0.4354, validation loss: 0.8655
2024-06-02 18:30:28 [INFO]: Epoch 026 - training loss: 0.4338, validation loss: 0.8525
2024-06-02 18:30:34 [INFO]: Epoch 027 - training loss: 0.4317, validation loss: 0.8344
2024-06-02 18:30:40 [INFO]: Epoch 028 - training loss: 0.4295, validation loss: 0.8229
2024-06-02 18:30:45 [INFO]: Epoch 029 - training loss: 0.4279, validation loss: 0.8121
2024-06-02 18:30:51 [INFO]: Epoch 030 - training loss: 0.4262, validation loss: 0.7967
2024-06-02 18:30:57 [INFO]: Epoch 031 - training loss: 0.4247, validation loss: 0.7894
2024-06-02 18:31:02 [INFO]: Epoch 032 - training loss: 0.4232, validation loss: 0.7756
2024-06-02 18:31:08 [INFO]: Epoch 033 - training loss: 0.4218, validation loss: 0.7718
2024-06-02 18:31:13 [INFO]: Epoch 034 - training loss: 0.4204, validation loss: 0.7572
2024-06-02 18:31:19 [INFO]: Epoch 035 - training loss: 0.4186, validation loss: 0.7439
2024-06-02 18:31:24 [INFO]: Epoch 036 - training loss: 0.4177, validation loss: 0.7480
2024-06-02 18:31:30 [INFO]: Epoch 037 - training loss: 0.4165, validation loss: 0.7308
2024-06-02 18:31:35 [INFO]: Epoch 038 - training loss: 0.4153, validation loss: 0.7257
2024-06-02 18:31:41 [INFO]: Epoch 039 - training loss: 0.4143, validation loss: 0.7197
2024-06-02 18:31:46 [INFO]: Epoch 040 - training loss: 0.4133, validation loss: 0.7174
2024-06-02 18:31:52 [INFO]: Epoch 041 - training loss: 0.4125, validation loss: 0.7004
2024-06-02 18:31:57 [INFO]: Epoch 042 - training loss: 0.4101, validation loss: 0.6965
2024-06-02 18:32:03 [INFO]: Epoch 043 - training loss: 0.4096, validation loss: 0.6920
2024-06-02 18:32:08 [INFO]: Epoch 044 - training loss: 0.4090, validation loss: 0.6892
2024-06-02 18:32:14 [INFO]: Epoch 045 - training loss: 0.4080, validation loss: 0.6819
2024-06-02 18:32:19 [INFO]: Epoch 046 - training loss: 0.4071, validation loss: 0.6767
2024-06-02 18:32:25 [INFO]: Epoch 047 - training loss: 0.4062, validation loss: 0.6750
2024-06-02 18:32:30 [INFO]: Epoch 048 - training loss: 0.4056, validation loss: 0.6682
2024-06-02 18:32:36 [INFO]: Epoch 049 - training loss: 0.4047, validation loss: 0.6701
2024-06-02 18:32:41 [INFO]: Epoch 050 - training loss: 0.4043, validation loss: 0.6697
2024-06-02 18:32:47 [INFO]: Epoch 051 - training loss: 0.4038, validation loss: 0.6661
2024-06-02 18:32:53 [INFO]: Epoch 052 - training loss: 0.4031, validation loss: 0.6636
2024-06-02 18:32:59 [INFO]: Epoch 053 - training loss: 0.4031, validation loss: 0.6664
2024-06-02 18:33:04 [INFO]: Epoch 054 - training loss: 0.4019, validation loss: 0.6645
2024-06-02 18:33:10 [INFO]: Epoch 055 - training loss: 0.4015, validation loss: 0.6579
2024-06-02 18:33:16 [INFO]: Epoch 056 - training loss: 0.4007, validation loss: 0.6549
2024-06-02 18:33:21 [INFO]: Epoch 057 - training loss: 0.4000, validation loss: 0.6527
2024-06-02 18:33:27 [INFO]: Epoch 058 - training loss: 0.3995, validation loss: 0.6529
2024-06-02 18:33:33 [INFO]: Epoch 059 - training loss: 0.3991, validation loss: 0.6527
2024-06-02 18:33:39 [INFO]: Epoch 060 - training loss: 0.3986, validation loss: 0.6531
2024-06-02 18:33:44 [INFO]: Epoch 061 - training loss: 0.3975, validation loss: 0.6471
2024-06-02 18:33:50 [INFO]: Epoch 062 - training loss: 0.3983, validation loss: 0.6441
2024-06-02 18:33:56 [INFO]: Epoch 063 - training loss: 0.3973, validation loss: 0.6450
2024-06-02 18:34:01 [INFO]: Epoch 064 - training loss: 0.3970, validation loss: 0.6464
2024-06-02 18:34:07 [INFO]: Epoch 065 - training loss: 0.3968, validation loss: 0.6453
2024-06-02 18:34:13 [INFO]: Epoch 066 - training loss: 0.3959, validation loss: 0.6491
2024-06-02 18:34:18 [INFO]: Epoch 067 - training loss: 0.3961, validation loss: 0.6464
2024-06-02 18:34:24 [INFO]: Epoch 068 - training loss: 0.3951, validation loss: 0.6441
2024-06-02 18:34:30 [INFO]: Epoch 069 - training loss: 0.3947, validation loss: 0.6462
2024-06-02 18:34:35 [INFO]: Epoch 070 - training loss: 0.3945, validation loss: 0.6388
2024-06-02 18:34:41 [INFO]: Epoch 071 - training loss: 0.3938, validation loss: 0.6384
2024-06-02 18:34:47 [INFO]: Epoch 072 - training loss: 0.3934, validation loss: 0.6395
2024-06-02 18:34:52 [INFO]: Epoch 073 - training loss: 0.3937, validation loss: 0.6427
2024-06-02 18:34:57 [INFO]: Epoch 074 - training loss: 0.3935, validation loss: 0.6425
2024-06-02 18:35:03 [INFO]: Epoch 075 - training loss: 0.3928, validation loss: 0.6378
2024-06-02 18:35:08 [INFO]: Epoch 076 - training loss: 0.3928, validation loss: 0.6357
2024-06-02 18:35:14 [INFO]: Epoch 077 - training loss: 0.3921, validation loss: 0.6305
2024-06-02 18:35:19 [INFO]: Epoch 078 - training loss: 0.3917, validation loss: 0.6360
2024-06-02 18:35:25 [INFO]: Epoch 079 - training loss: 0.3918, validation loss: 0.6378
2024-06-02 18:35:31 [INFO]: Epoch 080 - training loss: 0.3914, validation loss: 0.6360
2024-06-02 18:35:36 [INFO]: Epoch 081 - training loss: 0.3909, validation loss: 0.6367
2024-06-02 18:35:42 [INFO]: Epoch 082 - training loss: 0.3901, validation loss: 0.6350
2024-06-02 18:35:48 [INFO]: Epoch 083 - training loss: 0.3903, validation loss: 0.6377
2024-06-02 18:35:53 [INFO]: Epoch 084 - training loss: 0.3906, validation loss: 0.6369
2024-06-02 18:35:58 [INFO]: Epoch 085 - training loss: 0.3906, validation loss: 0.6298
2024-06-02 18:36:04 [INFO]: Epoch 086 - training loss: 0.3898, validation loss: 0.6313
2024-06-02 18:36:10 [INFO]: Epoch 087 - training loss: 0.3896, validation loss: 0.6336
2024-06-02 18:36:15 [INFO]: Epoch 088 - training loss: 0.3895, validation loss: 0.6309
2024-06-02 18:36:21 [INFO]: Epoch 089 - training loss: 0.3889, validation loss: 0.6296
2024-06-02 18:36:26 [INFO]: Epoch 090 - training loss: 0.3887, validation loss: 0.6321
2024-06-02 18:36:32 [INFO]: Epoch 091 - training loss: 0.3889, validation loss: 0.6350
2024-06-02 18:36:37 [INFO]: Epoch 092 - training loss: 0.3880, validation loss: 0.6286
2024-06-02 18:36:43 [INFO]: Epoch 093 - training loss: 0.3879, validation loss: 0.6295
2024-06-02 18:36:49 [INFO]: Epoch 094 - training loss: 0.3880, validation loss: 0.6302
2024-06-02 18:36:54 [INFO]: Epoch 095 - training loss: 0.3880, validation loss: 0.6333
2024-06-02 18:37:00 [INFO]: Epoch 096 - training loss: 0.3873, validation loss: 0.6312
2024-06-02 18:37:06 [INFO]: Epoch 097 - training loss: 0.3866, validation loss: 0.6327
2024-06-02 18:37:12 [INFO]: Epoch 098 - training loss: 0.3865, validation loss: 0.6309
2024-06-02 18:37:18 [INFO]: Epoch 099 - training loss: 0.3867, validation loss: 0.6297
2024-06-02 18:37:23 [INFO]: Epoch 100 - training loss: 0.3869, validation loss: 0.6276
2024-06-02 18:37:23 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 18:37:23 [INFO]: Saved the model to results_point_rate01/Electricity/ETSformer_Electricity/round_1/20240602_T182801/ETSformer.pypots
2024-06-02 18:37:24 [INFO]: Successfully saved to results_point_rate01/Electricity/ETSformer_Electricity/round_1/imputation.pkl
2024-06-02 18:37:24 [INFO]: Round1 - ETSformer on Electricity: MAE=0.4130, MSE=0.3502, MRE=0.2209
2024-06-02 18:37:24 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 18:37:24 [INFO]: Using the given device: cuda:0
2024-06-02 18:37:24 [INFO]: Model files will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_2/20240602_T183724
2024-06-02 18:37:24 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_2/20240602_T183724/tensorboard
2024-06-02 18:37:24 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 10,518,266
2024-06-02 18:37:30 [INFO]: Epoch 001 - training loss: 0.8626, validation loss: 1.4103
2024-06-02 18:37:35 [INFO]: Epoch 002 - training loss: 0.6284, validation loss: 1.3700
2024-06-02 18:37:39 [INFO]: Epoch 003 - training loss: 0.5876, validation loss: 1.3404
2024-06-02 18:37:44 [INFO]: Epoch 004 - training loss: 0.5667, validation loss: 1.3237
2024-06-02 18:37:49 [INFO]: Epoch 005 - training loss: 0.5514, validation loss: 1.3043
2024-06-02 18:37:53 [INFO]: Epoch 006 - training loss: 0.5403, validation loss: 1.2767
2024-06-02 18:37:58 [INFO]: Epoch 007 - training loss: 0.5296, validation loss: 1.2506
2024-06-02 18:38:04 [INFO]: Epoch 008 - training loss: 0.5196, validation loss: 1.2272
2024-06-02 18:38:08 [INFO]: Epoch 009 - training loss: 0.5116, validation loss: 1.2031
2024-06-02 18:38:13 [INFO]: Epoch 010 - training loss: 0.5030, validation loss: 1.1835
2024-06-02 18:38:17 [INFO]: Epoch 011 - training loss: 0.4969, validation loss: 1.1640
2024-06-02 18:38:22 [INFO]: Epoch 012 - training loss: 0.4898, validation loss: 1.1426
2024-06-02 18:38:27 [INFO]: Epoch 013 - training loss: 0.4846, validation loss: 1.1204
2024-06-02 18:38:32 [INFO]: Epoch 014 - training loss: 0.4786, validation loss: 1.0994
2024-06-02 18:38:37 [INFO]: Epoch 015 - training loss: 0.4737, validation loss: 1.0956
2024-06-02 18:38:42 [INFO]: Epoch 016 - training loss: 0.4693, validation loss: 1.0718
2024-06-02 18:38:47 [INFO]: Epoch 017 - training loss: 0.4655, validation loss: 1.0590
2024-06-02 18:38:51 [INFO]: Epoch 018 - training loss: 0.4608, validation loss: 1.0358
2024-06-02 18:38:57 [INFO]: Epoch 019 - training loss: 0.4567, validation loss: 1.0203
2024-06-02 18:39:01 [INFO]: Epoch 020 - training loss: 0.4534, validation loss: 1.0082
2024-06-02 18:39:05 [INFO]: Epoch 021 - training loss: 0.4506, validation loss: 0.9891
2024-06-02 18:39:10 [INFO]: Epoch 022 - training loss: 0.4472, validation loss: 0.9787
2024-06-02 18:39:15 [INFO]: Epoch 023 - training loss: 0.4450, validation loss: 0.9584
2024-06-02 18:39:21 [INFO]: Epoch 024 - training loss: 0.4422, validation loss: 0.9483
2024-06-02 18:39:26 [INFO]: Epoch 025 - training loss: 0.4401, validation loss: 0.9397
2024-06-02 18:39:32 [INFO]: Epoch 026 - training loss: 0.4372, validation loss: 0.9252
2024-06-02 18:39:37 [INFO]: Epoch 027 - training loss: 0.4347, validation loss: 0.9059
2024-06-02 18:39:41 [INFO]: Epoch 028 - training loss: 0.4328, validation loss: 0.8959
2024-06-02 18:39:46 [INFO]: Epoch 029 - training loss: 0.4310, validation loss: 0.8853
2024-06-02 18:39:51 [INFO]: Epoch 030 - training loss: 0.4287, validation loss: 0.8773
2024-06-02 18:39:56 [INFO]: Epoch 031 - training loss: 0.4268, validation loss: 0.8584
2024-06-02 18:40:01 [INFO]: Epoch 032 - training loss: 0.4254, validation loss: 0.8503
2024-06-02 18:40:06 [INFO]: Epoch 033 - training loss: 0.4235, validation loss: 0.8418
2024-06-02 18:40:11 [INFO]: Epoch 034 - training loss: 0.4224, validation loss: 0.8304
2024-06-02 18:40:16 [INFO]: Epoch 035 - training loss: 0.4207, validation loss: 0.8113
2024-06-02 18:40:21 [INFO]: Epoch 036 - training loss: 0.4191, validation loss: 0.8110
2024-06-02 18:40:25 [INFO]: Epoch 037 - training loss: 0.4182, validation loss: 0.8010
2024-06-02 18:40:30 [INFO]: Epoch 038 - training loss: 0.4175, validation loss: 0.7969
2024-06-02 18:40:36 [INFO]: Epoch 039 - training loss: 0.4158, validation loss: 0.7885
2024-06-02 18:40:41 [INFO]: Epoch 040 - training loss: 0.4150, validation loss: 0.7795
2024-06-02 18:40:46 [INFO]: Epoch 041 - training loss: 0.4135, validation loss: 0.7670
2024-06-02 18:40:50 [INFO]: Epoch 042 - training loss: 0.4123, validation loss: 0.7665
2024-06-02 18:40:55 [INFO]: Epoch 043 - training loss: 0.4115, validation loss: 0.7558
2024-06-02 18:41:00 [INFO]: Epoch 044 - training loss: 0.4105, validation loss: 0.7507
2024-06-02 18:41:04 [INFO]: Epoch 045 - training loss: 0.4096, validation loss: 0.7477
2024-06-02 18:41:10 [INFO]: Epoch 046 - training loss: 0.4088, validation loss: 0.7397
2024-06-02 18:41:15 [INFO]: Epoch 047 - training loss: 0.4077, validation loss: 0.7350
2024-06-02 18:41:20 [INFO]: Epoch 048 - training loss: 0.4071, validation loss: 0.7236
2024-06-02 18:41:24 [INFO]: Epoch 049 - training loss: 0.4066, validation loss: 0.7212
2024-06-02 18:41:30 [INFO]: Epoch 050 - training loss: 0.4069, validation loss: 0.7177
2024-06-02 18:41:35 [INFO]: Epoch 051 - training loss: 0.4056, validation loss: 0.7194
2024-06-02 18:41:39 [INFO]: Epoch 052 - training loss: 0.4049, validation loss: 0.7069
2024-06-02 18:41:44 [INFO]: Epoch 053 - training loss: 0.4036, validation loss: 0.7016
2024-06-02 18:41:49 [INFO]: Epoch 054 - training loss: 0.4031, validation loss: 0.7002
2024-06-02 18:41:53 [INFO]: Epoch 055 - training loss: 0.4022, validation loss: 0.7007
2024-06-02 18:41:58 [INFO]: Epoch 056 - training loss: 0.4018, validation loss: 0.6951
2024-06-02 18:42:03 [INFO]: Epoch 057 - training loss: 0.4012, validation loss: 0.6892
2024-06-02 18:42:07 [INFO]: Epoch 058 - training loss: 0.4008, validation loss: 0.6872
2024-06-02 18:42:12 [INFO]: Epoch 059 - training loss: 0.4005, validation loss: 0.6846
2024-06-02 18:42:17 [INFO]: Epoch 060 - training loss: 0.4002, validation loss: 0.6787
2024-06-02 18:42:22 [INFO]: Epoch 061 - training loss: 0.4004, validation loss: 0.6881
2024-06-02 18:42:27 [INFO]: Epoch 062 - training loss: 0.3988, validation loss: 0.6755
2024-06-02 18:42:32 [INFO]: Epoch 063 - training loss: 0.3979, validation loss: 0.6682
2024-06-02 18:42:37 [INFO]: Epoch 064 - training loss: 0.3975, validation loss: 0.6703
2024-06-02 18:42:42 [INFO]: Epoch 065 - training loss: 0.3970, validation loss: 0.6665
2024-06-02 18:42:46 [INFO]: Epoch 066 - training loss: 0.3966, validation loss: 0.6704
2024-06-02 18:42:50 [INFO]: Epoch 067 - training loss: 0.3964, validation loss: 0.6630
2024-06-02 18:42:55 [INFO]: Epoch 068 - training loss: 0.3960, validation loss: 0.6620
2024-06-02 18:43:00 [INFO]: Epoch 069 - training loss: 0.3960, validation loss: 0.6572
2024-06-02 18:43:05 [INFO]: Epoch 070 - training loss: 0.3957, validation loss: 0.6591
2024-06-02 18:43:09 [INFO]: Epoch 071 - training loss: 0.3945, validation loss: 0.6506
2024-06-02 18:43:13 [INFO]: Epoch 072 - training loss: 0.3942, validation loss: 0.6507
2024-06-02 18:43:18 [INFO]: Epoch 073 - training loss: 0.3941, validation loss: 0.6500
2024-06-02 18:43:24 [INFO]: Epoch 074 - training loss: 0.3935, validation loss: 0.6453
2024-06-02 18:43:28 [INFO]: Epoch 075 - training loss: 0.3932, validation loss: 0.6446
2024-06-02 18:43:33 [INFO]: Epoch 076 - training loss: 0.3928, validation loss: 0.6449
2024-06-02 18:43:39 [INFO]: Epoch 077 - training loss: 0.3921, validation loss: 0.6418
2024-06-02 18:43:44 [INFO]: Epoch 078 - training loss: 0.3924, validation loss: 0.6415
2024-06-02 18:43:49 [INFO]: Epoch 079 - training loss: 0.3924, validation loss: 0.6414
2024-06-02 18:43:54 [INFO]: Epoch 080 - training loss: 0.3924, validation loss: 0.6382
2024-06-02 18:43:58 [INFO]: Epoch 081 - training loss: 0.3922, validation loss: 0.6364
2024-06-02 18:44:03 [INFO]: Epoch 082 - training loss: 0.3913, validation loss: 0.6315
2024-06-02 18:44:08 [INFO]: Epoch 083 - training loss: 0.3907, validation loss: 0.6331
2024-06-02 18:44:13 [INFO]: Epoch 084 - training loss: 0.3911, validation loss: 0.6299
2024-06-02 18:44:17 [INFO]: Epoch 085 - training loss: 0.3902, validation loss: 0.6283
2024-06-02 18:44:22 [INFO]: Epoch 086 - training loss: 0.3908, validation loss: 0.6309
2024-06-02 18:44:27 [INFO]: Epoch 087 - training loss: 0.3902, validation loss: 0.6330
2024-06-02 18:44:32 [INFO]: Epoch 088 - training loss: 0.3899, validation loss: 0.6293
2024-06-02 18:44:36 [INFO]: Epoch 089 - training loss: 0.3892, validation loss: 0.6281
2024-06-02 18:44:41 [INFO]: Epoch 090 - training loss: 0.3897, validation loss: 0.6242
2024-06-02 18:44:47 [INFO]: Epoch 091 - training loss: 0.3889, validation loss: 0.6239
2024-06-02 18:44:52 [INFO]: Epoch 092 - training loss: 0.3891, validation loss: 0.6186
2024-06-02 18:44:56 [INFO]: Epoch 093 - training loss: 0.3884, validation loss: 0.6211
2024-06-02 18:45:01 [INFO]: Epoch 094 - training loss: 0.3881, validation loss: 0.6188
2024-06-02 18:45:06 [INFO]: Epoch 095 - training loss: 0.3876, validation loss: 0.6233
2024-06-02 18:45:11 [INFO]: Epoch 096 - training loss: 0.3883, validation loss: 0.6200
2024-06-02 18:45:16 [INFO]: Epoch 097 - training loss: 0.3879, validation loss: 0.6186
2024-06-02 18:45:22 [INFO]: Epoch 098 - training loss: 0.3878, validation loss: 0.6242
2024-06-02 18:45:27 [INFO]: Epoch 099 - training loss: 0.3872, validation loss: 0.6234
2024-06-02 18:45:32 [INFO]: Epoch 100 - training loss: 0.3868, validation loss: 0.6173
2024-06-02 18:45:32 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 18:45:32 [INFO]: Saved the model to results_point_rate01/Electricity/ETSformer_Electricity/round_2/20240602_T183724/ETSformer.pypots
2024-06-02 18:45:33 [INFO]: Successfully saved to results_point_rate01/Electricity/ETSformer_Electricity/round_2/imputation.pkl
2024-06-02 18:45:33 [INFO]: Round2 - ETSformer on Electricity: MAE=0.4037, MSE=0.3350, MRE=0.2159
2024-06-02 18:45:33 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 18:45:33 [INFO]: Using the given device: cuda:0
2024-06-02 18:45:33 [INFO]: Model files will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_3/20240602_T184533
2024-06-02 18:45:33 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_3/20240602_T184533/tensorboard
2024-06-02 18:45:33 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 10,518,266
2024-06-02 18:45:38 [INFO]: Epoch 001 - training loss: 0.8589, validation loss: 1.3620
2024-06-02 18:45:43 [INFO]: Epoch 002 - training loss: 0.6273, validation loss: 1.3090
2024-06-02 18:45:48 [INFO]: Epoch 003 - training loss: 0.5864, validation loss: 1.2764
2024-06-02 18:45:53 [INFO]: Epoch 004 - training loss: 0.5661, validation loss: 1.2500
2024-06-02 18:45:58 [INFO]: Epoch 005 - training loss: 0.5500, validation loss: 1.2314
2024-06-02 18:46:02 [INFO]: Epoch 006 - training loss: 0.5374, validation loss: 1.2031
2024-06-02 18:46:07 [INFO]: Epoch 007 - training loss: 0.5271, validation loss: 1.1750
2024-06-02 18:46:12 [INFO]: Epoch 008 - training loss: 0.5177, validation loss: 1.1455
2024-06-02 18:46:18 [INFO]: Epoch 009 - training loss: 0.5092, validation loss: 1.1233
2024-06-02 18:46:23 [INFO]: Epoch 010 - training loss: 0.5018, validation loss: 1.1004
2024-06-02 18:46:28 [INFO]: Epoch 011 - training loss: 0.4950, validation loss: 1.0807
2024-06-02 18:46:33 [INFO]: Epoch 012 - training loss: 0.4884, validation loss: 1.0648
2024-06-02 18:46:38 [INFO]: Epoch 013 - training loss: 0.4829, validation loss: 1.0422
2024-06-02 18:46:43 [INFO]: Epoch 014 - training loss: 0.4769, validation loss: 1.0206
2024-06-02 18:46:47 [INFO]: Epoch 015 - training loss: 0.4723, validation loss: 1.0022
2024-06-02 18:46:52 [INFO]: Epoch 016 - training loss: 0.4670, validation loss: 0.9758
2024-06-02 18:46:56 [INFO]: Epoch 017 - training loss: 0.4634, validation loss: 0.9675
2024-06-02 18:47:02 [INFO]: Epoch 018 - training loss: 0.4587, validation loss: 0.9490
2024-06-02 18:47:06 [INFO]: Epoch 019 - training loss: 0.4554, validation loss: 0.9376
2024-06-02 18:47:11 [INFO]: Epoch 020 - training loss: 0.4517, validation loss: 0.9193
2024-06-02 18:47:15 [INFO]: Epoch 021 - training loss: 0.4485, validation loss: 0.9077
2024-06-02 18:47:20 [INFO]: Epoch 022 - training loss: 0.4460, validation loss: 0.9010
2024-06-02 18:47:25 [INFO]: Epoch 023 - training loss: 0.4430, validation loss: 0.8843
2024-06-02 18:47:29 [INFO]: Epoch 024 - training loss: 0.4404, validation loss: 0.8737
2024-06-02 18:47:35 [INFO]: Epoch 025 - training loss: 0.4376, validation loss: 0.8588
2024-06-02 18:47:40 [INFO]: Epoch 026 - training loss: 0.4353, validation loss: 0.8454
2024-06-02 18:47:46 [INFO]: Epoch 027 - training loss: 0.4331, validation loss: 0.8387
2024-06-02 18:47:51 [INFO]: Epoch 028 - training loss: 0.4307, validation loss: 0.8319
2024-06-02 18:47:55 [INFO]: Epoch 029 - training loss: 0.4291, validation loss: 0.8078
2024-06-02 18:48:00 [INFO]: Epoch 030 - training loss: 0.4270, validation loss: 0.7984
2024-06-02 18:48:04 [INFO]: Epoch 031 - training loss: 0.4251, validation loss: 0.7960
2024-06-02 18:48:08 [INFO]: Epoch 032 - training loss: 0.4240, validation loss: 0.7827
2024-06-02 18:48:13 [INFO]: Epoch 033 - training loss: 0.4213, validation loss: 0.7797
2024-06-02 18:48:16 [INFO]: Epoch 034 - training loss: 0.4200, validation loss: 0.7640
2024-06-02 18:48:19 [INFO]: Epoch 035 - training loss: 0.4196, validation loss: 0.7640
2024-06-02 18:48:23 [INFO]: Epoch 036 - training loss: 0.4177, validation loss: 0.7541
2024-06-02 18:48:26 [INFO]: Epoch 037 - training loss: 0.4164, validation loss: 0.7434
2024-06-02 18:48:29 [INFO]: Epoch 038 - training loss: 0.4149, validation loss: 0.7440
2024-06-02 18:48:32 [INFO]: Epoch 039 - training loss: 0.4143, validation loss: 0.7331
2024-06-02 18:48:35 [INFO]: Epoch 040 - training loss: 0.4135, validation loss: 0.7250
2024-06-02 18:48:39 [INFO]: Epoch 041 - training loss: 0.4121, validation loss: 0.7198
2024-06-02 18:48:42 [INFO]: Epoch 042 - training loss: 0.4106, validation loss: 0.7188
2024-06-02 18:48:45 [INFO]: Epoch 043 - training loss: 0.4099, validation loss: 0.7126
2024-06-02 18:48:48 [INFO]: Epoch 044 - training loss: 0.4094, validation loss: 0.7091
2024-06-02 18:48:51 [INFO]: Epoch 045 - training loss: 0.4083, validation loss: 0.7053
2024-06-02 18:48:54 [INFO]: Epoch 046 - training loss: 0.4075, validation loss: 0.7021
2024-06-02 18:48:57 [INFO]: Epoch 047 - training loss: 0.4061, validation loss: 0.6936
2024-06-02 18:49:01 [INFO]: Epoch 048 - training loss: 0.4054, validation loss: 0.6910
2024-06-02 18:49:04 [INFO]: Epoch 049 - training loss: 0.4047, validation loss: 0.6904
2024-06-02 18:49:07 [INFO]: Epoch 050 - training loss: 0.4038, validation loss: 0.6850
2024-06-02 18:49:10 [INFO]: Epoch 051 - training loss: 0.4039, validation loss: 0.6824
2024-06-02 18:49:14 [INFO]: Epoch 052 - training loss: 0.4033, validation loss: 0.6782
2024-06-02 18:49:17 [INFO]: Epoch 053 - training loss: 0.4019, validation loss: 0.6785
2024-06-02 18:49:20 [INFO]: Epoch 054 - training loss: 0.4017, validation loss: 0.6690
2024-06-02 18:49:23 [INFO]: Epoch 055 - training loss: 0.4017, validation loss: 0.6677
2024-06-02 18:49:26 [INFO]: Epoch 056 - training loss: 0.4003, validation loss: 0.6731
2024-06-02 18:49:29 [INFO]: Epoch 057 - training loss: 0.4000, validation loss: 0.6621
2024-06-02 18:49:32 [INFO]: Epoch 058 - training loss: 0.3994, validation loss: 0.6655
2024-06-02 18:49:36 [INFO]: Epoch 059 - training loss: 0.3983, validation loss: 0.6562
2024-06-02 18:49:39 [INFO]: Epoch 060 - training loss: 0.3991, validation loss: 0.6598
2024-06-02 18:49:42 [INFO]: Epoch 061 - training loss: 0.3980, validation loss: 0.6616
2024-06-02 18:49:45 [INFO]: Epoch 062 - training loss: 0.3978, validation loss: 0.6595
2024-06-02 18:49:48 [INFO]: Epoch 063 - training loss: 0.3972, validation loss: 0.6535
2024-06-02 18:49:52 [INFO]: Epoch 064 - training loss: 0.3965, validation loss: 0.6542
2024-06-02 18:49:55 [INFO]: Epoch 065 - training loss: 0.3961, validation loss: 0.6497
2024-06-02 18:49:58 [INFO]: Epoch 066 - training loss: 0.3961, validation loss: 0.6551
2024-06-02 18:50:01 [INFO]: Epoch 067 - training loss: 0.3951, validation loss: 0.6562
2024-06-02 18:50:04 [INFO]: Epoch 068 - training loss: 0.3951, validation loss: 0.6469
2024-06-02 18:50:07 [INFO]: Epoch 069 - training loss: 0.3941, validation loss: 0.6549
2024-06-02 18:50:10 [INFO]: Epoch 070 - training loss: 0.3941, validation loss: 0.6496
2024-06-02 18:50:14 [INFO]: Epoch 071 - training loss: 0.3932, validation loss: 0.6466
2024-06-02 18:50:17 [INFO]: Epoch 072 - training loss: 0.3934, validation loss: 0.6440
2024-06-02 18:50:20 [INFO]: Epoch 073 - training loss: 0.3929, validation loss: 0.6426
2024-06-02 18:50:23 [INFO]: Epoch 074 - training loss: 0.3924, validation loss: 0.6448
2024-06-02 18:50:26 [INFO]: Epoch 075 - training loss: 0.3920, validation loss: 0.6396
2024-06-02 18:50:30 [INFO]: Epoch 076 - training loss: 0.3923, validation loss: 0.6534
2024-06-02 18:50:33 [INFO]: Epoch 077 - training loss: 0.3914, validation loss: 0.6459
2024-06-02 18:50:36 [INFO]: Epoch 078 - training loss: 0.3913, validation loss: 0.6450
2024-06-02 18:50:39 [INFO]: Epoch 079 - training loss: 0.3907, validation loss: 0.6417
2024-06-02 18:50:42 [INFO]: Epoch 080 - training loss: 0.3903, validation loss: 0.6369
2024-06-02 18:50:45 [INFO]: Epoch 081 - training loss: 0.3903, validation loss: 0.6410
2024-06-02 18:50:48 [INFO]: Epoch 082 - training loss: 0.3900, validation loss: 0.6349
2024-06-02 18:50:52 [INFO]: Epoch 083 - training loss: 0.3898, validation loss: 0.6399
2024-06-02 18:50:55 [INFO]: Epoch 084 - training loss: 0.3897, validation loss: 0.6401
2024-06-02 18:50:57 [INFO]: Epoch 085 - training loss: 0.3894, validation loss: 0.6355
2024-06-02 18:50:59 [INFO]: Epoch 086 - training loss: 0.3890, validation loss: 0.6408
2024-06-02 18:51:01 [INFO]: Epoch 087 - training loss: 0.3889, validation loss: 0.6403
2024-06-02 18:51:02 [INFO]: Epoch 088 - training loss: 0.3888, validation loss: 0.6374
2024-06-02 18:51:04 [INFO]: Epoch 089 - training loss: 0.3888, validation loss: 0.6378
2024-06-02 18:51:06 [INFO]: Epoch 090 - training loss: 0.3883, validation loss: 0.6360
2024-06-02 18:51:07 [INFO]: Epoch 091 - training loss: 0.3885, validation loss: 0.6396
2024-06-02 18:51:09 [INFO]: Epoch 092 - training loss: 0.3875, validation loss: 0.6410
2024-06-02 18:51:09 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:51:09 [INFO]: Finished training. The best model is from epoch#82.
2024-06-02 18:51:09 [INFO]: Saved the model to results_point_rate01/Electricity/ETSformer_Electricity/round_3/20240602_T184533/ETSformer.pypots
2024-06-02 18:51:09 [INFO]: Successfully saved to results_point_rate01/Electricity/ETSformer_Electricity/round_3/imputation.pkl
2024-06-02 18:51:09 [INFO]: Round3 - ETSformer on Electricity: MAE=0.4194, MSE=0.3599, MRE=0.2244
2024-06-02 18:51:09 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 18:51:09 [INFO]: Using the given device: cuda:0
2024-06-02 18:51:09 [INFO]: Model files will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_4/20240602_T185109
2024-06-02 18:51:09 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/ETSformer_Electricity/round_4/20240602_T185109/tensorboard
2024-06-02 18:51:09 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 10,518,266
2024-06-02 18:51:11 [INFO]: Epoch 001 - training loss: 0.9126, validation loss: 1.3324
2024-06-02 18:51:13 [INFO]: Epoch 002 - training loss: 0.6494, validation loss: 1.2709
2024-06-02 18:51:14 [INFO]: Epoch 003 - training loss: 0.5972, validation loss: 1.2325
2024-06-02 18:51:16 [INFO]: Epoch 004 - training loss: 0.5738, validation loss: 1.2091
2024-06-02 18:51:18 [INFO]: Epoch 005 - training loss: 0.5578, validation loss: 1.1864
2024-06-02 18:51:19 [INFO]: Epoch 006 - training loss: 0.5448, validation loss: 1.1718
2024-06-02 18:51:21 [INFO]: Epoch 007 - training loss: 0.5338, validation loss: 1.1440
2024-06-02 18:51:23 [INFO]: Epoch 008 - training loss: 0.5245, validation loss: 1.1321
2024-06-02 18:51:24 [INFO]: Epoch 009 - training loss: 0.5171, validation loss: 1.1163
2024-06-02 18:51:26 [INFO]: Epoch 010 - training loss: 0.5094, validation loss: 1.0988
2024-06-02 18:51:28 [INFO]: Epoch 011 - training loss: 0.5020, validation loss: 1.0896
2024-06-02 18:51:29 [INFO]: Epoch 012 - training loss: 0.4962, validation loss: 1.0664
2024-06-02 18:51:31 [INFO]: Epoch 013 - training loss: 0.4905, validation loss: 1.0459
2024-06-02 18:51:32 [INFO]: Epoch 014 - training loss: 0.4844, validation loss: 1.0399
2024-06-02 18:51:34 [INFO]: Epoch 015 - training loss: 0.4813, validation loss: 1.0269
2024-06-02 18:51:36 [INFO]: Epoch 016 - training loss: 0.4758, validation loss: 1.0157
2024-06-02 18:51:37 [INFO]: Epoch 017 - training loss: 0.4711, validation loss: 1.0026
2024-06-02 18:51:39 [INFO]: Epoch 018 - training loss: 0.4682, validation loss: 0.9845
2024-06-02 18:51:41 [INFO]: Epoch 019 - training loss: 0.4628, validation loss: 0.9690
2024-06-02 18:51:42 [INFO]: Epoch 020 - training loss: 0.4590, validation loss: 0.9564
2024-06-02 18:51:44 [INFO]: Epoch 021 - training loss: 0.4561, validation loss: 0.9463
2024-06-02 18:51:46 [INFO]: Epoch 022 - training loss: 0.4523, validation loss: 0.9359
2024-06-02 18:51:47 [INFO]: Epoch 023 - training loss: 0.4494, validation loss: 0.9244
2024-06-02 18:51:49 [INFO]: Epoch 024 - training loss: 0.4459, validation loss: 0.9151
2024-06-02 18:51:50 [INFO]: Epoch 025 - training loss: 0.4445, validation loss: 0.9002
2024-06-02 18:51:52 [INFO]: Epoch 026 - training loss: 0.4418, validation loss: 0.8970
2024-06-02 18:51:54 [INFO]: Epoch 027 - training loss: 0.4391, validation loss: 0.8886
2024-06-02 18:51:55 [INFO]: Epoch 028 - training loss: 0.4366, validation loss: 0.8797
2024-06-02 18:51:57 [INFO]: Epoch 029 - training loss: 0.4341, validation loss: 0.8744
2024-06-02 18:51:59 [INFO]: Epoch 030 - training loss: 0.4325, validation loss: 0.8643
2024-06-02 18:52:00 [INFO]: Epoch 031 - training loss: 0.4309, validation loss: 0.8490
2024-06-02 18:52:02 [INFO]: Epoch 032 - training loss: 0.4290, validation loss: 0.8405
2024-06-02 18:52:03 [INFO]: Epoch 033 - training loss: 0.4272, validation loss: 0.8320
2024-06-02 18:52:05 [INFO]: Epoch 034 - training loss: 0.4253, validation loss: 0.8316
2024-06-02 18:52:07 [INFO]: Epoch 035 - training loss: 0.4237, validation loss: 0.8205
2024-06-02 18:52:08 [INFO]: Epoch 036 - training loss: 0.4221, validation loss: 0.8104
2024-06-02 18:52:10 [INFO]: Epoch 037 - training loss: 0.4208, validation loss: 0.8057
2024-06-02 18:52:12 [INFO]: Epoch 038 - training loss: 0.4194, validation loss: 0.7930
2024-06-02 18:52:13 [INFO]: Epoch 039 - training loss: 0.4183, validation loss: 0.7904
2024-06-02 18:52:15 [INFO]: Epoch 040 - training loss: 0.4172, validation loss: 0.7842
2024-06-02 18:52:17 [INFO]: Epoch 041 - training loss: 0.4157, validation loss: 0.7787
2024-06-02 18:52:18 [INFO]: Epoch 042 - training loss: 0.4149, validation loss: 0.7699
2024-06-02 18:52:20 [INFO]: Epoch 043 - training loss: 0.4137, validation loss: 0.7666
2024-06-02 18:52:21 [INFO]: Epoch 044 - training loss: 0.4127, validation loss: 0.7577
2024-06-02 18:52:23 [INFO]: Epoch 045 - training loss: 0.4118, validation loss: 0.7584
2024-06-02 18:52:25 [INFO]: Epoch 046 - training loss: 0.4106, validation loss: 0.7493
2024-06-02 18:52:26 [INFO]: Epoch 047 - training loss: 0.4099, validation loss: 0.7474
2024-06-02 18:52:28 [INFO]: Epoch 048 - training loss: 0.4089, validation loss: 0.7410
2024-06-02 18:52:30 [INFO]: Epoch 049 - training loss: 0.4080, validation loss: 0.7402
2024-06-02 18:52:31 [INFO]: Epoch 050 - training loss: 0.4069, validation loss: 0.7358
2024-06-02 18:52:33 [INFO]: Epoch 051 - training loss: 0.4065, validation loss: 0.7294
2024-06-02 18:52:35 [INFO]: Epoch 052 - training loss: 0.4057, validation loss: 0.7322
2024-06-02 18:52:36 [INFO]: Epoch 053 - training loss: 0.4061, validation loss: 0.7317
2024-06-02 18:52:38 [INFO]: Epoch 054 - training loss: 0.4051, validation loss: 0.7271
2024-06-02 18:52:39 [INFO]: Epoch 055 - training loss: 0.4042, validation loss: 0.7221
2024-06-02 18:52:41 [INFO]: Epoch 056 - training loss: 0.4033, validation loss: 0.7141
2024-06-02 18:52:43 [INFO]: Epoch 057 - training loss: 0.4023, validation loss: 0.7114
2024-06-02 18:52:44 [INFO]: Epoch 058 - training loss: 0.4022, validation loss: 0.7112
2024-06-02 18:52:46 [INFO]: Epoch 059 - training loss: 0.4015, validation loss: 0.7115
2024-06-02 18:52:48 [INFO]: Epoch 060 - training loss: 0.4006, validation loss: 0.7041
2024-06-02 18:52:49 [INFO]: Epoch 061 - training loss: 0.4003, validation loss: 0.7019
2024-06-02 18:52:51 [INFO]: Epoch 062 - training loss: 0.4001, validation loss: 0.6988
2024-06-02 18:52:52 [INFO]: Epoch 063 - training loss: 0.3995, validation loss: 0.6957
2024-06-02 18:52:54 [INFO]: Epoch 064 - training loss: 0.3990, validation loss: 0.6956
2024-06-02 18:52:56 [INFO]: Epoch 065 - training loss: 0.3984, validation loss: 0.6952
2024-06-02 18:52:57 [INFO]: Epoch 066 - training loss: 0.3981, validation loss: 0.6908
2024-06-02 18:52:59 [INFO]: Epoch 067 - training loss: 0.3978, validation loss: 0.6963
2024-06-02 18:53:01 [INFO]: Epoch 068 - training loss: 0.3978, validation loss: 0.6938
2024-06-02 18:53:02 [INFO]: Epoch 069 - training loss: 0.3969, validation loss: 0.6862
2024-06-02 18:53:04 [INFO]: Epoch 070 - training loss: 0.3967, validation loss: 0.6898
2024-06-02 18:53:06 [INFO]: Epoch 071 - training loss: 0.3970, validation loss: 0.6874
2024-06-02 18:53:07 [INFO]: Epoch 072 - training loss: 0.3960, validation loss: 0.6816
2024-06-02 18:53:09 [INFO]: Epoch 073 - training loss: 0.3952, validation loss: 0.6771
2024-06-02 18:53:10 [INFO]: Epoch 074 - training loss: 0.3961, validation loss: 0.6763
2024-06-02 18:53:12 [INFO]: Epoch 075 - training loss: 0.3952, validation loss: 0.6806
2024-06-02 18:53:14 [INFO]: Epoch 076 - training loss: 0.3950, validation loss: 0.6759
2024-06-02 18:53:15 [INFO]: Epoch 077 - training loss: 0.3942, validation loss: 0.6803
2024-06-02 18:53:17 [INFO]: Epoch 078 - training loss: 0.3940, validation loss: 0.6841
2024-06-02 18:53:19 [INFO]: Epoch 079 - training loss: 0.3937, validation loss: 0.6796
2024-06-02 18:53:20 [INFO]: Epoch 080 - training loss: 0.3938, validation loss: 0.6837
2024-06-02 18:53:22 [INFO]: Epoch 081 - training loss: 0.3942, validation loss: 0.6768
2024-06-02 18:53:24 [INFO]: Epoch 082 - training loss: 0.3928, validation loss: 0.6765
2024-06-02 18:53:25 [INFO]: Epoch 083 - training loss: 0.3923, validation loss: 0.6707
2024-06-02 18:53:27 [INFO]: Epoch 084 - training loss: 0.3923, validation loss: 0.6705
2024-06-02 18:53:29 [INFO]: Epoch 085 - training loss: 0.3924, validation loss: 0.6704
2024-06-02 18:53:30 [INFO]: Epoch 086 - training loss: 0.3918, validation loss: 0.6722
2024-06-02 18:53:32 [INFO]: Epoch 087 - training loss: 0.3914, validation loss: 0.6725
2024-06-02 18:53:33 [INFO]: Epoch 088 - training loss: 0.3918, validation loss: 0.6665
2024-06-02 18:53:35 [INFO]: Epoch 089 - training loss: 0.3910, validation loss: 0.6696
2024-06-02 18:53:37 [INFO]: Epoch 090 - training loss: 0.3905, validation loss: 0.6671
2024-06-02 18:53:38 [INFO]: Epoch 091 - training loss: 0.3905, validation loss: 0.6674
2024-06-02 18:53:40 [INFO]: Epoch 092 - training loss: 0.3899, validation loss: 0.6664
2024-06-02 18:53:42 [INFO]: Epoch 093 - training loss: 0.3899, validation loss: 0.6653
2024-06-02 18:53:43 [INFO]: Epoch 094 - training loss: 0.3900, validation loss: 0.6666
2024-06-02 18:53:45 [INFO]: Epoch 095 - training loss: 0.3894, validation loss: 0.6717
2024-06-02 18:53:46 [INFO]: Epoch 096 - training loss: 0.3894, validation loss: 0.6631
2024-06-02 18:53:48 [INFO]: Epoch 097 - training loss: 0.3892, validation loss: 0.6708
2024-06-02 18:53:50 [INFO]: Epoch 098 - training loss: 0.3895, validation loss: 0.6642
2024-06-02 18:53:51 [INFO]: Epoch 099 - training loss: 0.3885, validation loss: 0.6680
2024-06-02 18:53:53 [INFO]: Epoch 100 - training loss: 0.3888, validation loss: 0.6674
2024-06-02 18:53:53 [INFO]: Finished training. The best model is from epoch#96.
2024-06-02 18:53:53 [INFO]: Saved the model to results_point_rate01/Electricity/ETSformer_Electricity/round_4/20240602_T185109/ETSformer.pypots
2024-06-02 18:53:53 [INFO]: Successfully saved to results_point_rate01/Electricity/ETSformer_Electricity/round_4/imputation.pkl
2024-06-02 18:53:53 [INFO]: Round4 - ETSformer on Electricity: MAE=0.4090, MSE=0.3400, MRE=0.2188
2024-06-02 18:53:53 [INFO]: Done! Final results:
Averaged ETSformer (10,518,266 params) on Electricity: MAE=0.4121 ± 0.005422573170254566, MSE=0.3470 ± 0.008713782557411653, MRE=0.2205 ± 0.0029008241158800783, average inference time=0.56
