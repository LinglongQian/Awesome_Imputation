2024-06-02 18:18:29 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 18:18:29 [INFO]: Using the given device: cuda:0
2024-06-02 18:18:29 [INFO]: Model files will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_0/20240602_T181829
2024-06-02 18:18:29 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_0/20240602_T181829/tensorboard
2024-06-02 18:18:30 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 18:18:35 [INFO]: Epoch 001 - training loss: 1.2989, validation loss: 3.8360
2024-06-02 18:18:39 [INFO]: Epoch 002 - training loss: 0.8285, validation loss: 3.1560
2024-06-02 18:18:43 [INFO]: Epoch 003 - training loss: 0.6952, validation loss: 2.8972
2024-06-02 18:18:47 [INFO]: Epoch 004 - training loss: 0.6420, validation loss: 2.7543
2024-06-02 18:18:52 [INFO]: Epoch 005 - training loss: 0.6129, validation loss: 2.5695
2024-06-02 18:18:56 [INFO]: Epoch 006 - training loss: 0.5923, validation loss: 2.4895
2024-06-02 18:19:00 [INFO]: Epoch 007 - training loss: 0.5750, validation loss: 2.3381
2024-06-02 18:19:04 [INFO]: Epoch 008 - training loss: 0.5609, validation loss: 2.3356
2024-06-02 18:19:09 [INFO]: Epoch 009 - training loss: 0.5504, validation loss: 2.3080
2024-06-02 18:19:13 [INFO]: Epoch 010 - training loss: 0.5365, validation loss: 2.2216
2024-06-02 18:19:17 [INFO]: Epoch 011 - training loss: 0.5297, validation loss: 2.2026
2024-06-02 18:19:21 [INFO]: Epoch 012 - training loss: 0.5185, validation loss: 2.1360
2024-06-02 18:19:25 [INFO]: Epoch 013 - training loss: 0.5170, validation loss: 2.1258
2024-06-02 18:19:29 [INFO]: Epoch 014 - training loss: 0.5076, validation loss: 2.0657
2024-06-02 18:19:33 [INFO]: Epoch 015 - training loss: 0.5031, validation loss: 2.0155
2024-06-02 18:19:38 [INFO]: Epoch 016 - training loss: 0.4949, validation loss: 1.9774
2024-06-02 18:19:42 [INFO]: Epoch 017 - training loss: 0.4908, validation loss: 1.9901
2024-06-02 18:19:46 [INFO]: Epoch 018 - training loss: 0.4873, validation loss: 1.9383
2024-06-02 18:19:50 [INFO]: Epoch 019 - training loss: 0.4835, validation loss: 1.9367
2024-06-02 18:19:55 [INFO]: Epoch 020 - training loss: 0.4821, validation loss: 1.8482
2024-06-02 18:19:59 [INFO]: Epoch 021 - training loss: 0.4753, validation loss: 1.8493
2024-06-02 18:20:03 [INFO]: Epoch 022 - training loss: 0.4764, validation loss: 1.8366
2024-06-02 18:20:08 [INFO]: Epoch 023 - training loss: 0.4700, validation loss: 1.7865
2024-06-02 18:20:12 [INFO]: Epoch 024 - training loss: 0.4652, validation loss: 1.7872
2024-06-02 18:20:16 [INFO]: Epoch 025 - training loss: 0.4610, validation loss: 1.7756
2024-06-02 18:20:21 [INFO]: Epoch 026 - training loss: 0.4580, validation loss: 1.7274
2024-06-02 18:20:25 [INFO]: Epoch 027 - training loss: 0.4590, validation loss: 1.7052
2024-06-02 18:20:29 [INFO]: Epoch 028 - training loss: 0.4555, validation loss: 1.6646
2024-06-02 18:20:33 [INFO]: Epoch 029 - training loss: 0.4519, validation loss: 1.6656
2024-06-02 18:20:38 [INFO]: Epoch 030 - training loss: 0.4510, validation loss: 1.6518
2024-06-02 18:20:42 [INFO]: Epoch 031 - training loss: 0.4528, validation loss: 1.6369
2024-06-02 18:20:46 [INFO]: Epoch 032 - training loss: 0.4495, validation loss: 1.5576
2024-06-02 18:20:51 [INFO]: Epoch 033 - training loss: 0.4446, validation loss: 1.5720
2024-06-02 18:20:55 [INFO]: Epoch 034 - training loss: 0.4420, validation loss: 1.5764
2024-06-02 18:20:59 [INFO]: Epoch 035 - training loss: 0.4403, validation loss: 1.5444
2024-06-02 18:21:03 [INFO]: Epoch 036 - training loss: 0.4405, validation loss: 1.4998
2024-06-02 18:21:08 [INFO]: Epoch 037 - training loss: 0.4372, validation loss: 1.4750
2024-06-02 18:21:12 [INFO]: Epoch 038 - training loss: 0.4359, validation loss: 1.4845
2024-06-02 18:21:16 [INFO]: Epoch 039 - training loss: 0.4371, validation loss: 1.4448
2024-06-02 18:21:20 [INFO]: Epoch 040 - training loss: 0.4322, validation loss: 1.4440
2024-06-02 18:21:25 [INFO]: Epoch 041 - training loss: 0.4304, validation loss: 1.4243
2024-06-02 18:21:29 [INFO]: Epoch 042 - training loss: 0.4313, validation loss: 1.4260
2024-06-02 18:21:33 [INFO]: Epoch 043 - training loss: 0.4303, validation loss: 1.3726
2024-06-02 18:21:38 [INFO]: Epoch 044 - training loss: 0.4277, validation loss: 1.3342
2024-06-02 18:21:42 [INFO]: Epoch 045 - training loss: 0.4286, validation loss: 1.3542
2024-06-02 18:21:46 [INFO]: Epoch 046 - training loss: 0.4252, validation loss: 1.3520
2024-06-02 18:21:50 [INFO]: Epoch 047 - training loss: 0.4270, validation loss: 1.3007
2024-06-02 18:21:55 [INFO]: Epoch 048 - training loss: 0.4247, validation loss: 1.3073
2024-06-02 18:21:59 [INFO]: Epoch 049 - training loss: 0.4242, validation loss: 1.2558
2024-06-02 18:22:03 [INFO]: Epoch 050 - training loss: 0.4213, validation loss: 1.2586
2024-06-02 18:22:07 [INFO]: Epoch 051 - training loss: 0.4203, validation loss: 1.2319
2024-06-02 18:22:11 [INFO]: Epoch 052 - training loss: 0.4196, validation loss: 1.2180
2024-06-02 18:22:16 [INFO]: Epoch 053 - training loss: 0.4199, validation loss: 1.1794
2024-06-02 18:22:20 [INFO]: Epoch 054 - training loss: 0.4164, validation loss: 1.1720
2024-06-02 18:22:24 [INFO]: Epoch 055 - training loss: 0.4179, validation loss: 1.1805
2024-06-02 18:22:29 [INFO]: Epoch 056 - training loss: 0.4168, validation loss: 1.1489
2024-06-02 18:22:33 [INFO]: Epoch 057 - training loss: 0.4180, validation loss: 1.1443
2024-06-02 18:22:37 [INFO]: Epoch 058 - training loss: 0.4161, validation loss: 1.1219
2024-06-02 18:22:42 [INFO]: Epoch 059 - training loss: 0.4140, validation loss: 1.0971
2024-06-02 18:22:46 [INFO]: Epoch 060 - training loss: 0.4134, validation loss: 1.1007
2024-06-02 18:22:50 [INFO]: Epoch 061 - training loss: 0.4126, validation loss: 1.0856
2024-06-02 18:22:55 [INFO]: Epoch 062 - training loss: 0.4135, validation loss: 1.0625
2024-06-02 18:22:59 [INFO]: Epoch 063 - training loss: 0.4119, validation loss: 1.0609
2024-06-02 18:23:03 [INFO]: Epoch 064 - training loss: 0.4109, validation loss: 1.0011
2024-06-02 18:23:07 [INFO]: Epoch 065 - training loss: 0.4099, validation loss: 1.0209
2024-06-02 18:23:12 [INFO]: Epoch 066 - training loss: 0.4103, validation loss: 0.9962
2024-06-02 18:23:16 [INFO]: Epoch 067 - training loss: 0.4118, validation loss: 0.9656
2024-06-02 18:23:20 [INFO]: Epoch 068 - training loss: 0.4094, validation loss: 0.9673
2024-06-02 18:23:24 [INFO]: Epoch 069 - training loss: 0.4072, validation loss: 0.9486
2024-06-02 18:23:28 [INFO]: Epoch 070 - training loss: 0.4068, validation loss: 0.9314
2024-06-02 18:23:33 [INFO]: Epoch 071 - training loss: 0.4061, validation loss: 0.9390
2024-06-02 18:23:37 [INFO]: Epoch 072 - training loss: 0.4070, validation loss: 0.9095
2024-06-02 18:23:41 [INFO]: Epoch 073 - training loss: 0.4071, validation loss: 0.9399
2024-06-02 18:23:45 [INFO]: Epoch 074 - training loss: 0.4065, validation loss: 0.9094
2024-06-02 18:23:50 [INFO]: Epoch 075 - training loss: 0.4064, validation loss: 0.9144
2024-06-02 18:23:54 [INFO]: Epoch 076 - training loss: 0.4068, validation loss: 0.8872
2024-06-02 18:23:58 [INFO]: Epoch 077 - training loss: 0.4048, validation loss: 0.8916
2024-06-02 18:24:02 [INFO]: Epoch 078 - training loss: 0.4044, validation loss: 0.8651
2024-06-02 18:24:07 [INFO]: Epoch 079 - training loss: 0.4040, validation loss: 0.8586
2024-06-02 18:24:11 [INFO]: Epoch 080 - training loss: 0.4010, validation loss: 0.8250
2024-06-02 18:24:15 [INFO]: Epoch 081 - training loss: 0.4023, validation loss: 0.8449
2024-06-02 18:24:19 [INFO]: Epoch 082 - training loss: 0.4038, validation loss: 0.8526
2024-06-02 18:24:24 [INFO]: Epoch 083 - training loss: 0.4049, validation loss: 0.8355
2024-06-02 18:24:28 [INFO]: Epoch 084 - training loss: 0.4011, validation loss: 0.8153
2024-06-02 18:24:32 [INFO]: Epoch 085 - training loss: 0.4011, validation loss: 0.8199
2024-06-02 18:24:36 [INFO]: Epoch 086 - training loss: 0.4005, validation loss: 0.8130
2024-06-02 18:24:41 [INFO]: Epoch 087 - training loss: 0.4001, validation loss: 0.8052
2024-06-02 18:24:45 [INFO]: Epoch 088 - training loss: 0.3994, validation loss: 0.7886
2024-06-02 18:24:49 [INFO]: Epoch 089 - training loss: 0.4006, validation loss: 0.7820
2024-06-02 18:24:54 [INFO]: Epoch 090 - training loss: 0.3989, validation loss: 0.7840
2024-06-02 18:24:58 [INFO]: Epoch 091 - training loss: 0.3981, validation loss: 0.7872
2024-06-02 18:25:02 [INFO]: Epoch 092 - training loss: 0.4001, validation loss: 0.7772
2024-06-02 18:25:06 [INFO]: Epoch 093 - training loss: 0.3988, validation loss: 0.7658
2024-06-02 18:25:11 [INFO]: Epoch 094 - training loss: 0.3976, validation loss: 0.7612
2024-06-02 18:25:15 [INFO]: Epoch 095 - training loss: 0.3969, validation loss: 0.7667
2024-06-02 18:25:19 [INFO]: Epoch 096 - training loss: 0.3975, validation loss: 0.7583
2024-06-02 18:25:23 [INFO]: Epoch 097 - training loss: 0.3997, validation loss: 0.7640
2024-06-02 18:25:28 [INFO]: Epoch 098 - training loss: 0.3967, validation loss: 0.7671
2024-06-02 18:25:32 [INFO]: Epoch 099 - training loss: 0.3955, validation loss: 0.7370
2024-06-02 18:25:36 [INFO]: Epoch 100 - training loss: 0.3956, validation loss: 0.7512
2024-06-02 18:25:36 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 18:25:36 [INFO]: Saved the model to results_point_rate01/Electricity/FreTS_Electricity/round_0/20240602_T181829/FreTS.pypots
2024-06-02 18:25:37 [INFO]: Successfully saved to results_point_rate01/Electricity/FreTS_Electricity/round_0/imputation.pkl
2024-06-02 18:25:37 [INFO]: Round0 - FreTS on Electricity: MAE=0.7836, MSE=1.0030, MRE=0.4192
2024-06-02 18:25:37 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 18:25:37 [INFO]: Using the given device: cuda:0
2024-06-02 18:25:37 [INFO]: Model files will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_1/20240602_T182537
2024-06-02 18:25:37 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_1/20240602_T182537/tensorboard
2024-06-02 18:25:37 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 18:25:42 [INFO]: Epoch 001 - training loss: 1.4308, validation loss: 4.4188
2024-06-02 18:25:46 [INFO]: Epoch 002 - training loss: 0.9911, validation loss: 4.6576
2024-06-02 18:25:50 [INFO]: Epoch 003 - training loss: 0.7741, validation loss: 3.8218
2024-06-02 18:25:55 [INFO]: Epoch 004 - training loss: 0.6847, validation loss: 3.4511
2024-06-02 18:25:59 [INFO]: Epoch 005 - training loss: 0.6461, validation loss: 3.1855
2024-06-02 18:26:03 [INFO]: Epoch 006 - training loss: 0.6104, validation loss: 3.0430
2024-06-02 18:26:07 [INFO]: Epoch 007 - training loss: 0.5880, validation loss: 2.9114
2024-06-02 18:26:11 [INFO]: Epoch 008 - training loss: 0.5731, validation loss: 2.7676
2024-06-02 18:26:16 [INFO]: Epoch 009 - training loss: 0.5552, validation loss: 2.6631
2024-06-02 18:26:20 [INFO]: Epoch 010 - training loss: 0.5487, validation loss: 2.6082
2024-06-02 18:26:24 [INFO]: Epoch 011 - training loss: 0.5381, validation loss: 2.4785
2024-06-02 18:26:28 [INFO]: Epoch 012 - training loss: 0.5286, validation loss: 2.4506
2024-06-02 18:26:33 [INFO]: Epoch 013 - training loss: 0.5247, validation loss: 2.4191
2024-06-02 18:26:37 [INFO]: Epoch 014 - training loss: 0.5124, validation loss: 2.3209
2024-06-02 18:26:41 [INFO]: Epoch 015 - training loss: 0.5071, validation loss: 2.2932
2024-06-02 18:26:45 [INFO]: Epoch 016 - training loss: 0.4998, validation loss: 2.2841
2024-06-02 18:26:50 [INFO]: Epoch 017 - training loss: 0.4939, validation loss: 2.1934
2024-06-02 18:26:54 [INFO]: Epoch 018 - training loss: 0.4864, validation loss: 2.1544
2024-06-02 18:26:58 [INFO]: Epoch 019 - training loss: 0.4847, validation loss: 2.1669
2024-06-02 18:27:02 [INFO]: Epoch 020 - training loss: 0.4814, validation loss: 2.1146
2024-06-02 18:27:07 [INFO]: Epoch 021 - training loss: 0.4802, validation loss: 2.0613
2024-06-02 18:27:11 [INFO]: Epoch 022 - training loss: 0.4714, validation loss: 2.0281
2024-06-02 18:27:15 [INFO]: Epoch 023 - training loss: 0.4687, validation loss: 2.0269
2024-06-02 18:27:19 [INFO]: Epoch 024 - training loss: 0.4643, validation loss: 1.9465
2024-06-02 18:27:23 [INFO]: Epoch 025 - training loss: 0.4644, validation loss: 1.9485
2024-06-02 18:27:28 [INFO]: Epoch 026 - training loss: 0.4608, validation loss: 1.9294
2024-06-02 18:27:32 [INFO]: Epoch 027 - training loss: 0.4569, validation loss: 1.9083
2024-06-02 18:27:36 [INFO]: Epoch 028 - training loss: 0.4559, validation loss: 1.8841
2024-06-02 18:27:40 [INFO]: Epoch 029 - training loss: 0.4533, validation loss: 1.8916
2024-06-02 18:27:44 [INFO]: Epoch 030 - training loss: 0.4503, validation loss: 1.8889
2024-06-02 18:27:48 [INFO]: Epoch 031 - training loss: 0.4511, validation loss: 1.8415
2024-06-02 18:27:52 [INFO]: Epoch 032 - training loss: 0.4447, validation loss: 1.8415
2024-06-02 18:27:56 [INFO]: Epoch 033 - training loss: 0.4435, validation loss: 1.8012
2024-06-02 18:28:01 [INFO]: Epoch 034 - training loss: 0.4427, validation loss: 1.7968
2024-06-02 18:28:05 [INFO]: Epoch 035 - training loss: 0.4415, validation loss: 1.7641
2024-06-02 18:28:09 [INFO]: Epoch 036 - training loss: 0.4394, validation loss: 1.7807
2024-06-02 18:28:13 [INFO]: Epoch 037 - training loss: 0.4399, validation loss: 1.7492
2024-06-02 18:28:17 [INFO]: Epoch 038 - training loss: 0.4385, validation loss: 1.7166
2024-06-02 18:28:22 [INFO]: Epoch 039 - training loss: 0.4334, validation loss: 1.7226
2024-06-02 18:28:26 [INFO]: Epoch 040 - training loss: 0.4326, validation loss: 1.7249
2024-06-02 18:28:30 [INFO]: Epoch 041 - training loss: 0.4310, validation loss: 1.6936
2024-06-02 18:28:34 [INFO]: Epoch 042 - training loss: 0.4293, validation loss: 1.6810
2024-06-02 18:28:38 [INFO]: Epoch 043 - training loss: 0.4293, validation loss: 1.6689
2024-06-02 18:28:43 [INFO]: Epoch 044 - training loss: 0.4269, validation loss: 1.6810
2024-06-02 18:28:47 [INFO]: Epoch 045 - training loss: 0.4269, validation loss: 1.6845
2024-06-02 18:28:51 [INFO]: Epoch 046 - training loss: 0.4250, validation loss: 1.6742
2024-06-02 18:28:55 [INFO]: Epoch 047 - training loss: 0.4254, validation loss: 1.6780
2024-06-02 18:29:00 [INFO]: Epoch 048 - training loss: 0.4224, validation loss: 1.6830
2024-06-02 18:29:04 [INFO]: Epoch 049 - training loss: 0.4225, validation loss: 1.6583
2024-06-02 18:29:08 [INFO]: Epoch 050 - training loss: 0.4244, validation loss: 1.6166
2024-06-02 18:29:13 [INFO]: Epoch 051 - training loss: 0.4220, validation loss: 1.6205
2024-06-02 18:29:17 [INFO]: Epoch 052 - training loss: 0.4237, validation loss: 1.6360
2024-06-02 18:29:21 [INFO]: Epoch 053 - training loss: 0.4177, validation loss: 1.5828
2024-06-02 18:29:25 [INFO]: Epoch 054 - training loss: 0.4165, validation loss: 1.5857
2024-06-02 18:29:29 [INFO]: Epoch 055 - training loss: 0.4166, validation loss: 1.5805
2024-06-02 18:29:34 [INFO]: Epoch 056 - training loss: 0.4156, validation loss: 1.5607
2024-06-02 18:29:38 [INFO]: Epoch 057 - training loss: 0.4145, validation loss: 1.5630
2024-06-02 18:29:42 [INFO]: Epoch 058 - training loss: 0.4138, validation loss: 1.5700
2024-06-02 18:29:47 [INFO]: Epoch 059 - training loss: 0.4138, validation loss: 1.5476
2024-06-02 18:29:51 [INFO]: Epoch 060 - training loss: 0.4134, validation loss: 1.5694
2024-06-02 18:29:55 [INFO]: Epoch 061 - training loss: 0.4103, validation loss: 1.5204
2024-06-02 18:30:00 [INFO]: Epoch 062 - training loss: 0.4112, validation loss: 1.5582
2024-06-02 18:30:04 [INFO]: Epoch 063 - training loss: 0.4120, validation loss: 1.5513
2024-06-02 18:30:08 [INFO]: Epoch 064 - training loss: 0.4079, validation loss: 1.5203
2024-06-02 18:30:12 [INFO]: Epoch 065 - training loss: 0.4087, validation loss: 1.5066
2024-06-02 18:30:17 [INFO]: Epoch 066 - training loss: 0.4096, validation loss: 1.4943
2024-06-02 18:30:21 [INFO]: Epoch 067 - training loss: 0.4131, validation loss: 1.5170
2024-06-02 18:30:25 [INFO]: Epoch 068 - training loss: 0.4083, validation loss: 1.5138
2024-06-02 18:30:29 [INFO]: Epoch 069 - training loss: 0.4066, validation loss: 1.4872
2024-06-02 18:30:33 [INFO]: Epoch 070 - training loss: 0.4075, validation loss: 1.4990
2024-06-02 18:30:38 [INFO]: Epoch 071 - training loss: 0.4122, validation loss: 1.5146
2024-06-02 18:30:42 [INFO]: Epoch 072 - training loss: 0.4040, validation loss: 1.5003
2024-06-02 18:30:46 [INFO]: Epoch 073 - training loss: 0.4040, validation loss: 1.5079
2024-06-02 18:30:51 [INFO]: Epoch 074 - training loss: 0.4032, validation loss: 1.4695
2024-06-02 18:30:55 [INFO]: Epoch 075 - training loss: 0.4041, validation loss: 1.4916
2024-06-02 18:30:59 [INFO]: Epoch 076 - training loss: 0.4028, validation loss: 1.4963
2024-06-02 18:31:03 [INFO]: Epoch 077 - training loss: 0.4024, validation loss: 1.4693
2024-06-02 18:31:07 [INFO]: Epoch 078 - training loss: 0.4023, validation loss: 1.4720
2024-06-02 18:31:12 [INFO]: Epoch 079 - training loss: 0.4022, validation loss: 1.5110
2024-06-02 18:31:16 [INFO]: Epoch 080 - training loss: 0.4015, validation loss: 1.5032
2024-06-02 18:31:20 [INFO]: Epoch 081 - training loss: 0.4052, validation loss: 1.5067
2024-06-02 18:31:25 [INFO]: Epoch 082 - training loss: 0.4010, validation loss: 1.4022
2024-06-02 18:31:29 [INFO]: Epoch 083 - training loss: 0.4039, validation loss: 1.4967
2024-06-02 18:31:33 [INFO]: Epoch 084 - training loss: 0.4009, validation loss: 1.4239
2024-06-02 18:31:37 [INFO]: Epoch 085 - training loss: 0.3984, validation loss: 1.3992
2024-06-02 18:31:42 [INFO]: Epoch 086 - training loss: 0.3982, validation loss: 1.4549
2024-06-02 18:31:46 [INFO]: Epoch 087 - training loss: 0.3979, validation loss: 1.4345
2024-06-02 18:31:50 [INFO]: Epoch 088 - training loss: 0.3975, validation loss: 1.4342
2024-06-02 18:31:54 [INFO]: Epoch 089 - training loss: 0.3965, validation loss: 1.4460
2024-06-02 18:31:59 [INFO]: Epoch 090 - training loss: 0.3965, validation loss: 1.4864
2024-06-02 18:32:03 [INFO]: Epoch 091 - training loss: 0.3977, validation loss: 1.4752
2024-06-02 18:32:07 [INFO]: Epoch 092 - training loss: 0.3963, validation loss: 1.4626
2024-06-02 18:32:11 [INFO]: Epoch 093 - training loss: 0.3972, validation loss: 1.4931
2024-06-02 18:32:15 [INFO]: Epoch 094 - training loss: 0.3964, validation loss: 1.4731
2024-06-02 18:32:20 [INFO]: Epoch 095 - training loss: 0.3964, validation loss: 1.5026
2024-06-02 18:32:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:32:20 [INFO]: Finished training. The best model is from epoch#85.
2024-06-02 18:32:20 [INFO]: Saved the model to results_point_rate01/Electricity/FreTS_Electricity/round_1/20240602_T182537/FreTS.pypots
2024-06-02 18:32:20 [INFO]: Successfully saved to results_point_rate01/Electricity/FreTS_Electricity/round_1/imputation.pkl
2024-06-02 18:32:21 [INFO]: Round1 - FreTS on Electricity: MAE=0.6602, MSE=0.7731, MRE=0.3532
2024-06-02 18:32:21 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 18:32:21 [INFO]: Using the given device: cuda:0
2024-06-02 18:32:21 [INFO]: Model files will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_2/20240602_T183221
2024-06-02 18:32:21 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_2/20240602_T183221/tensorboard
2024-06-02 18:32:21 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 18:32:25 [INFO]: Epoch 001 - training loss: 1.3766, validation loss: 4.0501
2024-06-02 18:32:30 [INFO]: Epoch 002 - training loss: 0.8989, validation loss: 3.3852
2024-06-02 18:32:34 [INFO]: Epoch 003 - training loss: 0.7202, validation loss: 2.9611
2024-06-02 18:32:38 [INFO]: Epoch 004 - training loss: 0.6575, validation loss: 2.6904
2024-06-02 18:32:42 [INFO]: Epoch 005 - training loss: 0.6244, validation loss: 2.5409
2024-06-02 18:32:47 [INFO]: Epoch 006 - training loss: 0.6010, validation loss: 2.4541
2024-06-02 18:32:51 [INFO]: Epoch 007 - training loss: 0.5795, validation loss: 2.3334
2024-06-02 18:32:55 [INFO]: Epoch 008 - training loss: 0.5676, validation loss: 2.2662
2024-06-02 18:32:59 [INFO]: Epoch 009 - training loss: 0.5539, validation loss: 2.1848
2024-06-02 18:33:04 [INFO]: Epoch 010 - training loss: 0.5492, validation loss: 2.1385
2024-06-02 18:33:08 [INFO]: Epoch 011 - training loss: 0.5314, validation loss: 2.1003
2024-06-02 18:33:12 [INFO]: Epoch 012 - training loss: 0.5245, validation loss: 2.0521
2024-06-02 18:33:16 [INFO]: Epoch 013 - training loss: 0.5186, validation loss: 2.0254
2024-06-02 18:33:21 [INFO]: Epoch 014 - training loss: 0.5081, validation loss: 1.9627
2024-06-02 18:33:25 [INFO]: Epoch 015 - training loss: 0.5035, validation loss: 1.9656
2024-06-02 18:33:29 [INFO]: Epoch 016 - training loss: 0.4975, validation loss: 1.9255
2024-06-02 18:33:33 [INFO]: Epoch 017 - training loss: 0.4938, validation loss: 1.8986
2024-06-02 18:33:37 [INFO]: Epoch 018 - training loss: 0.4910, validation loss: 1.8782
2024-06-02 18:33:42 [INFO]: Epoch 019 - training loss: 0.4872, validation loss: 1.8509
2024-06-02 18:33:46 [INFO]: Epoch 020 - training loss: 0.4788, validation loss: 1.8337
2024-06-02 18:33:50 [INFO]: Epoch 021 - training loss: 0.4768, validation loss: 1.8293
2024-06-02 18:33:54 [INFO]: Epoch 022 - training loss: 0.4762, validation loss: 1.7777
2024-06-02 18:33:58 [INFO]: Epoch 023 - training loss: 0.4709, validation loss: 1.7433
2024-06-02 18:34:03 [INFO]: Epoch 024 - training loss: 0.4674, validation loss: 1.7475
2024-06-02 18:34:07 [INFO]: Epoch 025 - training loss: 0.4664, validation loss: 1.7352
2024-06-02 18:34:11 [INFO]: Epoch 026 - training loss: 0.4632, validation loss: 1.7153
2024-06-02 18:34:15 [INFO]: Epoch 027 - training loss: 0.4565, validation loss: 1.7048
2024-06-02 18:34:20 [INFO]: Epoch 028 - training loss: 0.4529, validation loss: 1.6717
2024-06-02 18:34:24 [INFO]: Epoch 029 - training loss: 0.4526, validation loss: 1.6652
2024-06-02 18:34:28 [INFO]: Epoch 030 - training loss: 0.4514, validation loss: 1.6417
2024-06-02 18:34:33 [INFO]: Epoch 031 - training loss: 0.4484, validation loss: 1.6209
2024-06-02 18:34:37 [INFO]: Epoch 032 - training loss: 0.4481, validation loss: 1.5967
2024-06-02 18:34:41 [INFO]: Epoch 033 - training loss: 0.4469, validation loss: 1.5906
2024-06-02 18:34:45 [INFO]: Epoch 034 - training loss: 0.4440, validation loss: 1.5794
2024-06-02 18:34:50 [INFO]: Epoch 035 - training loss: 0.4423, validation loss: 1.5416
2024-06-02 18:34:54 [INFO]: Epoch 036 - training loss: 0.4419, validation loss: 1.5327
2024-06-02 18:34:58 [INFO]: Epoch 037 - training loss: 0.4394, validation loss: 1.5003
2024-06-02 18:35:02 [INFO]: Epoch 038 - training loss: 0.4394, validation loss: 1.4847
2024-06-02 18:35:07 [INFO]: Epoch 039 - training loss: 0.4353, validation loss: 1.4994
2024-06-02 18:35:11 [INFO]: Epoch 040 - training loss: 0.4349, validation loss: 1.4675
2024-06-02 18:35:16 [INFO]: Epoch 041 - training loss: 0.4335, validation loss: 1.4524
2024-06-02 18:35:20 [INFO]: Epoch 042 - training loss: 0.4318, validation loss: 1.4220
2024-06-02 18:35:24 [INFO]: Epoch 043 - training loss: 0.4302, validation loss: 1.3990
2024-06-02 18:35:29 [INFO]: Epoch 044 - training loss: 0.4288, validation loss: 1.3701
2024-06-02 18:35:33 [INFO]: Epoch 045 - training loss: 0.4275, validation loss: 1.3575
2024-06-02 18:35:37 [INFO]: Epoch 046 - training loss: 0.4280, validation loss: 1.3439
2024-06-02 18:35:42 [INFO]: Epoch 047 - training loss: 0.4253, validation loss: 1.3237
2024-06-02 18:35:46 [INFO]: Epoch 048 - training loss: 0.4252, validation loss: 1.3179
2024-06-02 18:35:50 [INFO]: Epoch 049 - training loss: 0.4233, validation loss: 1.2916
2024-06-02 18:35:55 [INFO]: Epoch 050 - training loss: 0.4211, validation loss: 1.2803
2024-06-02 18:35:59 [INFO]: Epoch 051 - training loss: 0.4199, validation loss: 1.2565
2024-06-02 18:36:03 [INFO]: Epoch 052 - training loss: 0.4230, validation loss: 1.2345
2024-06-02 18:36:08 [INFO]: Epoch 053 - training loss: 0.4194, validation loss: 1.2189
2024-06-02 18:36:12 [INFO]: Epoch 054 - training loss: 0.4187, validation loss: 1.1912
2024-06-02 18:36:16 [INFO]: Epoch 055 - training loss: 0.4201, validation loss: 1.1953
2024-06-02 18:36:20 [INFO]: Epoch 056 - training loss: 0.4192, validation loss: 1.1683
2024-06-02 18:36:25 [INFO]: Epoch 057 - training loss: 0.4158, validation loss: 1.1588
2024-06-02 18:36:29 [INFO]: Epoch 058 - training loss: 0.4161, validation loss: 1.1146
2024-06-02 18:36:33 [INFO]: Epoch 059 - training loss: 0.4135, validation loss: 1.1118
2024-06-02 18:36:37 [INFO]: Epoch 060 - training loss: 0.4130, validation loss: 1.1000
2024-06-02 18:36:42 [INFO]: Epoch 061 - training loss: 0.4136, validation loss: 1.0843
2024-06-02 18:36:46 [INFO]: Epoch 062 - training loss: 0.4129, validation loss: 1.0666
2024-06-02 18:36:50 [INFO]: Epoch 063 - training loss: 0.4122, validation loss: 1.0635
2024-06-02 18:36:54 [INFO]: Epoch 064 - training loss: 0.4124, validation loss: 1.0371
2024-06-02 18:36:58 [INFO]: Epoch 065 - training loss: 0.4103, validation loss: 1.0240
2024-06-02 18:37:03 [INFO]: Epoch 066 - training loss: 0.4095, validation loss: 1.0102
2024-06-02 18:37:07 [INFO]: Epoch 067 - training loss: 0.4139, validation loss: 1.0042
2024-06-02 18:37:11 [INFO]: Epoch 068 - training loss: 0.4096, validation loss: 0.9924
2024-06-02 18:37:15 [INFO]: Epoch 069 - training loss: 0.4086, validation loss: 0.9670
2024-06-02 18:37:19 [INFO]: Epoch 070 - training loss: 0.4071, validation loss: 0.9561
2024-06-02 18:37:23 [INFO]: Epoch 071 - training loss: 0.4077, validation loss: 0.9659
2024-06-02 18:37:26 [INFO]: Epoch 072 - training loss: 0.4071, validation loss: 0.9597
2024-06-02 18:37:30 [INFO]: Epoch 073 - training loss: 0.4064, validation loss: 0.9418
2024-06-02 18:37:34 [INFO]: Epoch 074 - training loss: 0.4054, validation loss: 0.9093
2024-06-02 18:37:38 [INFO]: Epoch 075 - training loss: 0.4040, validation loss: 0.9311
2024-06-02 18:37:41 [INFO]: Epoch 076 - training loss: 0.4043, validation loss: 0.8951
2024-06-02 18:37:45 [INFO]: Epoch 077 - training loss: 0.4047, validation loss: 0.9013
2024-06-02 18:37:49 [INFO]: Epoch 078 - training loss: 0.4050, validation loss: 0.8933
2024-06-02 18:37:52 [INFO]: Epoch 079 - training loss: 0.4047, validation loss: 0.8928
2024-06-02 18:37:56 [INFO]: Epoch 080 - training loss: 0.4044, validation loss: 0.8540
2024-06-02 18:38:00 [INFO]: Epoch 081 - training loss: 0.4023, validation loss: 0.8733
2024-06-02 18:38:03 [INFO]: Epoch 082 - training loss: 0.4021, validation loss: 0.8437
2024-06-02 18:38:07 [INFO]: Epoch 083 - training loss: 0.4002, validation loss: 0.8272
2024-06-02 18:38:11 [INFO]: Epoch 084 - training loss: 0.4016, validation loss: 0.8276
2024-06-02 18:38:15 [INFO]: Epoch 085 - training loss: 0.4006, validation loss: 0.8257
2024-06-02 18:38:19 [INFO]: Epoch 086 - training loss: 0.4017, validation loss: 0.8104
2024-06-02 18:38:22 [INFO]: Epoch 087 - training loss: 0.4025, validation loss: 0.7962
2024-06-02 18:38:26 [INFO]: Epoch 088 - training loss: 0.3993, validation loss: 0.7773
2024-06-02 18:38:30 [INFO]: Epoch 089 - training loss: 0.3979, validation loss: 0.7783
2024-06-02 18:38:33 [INFO]: Epoch 090 - training loss: 0.3974, validation loss: 0.7775
2024-06-02 18:38:37 [INFO]: Epoch 091 - training loss: 0.3988, validation loss: 0.7579
2024-06-02 18:38:41 [INFO]: Epoch 092 - training loss: 0.3969, validation loss: 0.7555
2024-06-02 18:38:44 [INFO]: Epoch 093 - training loss: 0.3973, validation loss: 0.7452
2024-06-02 18:38:48 [INFO]: Epoch 094 - training loss: 0.3964, validation loss: 0.7366
2024-06-02 18:38:52 [INFO]: Epoch 095 - training loss: 0.3957, validation loss: 0.7587
2024-06-02 18:38:55 [INFO]: Epoch 096 - training loss: 0.3964, validation loss: 0.7403
2024-06-02 18:38:59 [INFO]: Epoch 097 - training loss: 0.3965, validation loss: 0.7317
2024-06-02 18:39:03 [INFO]: Epoch 098 - training loss: 0.3978, validation loss: 0.7399
2024-06-02 18:39:07 [INFO]: Epoch 099 - training loss: 0.3976, validation loss: 0.7080
2024-06-02 18:39:11 [INFO]: Epoch 100 - training loss: 0.3957, validation loss: 0.7065
2024-06-02 18:39:11 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 18:39:11 [INFO]: Saved the model to results_point_rate01/Electricity/FreTS_Electricity/round_2/20240602_T183221/FreTS.pypots
2024-06-02 18:39:12 [INFO]: Successfully saved to results_point_rate01/Electricity/FreTS_Electricity/round_2/imputation.pkl
2024-06-02 18:39:12 [INFO]: Round2 - FreTS on Electricity: MAE=0.6823, MSE=0.7664, MRE=0.3650
2024-06-02 18:39:12 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 18:39:12 [INFO]: Using the given device: cuda:0
2024-06-02 18:39:12 [INFO]: Model files will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_3/20240602_T183912
2024-06-02 18:39:12 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_3/20240602_T183912/tensorboard
2024-06-02 18:39:12 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 18:39:16 [INFO]: Epoch 001 - training loss: 1.3447, validation loss: 3.9171
2024-06-02 18:39:19 [INFO]: Epoch 002 - training loss: 0.8298, validation loss: 3.1463
2024-06-02 18:39:23 [INFO]: Epoch 003 - training loss: 0.7110, validation loss: 2.9668
2024-06-02 18:39:26 [INFO]: Epoch 004 - training loss: 0.6532, validation loss: 2.7838
2024-06-02 18:39:30 [INFO]: Epoch 005 - training loss: 0.6247, validation loss: 2.6183
2024-06-02 18:39:33 [INFO]: Epoch 006 - training loss: 0.5992, validation loss: 2.4891
2024-06-02 18:39:37 [INFO]: Epoch 007 - training loss: 0.5795, validation loss: 2.3727
2024-06-02 18:39:41 [INFO]: Epoch 008 - training loss: 0.5662, validation loss: 2.3217
2024-06-02 18:39:45 [INFO]: Epoch 009 - training loss: 0.5528, validation loss: 2.2051
2024-06-02 18:39:49 [INFO]: Epoch 010 - training loss: 0.5416, validation loss: 2.1424
2024-06-02 18:39:52 [INFO]: Epoch 011 - training loss: 0.5305, validation loss: 2.0802
2024-06-02 18:39:56 [INFO]: Epoch 012 - training loss: 0.5265, validation loss: 2.0668
2024-06-02 18:40:00 [INFO]: Epoch 013 - training loss: 0.5212, validation loss: 2.0341
2024-06-02 18:40:04 [INFO]: Epoch 014 - training loss: 0.5088, validation loss: 1.9526
2024-06-02 18:40:07 [INFO]: Epoch 015 - training loss: 0.5022, validation loss: 1.9370
2024-06-02 18:40:11 [INFO]: Epoch 016 - training loss: 0.4970, validation loss: 1.8751
2024-06-02 18:40:15 [INFO]: Epoch 017 - training loss: 0.4943, validation loss: 1.8495
2024-06-02 18:40:19 [INFO]: Epoch 018 - training loss: 0.4866, validation loss: 1.8121
2024-06-02 18:40:22 [INFO]: Epoch 019 - training loss: 0.4818, validation loss: 1.7988
2024-06-02 18:40:27 [INFO]: Epoch 020 - training loss: 0.4800, validation loss: 1.7752
2024-06-02 18:40:30 [INFO]: Epoch 021 - training loss: 0.4764, validation loss: 1.7316
2024-06-02 18:40:34 [INFO]: Epoch 022 - training loss: 0.4715, validation loss: 1.7056
2024-06-02 18:40:37 [INFO]: Epoch 023 - training loss: 0.4731, validation loss: 1.6855
2024-06-02 18:40:41 [INFO]: Epoch 024 - training loss: 0.4662, validation loss: 1.6550
2024-06-02 18:40:45 [INFO]: Epoch 025 - training loss: 0.4638, validation loss: 1.6556
2024-06-02 18:40:48 [INFO]: Epoch 026 - training loss: 0.4628, validation loss: 1.6175
2024-06-02 18:40:52 [INFO]: Epoch 027 - training loss: 0.4585, validation loss: 1.6235
2024-06-02 18:40:56 [INFO]: Epoch 028 - training loss: 0.4564, validation loss: 1.5682
2024-06-02 18:41:00 [INFO]: Epoch 029 - training loss: 0.4531, validation loss: 1.5689
2024-06-02 18:41:04 [INFO]: Epoch 030 - training loss: 0.4496, validation loss: 1.5386
2024-06-02 18:41:07 [INFO]: Epoch 031 - training loss: 0.4468, validation loss: 1.5357
2024-06-02 18:41:11 [INFO]: Epoch 032 - training loss: 0.4465, validation loss: 1.5028
2024-06-02 18:41:14 [INFO]: Epoch 033 - training loss: 0.4477, validation loss: 1.4707
2024-06-02 18:41:18 [INFO]: Epoch 034 - training loss: 0.4438, validation loss: 1.4472
2024-06-02 18:41:22 [INFO]: Epoch 035 - training loss: 0.4428, validation loss: 1.4717
2024-06-02 18:41:26 [INFO]: Epoch 036 - training loss: 0.4413, validation loss: 1.4336
2024-06-02 18:41:29 [INFO]: Epoch 037 - training loss: 0.4385, validation loss: 1.4185
2024-06-02 18:41:33 [INFO]: Epoch 038 - training loss: 0.4368, validation loss: 1.3869
2024-06-02 18:41:37 [INFO]: Epoch 039 - training loss: 0.4346, validation loss: 1.3539
2024-06-02 18:41:41 [INFO]: Epoch 040 - training loss: 0.4347, validation loss: 1.3498
2024-06-02 18:41:45 [INFO]: Epoch 041 - training loss: 0.4316, validation loss: 1.3314
2024-06-02 18:41:49 [INFO]: Epoch 042 - training loss: 0.4317, validation loss: 1.2995
2024-06-02 18:41:52 [INFO]: Epoch 043 - training loss: 0.4326, validation loss: 1.3031
2024-06-02 18:41:56 [INFO]: Epoch 044 - training loss: 0.4285, validation loss: 1.2599
2024-06-02 18:42:00 [INFO]: Epoch 045 - training loss: 0.4278, validation loss: 1.2673
2024-06-02 18:42:04 [INFO]: Epoch 046 - training loss: 0.4287, validation loss: 1.2371
2024-06-02 18:42:07 [INFO]: Epoch 047 - training loss: 0.4271, validation loss: 1.2149
2024-06-02 18:42:11 [INFO]: Epoch 048 - training loss: 0.4246, validation loss: 1.1892
2024-06-02 18:42:15 [INFO]: Epoch 049 - training loss: 0.4242, validation loss: 1.1702
2024-06-02 18:42:18 [INFO]: Epoch 050 - training loss: 0.4223, validation loss: 1.1354
2024-06-02 18:42:22 [INFO]: Epoch 051 - training loss: 0.4225, validation loss: 1.1283
2024-06-02 18:42:26 [INFO]: Epoch 052 - training loss: 0.4226, validation loss: 1.1189
2024-06-02 18:42:29 [INFO]: Epoch 053 - training loss: 0.4213, validation loss: 1.1050
2024-06-02 18:42:33 [INFO]: Epoch 054 - training loss: 0.4191, validation loss: 1.0817
2024-06-02 18:42:37 [INFO]: Epoch 055 - training loss: 0.4186, validation loss: 1.0787
2024-06-02 18:42:41 [INFO]: Epoch 056 - training loss: 0.4197, validation loss: 1.0638
2024-06-02 18:42:45 [INFO]: Epoch 057 - training loss: 0.4165, validation loss: 1.0635
2024-06-02 18:42:49 [INFO]: Epoch 058 - training loss: 0.4175, validation loss: 1.0592
2024-06-02 18:42:52 [INFO]: Epoch 059 - training loss: 0.4173, validation loss: 1.0324
2024-06-02 18:42:56 [INFO]: Epoch 060 - training loss: 0.4152, validation loss: 0.9934
2024-06-02 18:43:00 [INFO]: Epoch 061 - training loss: 0.4136, validation loss: 0.9997
2024-06-02 18:43:04 [INFO]: Epoch 062 - training loss: 0.4148, validation loss: 0.9861
2024-06-02 18:43:07 [INFO]: Epoch 063 - training loss: 0.4195, validation loss: 0.9775
2024-06-02 18:43:11 [INFO]: Epoch 064 - training loss: 0.4153, validation loss: 0.9496
2024-06-02 18:43:15 [INFO]: Epoch 065 - training loss: 0.4125, validation loss: 0.9652
2024-06-02 18:43:19 [INFO]: Epoch 066 - training loss: 0.4133, validation loss: 0.9322
2024-06-02 18:43:22 [INFO]: Epoch 067 - training loss: 0.4108, validation loss: 0.9357
2024-06-02 18:43:26 [INFO]: Epoch 068 - training loss: 0.4140, validation loss: 0.9157
2024-06-02 18:43:30 [INFO]: Epoch 069 - training loss: 0.4113, validation loss: 0.8995
2024-06-02 18:43:33 [INFO]: Epoch 070 - training loss: 0.4088, validation loss: 0.8972
2024-06-02 18:43:37 [INFO]: Epoch 071 - training loss: 0.4085, validation loss: 0.8996
2024-06-02 18:43:41 [INFO]: Epoch 072 - training loss: 0.4071, validation loss: 0.8856
2024-06-02 18:43:45 [INFO]: Epoch 073 - training loss: 0.4080, validation loss: 0.8736
2024-06-02 18:43:49 [INFO]: Epoch 074 - training loss: 0.4071, validation loss: 0.8590
2024-06-02 18:43:53 [INFO]: Epoch 075 - training loss: 0.4065, validation loss: 0.8658
2024-06-02 18:43:57 [INFO]: Epoch 076 - training loss: 0.4056, validation loss: 0.8646
2024-06-02 18:44:00 [INFO]: Epoch 077 - training loss: 0.4080, validation loss: 0.8454
2024-06-02 18:44:04 [INFO]: Epoch 078 - training loss: 0.4071, validation loss: 0.8385
2024-06-02 18:44:08 [INFO]: Epoch 079 - training loss: 0.4079, validation loss: 0.8098
2024-06-02 18:44:12 [INFO]: Epoch 080 - training loss: 0.4082, validation loss: 0.8204
2024-06-02 18:44:15 [INFO]: Epoch 081 - training loss: 0.4037, validation loss: 0.8368
2024-06-02 18:44:19 [INFO]: Epoch 082 - training loss: 0.4053, validation loss: 0.8084
2024-06-02 18:44:23 [INFO]: Epoch 083 - training loss: 0.4052, validation loss: 0.8102
2024-06-02 18:44:27 [INFO]: Epoch 084 - training loss: 0.4033, validation loss: 0.8001
2024-06-02 18:44:31 [INFO]: Epoch 085 - training loss: 0.4038, validation loss: 0.7848
2024-06-02 18:44:34 [INFO]: Epoch 086 - training loss: 0.4022, validation loss: 0.7984
2024-06-02 18:44:38 [INFO]: Epoch 087 - training loss: 0.4021, validation loss: 0.8038
2024-06-02 18:44:42 [INFO]: Epoch 088 - training loss: 0.4028, validation loss: 0.7791
2024-06-02 18:44:46 [INFO]: Epoch 089 - training loss: 0.4087, validation loss: 0.7851
2024-06-02 18:44:49 [INFO]: Epoch 090 - training loss: 0.4027, validation loss: 0.7764
2024-06-02 18:44:53 [INFO]: Epoch 091 - training loss: 0.4020, validation loss: 0.7596
2024-06-02 18:44:57 [INFO]: Epoch 092 - training loss: 0.4027, validation loss: 0.7576
2024-06-02 18:45:01 [INFO]: Epoch 093 - training loss: 0.4008, validation loss: 0.7484
2024-06-02 18:45:05 [INFO]: Epoch 094 - training loss: 0.4010, validation loss: 0.7516
2024-06-02 18:45:09 [INFO]: Epoch 095 - training loss: 0.3997, validation loss: 0.7515
2024-06-02 18:45:12 [INFO]: Epoch 096 - training loss: 0.4005, validation loss: 0.7383
2024-06-02 18:45:16 [INFO]: Epoch 097 - training loss: 0.4001, validation loss: 0.7573
2024-06-02 18:45:20 [INFO]: Epoch 098 - training loss: 0.4013, validation loss: 0.7483
2024-06-02 18:45:23 [INFO]: Epoch 099 - training loss: 0.3990, validation loss: 0.7235
2024-06-02 18:45:27 [INFO]: Epoch 100 - training loss: 0.3999, validation loss: 0.7332
2024-06-02 18:45:27 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 18:45:27 [INFO]: Saved the model to results_point_rate01/Electricity/FreTS_Electricity/round_3/20240602_T183912/FreTS.pypots
2024-06-02 18:45:28 [INFO]: Successfully saved to results_point_rate01/Electricity/FreTS_Electricity/round_3/imputation.pkl
2024-06-02 18:45:28 [INFO]: Round3 - FreTS on Electricity: MAE=0.7274, MSE=0.8874, MRE=0.3891
2024-06-02 18:45:28 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 18:45:28 [INFO]: Using the given device: cuda:0
2024-06-02 18:45:28 [INFO]: Model files will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_4/20240602_T184528
2024-06-02 18:45:28 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/FreTS_Electricity/round_4/20240602_T184528/tensorboard
2024-06-02 18:45:28 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 18:45:32 [INFO]: Epoch 001 - training loss: 1.3393, validation loss: 3.7221
2024-06-02 18:45:36 [INFO]: Epoch 002 - training loss: 0.8250, validation loss: 3.1417
2024-06-02 18:45:39 [INFO]: Epoch 003 - training loss: 0.6993, validation loss: 3.0082
2024-06-02 18:45:43 [INFO]: Epoch 004 - training loss: 0.6420, validation loss: 2.7255
2024-06-02 18:45:47 [INFO]: Epoch 005 - training loss: 0.6119, validation loss: 2.5862
2024-06-02 18:45:51 [INFO]: Epoch 006 - training loss: 0.5931, validation loss: 2.4711
2024-06-02 18:45:54 [INFO]: Epoch 007 - training loss: 0.5746, validation loss: 2.3048
2024-06-02 18:45:58 [INFO]: Epoch 008 - training loss: 0.5581, validation loss: 2.2378
2024-06-02 18:46:02 [INFO]: Epoch 009 - training loss: 0.5471, validation loss: 2.1800
2024-06-02 18:46:06 [INFO]: Epoch 010 - training loss: 0.5381, validation loss: 2.1103
2024-06-02 18:46:09 [INFO]: Epoch 011 - training loss: 0.5292, validation loss: 2.0532
2024-06-02 18:46:13 [INFO]: Epoch 012 - training loss: 0.5220, validation loss: 1.9941
2024-06-02 18:46:17 [INFO]: Epoch 013 - training loss: 0.5133, validation loss: 1.9741
2024-06-02 18:46:20 [INFO]: Epoch 014 - training loss: 0.5044, validation loss: 1.9365
2024-06-02 18:46:24 [INFO]: Epoch 015 - training loss: 0.4997, validation loss: 1.8963
2024-06-02 18:46:27 [INFO]: Epoch 016 - training loss: 0.4960, validation loss: 1.8666
2024-06-02 18:46:31 [INFO]: Epoch 017 - training loss: 0.4900, validation loss: 1.8689
2024-06-02 18:46:34 [INFO]: Epoch 018 - training loss: 0.4851, validation loss: 1.8080
2024-06-02 18:46:38 [INFO]: Epoch 019 - training loss: 0.4832, validation loss: 1.7979
2024-06-02 18:46:42 [INFO]: Epoch 020 - training loss: 0.4767, validation loss: 1.7850
2024-06-02 18:46:46 [INFO]: Epoch 021 - training loss: 0.4740, validation loss: 1.7798
2024-06-02 18:46:49 [INFO]: Epoch 022 - training loss: 0.4719, validation loss: 1.7316
2024-06-02 18:46:53 [INFO]: Epoch 023 - training loss: 0.4674, validation loss: 1.7071
2024-06-02 18:46:57 [INFO]: Epoch 024 - training loss: 0.4656, validation loss: 1.6605
2024-06-02 18:47:01 [INFO]: Epoch 025 - training loss: 0.4607, validation loss: 1.6473
2024-06-02 18:47:05 [INFO]: Epoch 026 - training loss: 0.4591, validation loss: 1.6304
2024-06-02 18:47:08 [INFO]: Epoch 027 - training loss: 0.4568, validation loss: 1.6220
2024-06-02 18:47:12 [INFO]: Epoch 028 - training loss: 0.4573, validation loss: 1.6026
2024-06-02 18:47:16 [INFO]: Epoch 029 - training loss: 0.4532, validation loss: 1.5592
2024-06-02 18:47:20 [INFO]: Epoch 030 - training loss: 0.4499, validation loss: 1.5156
2024-06-02 18:47:24 [INFO]: Epoch 031 - training loss: 0.4467, validation loss: 1.5188
2024-06-02 18:47:27 [INFO]: Epoch 032 - training loss: 0.4454, validation loss: 1.5197
2024-06-02 18:47:31 [INFO]: Epoch 033 - training loss: 0.4431, validation loss: 1.4837
2024-06-02 18:47:35 [INFO]: Epoch 034 - training loss: 0.4427, validation loss: 1.4549
2024-06-02 18:47:38 [INFO]: Epoch 035 - training loss: 0.4427, validation loss: 1.4372
2024-06-02 18:47:42 [INFO]: Epoch 036 - training loss: 0.4387, validation loss: 1.4280
2024-06-02 18:47:46 [INFO]: Epoch 037 - training loss: 0.4356, validation loss: 1.3893
2024-06-02 18:47:49 [INFO]: Epoch 038 - training loss: 0.4372, validation loss: 1.3873
2024-06-02 18:47:53 [INFO]: Epoch 039 - training loss: 0.4355, validation loss: 1.3690
2024-06-02 18:47:57 [INFO]: Epoch 040 - training loss: 0.4320, validation loss: 1.3321
2024-06-02 18:48:01 [INFO]: Epoch 041 - training loss: 0.4323, validation loss: 1.3135
2024-06-02 18:48:04 [INFO]: Epoch 042 - training loss: 0.4288, validation loss: 1.3130
2024-06-02 18:48:09 [INFO]: Epoch 043 - training loss: 0.4288, validation loss: 1.2800
2024-06-02 18:48:12 [INFO]: Epoch 044 - training loss: 0.4300, validation loss: 1.2834
2024-06-02 18:48:15 [INFO]: Epoch 045 - training loss: 0.4316, validation loss: 1.2467
2024-06-02 18:48:18 [INFO]: Epoch 046 - training loss: 0.4256, validation loss: 1.2201
2024-06-02 18:48:21 [INFO]: Epoch 047 - training loss: 0.4235, validation loss: 1.2124
2024-06-02 18:48:24 [INFO]: Epoch 048 - training loss: 0.4220, validation loss: 1.1924
2024-06-02 18:48:27 [INFO]: Epoch 049 - training loss: 0.4226, validation loss: 1.1937
2024-06-02 18:48:30 [INFO]: Epoch 050 - training loss: 0.4244, validation loss: 1.1642
2024-06-02 18:48:33 [INFO]: Epoch 051 - training loss: 0.4249, validation loss: 1.1501
2024-06-02 18:48:36 [INFO]: Epoch 052 - training loss: 0.4227, validation loss: 1.1167
2024-06-02 18:48:39 [INFO]: Epoch 053 - training loss: 0.4218, validation loss: 1.0996
2024-06-02 18:48:42 [INFO]: Epoch 054 - training loss: 0.4162, validation loss: 1.0766
2024-06-02 18:48:44 [INFO]: Epoch 055 - training loss: 0.4151, validation loss: 1.0889
2024-06-02 18:48:47 [INFO]: Epoch 056 - training loss: 0.4145, validation loss: 1.0710
2024-06-02 18:48:50 [INFO]: Epoch 057 - training loss: 0.4143, validation loss: 1.0441
2024-06-02 18:48:53 [INFO]: Epoch 058 - training loss: 0.4144, validation loss: 1.0374
2024-06-02 18:48:56 [INFO]: Epoch 059 - training loss: 0.4130, validation loss: 1.0299
2024-06-02 18:48:59 [INFO]: Epoch 060 - training loss: 0.4129, validation loss: 1.0075
2024-06-02 18:49:02 [INFO]: Epoch 061 - training loss: 0.4126, validation loss: 1.0121
2024-06-02 18:49:05 [INFO]: Epoch 062 - training loss: 0.4140, validation loss: 0.9714
2024-06-02 18:49:08 [INFO]: Epoch 063 - training loss: 0.4114, validation loss: 0.9682
2024-06-02 18:49:11 [INFO]: Epoch 064 - training loss: 0.4094, validation loss: 0.9658
2024-06-02 18:49:14 [INFO]: Epoch 065 - training loss: 0.4099, validation loss: 0.9472
2024-06-02 18:49:17 [INFO]: Epoch 066 - training loss: 0.4110, validation loss: 0.9518
2024-06-02 18:49:20 [INFO]: Epoch 067 - training loss: 0.4112, validation loss: 0.9365
2024-06-02 18:49:23 [INFO]: Epoch 068 - training loss: 0.4087, validation loss: 0.9288
2024-06-02 18:49:25 [INFO]: Epoch 069 - training loss: 0.4085, validation loss: 0.8977
2024-06-02 18:49:28 [INFO]: Epoch 070 - training loss: 0.4064, validation loss: 0.9058
2024-06-02 18:49:31 [INFO]: Epoch 071 - training loss: 0.4059, validation loss: 0.8909
2024-06-02 18:49:34 [INFO]: Epoch 072 - training loss: 0.4069, validation loss: 0.9061
2024-06-02 18:49:37 [INFO]: Epoch 073 - training loss: 0.4086, validation loss: 0.8612
2024-06-02 18:49:40 [INFO]: Epoch 074 - training loss: 0.4070, validation loss: 0.8467
2024-06-02 18:49:43 [INFO]: Epoch 075 - training loss: 0.4048, validation loss: 0.8223
2024-06-02 18:49:46 [INFO]: Epoch 076 - training loss: 0.4050, validation loss: 0.8454
2024-06-02 18:49:49 [INFO]: Epoch 077 - training loss: 0.4033, validation loss: 0.8143
2024-06-02 18:49:52 [INFO]: Epoch 078 - training loss: 0.4025, validation loss: 0.8395
2024-06-02 18:49:55 [INFO]: Epoch 079 - training loss: 0.4021, validation loss: 0.8035
2024-06-02 18:49:58 [INFO]: Epoch 080 - training loss: 0.4020, validation loss: 0.8012
2024-06-02 18:50:01 [INFO]: Epoch 081 - training loss: 0.4020, validation loss: 0.7899
2024-06-02 18:50:04 [INFO]: Epoch 082 - training loss: 0.4017, validation loss: 0.7807
2024-06-02 18:50:07 [INFO]: Epoch 083 - training loss: 0.4010, validation loss: 0.7807
2024-06-02 18:50:10 [INFO]: Epoch 084 - training loss: 0.4036, validation loss: 0.7817
2024-06-02 18:50:12 [INFO]: Epoch 085 - training loss: 0.4003, validation loss: 0.7742
2024-06-02 18:50:15 [INFO]: Epoch 086 - training loss: 0.4017, validation loss: 0.7730
2024-06-02 18:50:18 [INFO]: Epoch 087 - training loss: 0.4006, validation loss: 0.7593
2024-06-02 18:50:21 [INFO]: Epoch 088 - training loss: 0.3994, validation loss: 0.7530
2024-06-02 18:50:24 [INFO]: Epoch 089 - training loss: 0.3985, validation loss: 0.7326
2024-06-02 18:50:27 [INFO]: Epoch 090 - training loss: 0.3985, validation loss: 0.7479
2024-06-02 18:50:30 [INFO]: Epoch 091 - training loss: 0.3983, validation loss: 0.7312
2024-06-02 18:50:33 [INFO]: Epoch 092 - training loss: 0.3988, validation loss: 0.7279
2024-06-02 18:50:36 [INFO]: Epoch 093 - training loss: 0.3983, validation loss: 0.7189
2024-06-02 18:50:39 [INFO]: Epoch 094 - training loss: 0.3973, validation loss: 0.6950
2024-06-02 18:50:42 [INFO]: Epoch 095 - training loss: 0.3984, validation loss: 0.7198
2024-06-02 18:50:45 [INFO]: Epoch 096 - training loss: 0.3983, validation loss: 0.7332
2024-06-02 18:50:48 [INFO]: Epoch 097 - training loss: 0.3975, validation loss: 0.7135
2024-06-02 18:50:51 [INFO]: Epoch 098 - training loss: 0.3974, validation loss: 0.6950
2024-06-02 18:50:54 [INFO]: Epoch 099 - training loss: 0.3994, validation loss: 0.6898
2024-06-02 18:50:56 [INFO]: Epoch 100 - training loss: 0.3971, validation loss: 0.6898
2024-06-02 18:50:56 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 18:50:56 [INFO]: Saved the model to results_point_rate01/Electricity/FreTS_Electricity/round_4/20240602_T184528/FreTS.pypots
2024-06-02 18:50:57 [INFO]: Successfully saved to results_point_rate01/Electricity/FreTS_Electricity/round_4/imputation.pkl
2024-06-02 18:50:57 [INFO]: Round4 - FreTS on Electricity: MAE=0.7363, MSE=0.9110, MRE=0.3939
2024-06-02 18:50:57 [INFO]: Done! Final results:
Averaged FreTS (3,706,194 params) on Electricity: MAE=0.7180 ± 0.04320024798447169, MSE=0.8682 ± 0.089189498881059, MRE=0.3841 ± 0.023110120828387443, average inference time=0.39
