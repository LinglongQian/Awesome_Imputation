2024-06-01 21:58:29 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-01 21:58:29 [INFO]: Using the given device: cuda:0
2024-06-01 21:58:29 [INFO]: Model files will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_0/20240601_T215829
2024-06-01 21:58:29 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_0/20240601_T215829/tensorboard
2024-06-01 21:58:30 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 17,082,800
2024-06-01 22:05:29 [INFO]: Epoch 001 - training loss: 0.9790, validation loss: 2.9088
2024-06-01 22:12:22 [INFO]: Epoch 002 - training loss: 0.6780, validation loss: 2.7227
2024-06-01 22:18:17 [INFO]: Epoch 003 - training loss: 0.6084, validation loss: 2.6144
2024-06-01 22:24:03 [INFO]: Epoch 004 - training loss: 0.5703, validation loss: 2.5696
2024-06-01 22:29:35 [INFO]: Epoch 005 - training loss: 0.5450, validation loss: 2.5286
2024-06-01 22:35:15 [INFO]: Epoch 006 - training loss: 0.5260, validation loss: 2.4977
2024-06-01 22:40:44 [INFO]: Epoch 007 - training loss: 0.5116, validation loss: 2.4706
2024-06-01 22:46:27 [INFO]: Epoch 008 - training loss: 0.5000, validation loss: 2.4480
2024-06-01 22:51:54 [INFO]: Epoch 009 - training loss: 0.4892, validation loss: 2.4264
2024-06-01 22:57:27 [INFO]: Epoch 010 - training loss: 0.4801, validation loss: 2.4030
2024-06-01 23:03:04 [INFO]: Epoch 011 - training loss: 0.4712, validation loss: 2.3866
2024-06-01 23:08:36 [INFO]: Epoch 012 - training loss: 0.4639, validation loss: 2.3655
2024-06-01 23:14:18 [INFO]: Epoch 013 - training loss: 0.4573, validation loss: 2.3451
2024-06-01 23:19:47 [INFO]: Epoch 014 - training loss: 0.4517, validation loss: 2.3318
2024-06-01 23:25:27 [INFO]: Epoch 015 - training loss: 0.4458, validation loss: 2.3182
2024-06-01 23:30:51 [INFO]: Epoch 016 - training loss: 0.4411, validation loss: 2.3002
2024-06-01 23:36:30 [INFO]: Epoch 017 - training loss: 0.4366, validation loss: 2.2889
2024-06-01 23:42:01 [INFO]: Epoch 018 - training loss: 0.4322, validation loss: 2.2720
2024-06-01 23:47:55 [INFO]: Epoch 019 - training loss: 0.4284, validation loss: 2.2565
2024-06-01 23:53:31 [INFO]: Epoch 020 - training loss: 0.4236, validation loss: 2.2451
2024-06-01 23:59:11 [INFO]: Epoch 021 - training loss: 0.4206, validation loss: 2.2294
2024-06-02 00:04:53 [INFO]: Epoch 022 - training loss: 0.4174, validation loss: 2.2175
2024-06-02 00:10:23 [INFO]: Epoch 023 - training loss: 0.4141, validation loss: 2.2108
2024-06-02 00:16:10 [INFO]: Epoch 024 - training loss: 0.4116, validation loss: 2.1988
2024-06-02 00:20:03 [INFO]: Epoch 025 - training loss: 0.4092, validation loss: 2.1892
2024-06-02 00:22:09 [INFO]: Epoch 026 - training loss: 0.4059, validation loss: 2.1750
2024-06-02 00:24:15 [INFO]: Epoch 027 - training loss: 0.4037, validation loss: 2.1623
2024-06-02 00:26:21 [INFO]: Epoch 028 - training loss: 0.4016, validation loss: 2.1538
2024-06-02 00:28:27 [INFO]: Epoch 029 - training loss: 0.3996, validation loss: 2.1439
2024-06-02 00:30:34 [INFO]: Epoch 030 - training loss: 0.3978, validation loss: 2.1348
2024-06-02 00:32:40 [INFO]: Epoch 031 - training loss: 0.3955, validation loss: 2.1301
2024-06-02 00:34:47 [INFO]: Epoch 032 - training loss: 0.3937, validation loss: 2.1198
2024-06-02 00:36:53 [INFO]: Epoch 033 - training loss: 0.3917, validation loss: 2.1083
2024-06-02 00:38:59 [INFO]: Epoch 034 - training loss: 0.3900, validation loss: 2.1027
2024-06-02 00:41:06 [INFO]: Epoch 035 - training loss: 0.3890, validation loss: 2.0942
2024-06-02 00:43:12 [INFO]: Epoch 036 - training loss: 0.3878, validation loss: 2.0895
2024-06-02 00:45:19 [INFO]: Epoch 037 - training loss: 0.3853, validation loss: 2.0856
2024-06-02 00:47:26 [INFO]: Epoch 038 - training loss: 0.3840, validation loss: 2.0691
2024-06-02 00:49:33 [INFO]: Epoch 039 - training loss: 0.3825, validation loss: 2.0692
2024-06-02 00:51:40 [INFO]: Epoch 040 - training loss: 0.3813, validation loss: 2.0629
2024-06-02 00:53:46 [INFO]: Epoch 041 - training loss: 0.3798, validation loss: 2.0635
2024-06-02 00:55:53 [INFO]: Epoch 042 - training loss: 0.3792, validation loss: 2.0516
2024-06-02 00:57:59 [INFO]: Epoch 043 - training loss: 0.3785, validation loss: 2.0521
2024-06-02 01:00:06 [INFO]: Epoch 044 - training loss: 0.3772, validation loss: 2.0442
2024-06-02 01:02:12 [INFO]: Epoch 045 - training loss: 0.3760, validation loss: 2.0408
2024-06-02 01:04:19 [INFO]: Epoch 046 - training loss: 0.3746, validation loss: 2.0324
2024-06-02 01:06:25 [INFO]: Epoch 047 - training loss: 0.3734, validation loss: 2.0278
2024-06-02 01:08:32 [INFO]: Epoch 048 - training loss: 0.3737, validation loss: 2.0275
2024-06-02 01:10:38 [INFO]: Epoch 049 - training loss: 0.3725, validation loss: 2.0215
2024-06-02 01:12:45 [INFO]: Epoch 050 - training loss: 0.3714, validation loss: 2.0183
2024-06-02 01:14:52 [INFO]: Epoch 051 - training loss: 0.3703, validation loss: 2.0178
2024-06-02 01:16:58 [INFO]: Epoch 052 - training loss: 0.3697, validation loss: 2.0118
2024-06-02 01:19:05 [INFO]: Epoch 053 - training loss: 0.3685, validation loss: 2.0123
2024-06-02 01:21:12 [INFO]: Epoch 054 - training loss: 0.3682, validation loss: 2.0084
2024-06-02 01:23:18 [INFO]: Epoch 055 - training loss: 0.3675, validation loss: 2.0058
2024-06-02 01:25:25 [INFO]: Epoch 056 - training loss: 0.3665, validation loss: 2.0021
2024-06-02 01:27:31 [INFO]: Epoch 057 - training loss: 0.3660, validation loss: 2.0022
2024-06-02 01:29:38 [INFO]: Epoch 058 - training loss: 0.3653, validation loss: 1.9966
2024-06-02 01:31:45 [INFO]: Epoch 059 - training loss: 0.3644, validation loss: 1.9915
2024-06-02 01:33:51 [INFO]: Epoch 060 - training loss: 0.3632, validation loss: 1.9878
2024-06-02 01:35:58 [INFO]: Epoch 061 - training loss: 0.3639, validation loss: 1.9866
2024-06-02 01:38:05 [INFO]: Epoch 062 - training loss: 0.3638, validation loss: 1.9859
2024-06-02 01:40:11 [INFO]: Epoch 063 - training loss: 0.3626, validation loss: 1.9818
2024-06-02 01:42:18 [INFO]: Epoch 064 - training loss: 0.3626, validation loss: 1.9750
2024-06-02 01:44:25 [INFO]: Epoch 065 - training loss: 0.3624, validation loss: 1.9736
2024-06-02 01:46:31 [INFO]: Epoch 066 - training loss: 0.3612, validation loss: 1.9733
2024-06-02 01:48:38 [INFO]: Epoch 067 - training loss: 0.3602, validation loss: 1.9672
2024-06-02 01:50:45 [INFO]: Epoch 068 - training loss: 0.3598, validation loss: 1.9657
2024-06-02 01:52:52 [INFO]: Epoch 069 - training loss: 0.3595, validation loss: 1.9652
2024-06-02 01:54:59 [INFO]: Epoch 070 - training loss: 0.3590, validation loss: 1.9608
2024-06-02 01:57:05 [INFO]: Epoch 071 - training loss: 0.3585, validation loss: 1.9615
2024-06-02 01:59:12 [INFO]: Epoch 072 - training loss: 0.3580, validation loss: 1.9598
2024-06-02 02:01:19 [INFO]: Epoch 073 - training loss: 0.3575, validation loss: 1.9598
2024-06-02 02:03:25 [INFO]: Epoch 074 - training loss: 0.3570, validation loss: 1.9549
2024-06-02 02:05:32 [INFO]: Epoch 075 - training loss: 0.3564, validation loss: 1.9560
2024-06-02 02:07:39 [INFO]: Epoch 076 - training loss: 0.3563, validation loss: 1.9526
2024-06-02 02:09:46 [INFO]: Epoch 077 - training loss: 0.3555, validation loss: 1.9525
2024-06-02 02:11:53 [INFO]: Epoch 078 - training loss: 0.3548, validation loss: 1.9494
2024-06-02 02:13:59 [INFO]: Epoch 079 - training loss: 0.3549, validation loss: 1.9477
2024-06-02 02:16:06 [INFO]: Epoch 080 - training loss: 0.3560, validation loss: 1.9488
2024-06-02 02:18:12 [INFO]: Epoch 081 - training loss: 0.3546, validation loss: 1.9503
2024-06-02 02:20:19 [INFO]: Epoch 082 - training loss: 0.3543, validation loss: 1.9490
2024-06-02 02:22:25 [INFO]: Epoch 083 - training loss: 0.3534, validation loss: 1.9466
2024-06-02 02:24:31 [INFO]: Epoch 084 - training loss: 0.3534, validation loss: 1.9477
2024-06-02 02:26:38 [INFO]: Epoch 085 - training loss: 0.3538, validation loss: 1.9442
2024-06-02 02:28:49 [INFO]: Epoch 086 - training loss: 0.3532, validation loss: 1.9410
2024-06-02 02:31:32 [INFO]: Epoch 087 - training loss: 0.3526, validation loss: 1.9403
2024-06-02 02:34:15 [INFO]: Epoch 088 - training loss: 0.3518, validation loss: 1.9408
2024-06-02 02:36:58 [INFO]: Epoch 089 - training loss: 0.3522, validation loss: 1.9389
2024-06-02 02:39:40 [INFO]: Epoch 090 - training loss: 0.3520, validation loss: 1.9366
2024-06-02 02:42:23 [INFO]: Epoch 091 - training loss: 0.3517, validation loss: 1.9398
2024-06-02 02:45:06 [INFO]: Epoch 092 - training loss: 0.3514, validation loss: 1.9365
2024-06-02 02:47:07 [INFO]: Epoch 093 - training loss: 0.3507, validation loss: 1.9349
2024-06-02 02:49:13 [INFO]: Epoch 094 - training loss: 0.3501, validation loss: 1.9322
2024-06-02 02:51:20 [INFO]: Epoch 095 - training loss: 0.3499, validation loss: 1.9293
2024-06-02 02:53:29 [INFO]: Epoch 096 - training loss: 0.3495, validation loss: 1.9311
2024-06-02 02:55:35 [INFO]: Epoch 097 - training loss: 0.3489, validation loss: 1.9330
2024-06-02 02:57:43 [INFO]: Epoch 098 - training loss: 0.3498, validation loss: 1.9297
2024-06-02 02:59:50 [INFO]: Epoch 099 - training loss: 0.3492, validation loss: 1.9310
2024-06-02 03:01:56 [INFO]: Epoch 100 - training loss: 0.3486, validation loss: 1.9305
2024-06-02 03:04:03 [INFO]: Epoch 101 - training loss: 0.3492, validation loss: 1.9324
2024-06-02 03:06:10 [INFO]: Epoch 102 - training loss: 0.3491, validation loss: 1.9286
2024-06-02 03:08:16 [INFO]: Epoch 103 - training loss: 0.3484, validation loss: 1.9280
2024-06-02 03:10:24 [INFO]: Epoch 104 - training loss: 0.3478, validation loss: 1.9259
2024-06-02 03:12:32 [INFO]: Epoch 105 - training loss: 0.3478, validation loss: 1.9272
2024-06-02 03:14:39 [INFO]: Epoch 106 - training loss: 0.3476, validation loss: 1.9290
2024-06-02 03:16:46 [INFO]: Epoch 107 - training loss: 0.3470, validation loss: 1.9229
2024-06-02 03:18:55 [INFO]: Epoch 108 - training loss: 0.3476, validation loss: 1.9307
2024-06-02 03:21:01 [INFO]: Epoch 109 - training loss: 0.3473, validation loss: 1.9228
2024-06-02 03:23:09 [INFO]: Epoch 110 - training loss: 0.3463, validation loss: 1.9212
2024-06-02 03:25:15 [INFO]: Epoch 111 - training loss: 0.3460, validation loss: 1.9238
2024-06-02 03:27:22 [INFO]: Epoch 112 - training loss: 0.3460, validation loss: 1.9211
2024-06-02 03:29:29 [INFO]: Epoch 113 - training loss: 0.3454, validation loss: 1.9278
2024-06-02 03:31:36 [INFO]: Epoch 114 - training loss: 0.3457, validation loss: 1.9234
2024-06-02 03:33:42 [INFO]: Epoch 115 - training loss: 0.3456, validation loss: 1.9212
2024-06-02 03:35:49 [INFO]: Epoch 116 - training loss: 0.3449, validation loss: 1.9256
2024-06-02 03:37:55 [INFO]: Epoch 117 - training loss: 0.3453, validation loss: 1.9248
2024-06-02 03:40:03 [INFO]: Epoch 118 - training loss: 0.3449, validation loss: 1.9223
2024-06-02 03:42:10 [INFO]: Epoch 119 - training loss: 0.3447, validation loss: 1.9227
2024-06-02 03:44:16 [INFO]: Epoch 120 - training loss: 0.3449, validation loss: 1.9234
2024-06-02 03:46:23 [INFO]: Epoch 121 - training loss: 0.3442, validation loss: 1.9197
2024-06-02 03:48:30 [INFO]: Epoch 122 - training loss: 0.3446, validation loss: 1.9224
2024-06-02 03:50:36 [INFO]: Epoch 123 - training loss: 0.3443, validation loss: 1.9219
2024-06-02 03:52:43 [INFO]: Epoch 124 - training loss: 0.3441, validation loss: 1.9228
2024-06-02 03:54:50 [INFO]: Epoch 125 - training loss: 0.3440, validation loss: 1.9211
2024-06-02 03:56:56 [INFO]: Epoch 126 - training loss: 0.3435, validation loss: 1.9220
2024-06-02 03:59:03 [INFO]: Epoch 127 - training loss: 0.3430, validation loss: 1.9204
2024-06-02 04:01:12 [INFO]: Epoch 128 - training loss: 0.3433, validation loss: 1.9215
2024-06-02 04:03:19 [INFO]: Epoch 129 - training loss: 0.3426, validation loss: 1.9204
2024-06-02 04:05:25 [INFO]: Epoch 130 - training loss: 0.3421, validation loss: 1.9184
2024-06-02 04:07:32 [INFO]: Epoch 131 - training loss: 0.3425, validation loss: 1.9243
2024-06-02 04:09:39 [INFO]: Epoch 132 - training loss: 0.3427, validation loss: 1.9230
2024-06-02 04:11:45 [INFO]: Epoch 133 - training loss: 0.3426, validation loss: 1.9246
2024-06-02 04:13:52 [INFO]: Epoch 134 - training loss: 0.3426, validation loss: 1.9227
2024-06-02 04:15:59 [INFO]: Epoch 135 - training loss: 0.3423, validation loss: 1.9229
2024-06-02 04:18:05 [INFO]: Epoch 136 - training loss: 0.3416, validation loss: 1.9246
2024-06-02 04:20:12 [INFO]: Epoch 137 - training loss: 0.3414, validation loss: 1.9228
2024-06-02 04:22:19 [INFO]: Epoch 138 - training loss: 0.3410, validation loss: 1.9238
2024-06-02 04:24:25 [INFO]: Epoch 139 - training loss: 0.3406, validation loss: 1.9216
2024-06-02 04:26:33 [INFO]: Epoch 140 - training loss: 0.3411, validation loss: 1.9199
2024-06-02 04:26:33 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 04:26:33 [INFO]: Finished training. The best model is from epoch#130.
2024-06-02 04:26:33 [INFO]: Saved the model to results_point_rate01/Electricity/BRITS_Electricity/round_0/20240601_T215829/BRITS.pypots
2024-06-02 04:27:12 [INFO]: Successfully saved to results_point_rate01/Electricity/BRITS_Electricity/round_0/imputation.pkl
2024-06-02 04:27:12 [INFO]: Round0 - BRITS on Electricity: MAE=0.9928, MSE=2.2533, MRE=0.5311
2024-06-02 04:27:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 04:27:12 [INFO]: Using the given device: cuda:0
2024-06-02 04:27:12 [INFO]: Model files will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_1/20240602_T042712
2024-06-02 04:27:12 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_1/20240602_T042712/tensorboard
2024-06-02 04:27:12 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 17,082,800
2024-06-02 04:29:22 [INFO]: Epoch 001 - training loss: 0.9733, validation loss: 2.8830
2024-06-02 04:31:29 [INFO]: Epoch 002 - training loss: 0.6740, validation loss: 2.7026
2024-06-02 04:33:35 [INFO]: Epoch 003 - training loss: 0.6048, validation loss: 2.6049
2024-06-02 04:35:41 [INFO]: Epoch 004 - training loss: 0.5675, validation loss: 2.5539
2024-06-02 04:37:48 [INFO]: Epoch 005 - training loss: 0.5432, validation loss: 2.5221
2024-06-02 04:39:55 [INFO]: Epoch 006 - training loss: 0.5245, validation loss: 2.4949
2024-06-02 04:42:02 [INFO]: Epoch 007 - training loss: 0.5095, validation loss: 2.4691
2024-06-02 04:44:08 [INFO]: Epoch 008 - training loss: 0.4971, validation loss: 2.4478
2024-06-02 04:46:15 [INFO]: Epoch 009 - training loss: 0.4869, validation loss: 2.4282
2024-06-02 04:48:22 [INFO]: Epoch 010 - training loss: 0.4783, validation loss: 2.4095
2024-06-02 04:50:28 [INFO]: Epoch 011 - training loss: 0.4702, validation loss: 2.3899
2024-06-02 04:52:35 [INFO]: Epoch 012 - training loss: 0.4630, validation loss: 2.3751
2024-06-02 04:54:42 [INFO]: Epoch 013 - training loss: 0.4576, validation loss: 2.3582
2024-06-02 04:56:49 [INFO]: Epoch 014 - training loss: 0.4506, validation loss: 2.3433
2024-06-02 04:58:55 [INFO]: Epoch 015 - training loss: 0.4450, validation loss: 2.3209
2024-06-02 05:01:01 [INFO]: Epoch 016 - training loss: 0.4399, validation loss: 2.3096
2024-06-02 05:03:08 [INFO]: Epoch 017 - training loss: 0.4356, validation loss: 2.2938
2024-06-02 05:05:15 [INFO]: Epoch 018 - training loss: 0.4315, validation loss: 2.2788
2024-06-02 05:07:22 [INFO]: Epoch 019 - training loss: 0.4271, validation loss: 2.2687
2024-06-02 05:09:29 [INFO]: Epoch 020 - training loss: 0.4233, validation loss: 2.2471
2024-06-02 05:11:36 [INFO]: Epoch 021 - training loss: 0.4208, validation loss: 2.2340
2024-06-02 05:13:43 [INFO]: Epoch 022 - training loss: 0.4174, validation loss: 2.2230
2024-06-02 05:15:49 [INFO]: Epoch 023 - training loss: 0.4143, validation loss: 2.2118
2024-06-02 05:17:56 [INFO]: Epoch 024 - training loss: 0.4116, validation loss: 2.1954
2024-06-02 05:20:03 [INFO]: Epoch 025 - training loss: 0.4091, validation loss: 2.1849
2024-06-02 05:22:09 [INFO]: Epoch 026 - training loss: 0.4060, validation loss: 2.1751
2024-06-02 05:24:16 [INFO]: Epoch 027 - training loss: 0.4040, validation loss: 2.1666
2024-06-02 05:26:23 [INFO]: Epoch 028 - training loss: 0.4022, validation loss: 2.1514
2024-06-02 05:28:29 [INFO]: Epoch 029 - training loss: 0.3993, validation loss: 2.1415
2024-06-02 05:30:36 [INFO]: Epoch 030 - training loss: 0.3969, validation loss: 2.1357
2024-06-02 05:32:43 [INFO]: Epoch 031 - training loss: 0.3953, validation loss: 2.1236
2024-06-02 05:34:49 [INFO]: Epoch 032 - training loss: 0.3942, validation loss: 2.1232
2024-06-02 05:36:56 [INFO]: Epoch 033 - training loss: 0.3917, validation loss: 2.1087
2024-06-02 05:39:02 [INFO]: Epoch 034 - training loss: 0.3894, validation loss: 2.1026
2024-06-02 05:41:09 [INFO]: Epoch 035 - training loss: 0.3891, validation loss: 2.0927
2024-06-02 05:43:16 [INFO]: Epoch 036 - training loss: 0.3872, validation loss: 2.0858
2024-06-02 05:45:23 [INFO]: Epoch 037 - training loss: 0.3850, validation loss: 2.0812
2024-06-02 05:47:29 [INFO]: Epoch 038 - training loss: 0.3841, validation loss: 2.0760
2024-06-02 05:49:37 [INFO]: Epoch 039 - training loss: 0.3824, validation loss: 2.0682
2024-06-02 05:51:43 [INFO]: Epoch 040 - training loss: 0.3822, validation loss: 2.0664
2024-06-02 05:53:50 [INFO]: Epoch 041 - training loss: 0.3804, validation loss: 2.0591
2024-06-02 05:55:57 [INFO]: Epoch 042 - training loss: 0.3790, validation loss: 2.0564
2024-06-02 05:58:04 [INFO]: Epoch 043 - training loss: 0.3777, validation loss: 2.0540
2024-06-02 06:00:10 [INFO]: Epoch 044 - training loss: 0.3770, validation loss: 2.0454
2024-06-02 06:02:17 [INFO]: Epoch 045 - training loss: 0.3762, validation loss: 2.0463
2024-06-02 06:04:24 [INFO]: Epoch 046 - training loss: 0.3747, validation loss: 2.0457
2024-06-02 06:06:30 [INFO]: Epoch 047 - training loss: 0.3741, validation loss: 2.0437
2024-06-02 06:08:37 [INFO]: Epoch 048 - training loss: 0.3733, validation loss: 2.0288
2024-06-02 06:10:44 [INFO]: Epoch 049 - training loss: 0.3720, validation loss: 2.0234
2024-06-02 06:12:51 [INFO]: Epoch 050 - training loss: 0.3714, validation loss: 2.0229
2024-06-02 06:14:58 [INFO]: Epoch 051 - training loss: 0.3704, validation loss: 2.0158
2024-06-02 06:17:06 [INFO]: Epoch 052 - training loss: 0.3694, validation loss: 2.0151
2024-06-02 06:19:49 [INFO]: Epoch 053 - training loss: 0.3692, validation loss: 2.0054
2024-06-02 06:22:32 [INFO]: Epoch 054 - training loss: 0.3684, validation loss: 2.0023
2024-06-02 06:25:15 [INFO]: Epoch 055 - training loss: 0.3673, validation loss: 2.0036
2024-06-02 06:27:58 [INFO]: Epoch 056 - training loss: 0.3676, validation loss: 1.9972
2024-06-02 06:30:41 [INFO]: Epoch 057 - training loss: 0.3669, validation loss: 1.9955
2024-06-02 06:33:24 [INFO]: Epoch 058 - training loss: 0.3655, validation loss: 1.9910
2024-06-02 06:35:24 [INFO]: Epoch 059 - training loss: 0.3646, validation loss: 1.9901
2024-06-02 06:37:31 [INFO]: Epoch 060 - training loss: 0.3644, validation loss: 1.9847
2024-06-02 06:39:39 [INFO]: Epoch 061 - training loss: 0.3638, validation loss: 1.9836
2024-06-02 06:41:45 [INFO]: Epoch 062 - training loss: 0.3630, validation loss: 1.9796
2024-06-02 06:43:52 [INFO]: Epoch 063 - training loss: 0.3624, validation loss: 1.9733
2024-06-02 06:45:58 [INFO]: Epoch 064 - training loss: 0.3627, validation loss: 1.9772
2024-06-02 06:48:05 [INFO]: Epoch 065 - training loss: 0.3613, validation loss: 1.9718
2024-06-02 06:50:12 [INFO]: Epoch 066 - training loss: 0.3608, validation loss: 1.9728
2024-06-02 06:52:19 [INFO]: Epoch 067 - training loss: 0.3610, validation loss: 1.9701
2024-06-02 06:54:26 [INFO]: Epoch 068 - training loss: 0.3604, validation loss: 1.9633
2024-06-02 06:56:35 [INFO]: Epoch 069 - training loss: 0.3591, validation loss: 1.9641
2024-06-02 06:58:42 [INFO]: Epoch 070 - training loss: 0.3586, validation loss: 1.9626
2024-06-02 07:00:49 [INFO]: Epoch 071 - training loss: 0.3593, validation loss: 1.9606
2024-06-02 07:02:56 [INFO]: Epoch 072 - training loss: 0.3588, validation loss: 1.9636
2024-06-02 07:05:02 [INFO]: Epoch 073 - training loss: 0.3591, validation loss: 1.9546
2024-06-02 07:07:09 [INFO]: Epoch 074 - training loss: 0.3581, validation loss: 1.9526
2024-06-02 07:09:17 [INFO]: Epoch 075 - training loss: 0.3566, validation loss: 1.9553
2024-06-02 07:11:24 [INFO]: Epoch 076 - training loss: 0.3560, validation loss: 1.9505
2024-06-02 07:13:30 [INFO]: Epoch 077 - training loss: 0.3555, validation loss: 1.9497
2024-06-02 07:15:37 [INFO]: Epoch 078 - training loss: 0.3555, validation loss: 1.9505
2024-06-02 07:17:44 [INFO]: Epoch 079 - training loss: 0.3548, validation loss: 1.9457
2024-06-02 07:19:51 [INFO]: Epoch 080 - training loss: 0.3549, validation loss: 1.9477
2024-06-02 07:21:58 [INFO]: Epoch 081 - training loss: 0.3544, validation loss: 1.9452
2024-06-02 07:24:05 [INFO]: Epoch 082 - training loss: 0.3543, validation loss: 1.9441
2024-06-02 07:26:12 [INFO]: Epoch 083 - training loss: 0.3541, validation loss: 1.9412
2024-06-02 07:28:18 [INFO]: Epoch 084 - training loss: 0.3538, validation loss: 1.9430
2024-06-02 07:30:25 [INFO]: Epoch 085 - training loss: 0.3537, validation loss: 1.9395
2024-06-02 07:32:32 [INFO]: Epoch 086 - training loss: 0.3533, validation loss: 1.9390
2024-06-02 07:34:41 [INFO]: Epoch 087 - training loss: 0.3542, validation loss: 1.9389
2024-06-02 07:36:47 [INFO]: Epoch 088 - training loss: 0.3533, validation loss: 1.9337
2024-06-02 07:38:54 [INFO]: Epoch 089 - training loss: 0.3534, validation loss: 1.9314
2024-06-02 07:41:01 [INFO]: Epoch 090 - training loss: 0.3517, validation loss: 1.9340
2024-06-02 07:43:07 [INFO]: Epoch 091 - training loss: 0.3512, validation loss: 1.9300
2024-06-02 07:45:14 [INFO]: Epoch 092 - training loss: 0.3509, validation loss: 1.9334
2024-06-02 07:47:21 [INFO]: Epoch 093 - training loss: 0.3511, validation loss: 1.9313
2024-06-02 07:49:28 [INFO]: Epoch 094 - training loss: 0.3503, validation loss: 1.9327
2024-06-02 07:51:34 [INFO]: Epoch 095 - training loss: 0.3511, validation loss: 1.9320
2024-06-02 07:53:44 [INFO]: Epoch 096 - training loss: 0.3507, validation loss: 1.9316
2024-06-02 07:55:51 [INFO]: Epoch 097 - training loss: 0.3502, validation loss: 1.9282
2024-06-02 07:57:57 [INFO]: Epoch 098 - training loss: 0.3494, validation loss: 1.9264
2024-06-02 08:00:05 [INFO]: Epoch 099 - training loss: 0.3495, validation loss: 1.9263
2024-06-02 08:02:11 [INFO]: Epoch 100 - training loss: 0.3491, validation loss: 1.9226
2024-06-02 08:04:18 [INFO]: Epoch 101 - training loss: 0.3487, validation loss: 1.9245
2024-06-02 08:06:24 [INFO]: Epoch 102 - training loss: 0.3488, validation loss: 1.9244
2024-06-02 08:08:32 [INFO]: Epoch 103 - training loss: 0.3484, validation loss: 1.9228
2024-06-02 08:10:38 [INFO]: Epoch 104 - training loss: 0.3484, validation loss: 1.9234
2024-06-02 08:12:47 [INFO]: Epoch 105 - training loss: 0.3479, validation loss: 1.9244
2024-06-02 08:14:54 [INFO]: Epoch 106 - training loss: 0.3477, validation loss: 1.9223
2024-06-02 08:17:01 [INFO]: Epoch 107 - training loss: 0.3477, validation loss: 1.9250
2024-06-02 08:19:09 [INFO]: Epoch 108 - training loss: 0.3474, validation loss: 1.9220
2024-06-02 08:21:14 [INFO]: Epoch 109 - training loss: 0.3473, validation loss: 1.9221
2024-06-02 08:23:21 [INFO]: Epoch 110 - training loss: 0.3467, validation loss: 1.9205
2024-06-02 08:25:28 [INFO]: Epoch 111 - training loss: 0.3462, validation loss: 1.9218
2024-06-02 08:27:34 [INFO]: Epoch 112 - training loss: 0.3461, validation loss: 1.9211
2024-06-02 08:29:42 [INFO]: Epoch 113 - training loss: 0.3467, validation loss: 1.9209
2024-06-02 08:31:48 [INFO]: Epoch 114 - training loss: 0.3467, validation loss: 1.9177
2024-06-02 08:33:55 [INFO]: Epoch 115 - training loss: 0.3464, validation loss: 1.9202
2024-06-02 08:36:01 [INFO]: Epoch 116 - training loss: 0.3456, validation loss: 1.9205
2024-06-02 08:38:08 [INFO]: Epoch 117 - training loss: 0.3450, validation loss: 1.9173
2024-06-02 08:40:15 [INFO]: Epoch 118 - training loss: 0.3451, validation loss: 1.9198
2024-06-02 08:42:22 [INFO]: Epoch 119 - training loss: 0.3451, validation loss: 1.9162
2024-06-02 08:44:29 [INFO]: Epoch 120 - training loss: 0.3447, validation loss: 1.9207
2024-06-02 08:46:35 [INFO]: Epoch 121 - training loss: 0.3449, validation loss: 1.9193
2024-06-02 08:48:42 [INFO]: Epoch 122 - training loss: 0.3444, validation loss: 1.9173
2024-06-02 08:50:49 [INFO]: Epoch 123 - training loss: 0.3443, validation loss: 1.9198
2024-06-02 08:52:57 [INFO]: Epoch 124 - training loss: 0.3439, validation loss: 1.9184
2024-06-02 08:55:04 [INFO]: Epoch 125 - training loss: 0.3438, validation loss: 1.9168
2024-06-02 08:57:10 [INFO]: Epoch 126 - training loss: 0.3434, validation loss: 1.9172
2024-06-02 08:59:17 [INFO]: Epoch 127 - training loss: 0.3438, validation loss: 1.9191
2024-06-02 09:01:26 [INFO]: Epoch 128 - training loss: 0.3439, validation loss: 1.9146
2024-06-02 09:03:33 [INFO]: Epoch 129 - training loss: 0.3435, validation loss: 1.9161
2024-06-02 09:05:40 [INFO]: Epoch 130 - training loss: 0.3439, validation loss: 1.9220
2024-06-02 09:07:46 [INFO]: Epoch 131 - training loss: 0.3430, validation loss: 1.9173
2024-06-02 09:09:53 [INFO]: Epoch 132 - training loss: 0.3428, validation loss: 1.9192
2024-06-02 09:11:59 [INFO]: Epoch 133 - training loss: 0.3422, validation loss: 1.9147
2024-06-02 09:14:07 [INFO]: Epoch 134 - training loss: 0.3420, validation loss: 1.9173
2024-06-02 09:16:14 [INFO]: Epoch 135 - training loss: 0.3426, validation loss: 1.9131
2024-06-02 09:18:20 [INFO]: Epoch 136 - training loss: 0.3430, validation loss: 1.9159
2024-06-02 09:20:29 [INFO]: Epoch 137 - training loss: 0.3422, validation loss: 1.9173
2024-06-02 09:22:35 [INFO]: Epoch 138 - training loss: 0.3417, validation loss: 1.9171
2024-06-02 09:24:42 [INFO]: Epoch 139 - training loss: 0.3416, validation loss: 1.9168
2024-06-02 09:26:48 [INFO]: Epoch 140 - training loss: 0.3410, validation loss: 1.9161
2024-06-02 09:28:55 [INFO]: Epoch 141 - training loss: 0.3406, validation loss: 1.9193
2024-06-02 09:31:01 [INFO]: Epoch 142 - training loss: 0.3408, validation loss: 1.9174
2024-06-02 09:33:08 [INFO]: Epoch 143 - training loss: 0.3407, validation loss: 1.9162
2024-06-02 09:35:15 [INFO]: Epoch 144 - training loss: 0.3401, validation loss: 1.9180
2024-06-02 09:37:21 [INFO]: Epoch 145 - training loss: 0.3402, validation loss: 1.9196
2024-06-02 09:37:21 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 09:37:21 [INFO]: Finished training. The best model is from epoch#135.
2024-06-02 09:37:22 [INFO]: Saved the model to results_point_rate01/Electricity/BRITS_Electricity/round_1/20240602_T042712/BRITS.pypots
2024-06-02 09:38:05 [INFO]: Successfully saved to results_point_rate01/Electricity/BRITS_Electricity/round_1/imputation.pkl
2024-06-02 09:38:05 [INFO]: Round1 - BRITS on Electricity: MAE=0.9720, MSE=2.2206, MRE=0.5200
2024-06-02 09:38:05 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 09:38:05 [INFO]: Using the given device: cuda:0
2024-06-02 09:38:05 [INFO]: Model files will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_2/20240602_T093805
2024-06-02 09:38:05 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_2/20240602_T093805/tensorboard
2024-06-02 09:38:05 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 17,082,800
2024-06-02 09:40:15 [INFO]: Epoch 001 - training loss: 0.9739, validation loss: 2.9015
2024-06-02 09:42:22 [INFO]: Epoch 002 - training loss: 0.6760, validation loss: 2.7171
2024-06-02 09:44:29 [INFO]: Epoch 003 - training loss: 0.6066, validation loss: 2.6147
2024-06-02 09:46:35 [INFO]: Epoch 004 - training loss: 0.5695, validation loss: 2.5701
2024-06-02 09:48:42 [INFO]: Epoch 005 - training loss: 0.5443, validation loss: 2.5341
2024-06-02 09:50:49 [INFO]: Epoch 006 - training loss: 0.5258, validation loss: 2.5088
2024-06-02 09:52:55 [INFO]: Epoch 007 - training loss: 0.5106, validation loss: 2.4782
2024-06-02 09:55:02 [INFO]: Epoch 008 - training loss: 0.4982, validation loss: 2.4547
2024-06-02 09:57:09 [INFO]: Epoch 009 - training loss: 0.4890, validation loss: 2.4321
2024-06-02 09:59:15 [INFO]: Epoch 010 - training loss: 0.4790, validation loss: 2.4084
2024-06-02 10:01:22 [INFO]: Epoch 011 - training loss: 0.4709, validation loss: 2.3885
2024-06-02 10:03:29 [INFO]: Epoch 012 - training loss: 0.4636, validation loss: 2.3688
2024-06-02 10:05:44 [INFO]: Epoch 013 - training loss: 0.4576, validation loss: 2.3524
2024-06-02 10:08:27 [INFO]: Epoch 014 - training loss: 0.4515, validation loss: 2.3352
2024-06-02 10:11:10 [INFO]: Epoch 015 - training loss: 0.4455, validation loss: 2.3199
2024-06-02 10:13:53 [INFO]: Epoch 016 - training loss: 0.4406, validation loss: 2.3002
2024-06-02 10:16:35 [INFO]: Epoch 017 - training loss: 0.4369, validation loss: 2.2896
2024-06-02 10:19:18 [INFO]: Epoch 018 - training loss: 0.4321, validation loss: 2.2759
2024-06-02 10:22:01 [INFO]: Epoch 019 - training loss: 0.4278, validation loss: 2.2585
2024-06-02 10:24:00 [INFO]: Epoch 020 - training loss: 0.4247, validation loss: 2.2449
2024-06-02 10:26:06 [INFO]: Epoch 021 - training loss: 0.4211, validation loss: 2.2308
2024-06-02 10:28:13 [INFO]: Epoch 022 - training loss: 0.4180, validation loss: 2.2203
2024-06-02 10:30:20 [INFO]: Epoch 023 - training loss: 0.4159, validation loss: 2.2089
2024-06-02 10:32:27 [INFO]: Epoch 024 - training loss: 0.4117, validation loss: 2.1908
2024-06-02 10:34:33 [INFO]: Epoch 025 - training loss: 0.4091, validation loss: 2.1764
2024-06-02 10:36:40 [INFO]: Epoch 026 - training loss: 0.4066, validation loss: 2.1698
2024-06-02 10:38:47 [INFO]: Epoch 027 - training loss: 0.4045, validation loss: 2.1592
2024-06-02 10:40:54 [INFO]: Epoch 028 - training loss: 0.4019, validation loss: 2.1438
2024-06-02 10:43:00 [INFO]: Epoch 029 - training loss: 0.3993, validation loss: 2.1319
2024-06-02 10:45:07 [INFO]: Epoch 030 - training loss: 0.3975, validation loss: 2.1223
2024-06-02 10:47:14 [INFO]: Epoch 031 - training loss: 0.3954, validation loss: 2.1125
2024-06-02 10:49:22 [INFO]: Epoch 032 - training loss: 0.3935, validation loss: 2.1057
2024-06-02 10:51:29 [INFO]: Epoch 033 - training loss: 0.3916, validation loss: 2.0961
2024-06-02 10:53:38 [INFO]: Epoch 034 - training loss: 0.3901, validation loss: 2.0860
2024-06-02 10:55:46 [INFO]: Epoch 035 - training loss: 0.3888, validation loss: 2.0779
2024-06-02 10:57:52 [INFO]: Epoch 036 - training loss: 0.3877, validation loss: 2.0717
2024-06-02 10:59:58 [INFO]: Epoch 037 - training loss: 0.3861, validation loss: 2.0627
2024-06-02 11:02:05 [INFO]: Epoch 038 - training loss: 0.3840, validation loss: 2.0561
2024-06-02 11:04:12 [INFO]: Epoch 039 - training loss: 0.3836, validation loss: 2.0527
2024-06-02 11:06:19 [INFO]: Epoch 040 - training loss: 0.3818, validation loss: 2.0479
2024-06-02 11:08:25 [INFO]: Epoch 041 - training loss: 0.3800, validation loss: 2.0430
2024-06-02 11:10:33 [INFO]: Epoch 042 - training loss: 0.3794, validation loss: 2.0377
2024-06-02 11:12:41 [INFO]: Epoch 043 - training loss: 0.3776, validation loss: 2.0308
2024-06-02 11:14:48 [INFO]: Epoch 044 - training loss: 0.3768, validation loss: 2.0303
2024-06-02 11:16:53 [INFO]: Epoch 045 - training loss: 0.3766, validation loss: 2.0198
2024-06-02 11:18:59 [INFO]: Epoch 046 - training loss: 0.3755, validation loss: 2.0177
2024-06-02 11:21:06 [INFO]: Epoch 047 - training loss: 0.3745, validation loss: 2.0126
2024-06-02 11:23:13 [INFO]: Epoch 048 - training loss: 0.3736, validation loss: 2.0079
2024-06-02 11:25:19 [INFO]: Epoch 049 - training loss: 0.3727, validation loss: 2.0079
2024-06-02 11:27:26 [INFO]: Epoch 050 - training loss: 0.3715, validation loss: 2.0060
2024-06-02 11:29:33 [INFO]: Epoch 051 - training loss: 0.3704, validation loss: 1.9998
2024-06-02 11:31:39 [INFO]: Epoch 052 - training loss: 0.3709, validation loss: 1.9903
2024-06-02 11:33:46 [INFO]: Epoch 053 - training loss: 0.3698, validation loss: 1.9843
2024-06-02 11:35:53 [INFO]: Epoch 054 - training loss: 0.3677, validation loss: 1.9792
2024-06-02 11:37:59 [INFO]: Epoch 055 - training loss: 0.3675, validation loss: 1.9782
2024-06-02 11:40:06 [INFO]: Epoch 056 - training loss: 0.3670, validation loss: 1.9801
2024-06-02 11:42:13 [INFO]: Epoch 057 - training loss: 0.3665, validation loss: 1.9734
2024-06-02 11:44:19 [INFO]: Epoch 058 - training loss: 0.3654, validation loss: 1.9672
2024-06-02 11:46:27 [INFO]: Epoch 059 - training loss: 0.3647, validation loss: 1.9688
2024-06-02 11:48:33 [INFO]: Epoch 060 - training loss: 0.3646, validation loss: 1.9654
2024-06-02 11:50:41 [INFO]: Epoch 061 - training loss: 0.3644, validation loss: 1.9582
2024-06-02 11:52:47 [INFO]: Epoch 062 - training loss: 0.3632, validation loss: 1.9540
2024-06-02 11:54:54 [INFO]: Epoch 063 - training loss: 0.3634, validation loss: 1.9558
2024-06-02 11:57:01 [INFO]: Epoch 064 - training loss: 0.3620, validation loss: 1.9519
2024-06-02 11:59:07 [INFO]: Epoch 065 - training loss: 0.3617, validation loss: 1.9478
2024-06-02 12:01:14 [INFO]: Epoch 066 - training loss: 0.3617, validation loss: 1.9462
2024-06-02 12:03:21 [INFO]: Epoch 067 - training loss: 0.3605, validation loss: 1.9422
2024-06-02 12:05:27 [INFO]: Epoch 068 - training loss: 0.3600, validation loss: 1.9415
2024-06-02 12:07:33 [INFO]: Epoch 069 - training loss: 0.3594, validation loss: 1.9407
2024-06-02 12:09:41 [INFO]: Epoch 070 - training loss: 0.3597, validation loss: 1.9345
2024-06-02 12:11:50 [INFO]: Epoch 071 - training loss: 0.3590, validation loss: 1.9395
2024-06-02 12:13:57 [INFO]: Epoch 072 - training loss: 0.3586, validation loss: 1.9331
2024-06-02 12:16:05 [INFO]: Epoch 073 - training loss: 0.3578, validation loss: 1.9315
2024-06-02 12:18:12 [INFO]: Epoch 074 - training loss: 0.3573, validation loss: 1.9250
2024-06-02 12:20:19 [INFO]: Epoch 075 - training loss: 0.3564, validation loss: 1.9314
2024-06-02 12:22:25 [INFO]: Epoch 076 - training loss: 0.3558, validation loss: 1.9272
2024-06-02 12:24:32 [INFO]: Epoch 077 - training loss: 0.3556, validation loss: 1.9288
2024-06-02 12:26:39 [INFO]: Epoch 078 - training loss: 0.3558, validation loss: 1.9218
2024-06-02 12:28:45 [INFO]: Epoch 079 - training loss: 0.3555, validation loss: 1.9258
2024-06-02 12:30:52 [INFO]: Epoch 080 - training loss: 0.3549, validation loss: 1.9213
2024-06-02 12:32:58 [INFO]: Epoch 081 - training loss: 0.3542, validation loss: 1.9190
2024-06-02 12:35:07 [INFO]: Epoch 082 - training loss: 0.3542, validation loss: 1.9196
2024-06-02 12:37:14 [INFO]: Epoch 083 - training loss: 0.3541, validation loss: 1.9183
2024-06-02 12:39:21 [INFO]: Epoch 084 - training loss: 0.3531, validation loss: 1.9178
2024-06-02 12:41:27 [INFO]: Epoch 085 - training loss: 0.3527, validation loss: 1.9166
2024-06-02 12:43:34 [INFO]: Epoch 086 - training loss: 0.3527, validation loss: 1.9103
2024-06-02 12:45:41 [INFO]: Epoch 087 - training loss: 0.3529, validation loss: 1.9122
2024-06-02 12:47:47 [INFO]: Epoch 088 - training loss: 0.3523, validation loss: 1.9145
2024-06-02 12:49:54 [INFO]: Epoch 089 - training loss: 0.3525, validation loss: 1.9117
2024-06-02 12:52:00 [INFO]: Epoch 090 - training loss: 0.3520, validation loss: 1.9093
2024-06-02 12:54:09 [INFO]: Epoch 091 - training loss: 0.3513, validation loss: 1.9122
2024-06-02 12:56:16 [INFO]: Epoch 092 - training loss: 0.3516, validation loss: 1.9092
2024-06-02 12:58:23 [INFO]: Epoch 093 - training loss: 0.3512, validation loss: 1.9086
2024-06-02 13:00:31 [INFO]: Epoch 094 - training loss: 0.3504, validation loss: 1.9037
2024-06-02 13:02:38 [INFO]: Epoch 095 - training loss: 0.3498, validation loss: 1.9050
2024-06-02 13:04:45 [INFO]: Epoch 096 - training loss: 0.3495, validation loss: 1.9053
2024-06-02 13:06:51 [INFO]: Epoch 097 - training loss: 0.3493, validation loss: 1.9057
2024-06-02 13:08:58 [INFO]: Epoch 098 - training loss: 0.3489, validation loss: 1.9019
2024-06-02 13:11:05 [INFO]: Epoch 099 - training loss: 0.3501, validation loss: 1.9026
2024-06-02 13:13:11 [INFO]: Epoch 100 - training loss: 0.3496, validation loss: 1.9019
2024-06-02 13:15:18 [INFO]: Epoch 101 - training loss: 0.3494, validation loss: 1.8996
2024-06-02 13:17:25 [INFO]: Epoch 102 - training loss: 0.3483, validation loss: 1.9022
2024-06-02 13:19:31 [INFO]: Epoch 103 - training loss: 0.3478, validation loss: 1.8990
2024-06-02 13:21:38 [INFO]: Epoch 104 - training loss: 0.3476, validation loss: 1.9000
2024-06-02 13:23:45 [INFO]: Epoch 105 - training loss: 0.3478, validation loss: 1.8974
2024-06-02 13:25:51 [INFO]: Epoch 106 - training loss: 0.3480, validation loss: 1.8955
2024-06-02 13:27:58 [INFO]: Epoch 107 - training loss: 0.3476, validation loss: 1.8965
2024-06-02 13:30:05 [INFO]: Epoch 108 - training loss: 0.3473, validation loss: 1.8960
2024-06-02 13:32:11 [INFO]: Epoch 109 - training loss: 0.3468, validation loss: 1.8988
2024-06-02 13:34:18 [INFO]: Epoch 110 - training loss: 0.3462, validation loss: 1.8922
2024-06-02 13:36:25 [INFO]: Epoch 111 - training loss: 0.3462, validation loss: 1.8955
2024-06-02 13:38:34 [INFO]: Epoch 112 - training loss: 0.3465, validation loss: 1.8975
2024-06-02 13:40:40 [INFO]: Epoch 113 - training loss: 0.3459, validation loss: 1.8907
2024-06-02 13:42:47 [INFO]: Epoch 114 - training loss: 0.3461, validation loss: 1.8957
2024-06-02 13:44:54 [INFO]: Epoch 115 - training loss: 0.3455, validation loss: 1.8922
2024-06-02 13:47:35 [INFO]: Epoch 116 - training loss: 0.3464, validation loss: 1.8927
2024-06-02 13:50:18 [INFO]: Epoch 117 - training loss: 0.3471, validation loss: 1.8953
2024-06-02 13:53:01 [INFO]: Epoch 118 - training loss: 0.3458, validation loss: 1.8925
2024-06-02 13:55:44 [INFO]: Epoch 119 - training loss: 0.3448, validation loss: 1.8949
2024-06-02 13:58:26 [INFO]: Epoch 120 - training loss: 0.3447, validation loss: 1.8919
2024-06-02 14:01:10 [INFO]: Epoch 121 - training loss: 0.3441, validation loss: 1.8894
2024-06-02 14:03:07 [INFO]: Epoch 122 - training loss: 0.3440, validation loss: 1.8912
2024-06-02 14:05:14 [INFO]: Epoch 123 - training loss: 0.3440, validation loss: 1.8878
2024-06-02 14:07:20 [INFO]: Epoch 124 - training loss: 0.3435, validation loss: 1.8884
2024-06-02 14:09:27 [INFO]: Epoch 125 - training loss: 0.3433, validation loss: 1.8868
2024-06-02 14:11:34 [INFO]: Epoch 126 - training loss: 0.3436, validation loss: 1.8904
2024-06-02 14:13:40 [INFO]: Epoch 127 - training loss: 0.3431, validation loss: 1.8885
2024-06-02 14:15:45 [INFO]: Epoch 128 - training loss: 0.3429, validation loss: 1.8882
2024-06-02 14:17:50 [INFO]: Epoch 129 - training loss: 0.3426, validation loss: 1.8839
2024-06-02 14:19:55 [INFO]: Epoch 130 - training loss: 0.3427, validation loss: 1.8898
2024-06-02 14:22:00 [INFO]: Epoch 131 - training loss: 0.3425, validation loss: 1.8897
2024-06-02 14:24:05 [INFO]: Epoch 132 - training loss: 0.3424, validation loss: 1.8884
2024-06-02 14:26:09 [INFO]: Epoch 133 - training loss: 0.3425, validation loss: 1.8886
2024-06-02 14:28:13 [INFO]: Epoch 134 - training loss: 0.3421, validation loss: 1.8903
2024-06-02 14:30:18 [INFO]: Epoch 135 - training loss: 0.3424, validation loss: 1.8866
2024-06-02 14:32:22 [INFO]: Epoch 136 - training loss: 0.3427, validation loss: 1.8877
2024-06-02 14:34:28 [INFO]: Epoch 137 - training loss: 0.3416, validation loss: 1.8884
2024-06-02 14:36:34 [INFO]: Epoch 138 - training loss: 0.3418, validation loss: 1.8868
2024-06-02 14:38:39 [INFO]: Epoch 139 - training loss: 0.3416, validation loss: 1.8871
2024-06-02 14:38:39 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 14:38:39 [INFO]: Finished training. The best model is from epoch#129.
2024-06-02 14:38:39 [INFO]: Saved the model to results_point_rate01/Electricity/BRITS_Electricity/round_2/20240602_T093805/BRITS.pypots
2024-06-02 14:39:20 [INFO]: Successfully saved to results_point_rate01/Electricity/BRITS_Electricity/round_2/imputation.pkl
2024-06-02 14:39:20 [INFO]: Round2 - BRITS on Electricity: MAE=0.9453, MSE=2.1094, MRE=0.5057
2024-06-02 14:39:20 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 14:39:20 [INFO]: Using the given device: cuda:0
2024-06-02 14:39:20 [INFO]: Model files will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_3/20240602_T143920
2024-06-02 14:39:20 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_3/20240602_T143920/tensorboard
2024-06-02 14:39:20 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 17,082,800
2024-06-02 14:41:31 [INFO]: Epoch 001 - training loss: 0.9762, validation loss: 2.8806
2024-06-02 14:43:40 [INFO]: Epoch 002 - training loss: 0.6762, validation loss: 2.7098
2024-06-02 14:45:46 [INFO]: Epoch 003 - training loss: 0.6064, validation loss: 2.6054
2024-06-02 14:47:53 [INFO]: Epoch 004 - training loss: 0.5684, validation loss: 2.5505
2024-06-02 14:50:00 [INFO]: Epoch 005 - training loss: 0.5434, validation loss: 2.5156
2024-06-02 14:52:06 [INFO]: Epoch 006 - training loss: 0.5245, validation loss: 2.4896
2024-06-02 14:54:13 [INFO]: Epoch 007 - training loss: 0.5100, validation loss: 2.4638
2024-06-02 14:56:20 [INFO]: Epoch 008 - training loss: 0.4979, validation loss: 2.4399
2024-06-02 14:58:26 [INFO]: Epoch 009 - training loss: 0.4884, validation loss: 2.4176
2024-06-02 15:00:33 [INFO]: Epoch 010 - training loss: 0.4790, validation loss: 2.3944
2024-06-02 15:02:40 [INFO]: Epoch 011 - training loss: 0.4706, validation loss: 2.3735
2024-06-02 15:04:46 [INFO]: Epoch 012 - training loss: 0.4631, validation loss: 2.3610
2024-06-02 15:06:53 [INFO]: Epoch 013 - training loss: 0.4567, validation loss: 2.3424
2024-06-02 15:08:59 [INFO]: Epoch 014 - training loss: 0.4509, validation loss: 2.3202
2024-06-02 15:11:07 [INFO]: Epoch 015 - training loss: 0.4451, validation loss: 2.3062
2024-06-02 15:13:13 [INFO]: Epoch 016 - training loss: 0.4405, validation loss: 2.2948
2024-06-02 15:15:20 [INFO]: Epoch 017 - training loss: 0.4358, validation loss: 2.2782
2024-06-02 15:17:27 [INFO]: Epoch 018 - training loss: 0.4315, validation loss: 2.2636
2024-06-02 15:19:34 [INFO]: Epoch 019 - training loss: 0.4274, validation loss: 2.2452
2024-06-02 15:21:40 [INFO]: Epoch 020 - training loss: 0.4232, validation loss: 2.2326
2024-06-02 15:23:47 [INFO]: Epoch 021 - training loss: 0.4201, validation loss: 2.2189
2024-06-02 15:25:56 [INFO]: Epoch 022 - training loss: 0.4167, validation loss: 2.2087
2024-06-02 15:28:03 [INFO]: Epoch 023 - training loss: 0.4136, validation loss: 2.1959
2024-06-02 15:30:09 [INFO]: Epoch 024 - training loss: 0.4109, validation loss: 2.1793
2024-06-02 15:32:16 [INFO]: Epoch 025 - training loss: 0.4080, validation loss: 2.1706
2024-06-02 15:34:22 [INFO]: Epoch 026 - training loss: 0.4061, validation loss: 2.1586
2024-06-02 15:36:30 [INFO]: Epoch 027 - training loss: 0.4037, validation loss: 2.1507
2024-06-02 15:38:37 [INFO]: Epoch 028 - training loss: 0.4017, validation loss: 2.1364
2024-06-02 15:40:43 [INFO]: Epoch 029 - training loss: 0.3996, validation loss: 2.1226
2024-06-02 15:42:50 [INFO]: Epoch 030 - training loss: 0.3973, validation loss: 2.1104
2024-06-02 15:44:59 [INFO]: Epoch 031 - training loss: 0.3952, validation loss: 2.1037
2024-06-02 15:47:06 [INFO]: Epoch 032 - training loss: 0.3933, validation loss: 2.0919
2024-06-02 15:49:12 [INFO]: Epoch 033 - training loss: 0.3912, validation loss: 2.0867
2024-06-02 15:51:19 [INFO]: Epoch 034 - training loss: 0.3896, validation loss: 2.0798
2024-06-02 15:53:26 [INFO]: Epoch 035 - training loss: 0.3878, validation loss: 2.0703
2024-06-02 15:55:32 [INFO]: Epoch 036 - training loss: 0.3871, validation loss: 2.0681
2024-06-02 15:57:39 [INFO]: Epoch 037 - training loss: 0.3857, validation loss: 2.0632
2024-06-02 15:59:45 [INFO]: Epoch 038 - training loss: 0.3839, validation loss: 2.0513
2024-06-02 16:01:52 [INFO]: Epoch 039 - training loss: 0.3824, validation loss: 2.0478
2024-06-02 16:04:01 [INFO]: Epoch 040 - training loss: 0.3811, validation loss: 2.0406
2024-06-02 16:06:08 [INFO]: Epoch 041 - training loss: 0.3803, validation loss: 2.0357
2024-06-02 16:08:14 [INFO]: Epoch 042 - training loss: 0.3787, validation loss: 2.0301
2024-06-02 16:10:23 [INFO]: Epoch 043 - training loss: 0.3780, validation loss: 2.0295
2024-06-02 16:12:30 [INFO]: Epoch 044 - training loss: 0.3771, validation loss: 2.0267
2024-06-02 16:14:36 [INFO]: Epoch 045 - training loss: 0.3757, validation loss: 2.0229
2024-06-02 16:16:43 [INFO]: Epoch 046 - training loss: 0.3745, validation loss: 2.0151
2024-06-02 16:18:50 [INFO]: Epoch 047 - training loss: 0.3735, validation loss: 2.0050
2024-06-02 16:20:57 [INFO]: Epoch 048 - training loss: 0.3727, validation loss: 2.0030
2024-06-02 16:23:04 [INFO]: Epoch 049 - training loss: 0.3717, validation loss: 2.0009
2024-06-02 16:25:10 [INFO]: Epoch 050 - training loss: 0.3711, validation loss: 1.9991
2024-06-02 16:27:17 [INFO]: Epoch 051 - training loss: 0.3710, validation loss: 2.0026
2024-06-02 16:29:23 [INFO]: Epoch 052 - training loss: 0.3699, validation loss: 1.9881
2024-06-02 16:31:30 [INFO]: Epoch 053 - training loss: 0.3684, validation loss: 1.9802
2024-06-02 16:33:37 [INFO]: Epoch 054 - training loss: 0.3679, validation loss: 1.9857
2024-06-02 16:35:43 [INFO]: Epoch 055 - training loss: 0.3676, validation loss: 1.9735
2024-06-02 16:37:50 [INFO]: Epoch 056 - training loss: 0.3660, validation loss: 1.9691
2024-06-02 16:39:56 [INFO]: Epoch 057 - training loss: 0.3658, validation loss: 1.9717
2024-06-02 16:42:03 [INFO]: Epoch 058 - training loss: 0.3652, validation loss: 1.9648
2024-06-02 16:44:10 [INFO]: Epoch 059 - training loss: 0.3649, validation loss: 1.9591
2024-06-02 16:46:16 [INFO]: Epoch 060 - training loss: 0.3650, validation loss: 1.9588
2024-06-02 16:48:25 [INFO]: Epoch 061 - training loss: 0.3633, validation loss: 1.9548
2024-06-02 16:50:32 [INFO]: Epoch 062 - training loss: 0.3629, validation loss: 1.9547
2024-06-02 16:52:38 [INFO]: Epoch 063 - training loss: 0.3621, validation loss: 1.9486
2024-06-02 16:54:45 [INFO]: Epoch 064 - training loss: 0.3613, validation loss: 1.9453
2024-06-02 16:56:51 [INFO]: Epoch 065 - training loss: 0.3609, validation loss: 1.9395
2024-06-02 16:58:58 [INFO]: Epoch 066 - training loss: 0.3608, validation loss: 1.9395
2024-06-02 17:01:05 [INFO]: Epoch 067 - training loss: 0.3605, validation loss: 1.9371
2024-06-02 17:03:11 [INFO]: Epoch 068 - training loss: 0.3599, validation loss: 1.9346
2024-06-02 17:05:18 [INFO]: Epoch 069 - training loss: 0.3594, validation loss: 1.9340
2024-06-02 17:07:26 [INFO]: Epoch 070 - training loss: 0.3589, validation loss: 1.9305
2024-06-02 17:09:33 [INFO]: Epoch 071 - training loss: 0.3580, validation loss: 1.9284
2024-06-02 17:11:40 [INFO]: Epoch 072 - training loss: 0.3576, validation loss: 1.9315
2024-06-02 17:13:47 [INFO]: Epoch 073 - training loss: 0.3581, validation loss: 1.9268
2024-06-02 17:15:53 [INFO]: Epoch 074 - training loss: 0.3566, validation loss: 1.9230
2024-06-02 17:18:00 [INFO]: Epoch 075 - training loss: 0.3564, validation loss: 1.9189
2024-06-02 17:20:06 [INFO]: Epoch 076 - training loss: 0.3567, validation loss: 1.9197
2024-06-02 17:22:13 [INFO]: Epoch 077 - training loss: 0.3562, validation loss: 1.9200
2024-06-02 17:24:19 [INFO]: Epoch 078 - training loss: 0.3557, validation loss: 1.9185
2024-06-02 17:26:28 [INFO]: Epoch 079 - training loss: 0.3553, validation loss: 1.9134
2024-06-02 17:28:35 [INFO]: Epoch 080 - training loss: 0.3551, validation loss: 1.9144
2024-06-02 17:30:41 [INFO]: Epoch 081 - training loss: 0.3550, validation loss: 1.9103
2024-06-02 17:32:50 [INFO]: Epoch 082 - training loss: 0.3547, validation loss: 1.9107
2024-06-02 17:35:32 [INFO]: Epoch 083 - training loss: 0.3537, validation loss: 1.9089
2024-06-02 17:38:15 [INFO]: Epoch 084 - training loss: 0.3532, validation loss: 1.9104
2024-06-02 17:40:58 [INFO]: Epoch 085 - training loss: 0.3532, validation loss: 1.9082
2024-06-02 17:43:40 [INFO]: Epoch 086 - training loss: 0.3531, validation loss: 1.9102
2024-06-02 17:46:23 [INFO]: Epoch 087 - training loss: 0.3529, validation loss: 1.9038
2024-06-02 17:49:06 [INFO]: Epoch 088 - training loss: 0.3524, validation loss: 1.9062
2024-06-02 17:50:39 [INFO]: Epoch 089 - training loss: 0.3522, validation loss: 1.9020
2024-06-02 17:50:48 [INFO]: Epoch 090 - training loss: 0.3519, validation loss: 1.9017
2024-06-02 17:50:57 [INFO]: Epoch 091 - training loss: 0.3524, validation loss: 1.8997
2024-06-02 17:51:05 [INFO]: Epoch 092 - training loss: 0.3515, validation loss: 1.9029
2024-06-02 17:51:14 [INFO]: Epoch 093 - training loss: 0.3512, validation loss: 1.8982
2024-06-02 17:51:22 [INFO]: Epoch 094 - training loss: 0.3505, validation loss: 1.8925
2024-06-02 17:51:31 [INFO]: Epoch 095 - training loss: 0.3502, validation loss: 1.8962
2024-06-02 17:51:39 [INFO]: Epoch 096 - training loss: 0.3498, validation loss: 1.8977
2024-06-02 17:51:48 [INFO]: Epoch 097 - training loss: 0.3497, validation loss: 1.8970
2024-06-02 17:51:57 [INFO]: Epoch 098 - training loss: 0.3488, validation loss: 1.8932
2024-06-02 17:52:05 [INFO]: Epoch 099 - training loss: 0.3487, validation loss: 1.8942
2024-06-02 17:52:14 [INFO]: Epoch 100 - training loss: 0.3487, validation loss: 1.8918
2024-06-02 17:52:22 [INFO]: Epoch 101 - training loss: 0.3484, validation loss: 1.8910
2024-06-02 17:52:31 [INFO]: Epoch 102 - training loss: 0.3483, validation loss: 1.8901
2024-06-02 17:52:40 [INFO]: Epoch 103 - training loss: 0.3482, validation loss: 1.8891
2024-06-02 17:52:48 [INFO]: Epoch 104 - training loss: 0.3478, validation loss: 1.8948
2024-06-02 17:52:57 [INFO]: Epoch 105 - training loss: 0.3486, validation loss: 1.8888
2024-06-02 17:53:05 [INFO]: Epoch 106 - training loss: 0.3475, validation loss: 1.8873
2024-06-02 17:53:14 [INFO]: Epoch 107 - training loss: 0.3474, validation loss: 1.8884
2024-06-02 17:53:22 [INFO]: Epoch 108 - training loss: 0.3473, validation loss: 1.8899
2024-06-02 17:53:31 [INFO]: Epoch 109 - training loss: 0.3466, validation loss: 1.8870
2024-06-02 17:53:40 [INFO]: Epoch 110 - training loss: 0.3462, validation loss: 1.8853
2024-06-02 17:53:48 [INFO]: Epoch 111 - training loss: 0.3462, validation loss: 1.8847
2024-06-02 17:53:57 [INFO]: Epoch 112 - training loss: 0.3462, validation loss: 1.8853
2024-06-02 17:54:05 [INFO]: Epoch 113 - training loss: 0.3455, validation loss: 1.8877
2024-06-02 17:54:14 [INFO]: Epoch 114 - training loss: 0.3456, validation loss: 1.8830
2024-06-02 17:54:22 [INFO]: Epoch 115 - training loss: 0.3453, validation loss: 1.8866
2024-06-02 17:54:31 [INFO]: Epoch 116 - training loss: 0.3457, validation loss: 1.8867
2024-06-02 17:54:40 [INFO]: Epoch 117 - training loss: 0.3455, validation loss: 1.8838
2024-06-02 17:54:48 [INFO]: Epoch 118 - training loss: 0.3459, validation loss: 1.8847
2024-06-02 17:54:57 [INFO]: Epoch 119 - training loss: 0.3455, validation loss: 1.8855
2024-06-02 17:55:05 [INFO]: Epoch 120 - training loss: 0.3450, validation loss: 1.8827
2024-06-02 17:55:14 [INFO]: Epoch 121 - training loss: 0.3439, validation loss: 1.8856
2024-06-02 17:55:23 [INFO]: Epoch 122 - training loss: 0.3440, validation loss: 1.8836
2024-06-02 17:55:31 [INFO]: Epoch 123 - training loss: 0.3445, validation loss: 1.8844
2024-06-02 17:55:40 [INFO]: Epoch 124 - training loss: 0.3435, validation loss: 1.8849
2024-06-02 17:55:48 [INFO]: Epoch 125 - training loss: 0.3442, validation loss: 1.8878
2024-06-02 17:55:57 [INFO]: Epoch 126 - training loss: 0.3438, validation loss: 1.8859
2024-06-02 17:56:05 [INFO]: Epoch 127 - training loss: 0.3433, validation loss: 1.8872
2024-06-02 17:56:14 [INFO]: Epoch 128 - training loss: 0.3434, validation loss: 1.8818
2024-06-02 17:56:23 [INFO]: Epoch 129 - training loss: 0.3426, validation loss: 1.8812
2024-06-02 17:56:31 [INFO]: Epoch 130 - training loss: 0.3427, validation loss: 1.8832
2024-06-02 17:56:40 [INFO]: Epoch 131 - training loss: 0.3430, validation loss: 1.8841
2024-06-02 17:56:48 [INFO]: Epoch 132 - training loss: 0.3429, validation loss: 1.8850
2024-06-02 17:56:57 [INFO]: Epoch 133 - training loss: 0.3423, validation loss: 1.8834
2024-06-02 17:57:05 [INFO]: Epoch 134 - training loss: 0.3422, validation loss: 1.8814
2024-06-02 17:57:14 [INFO]: Epoch 135 - training loss: 0.3415, validation loss: 1.8828
2024-06-02 17:57:23 [INFO]: Epoch 136 - training loss: 0.3417, validation loss: 1.8804
2024-06-02 17:57:31 [INFO]: Epoch 137 - training loss: 0.3421, validation loss: 1.8854
2024-06-02 17:57:40 [INFO]: Epoch 138 - training loss: 0.3424, validation loss: 1.8887
2024-06-02 17:57:48 [INFO]: Epoch 139 - training loss: 0.3418, validation loss: 1.8823
2024-06-02 17:57:57 [INFO]: Epoch 140 - training loss: 0.3412, validation loss: 1.8863
2024-06-02 17:58:06 [INFO]: Epoch 141 - training loss: 0.3410, validation loss: 1.8801
2024-06-02 17:58:14 [INFO]: Epoch 142 - training loss: 0.3411, validation loss: 1.8827
2024-06-02 17:58:23 [INFO]: Epoch 143 - training loss: 0.3420, validation loss: 1.8844
2024-06-02 17:58:31 [INFO]: Epoch 144 - training loss: 0.3411, validation loss: 1.8827
2024-06-02 17:58:40 [INFO]: Epoch 145 - training loss: 0.3403, validation loss: 1.8874
2024-06-02 17:58:48 [INFO]: Epoch 146 - training loss: 0.3406, validation loss: 1.8877
2024-06-02 17:58:57 [INFO]: Epoch 147 - training loss: 0.3401, validation loss: 1.8843
2024-06-02 17:59:05 [INFO]: Epoch 148 - training loss: 0.3398, validation loss: 1.8844
2024-06-02 17:59:14 [INFO]: Epoch 149 - training loss: 0.3404, validation loss: 1.8849
2024-06-02 17:59:23 [INFO]: Epoch 150 - training loss: 0.3400, validation loss: 1.8845
2024-06-02 17:59:31 [INFO]: Epoch 151 - training loss: 0.3403, validation loss: 1.8865
2024-06-02 17:59:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 17:59:31 [INFO]: Finished training. The best model is from epoch#141.
2024-06-02 17:59:31 [INFO]: Saved the model to results_point_rate01/Electricity/BRITS_Electricity/round_3/20240602_T143920/BRITS.pypots
2024-06-02 17:59:33 [INFO]: Successfully saved to results_point_rate01/Electricity/BRITS_Electricity/round_3/imputation.pkl
2024-06-02 17:59:33 [INFO]: Round3 - BRITS on Electricity: MAE=0.9641, MSE=2.1211, MRE=0.5157
2024-06-02 17:59:33 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 17:59:33 [INFO]: Using the given device: cuda:0
2024-06-02 17:59:33 [INFO]: Model files will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_4/20240602_T175933
2024-06-02 17:59:33 [INFO]: Tensorboard file will be saved to results_point_rate01/Electricity/BRITS_Electricity/round_4/20240602_T175933/tensorboard
2024-06-02 17:59:33 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 17,082,800
2024-06-02 17:59:45 [INFO]: Epoch 001 - training loss: 0.9766, validation loss: 2.8808
2024-06-02 17:59:53 [INFO]: Epoch 002 - training loss: 0.6747, validation loss: 2.6930
2024-06-02 18:00:02 [INFO]: Epoch 003 - training loss: 0.6064, validation loss: 2.5936
2024-06-02 18:00:10 [INFO]: Epoch 004 - training loss: 0.5685, validation loss: 2.5482
2024-06-02 18:00:19 [INFO]: Epoch 005 - training loss: 0.5439, validation loss: 2.5156
2024-06-02 18:00:28 [INFO]: Epoch 006 - training loss: 0.5248, validation loss: 2.4917
2024-06-02 18:00:36 [INFO]: Epoch 007 - training loss: 0.5103, validation loss: 2.4662
2024-06-02 18:00:45 [INFO]: Epoch 008 - training loss: 0.4987, validation loss: 2.4409
2024-06-02 18:00:53 [INFO]: Epoch 009 - training loss: 0.4881, validation loss: 2.4214
2024-06-02 18:01:02 [INFO]: Epoch 010 - training loss: 0.4788, validation loss: 2.4014
2024-06-02 18:01:10 [INFO]: Epoch 011 - training loss: 0.4707, validation loss: 2.3819
2024-06-02 18:01:19 [INFO]: Epoch 012 - training loss: 0.4637, validation loss: 2.3629
2024-06-02 18:01:27 [INFO]: Epoch 013 - training loss: 0.4570, validation loss: 2.3447
2024-06-02 18:01:36 [INFO]: Epoch 014 - training loss: 0.4513, validation loss: 2.3273
2024-06-02 18:01:44 [INFO]: Epoch 015 - training loss: 0.4452, validation loss: 2.3098
2024-06-02 18:01:53 [INFO]: Epoch 016 - training loss: 0.4405, validation loss: 2.2950
2024-06-02 18:02:02 [INFO]: Epoch 017 - training loss: 0.4354, validation loss: 2.2802
2024-06-02 18:02:10 [INFO]: Epoch 018 - training loss: 0.4314, validation loss: 2.2661
2024-06-02 18:02:19 [INFO]: Epoch 019 - training loss: 0.4274, validation loss: 2.2507
2024-06-02 18:02:27 [INFO]: Epoch 020 - training loss: 0.4234, validation loss: 2.2381
2024-06-02 18:02:36 [INFO]: Epoch 021 - training loss: 0.4200, validation loss: 2.2236
2024-06-02 18:02:44 [INFO]: Epoch 022 - training loss: 0.4171, validation loss: 2.2114
2024-06-02 18:02:53 [INFO]: Epoch 023 - training loss: 0.4144, validation loss: 2.1975
2024-06-02 18:03:01 [INFO]: Epoch 024 - training loss: 0.4124, validation loss: 2.1851
2024-06-02 18:03:10 [INFO]: Epoch 025 - training loss: 0.4088, validation loss: 2.1719
2024-06-02 18:03:19 [INFO]: Epoch 026 - training loss: 0.4067, validation loss: 2.1626
2024-06-02 18:03:27 [INFO]: Epoch 027 - training loss: 0.4040, validation loss: 2.1558
2024-06-02 18:03:36 [INFO]: Epoch 028 - training loss: 0.4016, validation loss: 2.1445
2024-06-02 18:03:45 [INFO]: Epoch 029 - training loss: 0.3998, validation loss: 2.1312
2024-06-02 18:03:53 [INFO]: Epoch 030 - training loss: 0.3980, validation loss: 2.1206
2024-06-02 18:04:02 [INFO]: Epoch 031 - training loss: 0.3959, validation loss: 2.1102
2024-06-02 18:04:10 [INFO]: Epoch 032 - training loss: 0.3936, validation loss: 2.1087
2024-06-02 18:04:19 [INFO]: Epoch 033 - training loss: 0.3925, validation loss: 2.0964
2024-06-02 18:04:27 [INFO]: Epoch 034 - training loss: 0.3905, validation loss: 2.0909
2024-06-02 18:04:36 [INFO]: Epoch 035 - training loss: 0.3885, validation loss: 2.0871
2024-06-02 18:04:44 [INFO]: Epoch 036 - training loss: 0.3877, validation loss: 2.0783
2024-06-02 18:04:53 [INFO]: Epoch 037 - training loss: 0.3854, validation loss: 2.0672
2024-06-02 18:05:02 [INFO]: Epoch 038 - training loss: 0.3836, validation loss: 2.0581
2024-06-02 18:05:10 [INFO]: Epoch 039 - training loss: 0.3829, validation loss: 2.0586
2024-06-02 18:05:19 [INFO]: Epoch 040 - training loss: 0.3815, validation loss: 2.0504
2024-06-02 18:05:27 [INFO]: Epoch 041 - training loss: 0.3804, validation loss: 2.0468
2024-06-02 18:05:36 [INFO]: Epoch 042 - training loss: 0.3790, validation loss: 2.0386
2024-06-02 18:05:44 [INFO]: Epoch 043 - training loss: 0.3780, validation loss: 2.0317
2024-06-02 18:05:53 [INFO]: Epoch 044 - training loss: 0.3772, validation loss: 2.0292
2024-06-02 18:06:02 [INFO]: Epoch 045 - training loss: 0.3772, validation loss: 2.0293
2024-06-02 18:06:10 [INFO]: Epoch 046 - training loss: 0.3753, validation loss: 2.0223
2024-06-02 18:06:19 [INFO]: Epoch 047 - training loss: 0.3749, validation loss: 2.0237
2024-06-02 18:06:27 [INFO]: Epoch 048 - training loss: 0.3741, validation loss: 2.0208
2024-06-02 18:06:36 [INFO]: Epoch 049 - training loss: 0.3729, validation loss: 2.0156
2024-06-02 18:06:45 [INFO]: Epoch 050 - training loss: 0.3714, validation loss: 2.0132
2024-06-02 18:06:53 [INFO]: Epoch 051 - training loss: 0.3701, validation loss: 2.0024
2024-06-02 18:07:02 [INFO]: Epoch 052 - training loss: 0.3697, validation loss: 1.9989
2024-06-02 18:07:10 [INFO]: Epoch 053 - training loss: 0.3689, validation loss: 1.9924
2024-06-02 18:07:19 [INFO]: Epoch 054 - training loss: 0.3682, validation loss: 1.9973
2024-06-02 18:07:27 [INFO]: Epoch 055 - training loss: 0.3671, validation loss: 1.9955
2024-06-02 18:07:36 [INFO]: Epoch 056 - training loss: 0.3669, validation loss: 1.9858
2024-06-02 18:07:44 [INFO]: Epoch 057 - training loss: 0.3661, validation loss: 1.9814
2024-06-02 18:07:53 [INFO]: Epoch 058 - training loss: 0.3659, validation loss: 1.9844
2024-06-02 18:08:01 [INFO]: Epoch 059 - training loss: 0.3653, validation loss: 1.9742
2024-06-02 18:08:10 [INFO]: Epoch 060 - training loss: 0.3651, validation loss: 1.9800
2024-06-02 18:08:19 [INFO]: Epoch 061 - training loss: 0.3642, validation loss: 1.9681
2024-06-02 18:08:27 [INFO]: Epoch 062 - training loss: 0.3635, validation loss: 1.9741
2024-06-02 18:08:36 [INFO]: Epoch 063 - training loss: 0.3623, validation loss: 1.9642
2024-06-02 18:08:44 [INFO]: Epoch 064 - training loss: 0.3622, validation loss: 1.9668
2024-06-02 18:08:53 [INFO]: Epoch 065 - training loss: 0.3617, validation loss: 1.9590
2024-06-02 18:09:02 [INFO]: Epoch 066 - training loss: 0.3613, validation loss: 1.9549
2024-06-02 18:09:10 [INFO]: Epoch 067 - training loss: 0.3608, validation loss: 1.9531
2024-06-02 18:09:19 [INFO]: Epoch 068 - training loss: 0.3617, validation loss: 1.9541
2024-06-02 18:09:27 [INFO]: Epoch 069 - training loss: 0.3602, validation loss: 1.9516
2024-06-02 18:09:36 [INFO]: Epoch 070 - training loss: 0.3591, validation loss: 1.9484
2024-06-02 18:09:44 [INFO]: Epoch 071 - training loss: 0.3584, validation loss: 1.9445
2024-06-02 18:09:53 [INFO]: Epoch 072 - training loss: 0.3580, validation loss: 1.9460
2024-06-02 18:10:02 [INFO]: Epoch 073 - training loss: 0.3579, validation loss: 1.9482
2024-06-02 18:10:10 [INFO]: Epoch 074 - training loss: 0.3576, validation loss: 1.9448
2024-06-02 18:10:19 [INFO]: Epoch 075 - training loss: 0.3573, validation loss: 1.9374
2024-06-02 18:10:27 [INFO]: Epoch 076 - training loss: 0.3563, validation loss: 1.9393
2024-06-02 18:10:36 [INFO]: Epoch 077 - training loss: 0.3559, validation loss: 1.9344
2024-06-02 18:10:44 [INFO]: Epoch 078 - training loss: 0.3558, validation loss: 1.9374
2024-06-02 18:10:53 [INFO]: Epoch 079 - training loss: 0.3559, validation loss: 1.9341
2024-06-02 18:11:01 [INFO]: Epoch 080 - training loss: 0.3556, validation loss: 1.9277
2024-06-02 18:11:10 [INFO]: Epoch 081 - training loss: 0.3550, validation loss: 1.9296
2024-06-02 18:11:19 [INFO]: Epoch 082 - training loss: 0.3544, validation loss: 1.9310
2024-06-02 18:11:27 [INFO]: Epoch 083 - training loss: 0.3545, validation loss: 1.9301
2024-06-02 18:11:36 [INFO]: Epoch 084 - training loss: 0.3538, validation loss: 1.9245
2024-06-02 18:11:44 [INFO]: Epoch 085 - training loss: 0.3535, validation loss: 1.9257
2024-06-02 18:11:53 [INFO]: Epoch 086 - training loss: 0.3537, validation loss: 1.9270
2024-06-02 18:12:01 [INFO]: Epoch 087 - training loss: 0.3532, validation loss: 1.9250
2024-06-02 18:12:10 [INFO]: Epoch 088 - training loss: 0.3529, validation loss: 1.9233
2024-06-02 18:12:19 [INFO]: Epoch 089 - training loss: 0.3533, validation loss: 1.9233
2024-06-02 18:12:27 [INFO]: Epoch 090 - training loss: 0.3526, validation loss: 1.9232
2024-06-02 18:12:36 [INFO]: Epoch 091 - training loss: 0.3522, validation loss: 1.9211
2024-06-02 18:12:44 [INFO]: Epoch 092 - training loss: 0.3517, validation loss: 1.9227
2024-06-02 18:12:53 [INFO]: Epoch 093 - training loss: 0.3511, validation loss: 1.9174
2024-06-02 18:13:01 [INFO]: Epoch 094 - training loss: 0.3506, validation loss: 1.9156
2024-06-02 18:13:10 [INFO]: Epoch 095 - training loss: 0.3500, validation loss: 1.9162
2024-06-02 18:13:19 [INFO]: Epoch 096 - training loss: 0.3499, validation loss: 1.9162
2024-06-02 18:13:27 [INFO]: Epoch 097 - training loss: 0.3500, validation loss: 1.9168
2024-06-02 18:13:36 [INFO]: Epoch 098 - training loss: 0.3494, validation loss: 1.9198
2024-06-02 18:13:44 [INFO]: Epoch 099 - training loss: 0.3487, validation loss: 1.9128
2024-06-02 18:13:53 [INFO]: Epoch 100 - training loss: 0.3490, validation loss: 1.9148
2024-06-02 18:14:01 [INFO]: Epoch 101 - training loss: 0.3486, validation loss: 1.9167
2024-06-02 18:14:10 [INFO]: Epoch 102 - training loss: 0.3490, validation loss: 1.9102
2024-06-02 18:14:18 [INFO]: Epoch 103 - training loss: 0.3488, validation loss: 1.9120
2024-06-02 18:14:27 [INFO]: Epoch 104 - training loss: 0.3483, validation loss: 1.9137
2024-06-02 18:14:36 [INFO]: Epoch 105 - training loss: 0.3480, validation loss: 1.9085
2024-06-02 18:14:44 [INFO]: Epoch 106 - training loss: 0.3482, validation loss: 1.9093
2024-06-02 18:14:53 [INFO]: Epoch 107 - training loss: 0.3476, validation loss: 1.9097
2024-06-02 18:15:01 [INFO]: Epoch 108 - training loss: 0.3471, validation loss: 1.9099
2024-06-02 18:15:10 [INFO]: Epoch 109 - training loss: 0.3474, validation loss: 1.9118
2024-06-02 18:15:19 [INFO]: Epoch 110 - training loss: 0.3467, validation loss: 1.9105
2024-06-02 18:15:27 [INFO]: Epoch 111 - training loss: 0.3464, validation loss: 1.9088
2024-06-02 18:15:36 [INFO]: Epoch 112 - training loss: 0.3464, validation loss: 1.9079
2024-06-02 18:15:44 [INFO]: Epoch 113 - training loss: 0.3456, validation loss: 1.9055
2024-06-02 18:15:53 [INFO]: Epoch 114 - training loss: 0.3465, validation loss: 1.9083
2024-06-02 18:16:01 [INFO]: Epoch 115 - training loss: 0.3460, validation loss: 1.9084
2024-06-02 18:16:10 [INFO]: Epoch 116 - training loss: 0.3456, validation loss: 1.9080
2024-06-02 18:16:18 [INFO]: Epoch 117 - training loss: 0.3456, validation loss: 1.9062
2024-06-02 18:16:27 [INFO]: Epoch 118 - training loss: 0.3461, validation loss: 1.9028
2024-06-02 18:16:36 [INFO]: Epoch 119 - training loss: 0.3458, validation loss: 1.9063
2024-06-02 18:16:44 [INFO]: Epoch 120 - training loss: 0.3454, validation loss: 1.9058
2024-06-02 18:16:53 [INFO]: Epoch 121 - training loss: 0.3450, validation loss: 1.9020
2024-06-02 18:17:01 [INFO]: Epoch 122 - training loss: 0.3444, validation loss: 1.9060
2024-06-02 18:17:10 [INFO]: Epoch 123 - training loss: 0.3436, validation loss: 1.9061
2024-06-02 18:17:18 [INFO]: Epoch 124 - training loss: 0.3435, validation loss: 1.9042
2024-06-02 18:17:27 [INFO]: Epoch 125 - training loss: 0.3436, validation loss: 1.9053
2024-06-02 18:17:35 [INFO]: Epoch 126 - training loss: 0.3433, validation loss: 1.9035
2024-06-02 18:17:44 [INFO]: Epoch 127 - training loss: 0.3434, validation loss: 1.9065
2024-06-02 18:17:53 [INFO]: Epoch 128 - training loss: 0.3434, validation loss: 1.9053
2024-06-02 18:18:01 [INFO]: Epoch 129 - training loss: 0.3433, validation loss: 1.9096
2024-06-02 18:18:10 [INFO]: Epoch 130 - training loss: 0.3431, validation loss: 1.9061
2024-06-02 18:18:18 [INFO]: Epoch 131 - training loss: 0.3430, validation loss: 1.9086
2024-06-02 18:18:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 18:18:18 [INFO]: Finished training. The best model is from epoch#121.
2024-06-02 18:18:19 [INFO]: Saved the model to results_point_rate01/Electricity/BRITS_Electricity/round_4/20240602_T175933/BRITS.pypots
2024-06-02 18:18:21 [INFO]: Successfully saved to results_point_rate01/Electricity/BRITS_Electricity/round_4/imputation.pkl
2024-06-02 18:18:21 [INFO]: Round4 - BRITS on Electricity: MAE=0.9826, MSE=2.2647, MRE=0.5256
2024-06-02 18:18:21 [INFO]: Done! Final results:
Averaged BRITS (n params: 17,082,800) on Electricity: MAE=0.9713 ± 0.016218907708001377, MSE=2.1938 ± 0.06586444157765795, MRE=0.5196 ± 0.008676360306336747, average inference time=25.41
