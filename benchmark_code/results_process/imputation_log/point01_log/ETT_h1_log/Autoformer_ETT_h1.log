2024-06-01 22:01:33 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-01 22:01:33 [INFO]: Using the given device: cuda:0
2024-06-01 22:01:34 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_0/20240601_T220134
2024-06-01 22:01:34 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_0/20240601_T220134/tensorboard
2024-06-01 22:01:34 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 166,919
2024-06-01 22:01:37 [INFO]: Epoch 001 - training loss: 1.5013, validation loss: 0.4748
2024-06-01 22:01:38 [INFO]: Epoch 002 - training loss: 1.3251, validation loss: 0.3652
2024-06-01 22:01:38 [INFO]: Epoch 003 - training loss: 1.2852, validation loss: 0.3128
2024-06-01 22:01:38 [INFO]: Epoch 004 - training loss: 1.2546, validation loss: 0.2702
2024-06-01 22:01:39 [INFO]: Epoch 005 - training loss: 1.2443, validation loss: 0.2705
2024-06-01 22:01:39 [INFO]: Epoch 006 - training loss: 1.2217, validation loss: 0.2169
2024-06-01 22:01:39 [INFO]: Epoch 007 - training loss: 1.2003, validation loss: 0.1979
2024-06-01 22:01:40 [INFO]: Epoch 008 - training loss: 1.1935, validation loss: 0.1903
2024-06-01 22:01:40 [INFO]: Epoch 009 - training loss: 1.1985, validation loss: 0.1690
2024-06-01 22:01:41 [INFO]: Epoch 010 - training loss: 1.1916, validation loss: 0.1707
2024-06-01 22:01:41 [INFO]: Epoch 011 - training loss: 1.1873, validation loss: 0.1705
2024-06-01 22:01:42 [INFO]: Epoch 012 - training loss: 1.1722, validation loss: 0.1608
2024-06-01 22:01:42 [INFO]: Epoch 013 - training loss: 1.1795, validation loss: 0.1477
2024-06-01 22:01:43 [INFO]: Epoch 014 - training loss: 1.1687, validation loss: 0.1489
2024-06-01 22:01:43 [INFO]: Epoch 015 - training loss: 1.1668, validation loss: 0.1428
2024-06-01 22:01:44 [INFO]: Epoch 016 - training loss: 1.1577, validation loss: 0.1338
2024-06-01 22:01:44 [INFO]: Epoch 017 - training loss: 1.1523, validation loss: 0.1360
2024-06-01 22:01:45 [INFO]: Epoch 018 - training loss: 1.1508, validation loss: 0.1308
2024-06-01 22:01:46 [INFO]: Epoch 019 - training loss: 1.1521, validation loss: 0.1429
2024-06-01 22:01:46 [INFO]: Epoch 020 - training loss: 1.1470, validation loss: 0.1277
2024-06-01 22:01:47 [INFO]: Epoch 021 - training loss: 1.1512, validation loss: 0.1307
2024-06-01 22:01:47 [INFO]: Epoch 022 - training loss: 1.1472, validation loss: 0.1284
2024-06-01 22:01:48 [INFO]: Epoch 023 - training loss: 1.1386, validation loss: 0.1238
2024-06-01 22:01:48 [INFO]: Epoch 024 - training loss: 1.1300, validation loss: 0.1244
2024-06-01 22:01:49 [INFO]: Epoch 025 - training loss: 1.1306, validation loss: 0.1237
2024-06-01 22:01:49 [INFO]: Epoch 026 - training loss: 1.1225, validation loss: 0.1191
2024-06-01 22:01:49 [INFO]: Epoch 027 - training loss: 1.1350, validation loss: 0.1260
2024-06-01 22:01:50 [INFO]: Epoch 028 - training loss: 1.1273, validation loss: 0.1244
2024-06-01 22:01:50 [INFO]: Epoch 029 - training loss: 1.1236, validation loss: 0.1205
2024-06-01 22:01:51 [INFO]: Epoch 030 - training loss: 1.1439, validation loss: 0.1264
2024-06-01 22:01:52 [INFO]: Epoch 031 - training loss: 1.1254, validation loss: 0.1245
2024-06-01 22:01:52 [INFO]: Epoch 032 - training loss: 1.1294, validation loss: 0.1203
2024-06-01 22:01:52 [INFO]: Epoch 033 - training loss: 1.1266, validation loss: 0.1133
2024-06-01 22:01:53 [INFO]: Epoch 034 - training loss: 1.1289, validation loss: 0.1209
2024-06-01 22:01:53 [INFO]: Epoch 035 - training loss: 1.1267, validation loss: 0.1188
2024-06-01 22:01:54 [INFO]: Epoch 036 - training loss: 1.1216, validation loss: 0.1162
2024-06-01 22:01:54 [INFO]: Epoch 037 - training loss: 1.1215, validation loss: 0.1156
2024-06-01 22:01:55 [INFO]: Epoch 038 - training loss: 1.1206, validation loss: 0.1151
2024-06-01 22:01:55 [INFO]: Epoch 039 - training loss: 1.1187, validation loss: 0.1097
2024-06-01 22:01:55 [INFO]: Epoch 040 - training loss: 1.1211, validation loss: 0.1116
2024-06-01 22:01:56 [INFO]: Epoch 041 - training loss: 1.1131, validation loss: 0.1195
2024-06-01 22:01:56 [INFO]: Epoch 042 - training loss: 1.1109, validation loss: 0.1067
2024-06-01 22:01:57 [INFO]: Epoch 043 - training loss: 1.1220, validation loss: 0.1135
2024-06-01 22:01:57 [INFO]: Epoch 044 - training loss: 1.1187, validation loss: 0.1111
2024-06-01 22:01:58 [INFO]: Epoch 045 - training loss: 1.1078, validation loss: 0.1156
2024-06-01 22:01:58 [INFO]: Epoch 046 - training loss: 1.0979, validation loss: 0.1187
2024-06-01 22:01:58 [INFO]: Epoch 047 - training loss: 1.1030, validation loss: 0.1165
2024-06-01 22:01:59 [INFO]: Epoch 048 - training loss: 1.1087, validation loss: 0.1139
2024-06-01 22:01:59 [INFO]: Epoch 049 - training loss: 1.1047, validation loss: 0.1085
2024-06-01 22:02:00 [INFO]: Epoch 050 - training loss: 1.1116, validation loss: 0.1096
2024-06-01 22:02:00 [INFO]: Epoch 051 - training loss: 1.1106, validation loss: 0.1077
2024-06-01 22:02:01 [INFO]: Epoch 052 - training loss: 1.1196, validation loss: 0.1160
2024-06-01 22:02:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-01 22:02:01 [INFO]: Finished training. The best model is from epoch#42.
2024-06-01 22:02:01 [INFO]: Saved the model to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_0/20240601_T220134/Autoformer.pypots
2024-06-01 22:02:01 [INFO]: Successfully saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_0/imputation.pkl
2024-06-01 22:02:01 [INFO]: Round0 - Autoformer on ETT_h1: MAE=0.2813, MSE=0.1550, MRE=0.3319
2024-06-01 22:02:01 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-01 22:02:01 [INFO]: Using the given device: cuda:0
2024-06-01 22:02:01 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_1/20240601_T220201
2024-06-01 22:02:01 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_1/20240601_T220201/tensorboard
2024-06-01 22:02:01 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 166,919
2024-06-01 22:02:01 [INFO]: Epoch 001 - training loss: 1.4410, validation loss: 0.3420
2024-06-01 22:02:02 [INFO]: Epoch 002 - training loss: 1.3050, validation loss: 0.2806
2024-06-01 22:02:02 [INFO]: Epoch 003 - training loss: 1.2611, validation loss: 0.2642
2024-06-01 22:02:03 [INFO]: Epoch 004 - training loss: 1.2337, validation loss: 0.2319
2024-06-01 22:02:03 [INFO]: Epoch 005 - training loss: 1.2233, validation loss: 0.2222
2024-06-01 22:02:04 [INFO]: Epoch 006 - training loss: 1.2072, validation loss: 0.2085
2024-06-01 22:02:05 [INFO]: Epoch 007 - training loss: 1.2006, validation loss: 0.1949
2024-06-01 22:02:05 [INFO]: Epoch 008 - training loss: 1.1781, validation loss: 0.1736
2024-06-01 22:02:06 [INFO]: Epoch 009 - training loss: 1.1798, validation loss: 0.1818
2024-06-01 22:02:06 [INFO]: Epoch 010 - training loss: 1.1750, validation loss: 0.1647
2024-06-01 22:02:07 [INFO]: Epoch 011 - training loss: 1.1626, validation loss: 0.1616
2024-06-01 22:02:07 [INFO]: Epoch 012 - training loss: 1.1602, validation loss: 0.1787
2024-06-01 22:02:08 [INFO]: Epoch 013 - training loss: 1.1542, validation loss: 0.1535
2024-06-01 22:02:08 [INFO]: Epoch 014 - training loss: 1.1520, validation loss: 0.1593
2024-06-01 22:02:08 [INFO]: Epoch 015 - training loss: 1.1465, validation loss: 0.1425
2024-06-01 22:02:09 [INFO]: Epoch 016 - training loss: 1.1437, validation loss: 0.1559
2024-06-01 22:02:09 [INFO]: Epoch 017 - training loss: 1.1441, validation loss: 0.1412
2024-06-01 22:02:10 [INFO]: Epoch 018 - training loss: 1.1362, validation loss: 0.1359
2024-06-01 22:02:10 [INFO]: Epoch 019 - training loss: 1.1372, validation loss: 0.1336
2024-06-01 22:02:11 [INFO]: Epoch 020 - training loss: 1.1361, validation loss: 0.1275
2024-06-01 22:02:11 [INFO]: Epoch 021 - training loss: 1.1221, validation loss: 0.1321
2024-06-01 22:02:11 [INFO]: Epoch 022 - training loss: 1.1234, validation loss: 0.1243
2024-06-01 22:02:12 [INFO]: Epoch 023 - training loss: 1.1326, validation loss: 0.1159
2024-06-01 22:02:13 [INFO]: Epoch 024 - training loss: 1.1269, validation loss: 0.1285
2024-06-01 22:02:13 [INFO]: Epoch 025 - training loss: 1.1223, validation loss: 0.1174
2024-06-01 22:02:14 [INFO]: Epoch 026 - training loss: 1.1146, validation loss: 0.1197
2024-06-01 22:02:14 [INFO]: Epoch 027 - training loss: 1.1123, validation loss: 0.1170
2024-06-01 22:02:15 [INFO]: Epoch 028 - training loss: 1.1159, validation loss: 0.1091
2024-06-01 22:02:15 [INFO]: Epoch 029 - training loss: 1.1174, validation loss: 0.1125
2024-06-01 22:02:15 [INFO]: Epoch 030 - training loss: 1.1136, validation loss: 0.1293
2024-06-01 22:02:16 [INFO]: Epoch 031 - training loss: 1.1113, validation loss: 0.1072
2024-06-01 22:02:16 [INFO]: Epoch 032 - training loss: 1.1207, validation loss: 0.1006
2024-06-01 22:02:17 [INFO]: Epoch 033 - training loss: 1.1073, validation loss: 0.1083
2024-06-01 22:02:18 [INFO]: Epoch 034 - training loss: 1.1130, validation loss: 0.1129
2024-06-01 22:02:18 [INFO]: Epoch 035 - training loss: 1.1099, validation loss: 0.1038
2024-06-01 22:02:19 [INFO]: Epoch 036 - training loss: 1.1098, validation loss: 0.1090
2024-06-01 22:02:19 [INFO]: Epoch 037 - training loss: 1.1077, validation loss: 0.1132
2024-06-01 22:02:19 [INFO]: Epoch 038 - training loss: 1.1100, validation loss: 0.1085
2024-06-01 22:02:20 [INFO]: Epoch 039 - training loss: 1.0964, validation loss: 0.1113
2024-06-01 22:02:20 [INFO]: Epoch 040 - training loss: 1.1098, validation loss: 0.1061
2024-06-01 22:02:21 [INFO]: Epoch 041 - training loss: 1.0979, validation loss: 0.1017
2024-06-01 22:02:21 [INFO]: Epoch 042 - training loss: 1.0964, validation loss: 0.1005
2024-06-01 22:02:22 [INFO]: Epoch 043 - training loss: 1.0974, validation loss: 0.1148
2024-06-01 22:02:22 [INFO]: Epoch 044 - training loss: 1.0909, validation loss: 0.1055
2024-06-01 22:02:23 [INFO]: Epoch 045 - training loss: 1.1030, validation loss: 0.1067
2024-06-01 22:02:23 [INFO]: Epoch 046 - training loss: 1.0948, validation loss: 0.0993
2024-06-01 22:02:24 [INFO]: Epoch 047 - training loss: 1.0962, validation loss: 0.0995
2024-06-01 22:02:24 [INFO]: Epoch 048 - training loss: 1.0930, validation loss: 0.1131
2024-06-01 22:02:24 [INFO]: Epoch 049 - training loss: 1.0977, validation loss: 0.0986
2024-06-01 22:02:25 [INFO]: Epoch 050 - training loss: 1.0838, validation loss: 0.1009
2024-06-01 22:02:25 [INFO]: Epoch 051 - training loss: 1.0921, validation loss: 0.0973
2024-06-01 22:02:26 [INFO]: Epoch 052 - training loss: 1.0922, validation loss: 0.0967
2024-06-01 22:02:26 [INFO]: Epoch 053 - training loss: 1.0952, validation loss: 0.1014
2024-06-01 22:02:27 [INFO]: Epoch 054 - training loss: 1.0927, validation loss: 0.1002
2024-06-01 22:02:27 [INFO]: Epoch 055 - training loss: 1.0881, validation loss: 0.0963
2024-06-01 22:02:27 [INFO]: Epoch 056 - training loss: 1.0903, validation loss: 0.0969
2024-06-01 22:02:28 [INFO]: Epoch 057 - training loss: 1.0860, validation loss: 0.1079
2024-06-01 22:02:28 [INFO]: Epoch 058 - training loss: 1.0841, validation loss: 0.0952
2024-06-01 22:02:29 [INFO]: Epoch 059 - training loss: 1.0868, validation loss: 0.1048
2024-06-01 22:02:30 [INFO]: Epoch 060 - training loss: 1.0832, validation loss: 0.1021
2024-06-01 22:02:30 [INFO]: Epoch 061 - training loss: 1.0818, validation loss: 0.0971
2024-06-01 22:02:31 [INFO]: Epoch 062 - training loss: 1.0864, validation loss: 0.1082
2024-06-01 22:02:31 [INFO]: Epoch 063 - training loss: 1.0908, validation loss: 0.1050
2024-06-01 22:02:32 [INFO]: Epoch 064 - training loss: 1.0940, validation loss: 0.0886
2024-06-01 22:02:33 [INFO]: Epoch 065 - training loss: 1.0805, validation loss: 0.0949
2024-06-01 22:02:33 [INFO]: Epoch 066 - training loss: 1.0808, validation loss: 0.0998
2024-06-01 22:02:33 [INFO]: Epoch 067 - training loss: 1.0803, validation loss: 0.1028
2024-06-01 22:02:34 [INFO]: Epoch 068 - training loss: 1.0774, validation loss: 0.1057
2024-06-01 22:02:35 [INFO]: Epoch 069 - training loss: 1.0829, validation loss: 0.1013
2024-06-01 22:02:35 [INFO]: Epoch 070 - training loss: 1.0752, validation loss: 0.1007
2024-06-01 22:02:36 [INFO]: Epoch 071 - training loss: 1.0769, validation loss: 0.0909
2024-06-01 22:02:36 [INFO]: Epoch 072 - training loss: 1.0836, validation loss: 0.0994
2024-06-01 22:02:37 [INFO]: Epoch 073 - training loss: 1.0811, validation loss: 0.0969
2024-06-01 22:02:37 [INFO]: Epoch 074 - training loss: 1.0798, validation loss: 0.0918
2024-06-01 22:02:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-01 22:02:37 [INFO]: Finished training. The best model is from epoch#64.
2024-06-01 22:02:37 [INFO]: Saved the model to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_1/20240601_T220201/Autoformer.pypots
2024-06-01 22:02:37 [INFO]: Successfully saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_1/imputation.pkl
2024-06-01 22:02:37 [INFO]: Round1 - Autoformer on ETT_h1: MAE=0.2647, MSE=0.1411, MRE=0.3123
2024-06-01 22:02:37 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-01 22:02:37 [INFO]: Using the given device: cuda:0
2024-06-01 22:02:37 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_2/20240601_T220237
2024-06-01 22:02:37 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_2/20240601_T220237/tensorboard
2024-06-01 22:02:37 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 166,919
2024-06-01 22:02:38 [INFO]: Epoch 001 - training loss: 1.5017, validation loss: 0.4059
2024-06-01 22:02:39 [INFO]: Epoch 002 - training loss: 1.3258, validation loss: 0.2815
2024-06-01 22:02:39 [INFO]: Epoch 003 - training loss: 1.2805, validation loss: 0.3067
2024-06-01 22:02:40 [INFO]: Epoch 004 - training loss: 1.2642, validation loss: 0.2699
2024-06-01 22:02:40 [INFO]: Epoch 005 - training loss: 1.2328, validation loss: 0.2357
2024-06-01 22:02:41 [INFO]: Epoch 006 - training loss: 1.2343, validation loss: 0.2360
2024-06-01 22:02:42 [INFO]: Epoch 007 - training loss: 1.2227, validation loss: 0.1950
2024-06-01 22:02:42 [INFO]: Epoch 008 - training loss: 1.1979, validation loss: 0.1890
2024-06-01 22:02:43 [INFO]: Epoch 009 - training loss: 1.1988, validation loss: 0.1775
2024-06-01 22:02:43 [INFO]: Epoch 010 - training loss: 1.1846, validation loss: 0.1837
2024-06-01 22:02:44 [INFO]: Epoch 011 - training loss: 1.1930, validation loss: 0.1698
2024-06-01 22:02:44 [INFO]: Epoch 012 - training loss: 1.1836, validation loss: 0.1608
2024-06-01 22:02:45 [INFO]: Epoch 013 - training loss: 1.1927, validation loss: 0.1599
2024-06-01 22:02:45 [INFO]: Epoch 014 - training loss: 1.1649, validation loss: 0.1501
2024-06-01 22:02:46 [INFO]: Epoch 015 - training loss: 1.1755, validation loss: 0.1454
2024-06-01 22:02:46 [INFO]: Epoch 016 - training loss: 1.1657, validation loss: 0.1454
2024-06-01 22:02:47 [INFO]: Epoch 017 - training loss: 1.1576, validation loss: 0.1431
2024-06-01 22:02:47 [INFO]: Epoch 018 - training loss: 1.1505, validation loss: 0.1409
2024-06-01 22:02:48 [INFO]: Epoch 019 - training loss: 1.1592, validation loss: 0.1293
2024-06-01 22:02:48 [INFO]: Epoch 020 - training loss: 1.1502, validation loss: 0.1319
2024-06-01 22:02:48 [INFO]: Epoch 021 - training loss: 1.1466, validation loss: 0.1421
2024-06-01 22:02:49 [INFO]: Epoch 022 - training loss: 1.1510, validation loss: 0.1301
2024-06-01 22:02:49 [INFO]: Epoch 023 - training loss: 1.1462, validation loss: 0.1303
2024-06-01 22:02:50 [INFO]: Epoch 024 - training loss: 1.1379, validation loss: 0.1359
2024-06-01 22:02:50 [INFO]: Epoch 025 - training loss: 1.1407, validation loss: 0.1318
2024-06-01 22:02:51 [INFO]: Epoch 026 - training loss: 1.1448, validation loss: 0.1320
2024-06-01 22:02:51 [INFO]: Epoch 027 - training loss: 1.1423, validation loss: 0.1259
2024-06-01 22:02:52 [INFO]: Epoch 028 - training loss: 1.1313, validation loss: 0.1268
2024-06-01 22:02:53 [INFO]: Epoch 029 - training loss: 1.1281, validation loss: 0.1186
2024-06-01 22:02:53 [INFO]: Epoch 030 - training loss: 1.1278, validation loss: 0.1185
2024-06-01 22:02:54 [INFO]: Epoch 031 - training loss: 1.1355, validation loss: 0.1281
2024-06-01 22:02:54 [INFO]: Epoch 032 - training loss: 1.1252, validation loss: 0.1167
2024-06-01 22:02:55 [INFO]: Epoch 033 - training loss: 1.1227, validation loss: 0.1173
2024-06-01 22:02:55 [INFO]: Epoch 034 - training loss: 1.1253, validation loss: 0.1192
2024-06-01 22:02:56 [INFO]: Epoch 035 - training loss: 1.1239, validation loss: 0.1198
2024-06-01 22:02:56 [INFO]: Epoch 036 - training loss: 1.1239, validation loss: 0.1207
2024-06-01 22:02:57 [INFO]: Epoch 037 - training loss: 1.1314, validation loss: 0.1168
2024-06-01 22:02:57 [INFO]: Epoch 038 - training loss: 1.1201, validation loss: 0.1170
2024-06-01 22:02:58 [INFO]: Epoch 039 - training loss: 1.1159, validation loss: 0.1077
2024-06-01 22:02:58 [INFO]: Epoch 040 - training loss: 1.1277, validation loss: 0.1143
2024-06-01 22:02:58 [INFO]: Epoch 041 - training loss: 1.1192, validation loss: 0.1146
2024-06-01 22:02:59 [INFO]: Epoch 042 - training loss: 1.1131, validation loss: 0.1079
2024-06-01 22:02:59 [INFO]: Epoch 043 - training loss: 1.1141, validation loss: 0.1134
2024-06-01 22:02:59 [INFO]: Epoch 044 - training loss: 1.1139, validation loss: 0.1149
2024-06-01 22:02:59 [INFO]: Epoch 045 - training loss: 1.1107, validation loss: 0.1076
2024-06-01 22:03:00 [INFO]: Epoch 046 - training loss: 1.1116, validation loss: 0.1094
2024-06-01 22:03:00 [INFO]: Epoch 047 - training loss: 1.1192, validation loss: 0.1045
2024-06-01 22:03:00 [INFO]: Epoch 048 - training loss: 1.1079, validation loss: 0.1115
2024-06-01 22:03:01 [INFO]: Epoch 049 - training loss: 1.1103, validation loss: 0.1060
2024-06-01 22:03:01 [INFO]: Epoch 050 - training loss: 1.1089, validation loss: 0.1035
2024-06-01 22:03:02 [INFO]: Epoch 051 - training loss: 1.1077, validation loss: 0.1092
2024-06-01 22:03:02 [INFO]: Epoch 052 - training loss: 1.1028, validation loss: 0.1030
2024-06-01 22:03:03 [INFO]: Epoch 053 - training loss: 1.1148, validation loss: 0.1080
2024-06-01 22:03:03 [INFO]: Epoch 054 - training loss: 1.1171, validation loss: 0.1072
2024-06-01 22:03:03 [INFO]: Epoch 055 - training loss: 1.1056, validation loss: 0.1051
2024-06-01 22:03:04 [INFO]: Epoch 056 - training loss: 1.1080, validation loss: 0.1098
2024-06-01 22:03:04 [INFO]: Epoch 057 - training loss: 1.1019, validation loss: 0.1060
2024-06-01 22:03:05 [INFO]: Epoch 058 - training loss: 1.0988, validation loss: 0.1108
2024-06-01 22:03:05 [INFO]: Epoch 059 - training loss: 1.1002, validation loss: 0.1042
2024-06-01 22:03:05 [INFO]: Epoch 060 - training loss: 1.1031, validation loss: 0.1056
2024-06-01 22:03:06 [INFO]: Epoch 061 - training loss: 1.0975, validation loss: 0.1087
2024-06-01 22:03:06 [INFO]: Epoch 062 - training loss: 1.1011, validation loss: 0.1028
2024-06-01 22:03:07 [INFO]: Epoch 063 - training loss: 1.0947, validation loss: 0.1098
2024-06-01 22:03:07 [INFO]: Epoch 064 - training loss: 1.1014, validation loss: 0.1016
2024-06-01 22:03:08 [INFO]: Epoch 065 - training loss: 1.1016, validation loss: 0.1086
2024-06-01 22:03:08 [INFO]: Epoch 066 - training loss: 1.1086, validation loss: 0.1030
2024-06-01 22:03:08 [INFO]: Epoch 067 - training loss: 1.1071, validation loss: 0.1105
2024-06-01 22:03:09 [INFO]: Epoch 068 - training loss: 1.0984, validation loss: 0.1009
2024-06-01 22:03:09 [INFO]: Epoch 069 - training loss: 1.0969, validation loss: 0.1138
2024-06-01 22:03:10 [INFO]: Epoch 070 - training loss: 1.0958, validation loss: 0.1025
2024-06-01 22:03:10 [INFO]: Epoch 071 - training loss: 1.1018, validation loss: 0.1125
2024-06-01 22:03:10 [INFO]: Epoch 072 - training loss: 1.1027, validation loss: 0.1046
2024-06-01 22:03:11 [INFO]: Epoch 073 - training loss: 1.1036, validation loss: 0.1033
2024-06-01 22:03:11 [INFO]: Epoch 074 - training loss: 1.0948, validation loss: 0.1070
2024-06-01 22:03:11 [INFO]: Epoch 075 - training loss: 1.0962, validation loss: 0.1035
2024-06-01 22:03:12 [INFO]: Epoch 076 - training loss: 1.0939, validation loss: 0.1029
2024-06-01 22:03:12 [INFO]: Epoch 077 - training loss: 1.0944, validation loss: 0.0987
2024-06-01 22:03:13 [INFO]: Epoch 078 - training loss: 1.0915, validation loss: 0.0984
2024-06-01 22:03:13 [INFO]: Epoch 079 - training loss: 1.0886, validation loss: 0.0995
2024-06-01 22:03:14 [INFO]: Epoch 080 - training loss: 1.0938, validation loss: 0.1036
2024-06-01 22:03:14 [INFO]: Epoch 081 - training loss: 1.0928, validation loss: 0.1001
2024-06-01 22:03:15 [INFO]: Epoch 082 - training loss: 1.0861, validation loss: 0.1033
2024-06-01 22:03:15 [INFO]: Epoch 083 - training loss: 1.0863, validation loss: 0.1094
2024-06-01 22:03:16 [INFO]: Epoch 084 - training loss: 1.0886, validation loss: 0.1058
2024-06-01 22:03:16 [INFO]: Epoch 085 - training loss: 1.0908, validation loss: 0.1028
2024-06-01 22:03:17 [INFO]: Epoch 086 - training loss: 1.0901, validation loss: 0.0985
2024-06-01 22:03:17 [INFO]: Epoch 087 - training loss: 1.0838, validation loss: 0.1077
2024-06-01 22:03:17 [INFO]: Epoch 088 - training loss: 1.0981, validation loss: 0.1035
2024-06-01 22:03:17 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-01 22:03:17 [INFO]: Finished training. The best model is from epoch#78.
2024-06-01 22:03:17 [INFO]: Saved the model to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_2/20240601_T220237/Autoformer.pypots
2024-06-01 22:03:17 [INFO]: Successfully saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_2/imputation.pkl
2024-06-01 22:03:17 [INFO]: Round2 - Autoformer on ETT_h1: MAE=0.2601, MSE=0.1375, MRE=0.3070
2024-06-01 22:03:17 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-01 22:03:17 [INFO]: Using the given device: cuda:0
2024-06-01 22:03:17 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_3/20240601_T220317
2024-06-01 22:03:17 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_3/20240601_T220317/tensorboard
2024-06-01 22:03:17 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 166,919
2024-06-01 22:03:18 [INFO]: Epoch 001 - training loss: 1.4855, validation loss: 0.4041
2024-06-01 22:03:18 [INFO]: Epoch 002 - training loss: 1.3189, validation loss: 0.2916
2024-06-01 22:03:19 [INFO]: Epoch 003 - training loss: 1.2709, validation loss: 0.2707
2024-06-01 22:03:19 [INFO]: Epoch 004 - training loss: 1.2530, validation loss: 0.2505
2024-06-01 22:03:20 [INFO]: Epoch 005 - training loss: 1.2375, validation loss: 0.2144
2024-06-01 22:03:20 [INFO]: Epoch 006 - training loss: 1.2185, validation loss: 0.2081
2024-06-01 22:03:21 [INFO]: Epoch 007 - training loss: 1.1994, validation loss: 0.1915
2024-06-01 22:03:21 [INFO]: Epoch 008 - training loss: 1.1929, validation loss: 0.1960
2024-06-01 22:03:22 [INFO]: Epoch 009 - training loss: 1.1861, validation loss: 0.1774
2024-06-01 22:03:22 [INFO]: Epoch 010 - training loss: 1.1716, validation loss: 0.1768
2024-06-01 22:03:22 [INFO]: Epoch 011 - training loss: 1.1658, validation loss: 0.1699
2024-06-01 22:03:23 [INFO]: Epoch 012 - training loss: 1.1608, validation loss: 0.1725
2024-06-01 22:03:23 [INFO]: Epoch 013 - training loss: 1.1577, validation loss: 0.1791
2024-06-01 22:03:24 [INFO]: Epoch 014 - training loss: 1.1538, validation loss: 0.1529
2024-06-01 22:03:24 [INFO]: Epoch 015 - training loss: 1.1417, validation loss: 0.1653
2024-06-01 22:03:24 [INFO]: Epoch 016 - training loss: 1.1521, validation loss: 0.1532
2024-06-01 22:03:25 [INFO]: Epoch 017 - training loss: 1.1427, validation loss: 0.1647
2024-06-01 22:03:25 [INFO]: Epoch 018 - training loss: 1.1457, validation loss: 0.1586
2024-06-01 22:03:26 [INFO]: Epoch 019 - training loss: 1.1346, validation loss: 0.1502
2024-06-01 22:03:26 [INFO]: Epoch 020 - training loss: 1.1303, validation loss: 0.1598
2024-06-01 22:03:26 [INFO]: Epoch 021 - training loss: 1.1408, validation loss: 0.1502
2024-06-01 22:03:27 [INFO]: Epoch 022 - training loss: 1.1212, validation loss: 0.1358
2024-06-01 22:03:27 [INFO]: Epoch 023 - training loss: 1.1208, validation loss: 0.1433
2024-06-01 22:03:28 [INFO]: Epoch 024 - training loss: 1.1146, validation loss: 0.1434
2024-06-01 22:03:28 [INFO]: Epoch 025 - training loss: 1.1248, validation loss: 0.1588
2024-06-01 22:03:28 [INFO]: Epoch 026 - training loss: 1.1141, validation loss: 0.1594
2024-06-01 22:03:29 [INFO]: Epoch 027 - training loss: 1.1227, validation loss: 0.1478
2024-06-01 22:03:29 [INFO]: Epoch 028 - training loss: 1.1260, validation loss: 0.1406
2024-06-01 22:03:30 [INFO]: Epoch 029 - training loss: 1.1166, validation loss: 0.1370
2024-06-01 22:03:30 [INFO]: Epoch 030 - training loss: 1.1154, validation loss: 0.1353
2024-06-01 22:03:31 [INFO]: Epoch 031 - training loss: 1.1152, validation loss: 0.1263
2024-06-01 22:03:31 [INFO]: Epoch 032 - training loss: 1.1078, validation loss: 0.1226
2024-06-01 22:03:32 [INFO]: Epoch 033 - training loss: 1.1115, validation loss: 0.1222
2024-06-01 22:03:32 [INFO]: Epoch 034 - training loss: 1.1030, validation loss: 0.1150
2024-06-01 22:03:32 [INFO]: Epoch 035 - training loss: 1.1117, validation loss: 0.1141
2024-06-01 22:03:33 [INFO]: Epoch 036 - training loss: 1.0997, validation loss: 0.1179
2024-06-01 22:03:33 [INFO]: Epoch 037 - training loss: 1.1013, validation loss: 0.1128
2024-06-01 22:03:34 [INFO]: Epoch 038 - training loss: 1.0989, validation loss: 0.1169
2024-06-01 22:03:34 [INFO]: Epoch 039 - training loss: 1.1087, validation loss: 0.1142
2024-06-01 22:03:35 [INFO]: Epoch 040 - training loss: 1.1028, validation loss: 0.1155
2024-06-01 22:03:35 [INFO]: Epoch 041 - training loss: 1.1109, validation loss: 0.1058
2024-06-01 22:03:36 [INFO]: Epoch 042 - training loss: 1.0985, validation loss: 0.1033
2024-06-01 22:03:36 [INFO]: Epoch 043 - training loss: 1.1045, validation loss: 0.1062
2024-06-01 22:03:37 [INFO]: Epoch 044 - training loss: 1.0925, validation loss: 0.1164
2024-06-01 22:03:37 [INFO]: Epoch 045 - training loss: 1.0933, validation loss: 0.1157
2024-06-01 22:03:37 [INFO]: Epoch 046 - training loss: 1.0958, validation loss: 0.1053
2024-06-01 22:03:38 [INFO]: Epoch 047 - training loss: 1.0971, validation loss: 0.1128
2024-06-01 22:03:38 [INFO]: Epoch 048 - training loss: 1.0937, validation loss: 0.1079
2024-06-01 22:03:38 [INFO]: Epoch 049 - training loss: 1.0887, validation loss: 0.1033
2024-06-01 22:03:39 [INFO]: Epoch 050 - training loss: 1.1009, validation loss: 0.0980
2024-06-01 22:03:39 [INFO]: Epoch 051 - training loss: 1.0983, validation loss: 0.0988
2024-06-01 22:03:40 [INFO]: Epoch 052 - training loss: 1.0979, validation loss: 0.1036
2024-06-01 22:03:40 [INFO]: Epoch 053 - training loss: 1.0926, validation loss: 0.1004
2024-06-01 22:03:40 [INFO]: Epoch 054 - training loss: 1.0884, validation loss: 0.0974
2024-06-01 22:03:41 [INFO]: Epoch 055 - training loss: 1.0877, validation loss: 0.0978
2024-06-01 22:03:41 [INFO]: Epoch 056 - training loss: 1.0878, validation loss: 0.0955
2024-06-01 22:03:42 [INFO]: Epoch 057 - training loss: 1.0911, validation loss: 0.0888
2024-06-01 22:03:42 [INFO]: Epoch 058 - training loss: 1.0948, validation loss: 0.0899
2024-06-01 22:03:42 [INFO]: Epoch 059 - training loss: 1.0843, validation loss: 0.0903
2024-06-01 22:03:43 [INFO]: Epoch 060 - training loss: 1.0944, validation loss: 0.0916
2024-06-01 22:03:43 [INFO]: Epoch 061 - training loss: 1.0891, validation loss: 0.0891
2024-06-01 22:03:44 [INFO]: Epoch 062 - training loss: 1.0909, validation loss: 0.0881
2024-06-01 22:03:44 [INFO]: Epoch 063 - training loss: 1.0934, validation loss: 0.0887
2024-06-01 22:03:45 [INFO]: Epoch 064 - training loss: 1.0934, validation loss: 0.0793
2024-06-01 22:03:45 [INFO]: Epoch 065 - training loss: 1.1031, validation loss: 0.0886
2024-06-01 22:03:45 [INFO]: Epoch 066 - training loss: 1.0853, validation loss: 0.0810
2024-06-01 22:03:46 [INFO]: Epoch 067 - training loss: 1.0888, validation loss: 0.0849
2024-06-01 22:03:46 [INFO]: Epoch 068 - training loss: 1.0897, validation loss: 0.0882
2024-06-01 22:03:47 [INFO]: Epoch 069 - training loss: 1.0878, validation loss: 0.0862
2024-06-01 22:03:47 [INFO]: Epoch 070 - training loss: 1.0919, validation loss: 0.0880
2024-06-01 22:03:48 [INFO]: Epoch 071 - training loss: 1.0952, validation loss: 0.0846
2024-06-01 22:03:48 [INFO]: Epoch 072 - training loss: 1.0834, validation loss: 0.0894
2024-06-01 22:03:48 [INFO]: Epoch 073 - training loss: 1.0802, validation loss: 0.0870
2024-06-01 22:03:49 [INFO]: Epoch 074 - training loss: 1.0799, validation loss: 0.0902
2024-06-01 22:03:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-01 22:03:49 [INFO]: Finished training. The best model is from epoch#64.
2024-06-01 22:03:49 [INFO]: Saved the model to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_3/20240601_T220317/Autoformer.pypots
2024-06-01 22:03:49 [INFO]: Successfully saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_3/imputation.pkl
2024-06-01 22:03:49 [INFO]: Round3 - Autoformer on ETT_h1: MAE=0.2612, MSE=0.1359, MRE=0.3082
2024-06-01 22:03:49 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-01 22:03:49 [INFO]: Using the given device: cuda:0
2024-06-01 22:03:49 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_4/20240601_T220349
2024-06-01 22:03:49 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_4/20240601_T220349/tensorboard
2024-06-01 22:03:49 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 166,919
2024-06-01 22:03:49 [INFO]: Epoch 001 - training loss: 1.5376, validation loss: 0.4588
2024-06-01 22:03:50 [INFO]: Epoch 002 - training loss: 1.3489, validation loss: 0.3457
2024-06-01 22:03:50 [INFO]: Epoch 003 - training loss: 1.3024, validation loss: 0.2934
2024-06-01 22:03:51 [INFO]: Epoch 004 - training loss: 1.2663, validation loss: 0.2792
2024-06-01 22:03:51 [INFO]: Epoch 005 - training loss: 1.2410, validation loss: 0.2471
2024-06-01 22:03:51 [INFO]: Epoch 006 - training loss: 1.2320, validation loss: 0.2181
2024-06-01 22:03:52 [INFO]: Epoch 007 - training loss: 1.2126, validation loss: 0.2006
2024-06-01 22:03:52 [INFO]: Epoch 008 - training loss: 1.2120, validation loss: 0.2168
2024-06-01 22:03:53 [INFO]: Epoch 009 - training loss: 1.1982, validation loss: 0.1897
2024-06-01 22:03:53 [INFO]: Epoch 010 - training loss: 1.1993, validation loss: 0.1872
2024-06-01 22:03:54 [INFO]: Epoch 011 - training loss: 1.1914, validation loss: 0.1668
2024-06-01 22:03:54 [INFO]: Epoch 012 - training loss: 1.1728, validation loss: 0.1621
2024-06-01 22:03:55 [INFO]: Epoch 013 - training loss: 1.1722, validation loss: 0.1680
2024-06-01 22:03:55 [INFO]: Epoch 014 - training loss: 1.1716, validation loss: 0.1511
2024-06-01 22:03:55 [INFO]: Epoch 015 - training loss: 1.1510, validation loss: 0.1488
2024-06-01 22:03:56 [INFO]: Epoch 016 - training loss: 1.1655, validation loss: 0.1488
2024-06-01 22:03:56 [INFO]: Epoch 017 - training loss: 1.1634, validation loss: 0.1512
2024-06-01 22:03:57 [INFO]: Epoch 018 - training loss: 1.1497, validation loss: 0.1403
2024-06-01 22:03:57 [INFO]: Epoch 019 - training loss: 1.1493, validation loss: 0.1380
2024-06-01 22:03:58 [INFO]: Epoch 020 - training loss: 1.1492, validation loss: 0.1302
2024-06-01 22:03:58 [INFO]: Epoch 021 - training loss: 1.1351, validation loss: 0.1345
2024-06-01 22:03:59 [INFO]: Epoch 022 - training loss: 1.1360, validation loss: 0.1322
2024-06-01 22:03:59 [INFO]: Epoch 023 - training loss: 1.1376, validation loss: 0.1247
2024-06-01 22:04:00 [INFO]: Epoch 024 - training loss: 1.1354, validation loss: 0.1390
2024-06-01 22:04:00 [INFO]: Epoch 025 - training loss: 1.1373, validation loss: 0.1206
2024-06-01 22:04:00 [INFO]: Epoch 026 - training loss: 1.1396, validation loss: 0.1278
2024-06-01 22:04:01 [INFO]: Epoch 027 - training loss: 1.1332, validation loss: 0.1249
2024-06-01 22:04:01 [INFO]: Epoch 028 - training loss: 1.1296, validation loss: 0.1321
2024-06-01 22:04:02 [INFO]: Epoch 029 - training loss: 1.1295, validation loss: 0.1178
2024-06-01 22:04:02 [INFO]: Epoch 030 - training loss: 1.1211, validation loss: 0.1202
2024-06-01 22:04:03 [INFO]: Epoch 031 - training loss: 1.1267, validation loss: 0.1247
2024-06-01 22:04:03 [INFO]: Epoch 032 - training loss: 1.1377, validation loss: 0.1251
2024-06-01 22:04:03 [INFO]: Epoch 033 - training loss: 1.1241, validation loss: 0.1256
2024-06-01 22:04:04 [INFO]: Epoch 034 - training loss: 1.1258, validation loss: 0.1158
2024-06-01 22:04:04 [INFO]: Epoch 035 - training loss: 1.1197, validation loss: 0.1139
2024-06-01 22:04:05 [INFO]: Epoch 036 - training loss: 1.1230, validation loss: 0.1202
2024-06-01 22:04:05 [INFO]: Epoch 037 - training loss: 1.1192, validation loss: 0.1143
2024-06-01 22:04:05 [INFO]: Epoch 038 - training loss: 1.1196, validation loss: 0.1189
2024-06-01 22:04:06 [INFO]: Epoch 039 - training loss: 1.1231, validation loss: 0.1148
2024-06-01 22:04:06 [INFO]: Epoch 040 - training loss: 1.1167, validation loss: 0.1187
2024-06-01 22:04:07 [INFO]: Epoch 041 - training loss: 1.1105, validation loss: 0.1147
2024-06-01 22:04:07 [INFO]: Epoch 042 - training loss: 1.1153, validation loss: 0.1091
2024-06-01 22:04:07 [INFO]: Epoch 043 - training loss: 1.1198, validation loss: 0.1151
2024-06-01 22:04:08 [INFO]: Epoch 044 - training loss: 1.1152, validation loss: 0.1117
2024-06-01 22:04:08 [INFO]: Epoch 045 - training loss: 1.1196, validation loss: 0.1172
2024-06-01 22:04:08 [INFO]: Epoch 046 - training loss: 1.1161, validation loss: 0.1168
2024-06-01 22:04:09 [INFO]: Epoch 047 - training loss: 1.1106, validation loss: 0.1081
2024-06-01 22:04:09 [INFO]: Epoch 048 - training loss: 1.1120, validation loss: 0.1154
2024-06-01 22:04:10 [INFO]: Epoch 049 - training loss: 1.1101, validation loss: 0.1084
2024-06-01 22:04:10 [INFO]: Epoch 050 - training loss: 1.1121, validation loss: 0.1108
2024-06-01 22:04:11 [INFO]: Epoch 051 - training loss: 1.1093, validation loss: 0.1079
2024-06-01 22:04:11 [INFO]: Epoch 052 - training loss: 1.1090, validation loss: 0.1107
2024-06-01 22:04:12 [INFO]: Epoch 053 - training loss: 1.1020, validation loss: 0.1118
2024-06-01 22:04:12 [INFO]: Epoch 054 - training loss: 1.1077, validation loss: 0.1103
2024-06-01 22:04:12 [INFO]: Epoch 055 - training loss: 1.1159, validation loss: 0.1072
2024-06-01 22:04:13 [INFO]: Epoch 056 - training loss: 1.1075, validation loss: 0.1014
2024-06-01 22:04:13 [INFO]: Epoch 057 - training loss: 1.0996, validation loss: 0.1074
2024-06-01 22:04:14 [INFO]: Epoch 058 - training loss: 1.0928, validation loss: 0.1084
2024-06-01 22:04:15 [INFO]: Epoch 059 - training loss: 1.0979, validation loss: 0.1140
2024-06-01 22:04:15 [INFO]: Epoch 060 - training loss: 1.0998, validation loss: 0.1021
2024-06-01 22:04:16 [INFO]: Epoch 061 - training loss: 1.0945, validation loss: 0.1034
2024-06-01 22:04:16 [INFO]: Epoch 062 - training loss: 1.1017, validation loss: 0.1139
2024-06-01 22:04:17 [INFO]: Epoch 063 - training loss: 1.0950, validation loss: 0.1185
2024-06-01 22:04:17 [INFO]: Epoch 064 - training loss: 1.1021, validation loss: 0.1093
2024-06-01 22:04:18 [INFO]: Epoch 065 - training loss: 1.0946, validation loss: 0.1153
2024-06-01 22:04:18 [INFO]: Epoch 066 - training loss: 1.0964, validation loss: 0.1106
2024-06-01 22:04:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-01 22:04:18 [INFO]: Finished training. The best model is from epoch#56.
2024-06-01 22:04:18 [INFO]: Saved the model to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_4/20240601_T220349/Autoformer.pypots
2024-06-01 22:04:18 [INFO]: Successfully saved to results_point_rate01/ETT_h1/Autoformer_ETT_h1/round_4/imputation.pkl
2024-06-01 22:04:18 [INFO]: Round4 - Autoformer on ETT_h1: MAE=0.2699, MSE=0.1472, MRE=0.3185
2024-06-01 22:04:18 [INFO]: Done! Final results:
Averaged Autoformer (n params: 166,919) on ETT_h1: MAE=0.2674 ± 0.007724605985355162, MSE=0.1433 ± 0.006972186309184239, MRE=0.3156 ± 0.009115738507523055, average inference time=0.05
