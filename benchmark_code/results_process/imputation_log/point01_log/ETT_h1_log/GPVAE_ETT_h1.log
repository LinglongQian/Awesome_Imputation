2024-06-02 09:56:31 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 09:56:31 [INFO]: Using the given device: cuda:0
2024-06-02 09:56:31 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_0/20240602_T095631
2024-06-02 09:56:31 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_0/20240602_T095631/tensorboard
2024-06-02 09:56:32 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 384,796
2024-06-02 09:56:35 [INFO]: Epoch 001 - training loss: 6870.0470, validation loss: 0.9968
2024-06-02 09:56:35 [INFO]: Epoch 002 - training loss: 4656.9523, validation loss: 0.8797
2024-06-02 09:56:35 [INFO]: Epoch 003 - training loss: 4735.9290, validation loss: 0.9544
2024-06-02 09:56:35 [INFO]: Epoch 004 - training loss: 4682.2640, validation loss: 0.9433
2024-06-02 09:56:36 [INFO]: Epoch 005 - training loss: 4653.0645, validation loss: 0.8256
2024-06-02 09:56:36 [INFO]: Epoch 006 - training loss: 4621.6313, validation loss: 0.7493
2024-06-02 09:56:36 [INFO]: Epoch 007 - training loss: 4616.2021, validation loss: 0.7699
2024-06-02 09:56:37 [INFO]: Epoch 008 - training loss: 4607.2554, validation loss: 0.7557
2024-06-02 09:56:37 [INFO]: Epoch 009 - training loss: 4627.4311, validation loss: 0.7356
2024-06-02 09:56:37 [INFO]: Epoch 010 - training loss: 4603.3140, validation loss: 0.7057
2024-06-02 09:56:37 [INFO]: Epoch 011 - training loss: 4592.6706, validation loss: 0.6805
2024-06-02 09:56:38 [INFO]: Epoch 012 - training loss: 4583.4074, validation loss: 0.6160
2024-06-02 09:56:38 [INFO]: Epoch 013 - training loss: 4574.0182, validation loss: 0.5967
2024-06-02 09:56:38 [INFO]: Epoch 014 - training loss: 4571.1939, validation loss: 0.5751
2024-06-02 09:56:39 [INFO]: Epoch 015 - training loss: 4566.6773, validation loss: 0.5518
2024-06-02 09:56:39 [INFO]: Epoch 016 - training loss: 4563.8451, validation loss: 0.5794
2024-06-02 09:56:39 [INFO]: Epoch 017 - training loss: 4561.0914, validation loss: 0.5304
2024-06-02 09:56:39 [INFO]: Epoch 018 - training loss: 4557.2741, validation loss: 0.5269
2024-06-02 09:56:40 [INFO]: Epoch 019 - training loss: 4557.1037, validation loss: 0.5062
2024-06-02 09:56:40 [INFO]: Epoch 020 - training loss: 4556.5045, validation loss: 0.5077
2024-06-02 09:56:40 [INFO]: Epoch 021 - training loss: 4555.9765, validation loss: 0.4678
2024-06-02 09:56:40 [INFO]: Epoch 022 - training loss: 4554.2431, validation loss: 0.4532
2024-06-02 09:56:41 [INFO]: Epoch 023 - training loss: 4552.6848, validation loss: 0.4147
2024-06-02 09:56:41 [INFO]: Epoch 024 - training loss: 4550.3361, validation loss: 0.3619
2024-06-02 09:56:41 [INFO]: Epoch 025 - training loss: 4549.0015, validation loss: 0.2700
2024-06-02 09:56:42 [INFO]: Epoch 026 - training loss: 4547.7557, validation loss: 0.2208
2024-06-02 09:56:42 [INFO]: Epoch 027 - training loss: 4546.0250, validation loss: 0.2364
2024-06-02 09:56:42 [INFO]: Epoch 028 - training loss: 4545.7782, validation loss: 0.1929
2024-06-02 09:56:42 [INFO]: Epoch 029 - training loss: 4545.7180, validation loss: 0.2294
2024-06-02 09:56:43 [INFO]: Epoch 030 - training loss: 4549.2270, validation loss: 0.1817
2024-06-02 09:56:43 [INFO]: Epoch 031 - training loss: 4546.2226, validation loss: 0.2155
2024-06-02 09:56:43 [INFO]: Epoch 032 - training loss: 4544.4714, validation loss: 0.1916
2024-06-02 09:56:44 [INFO]: Epoch 033 - training loss: 4543.4439, validation loss: 0.1803
2024-06-02 09:56:44 [INFO]: Epoch 034 - training loss: 4543.1637, validation loss: 0.1752
2024-06-02 09:56:44 [INFO]: Epoch 035 - training loss: 4542.3072, validation loss: 0.1719
2024-06-02 09:56:44 [INFO]: Epoch 036 - training loss: 4541.5442, validation loss: 0.1629
2024-06-02 09:56:45 [INFO]: Epoch 037 - training loss: 4542.1706, validation loss: 0.1559
2024-06-02 09:56:45 [INFO]: Epoch 038 - training loss: 4542.3922, validation loss: 0.1693
2024-06-02 09:56:45 [INFO]: Epoch 039 - training loss: 4544.0225, validation loss: 0.1760
2024-06-02 09:56:45 [INFO]: Epoch 040 - training loss: 4543.8041, validation loss: 0.1897
2024-06-02 09:56:46 [INFO]: Epoch 041 - training loss: 4544.3009, validation loss: 0.1974
2024-06-02 09:56:46 [INFO]: Epoch 042 - training loss: 4543.2137, validation loss: 0.1526
2024-06-02 09:56:46 [INFO]: Epoch 043 - training loss: 4542.3714, validation loss: 0.1595
2024-06-02 09:56:47 [INFO]: Epoch 044 - training loss: 4542.1427, validation loss: 0.1652
2024-06-02 09:56:47 [INFO]: Epoch 045 - training loss: 4541.2379, validation loss: 0.1627
2024-06-02 09:56:47 [INFO]: Epoch 046 - training loss: 4540.5755, validation loss: 0.1585
2024-06-02 09:56:47 [INFO]: Epoch 047 - training loss: 4540.5377, validation loss: 0.1662
2024-06-02 09:56:48 [INFO]: Epoch 048 - training loss: 4540.0914, validation loss: 0.1594
2024-06-02 09:56:48 [INFO]: Epoch 049 - training loss: 4540.5967, validation loss: 0.1596
2024-06-02 09:56:48 [INFO]: Epoch 050 - training loss: 4541.1206, validation loss: 0.1703
2024-06-02 09:56:48 [INFO]: Epoch 051 - training loss: 4540.8274, validation loss: 0.1634
2024-06-02 09:56:49 [INFO]: Epoch 052 - training loss: 4540.4224, validation loss: 0.1645
2024-06-02 09:56:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 09:56:49 [INFO]: Finished training. The best model is from epoch#42.
2024-06-02 09:56:49 [INFO]: Saved the model to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_0/20240602_T095631/GPVAE.pypots
2024-06-02 09:56:49 [INFO]: Successfully saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_0/imputation.pkl
2024-06-02 09:56:49 [INFO]: Round0 - GPVAE on ETT_h1: MAE=0.3285, MSE=0.1988, MRE=0.3877
2024-06-02 09:56:49 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 09:56:49 [INFO]: Using the given device: cuda:0
2024-06-02 09:56:49 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_1/20240602_T095649
2024-06-02 09:56:49 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_1/20240602_T095649/tensorboard
2024-06-02 09:56:49 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 384,796
2024-06-02 09:56:49 [INFO]: Epoch 001 - training loss: 6783.0972, validation loss: 1.0231
2024-06-02 09:56:50 [INFO]: Epoch 002 - training loss: 4651.8425, validation loss: 0.8607
2024-06-02 09:56:50 [INFO]: Epoch 003 - training loss: 4704.2854, validation loss: 0.8110
2024-06-02 09:56:50 [INFO]: Epoch 004 - training loss: 4664.3160, validation loss: 0.7587
2024-06-02 09:56:50 [INFO]: Epoch 005 - training loss: 4626.2483, validation loss: 0.7784
2024-06-02 09:56:51 [INFO]: Epoch 006 - training loss: 4607.4538, validation loss: 0.6962
2024-06-02 09:56:51 [INFO]: Epoch 007 - training loss: 4590.7478, validation loss: 0.6759
2024-06-02 09:56:51 [INFO]: Epoch 008 - training loss: 4595.2819, validation loss: 0.6552
2024-06-02 09:56:51 [INFO]: Epoch 009 - training loss: 4580.1907, validation loss: 0.6436
2024-06-02 09:56:52 [INFO]: Epoch 010 - training loss: 4573.9391, validation loss: 0.5692
2024-06-02 09:56:52 [INFO]: Epoch 011 - training loss: 4570.8198, validation loss: 0.5977
2024-06-02 09:56:52 [INFO]: Epoch 012 - training loss: 4570.0755, validation loss: 0.5859
2024-06-02 09:56:53 [INFO]: Epoch 013 - training loss: 4571.1754, validation loss: 0.6232
2024-06-02 09:56:53 [INFO]: Epoch 014 - training loss: 4571.7101, validation loss: 0.6006
2024-06-02 09:56:53 [INFO]: Epoch 015 - training loss: 4569.1643, validation loss: 0.5679
2024-06-02 09:56:53 [INFO]: Epoch 016 - training loss: 4564.3186, validation loss: 0.5253
2024-06-02 09:56:54 [INFO]: Epoch 017 - training loss: 4568.3394, validation loss: 0.5386
2024-06-02 09:56:54 [INFO]: Epoch 018 - training loss: 4567.9565, validation loss: 0.5333
2024-06-02 09:56:54 [INFO]: Epoch 019 - training loss: 4568.8285, validation loss: 0.5160
2024-06-02 09:56:55 [INFO]: Epoch 020 - training loss: 4561.1970, validation loss: 0.5058
2024-06-02 09:56:55 [INFO]: Epoch 021 - training loss: 4556.0376, validation loss: 0.4942
2024-06-02 09:56:55 [INFO]: Epoch 022 - training loss: 4553.4418, validation loss: 0.4484
2024-06-02 09:56:55 [INFO]: Epoch 023 - training loss: 4552.0061, validation loss: 0.4322
2024-06-02 09:56:56 [INFO]: Epoch 024 - training loss: 4552.1889, validation loss: 0.4320
2024-06-02 09:56:56 [INFO]: Epoch 025 - training loss: 4553.2009, validation loss: 0.3877
2024-06-02 09:56:56 [INFO]: Epoch 026 - training loss: 4550.6182, validation loss: 0.3303
2024-06-02 09:56:57 [INFO]: Epoch 027 - training loss: 4548.0264, validation loss: 0.3183
2024-06-02 09:56:57 [INFO]: Epoch 028 - training loss: 4547.5242, validation loss: 0.2665
2024-06-02 09:56:57 [INFO]: Epoch 029 - training loss: 4545.5562, validation loss: 0.2509
2024-06-02 09:56:57 [INFO]: Epoch 030 - training loss: 4546.4301, validation loss: 0.2292
2024-06-02 09:56:58 [INFO]: Epoch 031 - training loss: 4544.5015, validation loss: 0.2197
2024-06-02 09:56:58 [INFO]: Epoch 032 - training loss: 4545.1880, validation loss: 0.2359
2024-06-02 09:56:58 [INFO]: Epoch 033 - training loss: 4544.6104, validation loss: 0.2190
2024-06-02 09:56:59 [INFO]: Epoch 034 - training loss: 4545.1045, validation loss: 0.2082
2024-06-02 09:56:59 [INFO]: Epoch 035 - training loss: 4544.2068, validation loss: 0.1983
2024-06-02 09:56:59 [INFO]: Epoch 036 - training loss: 4543.5824, validation loss: 0.2031
2024-06-02 09:56:59 [INFO]: Epoch 037 - training loss: 4542.9248, validation loss: 0.2067
2024-06-02 09:57:00 [INFO]: Epoch 038 - training loss: 4542.9205, validation loss: 0.1859
2024-06-02 09:57:00 [INFO]: Epoch 039 - training loss: 4543.2327, validation loss: 0.1935
2024-06-02 09:57:00 [INFO]: Epoch 040 - training loss: 4543.1061, validation loss: 0.1782
2024-06-02 09:57:01 [INFO]: Epoch 041 - training loss: 4542.2682, validation loss: 0.1988
2024-06-02 09:57:01 [INFO]: Epoch 042 - training loss: 4542.6238, validation loss: 0.2082
2024-06-02 09:57:01 [INFO]: Epoch 043 - training loss: 4542.6274, validation loss: 0.1946
2024-06-02 09:57:01 [INFO]: Epoch 044 - training loss: 4541.8949, validation loss: 0.1993
2024-06-02 09:57:02 [INFO]: Epoch 045 - training loss: 4541.3381, validation loss: 0.1826
2024-06-02 09:57:02 [INFO]: Epoch 046 - training loss: 4540.9217, validation loss: 0.1817
2024-06-02 09:57:02 [INFO]: Epoch 047 - training loss: 4540.8301, validation loss: 0.1848
2024-06-02 09:57:02 [INFO]: Epoch 048 - training loss: 4540.8128, validation loss: 0.1827
2024-06-02 09:57:03 [INFO]: Epoch 049 - training loss: 4540.4194, validation loss: 0.1792
2024-06-02 09:57:03 [INFO]: Epoch 050 - training loss: 4540.6528, validation loss: 0.1942
2024-06-02 09:57:03 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 09:57:03 [INFO]: Finished training. The best model is from epoch#40.
2024-06-02 09:57:03 [INFO]: Saved the model to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_1/20240602_T095649/GPVAE.pypots
2024-06-02 09:57:03 [INFO]: Successfully saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_1/imputation.pkl
2024-06-02 09:57:03 [INFO]: Round1 - GPVAE on ETT_h1: MAE=0.3565, MSE=0.2208, MRE=0.4207
2024-06-02 09:57:03 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 09:57:03 [INFO]: Using the given device: cuda:0
2024-06-02 09:57:03 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_2/20240602_T095703
2024-06-02 09:57:03 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_2/20240602_T095703/tensorboard
2024-06-02 09:57:03 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 384,796
2024-06-02 09:57:03 [INFO]: Epoch 001 - training loss: 6945.2987, validation loss: 1.0643
2024-06-02 09:57:04 [INFO]: Epoch 002 - training loss: 4694.1476, validation loss: 0.9927
2024-06-02 09:57:04 [INFO]: Epoch 003 - training loss: 4671.1934, validation loss: 1.0071
2024-06-02 09:57:04 [INFO]: Epoch 004 - training loss: 4668.0905, validation loss: 1.0115
2024-06-02 09:57:04 [INFO]: Epoch 005 - training loss: 4651.5061, validation loss: 0.9558
2024-06-02 09:57:05 [INFO]: Epoch 006 - training loss: 4630.2168, validation loss: 0.8393
2024-06-02 09:57:05 [INFO]: Epoch 007 - training loss: 4618.8474, validation loss: 0.9342
2024-06-02 09:57:05 [INFO]: Epoch 008 - training loss: 4620.4271, validation loss: 0.7545
2024-06-02 09:57:05 [INFO]: Epoch 009 - training loss: 4621.9346, validation loss: 0.7428
2024-06-02 09:57:06 [INFO]: Epoch 010 - training loss: 4602.2939, validation loss: 0.7158
2024-06-02 09:57:06 [INFO]: Epoch 011 - training loss: 4586.5910, validation loss: 0.6896
2024-06-02 09:57:06 [INFO]: Epoch 012 - training loss: 4583.1055, validation loss: 0.6434
2024-06-02 09:57:07 [INFO]: Epoch 013 - training loss: 4577.0823, validation loss: 0.6605
2024-06-02 09:57:07 [INFO]: Epoch 014 - training loss: 4572.5243, validation loss: 0.6175
2024-06-02 09:57:07 [INFO]: Epoch 015 - training loss: 4565.1716, validation loss: 0.5556
2024-06-02 09:57:07 [INFO]: Epoch 016 - training loss: 4559.9289, validation loss: 0.5429
2024-06-02 09:57:08 [INFO]: Epoch 017 - training loss: 4562.1543, validation loss: 0.5802
2024-06-02 09:57:08 [INFO]: Epoch 018 - training loss: 4575.1761, validation loss: 0.6469
2024-06-02 09:57:08 [INFO]: Epoch 019 - training loss: 4569.8511, validation loss: 0.5927
2024-06-02 09:57:08 [INFO]: Epoch 020 - training loss: 4564.8471, validation loss: 0.5256
2024-06-02 09:57:09 [INFO]: Epoch 021 - training loss: 4561.0807, validation loss: 0.5572
2024-06-02 09:57:09 [INFO]: Epoch 022 - training loss: 4556.3580, validation loss: 0.4812
2024-06-02 09:57:09 [INFO]: Epoch 023 - training loss: 4553.1991, validation loss: 0.4705
2024-06-02 09:57:10 [INFO]: Epoch 024 - training loss: 4550.7557, validation loss: 0.4353
2024-06-02 09:57:10 [INFO]: Epoch 025 - training loss: 4550.4271, validation loss: 0.3927
2024-06-02 09:57:10 [INFO]: Epoch 026 - training loss: 4548.8385, validation loss: 0.3858
2024-06-02 09:57:10 [INFO]: Epoch 027 - training loss: 4548.1054, validation loss: 0.3626
2024-06-02 09:57:11 [INFO]: Epoch 028 - training loss: 4546.8427, validation loss: 0.3127
2024-06-02 09:57:11 [INFO]: Epoch 029 - training loss: 4546.1368, validation loss: 0.2874
2024-06-02 09:57:11 [INFO]: Epoch 030 - training loss: 4546.0485, validation loss: 0.2579
2024-06-02 09:57:12 [INFO]: Epoch 031 - training loss: 4545.9992, validation loss: 0.2586
2024-06-02 09:57:12 [INFO]: Epoch 032 - training loss: 4546.8150, validation loss: 0.2407
2024-06-02 09:57:12 [INFO]: Epoch 033 - training loss: 4547.2757, validation loss: 0.2424
2024-06-02 09:57:12 [INFO]: Epoch 034 - training loss: 4545.9363, validation loss: 0.2312
2024-06-02 09:57:13 [INFO]: Epoch 035 - training loss: 4544.3154, validation loss: 0.2103
2024-06-02 09:57:13 [INFO]: Epoch 036 - training loss: 4544.5316, validation loss: 0.2034
2024-06-02 09:57:13 [INFO]: Epoch 037 - training loss: 4543.6893, validation loss: 0.2011
2024-06-02 09:57:14 [INFO]: Epoch 038 - training loss: 4542.1285, validation loss: 0.1765
2024-06-02 09:57:14 [INFO]: Epoch 039 - training loss: 4541.7859, validation loss: 0.1693
2024-06-02 09:57:14 [INFO]: Epoch 040 - training loss: 4541.5683, validation loss: 0.1651
2024-06-02 09:57:14 [INFO]: Epoch 041 - training loss: 4542.5016, validation loss: 0.1664
2024-06-02 09:57:15 [INFO]: Epoch 042 - training loss: 4542.6541, validation loss: 0.1635
2024-06-02 09:57:15 [INFO]: Epoch 043 - training loss: 4542.2190, validation loss: 0.1698
2024-06-02 09:57:15 [INFO]: Epoch 044 - training loss: 4541.7885, validation loss: 0.1687
2024-06-02 09:57:16 [INFO]: Epoch 045 - training loss: 4541.2369, validation loss: 0.1658
2024-06-02 09:57:16 [INFO]: Epoch 046 - training loss: 4541.2229, validation loss: 0.1535
2024-06-02 09:57:16 [INFO]: Epoch 047 - training loss: 4540.6368, validation loss: 0.1570
2024-06-02 09:57:16 [INFO]: Epoch 048 - training loss: 4540.1676, validation loss: 0.1601
2024-06-02 09:57:17 [INFO]: Epoch 049 - training loss: 4540.3870, validation loss: 0.1563
2024-06-02 09:57:17 [INFO]: Epoch 050 - training loss: 4540.1666, validation loss: 0.1498
2024-06-02 09:57:17 [INFO]: Epoch 051 - training loss: 4539.7436, validation loss: 0.1489
2024-06-02 09:57:18 [INFO]: Epoch 052 - training loss: 4540.1274, validation loss: 0.1429
2024-06-02 09:57:18 [INFO]: Epoch 053 - training loss: 4540.0202, validation loss: 0.1480
2024-06-02 09:57:18 [INFO]: Epoch 054 - training loss: 4539.9691, validation loss: 0.1486
2024-06-02 09:57:18 [INFO]: Epoch 055 - training loss: 4539.8650, validation loss: 0.1502
2024-06-02 09:57:19 [INFO]: Epoch 056 - training loss: 4539.7190, validation loss: 0.1467
2024-06-02 09:57:19 [INFO]: Epoch 057 - training loss: 4539.6076, validation loss: 0.1483
2024-06-02 09:57:19 [INFO]: Epoch 058 - training loss: 4539.4621, validation loss: 0.1460
2024-06-02 09:57:20 [INFO]: Epoch 059 - training loss: 4539.1570, validation loss: 0.1480
2024-06-02 09:57:20 [INFO]: Epoch 060 - training loss: 4539.5495, validation loss: 0.1556
2024-06-02 09:57:20 [INFO]: Epoch 061 - training loss: 4540.5439, validation loss: 0.1462
2024-06-02 09:57:20 [INFO]: Epoch 062 - training loss: 4541.5856, validation loss: 0.1534
2024-06-02 09:57:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 09:57:20 [INFO]: Finished training. The best model is from epoch#52.
2024-06-02 09:57:20 [INFO]: Saved the model to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_2/20240602_T095703/GPVAE.pypots
2024-06-02 09:57:20 [INFO]: Successfully saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_2/imputation.pkl
2024-06-02 09:57:20 [INFO]: Round2 - GPVAE on ETT_h1: MAE=0.3198, MSE=0.1930, MRE=0.3774
2024-06-02 09:57:20 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 09:57:20 [INFO]: Using the given device: cuda:0
2024-06-02 09:57:21 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_3/20240602_T095720
2024-06-02 09:57:21 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_3/20240602_T095720/tensorboard
2024-06-02 09:57:21 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 384,796
2024-06-02 09:57:21 [INFO]: Epoch 001 - training loss: 6799.1902, validation loss: 1.0820
2024-06-02 09:57:21 [INFO]: Epoch 002 - training loss: 4682.8639, validation loss: 1.0972
2024-06-02 09:57:21 [INFO]: Epoch 003 - training loss: 4672.4027, validation loss: 0.9945
2024-06-02 09:57:22 [INFO]: Epoch 004 - training loss: 4655.7947, validation loss: 0.9702
2024-06-02 09:57:22 [INFO]: Epoch 005 - training loss: 4661.3462, validation loss: 1.0039
2024-06-02 09:57:22 [INFO]: Epoch 006 - training loss: 4689.1237, validation loss: 0.9108
2024-06-02 09:57:23 [INFO]: Epoch 007 - training loss: 4648.5047, validation loss: 0.9154
2024-06-02 09:57:23 [INFO]: Epoch 008 - training loss: 4628.7084, validation loss: 0.8862
2024-06-02 09:57:23 [INFO]: Epoch 009 - training loss: 4613.6362, validation loss: 0.7893
2024-06-02 09:57:24 [INFO]: Epoch 010 - training loss: 4605.3384, validation loss: 0.7755
2024-06-02 09:57:24 [INFO]: Epoch 011 - training loss: 4606.3643, validation loss: 0.7811
2024-06-02 09:57:24 [INFO]: Epoch 012 - training loss: 4624.4918, validation loss: 0.7235
2024-06-02 09:57:24 [INFO]: Epoch 013 - training loss: 4604.4016, validation loss: 0.7129
2024-06-02 09:57:25 [INFO]: Epoch 014 - training loss: 4590.9871, validation loss: 0.6676
2024-06-02 09:57:25 [INFO]: Epoch 015 - training loss: 4584.1287, validation loss: 0.6620
2024-06-02 09:57:25 [INFO]: Epoch 016 - training loss: 4579.0748, validation loss: 0.6256
2024-06-02 09:57:25 [INFO]: Epoch 017 - training loss: 4581.9665, validation loss: 0.6205
2024-06-02 09:57:26 [INFO]: Epoch 018 - training loss: 4586.4841, validation loss: 0.6561
2024-06-02 09:57:26 [INFO]: Epoch 019 - training loss: 4579.7789, validation loss: 0.6260
2024-06-02 09:57:26 [INFO]: Epoch 020 - training loss: 4576.5260, validation loss: 0.6065
2024-06-02 09:57:27 [INFO]: Epoch 021 - training loss: 4571.7181, validation loss: 0.5935
2024-06-02 09:57:27 [INFO]: Epoch 022 - training loss: 4567.2513, validation loss: 0.6008
2024-06-02 09:57:27 [INFO]: Epoch 023 - training loss: 4563.3552, validation loss: 0.5705
2024-06-02 09:57:28 [INFO]: Epoch 024 - training loss: 4561.8388, validation loss: 0.5630
2024-06-02 09:57:28 [INFO]: Epoch 025 - training loss: 4559.0180, validation loss: 0.5488
2024-06-02 09:57:28 [INFO]: Epoch 026 - training loss: 4557.3982, validation loss: 0.5323
2024-06-02 09:57:28 [INFO]: Epoch 027 - training loss: 4556.2677, validation loss: 0.5535
2024-06-02 09:57:29 [INFO]: Epoch 028 - training loss: 4554.8302, validation loss: 0.5246
2024-06-02 09:57:29 [INFO]: Epoch 029 - training loss: 4553.6189, validation loss: 0.5559
2024-06-02 09:57:29 [INFO]: Epoch 030 - training loss: 4552.0762, validation loss: 0.5094
2024-06-02 09:57:29 [INFO]: Epoch 031 - training loss: 4552.8244, validation loss: 0.5565
2024-06-02 09:57:30 [INFO]: Epoch 032 - training loss: 4552.5920, validation loss: 0.5172
2024-06-02 09:57:30 [INFO]: Epoch 033 - training loss: 4554.0907, validation loss: 0.5611
2024-06-02 09:57:30 [INFO]: Epoch 034 - training loss: 4553.1614, validation loss: 0.5239
2024-06-02 09:57:31 [INFO]: Epoch 035 - training loss: 4551.9125, validation loss: 0.5276
2024-06-02 09:57:31 [INFO]: Epoch 036 - training loss: 4551.4847, validation loss: 0.5231
2024-06-02 09:57:31 [INFO]: Epoch 037 - training loss: 4551.0075, validation loss: 0.5374
2024-06-02 09:57:31 [INFO]: Epoch 038 - training loss: 4552.7104, validation loss: 0.5039
2024-06-02 09:57:32 [INFO]: Epoch 039 - training loss: 4553.9270, validation loss: 0.5364
2024-06-02 09:57:32 [INFO]: Epoch 040 - training loss: 4552.9258, validation loss: 0.5248
2024-06-02 09:57:32 [INFO]: Epoch 041 - training loss: 4552.3653, validation loss: 0.5130
2024-06-02 09:57:33 [INFO]: Epoch 042 - training loss: 4551.0861, validation loss: 0.5147
2024-06-02 09:57:33 [INFO]: Epoch 043 - training loss: 4550.9734, validation loss: 0.5123
2024-06-02 09:57:33 [INFO]: Epoch 044 - training loss: 4551.0004, validation loss: 0.4993
2024-06-02 09:57:33 [INFO]: Epoch 045 - training loss: 4551.1687, validation loss: 0.5286
2024-06-02 09:57:34 [INFO]: Epoch 046 - training loss: 4550.1187, validation loss: 0.5112
2024-06-02 09:57:34 [INFO]: Epoch 047 - training loss: 4550.4122, validation loss: 0.4970
2024-06-02 09:57:34 [INFO]: Epoch 048 - training loss: 4549.6445, validation loss: 0.5151
2024-06-02 09:57:35 [INFO]: Epoch 049 - training loss: 4549.6563, validation loss: 0.5151
2024-06-02 09:57:35 [INFO]: Epoch 050 - training loss: 4549.5287, validation loss: 0.4894
2024-06-02 09:57:35 [INFO]: Epoch 051 - training loss: 4549.6018, validation loss: 0.5222
2024-06-02 09:57:35 [INFO]: Epoch 052 - training loss: 4550.1159, validation loss: 0.5142
2024-06-02 09:57:36 [INFO]: Epoch 053 - training loss: 4549.7227, validation loss: 0.4933
2024-06-02 09:57:36 [INFO]: Epoch 054 - training loss: 4549.3604, validation loss: 0.5109
2024-06-02 09:57:36 [INFO]: Epoch 055 - training loss: 4548.8608, validation loss: 0.4970
2024-06-02 09:57:36 [INFO]: Epoch 056 - training loss: 4548.6272, validation loss: 0.4886
2024-06-02 09:57:37 [INFO]: Epoch 057 - training loss: 4548.2419, validation loss: 0.4867
2024-06-02 09:57:37 [INFO]: Epoch 058 - training loss: 4548.3664, validation loss: 0.4877
2024-06-02 09:57:37 [INFO]: Epoch 059 - training loss: 4548.9888, validation loss: 0.4864
2024-06-02 09:57:38 [INFO]: Epoch 060 - training loss: 4548.4662, validation loss: 0.4929
2024-06-02 09:57:38 [INFO]: Epoch 061 - training loss: 4548.3234, validation loss: 0.4730
2024-06-02 09:57:38 [INFO]: Epoch 062 - training loss: 4548.3547, validation loss: 0.4826
2024-06-02 09:57:38 [INFO]: Epoch 063 - training loss: 4550.0325, validation loss: 0.4599
2024-06-02 09:57:39 [INFO]: Epoch 064 - training loss: 4551.3551, validation loss: 0.4728
2024-06-02 09:57:39 [INFO]: Epoch 065 - training loss: 4549.1711, validation loss: 0.4880
2024-06-02 09:57:39 [INFO]: Epoch 066 - training loss: 4548.0959, validation loss: 0.4596
2024-06-02 09:57:40 [INFO]: Epoch 067 - training loss: 4547.5662, validation loss: 0.4269
2024-06-02 09:57:40 [INFO]: Epoch 068 - training loss: 4548.0299, validation loss: 0.4616
2024-06-02 09:57:40 [INFO]: Epoch 069 - training loss: 4547.9271, validation loss: 0.4250
2024-06-02 09:57:40 [INFO]: Epoch 070 - training loss: 4548.2203, validation loss: 0.4075
2024-06-02 09:57:41 [INFO]: Epoch 071 - training loss: 4547.7763, validation loss: 0.4197
2024-06-02 09:57:41 [INFO]: Epoch 072 - training loss: 4548.0400, validation loss: 0.3246
2024-06-02 09:57:41 [INFO]: Epoch 073 - training loss: 4547.7482, validation loss: 0.3453
2024-06-02 09:57:41 [INFO]: Epoch 074 - training loss: 4546.5089, validation loss: 0.2883
2024-06-02 09:57:42 [INFO]: Epoch 075 - training loss: 4546.1705, validation loss: 0.2606
2024-06-02 09:57:42 [INFO]: Epoch 076 - training loss: 4545.6902, validation loss: 0.3095
2024-06-02 09:57:42 [INFO]: Epoch 077 - training loss: 4545.9432, validation loss: 0.2460
2024-06-02 09:57:42 [INFO]: Epoch 078 - training loss: 4545.2379, validation loss: 0.1944
2024-06-02 09:57:43 [INFO]: Epoch 079 - training loss: 4545.5728, validation loss: 0.2364
2024-06-02 09:57:43 [INFO]: Epoch 080 - training loss: 4546.1396, validation loss: 0.2210
2024-06-02 09:57:43 [INFO]: Epoch 081 - training loss: 4544.6330, validation loss: 0.2004
2024-06-02 09:57:44 [INFO]: Epoch 082 - training loss: 4543.2575, validation loss: 0.1595
2024-06-02 09:57:44 [INFO]: Epoch 083 - training loss: 4542.8931, validation loss: 0.1589
2024-06-02 09:57:44 [INFO]: Epoch 084 - training loss: 4542.1685, validation loss: 0.1915
2024-06-02 09:57:44 [INFO]: Epoch 085 - training loss: 4542.0665, validation loss: 0.1751
2024-06-02 09:57:45 [INFO]: Epoch 086 - training loss: 4541.8370, validation loss: 0.1713
2024-06-02 09:57:45 [INFO]: Epoch 087 - training loss: 4542.0977, validation loss: 0.1570
2024-06-02 09:57:45 [INFO]: Epoch 088 - training loss: 4541.6966, validation loss: 0.1499
2024-06-02 09:57:46 [INFO]: Epoch 089 - training loss: 4541.3126, validation loss: 0.1576
2024-06-02 09:57:46 [INFO]: Epoch 090 - training loss: 4540.9964, validation loss: 0.1491
2024-06-02 09:57:46 [INFO]: Epoch 091 - training loss: 4541.0525, validation loss: 0.1654
2024-06-02 09:57:46 [INFO]: Epoch 092 - training loss: 4540.9743, validation loss: 0.1546
2024-06-02 09:57:47 [INFO]: Epoch 093 - training loss: 4540.4457, validation loss: 0.1402
2024-06-02 09:57:47 [INFO]: Epoch 094 - training loss: 4539.9822, validation loss: 0.1342
2024-06-02 09:57:47 [INFO]: Epoch 095 - training loss: 4540.1757, validation loss: 0.1426
2024-06-02 09:57:47 [INFO]: Epoch 096 - training loss: 4540.4780, validation loss: 0.1501
2024-06-02 09:57:48 [INFO]: Epoch 097 - training loss: 4540.4325, validation loss: 0.1630
2024-06-02 09:57:48 [INFO]: Epoch 098 - training loss: 4540.7705, validation loss: 0.1535
2024-06-02 09:57:48 [INFO]: Epoch 099 - training loss: 4540.2223, validation loss: 0.1347
2024-06-02 09:57:48 [INFO]: Epoch 100 - training loss: 4540.5388, validation loss: 0.1436
2024-06-02 09:57:48 [INFO]: Finished training. The best model is from epoch#94.
2024-06-02 09:57:48 [INFO]: Saved the model to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_3/20240602_T095720/GPVAE.pypots
2024-06-02 09:57:49 [INFO]: Successfully saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_3/imputation.pkl
2024-06-02 09:57:49 [INFO]: Round3 - GPVAE on ETT_h1: MAE=0.3040, MSE=0.1761, MRE=0.3587
2024-06-02 09:57:49 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 09:57:49 [INFO]: Using the given device: cuda:0
2024-06-02 09:57:49 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_4/20240602_T095749
2024-06-02 09:57:49 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_4/20240602_T095749/tensorboard
2024-06-02 09:57:49 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 384,796
2024-06-02 09:57:49 [INFO]: Epoch 001 - training loss: 6669.9192, validation loss: 1.0381
2024-06-02 09:57:49 [INFO]: Epoch 002 - training loss: 4671.8754, validation loss: 0.9179
2024-06-02 09:57:49 [INFO]: Epoch 003 - training loss: 4670.0474, validation loss: 0.8110
2024-06-02 09:57:50 [INFO]: Epoch 004 - training loss: 4657.8189, validation loss: 0.9697
2024-06-02 09:57:50 [INFO]: Epoch 005 - training loss: 4657.6903, validation loss: 0.8076
2024-06-02 09:57:50 [INFO]: Epoch 006 - training loss: 4670.8754, validation loss: 0.7591
2024-06-02 09:57:51 [INFO]: Epoch 007 - training loss: 4623.2615, validation loss: 0.6552
2024-06-02 09:57:51 [INFO]: Epoch 008 - training loss: 4598.3405, validation loss: 0.7037
2024-06-02 09:57:51 [INFO]: Epoch 009 - training loss: 4592.2726, validation loss: 0.6779
2024-06-02 09:57:51 [INFO]: Epoch 010 - training loss: 4610.7079, validation loss: 0.7014
2024-06-02 09:57:52 [INFO]: Epoch 011 - training loss: 4602.5404, validation loss: 0.6470
2024-06-02 09:57:52 [INFO]: Epoch 012 - training loss: 4586.1620, validation loss: 0.6273
2024-06-02 09:57:52 [INFO]: Epoch 013 - training loss: 4574.9018, validation loss: 0.6116
2024-06-02 09:57:52 [INFO]: Epoch 014 - training loss: 4569.0974, validation loss: 0.5867
2024-06-02 09:57:52 [INFO]: Epoch 015 - training loss: 4567.9785, validation loss: 0.5408
2024-06-02 09:57:53 [INFO]: Epoch 016 - training loss: 4562.1233, validation loss: 0.5240
2024-06-02 09:57:53 [INFO]: Epoch 017 - training loss: 4558.8523, validation loss: 0.5070
2024-06-02 09:57:53 [INFO]: Epoch 018 - training loss: 4555.5521, validation loss: 0.4761
2024-06-02 09:57:54 [INFO]: Epoch 019 - training loss: 4553.2460, validation loss: 0.4653
2024-06-02 09:57:54 [INFO]: Epoch 020 - training loss: 4552.1619, validation loss: 0.4539
2024-06-02 09:57:54 [INFO]: Epoch 021 - training loss: 4550.6340, validation loss: 0.4346
2024-06-02 09:57:54 [INFO]: Epoch 022 - training loss: 4549.7277, validation loss: 0.4089
2024-06-02 09:57:55 [INFO]: Epoch 023 - training loss: 4548.8913, validation loss: 0.3890
2024-06-02 09:57:55 [INFO]: Epoch 024 - training loss: 4548.4377, validation loss: 0.3748
2024-06-02 09:57:55 [INFO]: Epoch 025 - training loss: 4548.6662, validation loss: 0.3600
2024-06-02 09:57:55 [INFO]: Epoch 026 - training loss: 4547.4328, validation loss: 0.3242
2024-06-02 09:57:55 [INFO]: Epoch 027 - training loss: 4546.0043, validation loss: 0.3072
2024-06-02 09:57:56 [INFO]: Epoch 028 - training loss: 4545.8758, validation loss: 0.2859
2024-06-02 09:57:56 [INFO]: Epoch 029 - training loss: 4545.6064, validation loss: 0.2323
2024-06-02 09:57:56 [INFO]: Epoch 030 - training loss: 4544.3758, validation loss: 0.2063
2024-06-02 09:57:56 [INFO]: Epoch 031 - training loss: 4544.4082, validation loss: 0.1842
2024-06-02 09:57:57 [INFO]: Epoch 032 - training loss: 4543.7487, validation loss: 0.1949
2024-06-02 09:57:57 [INFO]: Epoch 033 - training loss: 4543.3878, validation loss: 0.2056
2024-06-02 09:57:57 [INFO]: Epoch 034 - training loss: 4549.2072, validation loss: 0.3439
2024-06-02 09:57:57 [INFO]: Epoch 035 - training loss: 4553.8635, validation loss: 0.2271
2024-06-02 09:57:57 [INFO]: Epoch 036 - training loss: 4548.9261, validation loss: 0.2024
2024-06-02 09:57:58 [INFO]: Epoch 037 - training loss: 4545.9581, validation loss: 0.1747
2024-06-02 09:57:58 [INFO]: Epoch 038 - training loss: 4542.9140, validation loss: 0.1604
2024-06-02 09:57:58 [INFO]: Epoch 039 - training loss: 4542.8726, validation loss: 0.1613
2024-06-02 09:57:58 [INFO]: Epoch 040 - training loss: 4541.6510, validation loss: 0.1636
2024-06-02 09:57:58 [INFO]: Epoch 041 - training loss: 4541.9960, validation loss: 0.1631
2024-06-02 09:57:59 [INFO]: Epoch 042 - training loss: 4541.8082, validation loss: 0.1492
2024-06-02 09:57:59 [INFO]: Epoch 043 - training loss: 4541.6602, validation loss: 0.1436
2024-06-02 09:57:59 [INFO]: Epoch 044 - training loss: 4540.9978, validation loss: 0.1476
2024-06-02 09:57:59 [INFO]: Epoch 045 - training loss: 4541.4372, validation loss: 0.1562
2024-06-02 09:58:00 [INFO]: Epoch 046 - training loss: 4543.5587, validation loss: 0.1705
2024-06-02 09:58:00 [INFO]: Epoch 047 - training loss: 4542.4865, validation loss: 0.1834
2024-06-02 09:58:00 [INFO]: Epoch 048 - training loss: 4542.1072, validation loss: 0.1728
2024-06-02 09:58:00 [INFO]: Epoch 049 - training loss: 4541.3675, validation loss: 0.1621
2024-06-02 09:58:00 [INFO]: Epoch 050 - training loss: 4540.6479, validation loss: 0.1535
2024-06-02 09:58:01 [INFO]: Epoch 051 - training loss: 4540.6972, validation loss: 0.1427
2024-06-02 09:58:01 [INFO]: Epoch 052 - training loss: 4539.8016, validation loss: 0.1475
2024-06-02 09:58:01 [INFO]: Epoch 053 - training loss: 4539.0964, validation loss: 0.1430
2024-06-02 09:58:01 [INFO]: Epoch 054 - training loss: 4539.1517, validation loss: 0.1485
2024-06-02 09:58:01 [INFO]: Epoch 055 - training loss: 4539.3354, validation loss: 0.1495
2024-06-02 09:58:02 [INFO]: Epoch 056 - training loss: 4539.4902, validation loss: 0.1457
2024-06-02 09:58:02 [INFO]: Epoch 057 - training loss: 4538.8186, validation loss: 0.1397
2024-06-02 09:58:02 [INFO]: Epoch 058 - training loss: 4539.1009, validation loss: 0.1484
2024-06-02 09:58:02 [INFO]: Epoch 059 - training loss: 4538.8760, validation loss: 0.1403
2024-06-02 09:58:02 [INFO]: Epoch 060 - training loss: 4539.0118, validation loss: 0.1579
2024-06-02 09:58:03 [INFO]: Epoch 061 - training loss: 4539.2670, validation loss: 0.1504
2024-06-02 09:58:03 [INFO]: Epoch 062 - training loss: 4539.4904, validation loss: 0.1436
2024-06-02 09:58:03 [INFO]: Epoch 063 - training loss: 4539.4849, validation loss: 0.1509
2024-06-02 09:58:03 [INFO]: Epoch 064 - training loss: 4539.5112, validation loss: 0.1392
2024-06-02 09:58:03 [INFO]: Epoch 065 - training loss: 4539.3138, validation loss: 0.1424
2024-06-02 09:58:03 [INFO]: Epoch 066 - training loss: 4538.9777, validation loss: 0.1477
2024-06-02 09:58:04 [INFO]: Epoch 067 - training loss: 4538.7599, validation loss: 0.1534
2024-06-02 09:58:04 [INFO]: Epoch 068 - training loss: 4539.1187, validation loss: 0.1456
2024-06-02 09:58:04 [INFO]: Epoch 069 - training loss: 4538.8387, validation loss: 0.1471
2024-06-02 09:58:04 [INFO]: Epoch 070 - training loss: 4538.6789, validation loss: 0.1449
2024-06-02 09:58:04 [INFO]: Epoch 071 - training loss: 4538.5523, validation loss: 0.1385
2024-06-02 09:58:05 [INFO]: Epoch 072 - training loss: 4538.6227, validation loss: 0.1360
2024-06-02 09:58:05 [INFO]: Epoch 073 - training loss: 4538.4681, validation loss: 0.1473
2024-06-02 09:58:05 [INFO]: Epoch 074 - training loss: 4538.0903, validation loss: 0.1581
2024-06-02 09:58:05 [INFO]: Epoch 075 - training loss: 4538.9743, validation loss: 0.1470
2024-06-02 09:58:06 [INFO]: Epoch 076 - training loss: 4539.3735, validation loss: 0.1541
2024-06-02 09:58:06 [INFO]: Epoch 077 - training loss: 4539.2853, validation loss: 0.1644
2024-06-02 09:58:06 [INFO]: Epoch 078 - training loss: 4539.1731, validation loss: 0.1490
2024-06-02 09:58:06 [INFO]: Epoch 079 - training loss: 4540.0047, validation loss: 0.1597
2024-06-02 09:58:06 [INFO]: Epoch 080 - training loss: 4538.8430, validation loss: 0.1622
2024-06-02 09:58:06 [INFO]: Epoch 081 - training loss: 4540.1556, validation loss: 0.1604
2024-06-02 09:58:07 [INFO]: Epoch 082 - training loss: 4539.8995, validation loss: 0.1635
2024-06-02 09:58:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 09:58:07 [INFO]: Finished training. The best model is from epoch#72.
2024-06-02 09:58:07 [INFO]: Saved the model to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_4/20240602_T095749/GPVAE.pypots
2024-06-02 09:58:07 [INFO]: Successfully saved to results_point_rate01/ETT_h1/GPVAE_ETT_h1/round_4/imputation.pkl
2024-06-02 09:58:07 [INFO]: Round4 - GPVAE on ETT_h1: MAE=0.3338, MSE=0.2194, MRE=0.3939
2024-06-02 09:58:07 [INFO]: Done! Final results:
Averaged GPVAE (384,796 params) on ETT_h1: MAE=0.3285 ± 0.017265060805653133, MSE=0.2016 ± 0.016854477700195703, MRE=0.3877 ± 0.020374344001389606, average inference time=0.11
