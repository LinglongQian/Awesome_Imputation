2024-06-02 11:04:30 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 11:04:30 [INFO]: Using the given device: cuda:0
2024-06-02 11:04:31 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_0/20240602_T110431
2024-06-02 11:04:31 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_0/20240602_T110431/tensorboard
2024-06-02 11:04:32 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-02 11:04:35 [INFO]: Epoch 001 - training loss: 1.0298, validation loss: 0.2457
2024-06-02 11:04:35 [INFO]: Epoch 002 - training loss: 0.6849, validation loss: 0.2081
2024-06-02 11:04:36 [INFO]: Epoch 003 - training loss: 0.6401, validation loss: 0.2024
2024-06-02 11:04:36 [INFO]: Epoch 004 - training loss: 0.6192, validation loss: 0.2003
2024-06-02 11:04:36 [INFO]: Epoch 005 - training loss: 0.6201, validation loss: 0.1882
2024-06-02 11:04:36 [INFO]: Epoch 006 - training loss: 0.6056, validation loss: 0.1861
2024-06-02 11:04:37 [INFO]: Epoch 007 - training loss: 0.6039, validation loss: 0.1763
2024-06-02 11:04:38 [INFO]: Epoch 008 - training loss: 0.6060, validation loss: 0.1745
2024-06-02 11:04:39 [INFO]: Epoch 009 - training loss: 0.5854, validation loss: 0.1700
2024-06-02 11:04:40 [INFO]: Epoch 010 - training loss: 0.5884, validation loss: 0.1758
2024-06-02 11:04:41 [INFO]: Epoch 011 - training loss: 0.5763, validation loss: 0.1753
2024-06-02 11:04:42 [INFO]: Epoch 012 - training loss: 0.5791, validation loss: 0.1652
2024-06-02 11:04:43 [INFO]: Epoch 013 - training loss: 0.5743, validation loss: 0.1658
2024-06-02 11:04:44 [INFO]: Epoch 014 - training loss: 0.5752, validation loss: 0.1705
2024-06-02 11:04:46 [INFO]: Epoch 015 - training loss: 0.5617, validation loss: 0.1631
2024-06-02 11:04:47 [INFO]: Epoch 016 - training loss: 0.5789, validation loss: 0.1604
2024-06-02 11:04:49 [INFO]: Epoch 017 - training loss: 0.5643, validation loss: 0.1580
2024-06-02 11:04:50 [INFO]: Epoch 018 - training loss: 0.5600, validation loss: 0.1596
2024-06-02 11:04:52 [INFO]: Epoch 019 - training loss: 0.5593, validation loss: 0.1678
2024-06-02 11:04:53 [INFO]: Epoch 020 - training loss: 0.5541, validation loss: 0.1495
2024-06-02 11:04:55 [INFO]: Epoch 021 - training loss: 0.5511, validation loss: 0.1576
2024-06-02 11:04:56 [INFO]: Epoch 022 - training loss: 0.5411, validation loss: 0.1438
2024-06-02 11:04:58 [INFO]: Epoch 023 - training loss: 0.5376, validation loss: 0.1504
2024-06-02 11:04:59 [INFO]: Epoch 024 - training loss: 0.5396, validation loss: 0.1523
2024-06-02 11:05:00 [INFO]: Epoch 025 - training loss: 0.5304, validation loss: 0.1454
2024-06-02 11:05:02 [INFO]: Epoch 026 - training loss: 0.5288, validation loss: 0.1388
2024-06-02 11:05:03 [INFO]: Epoch 027 - training loss: 0.5223, validation loss: 0.1387
2024-06-02 11:05:05 [INFO]: Epoch 028 - training loss: 0.5226, validation loss: 0.1336
2024-06-02 11:05:06 [INFO]: Epoch 029 - training loss: 0.5147, validation loss: 0.1351
2024-06-02 11:05:08 [INFO]: Epoch 030 - training loss: 0.5151, validation loss: 0.1351
2024-06-02 11:05:09 [INFO]: Epoch 031 - training loss: 0.5267, validation loss: 0.1362
2024-06-02 11:05:10 [INFO]: Epoch 032 - training loss: 0.5205, validation loss: 0.1329
2024-06-02 11:05:12 [INFO]: Epoch 033 - training loss: 0.5161, validation loss: 0.1350
2024-06-02 11:05:13 [INFO]: Epoch 034 - training loss: 0.5083, validation loss: 0.1223
2024-06-02 11:05:15 [INFO]: Epoch 035 - training loss: 0.4961, validation loss: 0.1245
2024-06-02 11:05:16 [INFO]: Epoch 036 - training loss: 0.5046, validation loss: 0.1301
2024-06-02 11:05:18 [INFO]: Epoch 037 - training loss: 0.5045, validation loss: 0.1289
2024-06-02 11:05:19 [INFO]: Epoch 038 - training loss: 0.5027, validation loss: 0.1324
2024-06-02 11:05:21 [INFO]: Epoch 039 - training loss: 0.5027, validation loss: 0.1216
2024-06-02 11:05:22 [INFO]: Epoch 040 - training loss: 0.4953, validation loss: 0.1241
2024-06-02 11:05:24 [INFO]: Epoch 041 - training loss: 0.4960, validation loss: 0.1188
2024-06-02 11:05:25 [INFO]: Epoch 042 - training loss: 0.4867, validation loss: 0.1209
2024-06-02 11:05:27 [INFO]: Epoch 043 - training loss: 0.4981, validation loss: 0.1265
2024-06-02 11:05:28 [INFO]: Epoch 044 - training loss: 0.4924, validation loss: 0.1308
2024-06-02 11:05:30 [INFO]: Epoch 045 - training loss: 0.4943, validation loss: 0.1233
2024-06-02 11:05:32 [INFO]: Epoch 046 - training loss: 0.4865, validation loss: 0.1194
2024-06-02 11:05:33 [INFO]: Epoch 047 - training loss: 0.4868, validation loss: 0.1199
2024-06-02 11:05:35 [INFO]: Epoch 048 - training loss: 0.4733, validation loss: 0.1130
2024-06-02 11:05:36 [INFO]: Epoch 049 - training loss: 0.4785, validation loss: 0.1139
2024-06-02 11:05:37 [INFO]: Epoch 050 - training loss: 0.4749, validation loss: 0.1118
2024-06-02 11:05:38 [INFO]: Epoch 051 - training loss: 0.4706, validation loss: 0.1153
2024-06-02 11:05:39 [INFO]: Epoch 052 - training loss: 0.4694, validation loss: 0.1171
2024-06-02 11:05:41 [INFO]: Epoch 053 - training loss: 0.4672, validation loss: 0.1149
2024-06-02 11:05:42 [INFO]: Epoch 054 - training loss: 0.4650, validation loss: 0.1256
2024-06-02 11:05:44 [INFO]: Epoch 055 - training loss: 0.4649, validation loss: 0.1149
2024-06-02 11:05:45 [INFO]: Epoch 056 - training loss: 0.4540, validation loss: 0.1214
2024-06-02 11:05:46 [INFO]: Epoch 057 - training loss: 0.4549, validation loss: 0.1197
2024-06-02 11:05:48 [INFO]: Epoch 058 - training loss: 0.4545, validation loss: 0.1082
2024-06-02 11:05:49 [INFO]: Epoch 059 - training loss: 0.4571, validation loss: 0.1096
2024-06-02 11:05:50 [INFO]: Epoch 060 - training loss: 0.4571, validation loss: 0.1113
2024-06-02 11:05:52 [INFO]: Epoch 061 - training loss: 0.4556, validation loss: 0.1102
2024-06-02 11:05:53 [INFO]: Epoch 062 - training loss: 0.4549, validation loss: 0.1162
2024-06-02 11:05:55 [INFO]: Epoch 063 - training loss: 0.4454, validation loss: 0.1088
2024-06-02 11:05:56 [INFO]: Epoch 064 - training loss: 0.4482, validation loss: 0.1180
2024-06-02 11:05:57 [INFO]: Epoch 065 - training loss: 0.4418, validation loss: 0.1088
2024-06-02 11:05:59 [INFO]: Epoch 066 - training loss: 0.4400, validation loss: 0.1032
2024-06-02 11:06:00 [INFO]: Epoch 067 - training loss: 0.4351, validation loss: 0.1090
2024-06-02 11:06:02 [INFO]: Epoch 068 - training loss: 0.4415, validation loss: 0.1037
2024-06-02 11:06:03 [INFO]: Epoch 069 - training loss: 0.4363, validation loss: 0.1082
2024-06-02 11:06:04 [INFO]: Epoch 070 - training loss: 0.4328, validation loss: 0.1072
2024-06-02 11:06:06 [INFO]: Epoch 071 - training loss: 0.4326, validation loss: 0.1059
2024-06-02 11:06:07 [INFO]: Epoch 072 - training loss: 0.4359, validation loss: 0.1018
2024-06-02 11:06:09 [INFO]: Epoch 073 - training loss: 0.4395, validation loss: 0.0969
2024-06-02 11:06:10 [INFO]: Epoch 074 - training loss: 0.4316, validation loss: 0.0991
2024-06-02 11:06:12 [INFO]: Epoch 075 - training loss: 0.4238, validation loss: 0.1019
2024-06-02 11:06:13 [INFO]: Epoch 076 - training loss: 0.4368, validation loss: 0.1070
2024-06-02 11:06:15 [INFO]: Epoch 077 - training loss: 0.4288, validation loss: 0.1108
2024-06-02 11:06:16 [INFO]: Epoch 078 - training loss: 0.4321, validation loss: 0.1051
2024-06-02 11:06:18 [INFO]: Epoch 079 - training loss: 0.4344, validation loss: 0.1035
2024-06-02 11:06:19 [INFO]: Epoch 080 - training loss: 0.4216, validation loss: 0.1042
2024-06-02 11:06:21 [INFO]: Epoch 081 - training loss: 0.4215, validation loss: 0.1021
2024-06-02 11:06:22 [INFO]: Epoch 082 - training loss: 0.4281, validation loss: 0.1106
2024-06-02 11:06:24 [INFO]: Epoch 083 - training loss: 0.4223, validation loss: 0.1053
2024-06-02 11:06:24 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:06:24 [INFO]: Finished training. The best model is from epoch#73.
2024-06-02 11:06:24 [INFO]: Saved the model to results_point_rate01/ETT_h1/MICN_ETT_h1/round_0/20240602_T110431/MICN.pypots
2024-06-02 11:06:24 [INFO]: Successfully saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_0/imputation.pkl
2024-06-02 11:06:24 [INFO]: Round0 - MICN on ETT_h1: MAE=0.2587, MSE=0.1388, MRE=0.3053
2024-06-02 11:06:24 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 11:06:24 [INFO]: Using the given device: cuda:0
2024-06-02 11:06:24 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_1/20240602_T110624
2024-06-02 11:06:24 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_1/20240602_T110624/tensorboard
2024-06-02 11:06:24 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-02 11:06:26 [INFO]: Epoch 001 - training loss: 1.0487, validation loss: 0.2282
2024-06-02 11:06:27 [INFO]: Epoch 002 - training loss: 0.6600, validation loss: 0.2055
2024-06-02 11:06:28 [INFO]: Epoch 003 - training loss: 0.6311, validation loss: 0.2006
2024-06-02 11:06:30 [INFO]: Epoch 004 - training loss: 0.6147, validation loss: 0.1924
2024-06-02 11:06:31 [INFO]: Epoch 005 - training loss: 0.6090, validation loss: 0.1928
2024-06-02 11:06:33 [INFO]: Epoch 006 - training loss: 0.6035, validation loss: 0.1897
2024-06-02 11:06:34 [INFO]: Epoch 007 - training loss: 0.6050, validation loss: 0.1802
2024-06-02 11:06:36 [INFO]: Epoch 008 - training loss: 0.5921, validation loss: 0.1747
2024-06-02 11:06:37 [INFO]: Epoch 009 - training loss: 0.5884, validation loss: 0.1711
2024-06-02 11:06:39 [INFO]: Epoch 010 - training loss: 0.5864, validation loss: 0.1700
2024-06-02 11:06:40 [INFO]: Epoch 011 - training loss: 0.5859, validation loss: 0.1700
2024-06-02 11:06:42 [INFO]: Epoch 012 - training loss: 0.5875, validation loss: 0.1706
2024-06-02 11:06:43 [INFO]: Epoch 013 - training loss: 0.5697, validation loss: 0.1698
2024-06-02 11:06:44 [INFO]: Epoch 014 - training loss: 0.5674, validation loss: 0.1616
2024-06-02 11:06:46 [INFO]: Epoch 015 - training loss: 0.5584, validation loss: 0.1624
2024-06-02 11:06:47 [INFO]: Epoch 016 - training loss: 0.5560, validation loss: 0.1612
2024-06-02 11:06:48 [INFO]: Epoch 017 - training loss: 0.5532, validation loss: 0.1573
2024-06-02 11:06:50 [INFO]: Epoch 018 - training loss: 0.5570, validation loss: 0.1602
2024-06-02 11:06:51 [INFO]: Epoch 019 - training loss: 0.5622, validation loss: 0.1561
2024-06-02 11:06:53 [INFO]: Epoch 020 - training loss: 0.5470, validation loss: 0.1592
2024-06-02 11:06:54 [INFO]: Epoch 021 - training loss: 0.5507, validation loss: 0.1570
2024-06-02 11:06:55 [INFO]: Epoch 022 - training loss: 0.5511, validation loss: 0.1543
2024-06-02 11:06:57 [INFO]: Epoch 023 - training loss: 0.5522, validation loss: 0.1519
2024-06-02 11:06:58 [INFO]: Epoch 024 - training loss: 0.5452, validation loss: 0.1473
2024-06-02 11:07:00 [INFO]: Epoch 025 - training loss: 0.5354, validation loss: 0.1518
2024-06-02 11:07:01 [INFO]: Epoch 026 - training loss: 0.5348, validation loss: 0.1472
2024-06-02 11:07:03 [INFO]: Epoch 027 - training loss: 0.5236, validation loss: 0.1465
2024-06-02 11:07:04 [INFO]: Epoch 028 - training loss: 0.5273, validation loss: 0.1405
2024-06-02 11:07:06 [INFO]: Epoch 029 - training loss: 0.5275, validation loss: 0.1512
2024-06-02 11:07:07 [INFO]: Epoch 030 - training loss: 0.5185, validation loss: 0.1472
2024-06-02 11:07:09 [INFO]: Epoch 031 - training loss: 0.5233, validation loss: 0.1445
2024-06-02 11:07:10 [INFO]: Epoch 032 - training loss: 0.5162, validation loss: 0.1438
2024-06-02 11:07:12 [INFO]: Epoch 033 - training loss: 0.5219, validation loss: 0.1417
2024-06-02 11:07:13 [INFO]: Epoch 034 - training loss: 0.5151, validation loss: 0.1484
2024-06-02 11:07:15 [INFO]: Epoch 035 - training loss: 0.5153, validation loss: 0.1449
2024-06-02 11:07:16 [INFO]: Epoch 036 - training loss: 0.5229, validation loss: 0.1440
2024-06-02 11:07:18 [INFO]: Epoch 037 - training loss: 0.5134, validation loss: 0.1417
2024-06-02 11:07:19 [INFO]: Epoch 038 - training loss: 0.5103, validation loss: 0.1383
2024-06-02 11:07:20 [INFO]: Epoch 039 - training loss: 0.5097, validation loss: 0.1471
2024-06-02 11:07:22 [INFO]: Epoch 040 - training loss: 0.5042, validation loss: 0.1406
2024-06-02 11:07:23 [INFO]: Epoch 041 - training loss: 0.5008, validation loss: 0.1434
2024-06-02 11:07:25 [INFO]: Epoch 042 - training loss: 0.4996, validation loss: 0.1458
2024-06-02 11:07:26 [INFO]: Epoch 043 - training loss: 0.5059, validation loss: 0.1405
2024-06-02 11:07:28 [INFO]: Epoch 044 - training loss: 0.4992, validation loss: 0.1391
2024-06-02 11:07:29 [INFO]: Epoch 045 - training loss: 0.5076, validation loss: 0.1348
2024-06-02 11:07:30 [INFO]: Epoch 046 - training loss: 0.4954, validation loss: 0.1380
2024-06-02 11:07:32 [INFO]: Epoch 047 - training loss: 0.4897, validation loss: 0.1363
2024-06-02 11:07:33 [INFO]: Epoch 048 - training loss: 0.4933, validation loss: 0.1425
2024-06-02 11:07:35 [INFO]: Epoch 049 - training loss: 0.4895, validation loss: 0.1359
2024-06-02 11:07:36 [INFO]: Epoch 050 - training loss: 0.4834, validation loss: 0.1346
2024-06-02 11:07:38 [INFO]: Epoch 051 - training loss: 0.4810, validation loss: 0.1339
2024-06-02 11:07:39 [INFO]: Epoch 052 - training loss: 0.4831, validation loss: 0.1291
2024-06-02 11:07:41 [INFO]: Epoch 053 - training loss: 0.4946, validation loss: 0.1367
2024-06-02 11:07:42 [INFO]: Epoch 054 - training loss: 0.4796, validation loss: 0.1296
2024-06-02 11:07:43 [INFO]: Epoch 055 - training loss: 0.4790, validation loss: 0.1382
2024-06-02 11:07:45 [INFO]: Epoch 056 - training loss: 0.4760, validation loss: 0.1240
2024-06-02 11:07:46 [INFO]: Epoch 057 - training loss: 0.4767, validation loss: 0.1263
2024-06-02 11:07:47 [INFO]: Epoch 058 - training loss: 0.4677, validation loss: 0.1261
2024-06-02 11:07:48 [INFO]: Epoch 059 - training loss: 0.4669, validation loss: 0.1248
2024-06-02 11:07:50 [INFO]: Epoch 060 - training loss: 0.4713, validation loss: 0.1270
2024-06-02 11:07:51 [INFO]: Epoch 061 - training loss: 0.4650, validation loss: 0.1296
2024-06-02 11:07:52 [INFO]: Epoch 062 - training loss: 0.4666, validation loss: 0.1435
2024-06-02 11:07:54 [INFO]: Epoch 063 - training loss: 0.4700, validation loss: 0.1302
2024-06-02 11:07:55 [INFO]: Epoch 064 - training loss: 0.4616, validation loss: 0.1234
2024-06-02 11:07:57 [INFO]: Epoch 065 - training loss: 0.4644, validation loss: 0.1288
2024-06-02 11:07:58 [INFO]: Epoch 066 - training loss: 0.4626, validation loss: 0.1209
2024-06-02 11:08:00 [INFO]: Epoch 067 - training loss: 0.4630, validation loss: 0.1248
2024-06-02 11:08:01 [INFO]: Epoch 068 - training loss: 0.4557, validation loss: 0.1325
2024-06-02 11:08:02 [INFO]: Epoch 069 - training loss: 0.4677, validation loss: 0.1225
2024-06-02 11:08:04 [INFO]: Epoch 070 - training loss: 0.4625, validation loss: 0.1286
2024-06-02 11:08:05 [INFO]: Epoch 071 - training loss: 0.4523, validation loss: 0.1309
2024-06-02 11:08:06 [INFO]: Epoch 072 - training loss: 0.4493, validation loss: 0.1267
2024-06-02 11:08:08 [INFO]: Epoch 073 - training loss: 0.4502, validation loss: 0.1258
2024-06-02 11:08:09 [INFO]: Epoch 074 - training loss: 0.4464, validation loss: 0.1221
2024-06-02 11:08:11 [INFO]: Epoch 075 - training loss: 0.4525, validation loss: 0.1171
2024-06-02 11:08:12 [INFO]: Epoch 076 - training loss: 0.4399, validation loss: 0.1089
2024-06-02 11:08:13 [INFO]: Epoch 077 - training loss: 0.4425, validation loss: 0.1135
2024-06-02 11:08:15 [INFO]: Epoch 078 - training loss: 0.4393, validation loss: 0.1251
2024-06-02 11:08:16 [INFO]: Epoch 079 - training loss: 0.4355, validation loss: 0.1156
2024-06-02 11:08:18 [INFO]: Epoch 080 - training loss: 0.4459, validation loss: 0.1115
2024-06-02 11:08:19 [INFO]: Epoch 081 - training loss: 0.4407, validation loss: 0.1289
2024-06-02 11:08:21 [INFO]: Epoch 082 - training loss: 0.4386, validation loss: 0.1161
2024-06-02 11:08:22 [INFO]: Epoch 083 - training loss: 0.4386, validation loss: 0.1120
2024-06-02 11:08:24 [INFO]: Epoch 084 - training loss: 0.4377, validation loss: 0.1116
2024-06-02 11:08:25 [INFO]: Epoch 085 - training loss: 0.4317, validation loss: 0.1096
2024-06-02 11:08:27 [INFO]: Epoch 086 - training loss: 0.4303, validation loss: 0.1163
2024-06-02 11:08:27 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:08:27 [INFO]: Finished training. The best model is from epoch#76.
2024-06-02 11:08:27 [INFO]: Saved the model to results_point_rate01/ETT_h1/MICN_ETT_h1/round_1/20240602_T110624/MICN.pypots
2024-06-02 11:08:27 [INFO]: Successfully saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_1/imputation.pkl
2024-06-02 11:08:27 [INFO]: Round1 - MICN on ETT_h1: MAE=0.2731, MSE=0.1538, MRE=0.3223
2024-06-02 11:08:27 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 11:08:27 [INFO]: Using the given device: cuda:0
2024-06-02 11:08:27 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_2/20240602_T110827
2024-06-02 11:08:27 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_2/20240602_T110827/tensorboard
2024-06-02 11:08:27 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-02 11:08:28 [INFO]: Epoch 001 - training loss: 1.0193, validation loss: 0.2540
2024-06-02 11:08:30 [INFO]: Epoch 002 - training loss: 0.6757, validation loss: 0.2020
2024-06-02 11:08:31 [INFO]: Epoch 003 - training loss: 0.6406, validation loss: 0.1973
2024-06-02 11:08:33 [INFO]: Epoch 004 - training loss: 0.6266, validation loss: 0.1983
2024-06-02 11:08:34 [INFO]: Epoch 005 - training loss: 0.6197, validation loss: 0.2019
2024-06-02 11:08:35 [INFO]: Epoch 006 - training loss: 0.6106, validation loss: 0.1949
2024-06-02 11:08:37 [INFO]: Epoch 007 - training loss: 0.6127, validation loss: 0.1860
2024-06-02 11:08:38 [INFO]: Epoch 008 - training loss: 0.6026, validation loss: 0.1825
2024-06-02 11:08:40 [INFO]: Epoch 009 - training loss: 0.5887, validation loss: 0.1835
2024-06-02 11:08:41 [INFO]: Epoch 010 - training loss: 0.5960, validation loss: 0.1735
2024-06-02 11:08:43 [INFO]: Epoch 011 - training loss: 0.5803, validation loss: 0.1884
2024-06-02 11:08:44 [INFO]: Epoch 012 - training loss: 0.5892, validation loss: 0.1714
2024-06-02 11:08:45 [INFO]: Epoch 013 - training loss: 0.5969, validation loss: 0.1833
2024-06-02 11:08:47 [INFO]: Epoch 014 - training loss: 0.5959, validation loss: 0.1667
2024-06-02 11:08:48 [INFO]: Epoch 015 - training loss: 0.5736, validation loss: 0.1633
2024-06-02 11:08:50 [INFO]: Epoch 016 - training loss: 0.5672, validation loss: 0.1701
2024-06-02 11:08:51 [INFO]: Epoch 017 - training loss: 0.5743, validation loss: 0.1714
2024-06-02 11:08:52 [INFO]: Epoch 018 - training loss: 0.5696, validation loss: 0.1654
2024-06-02 11:08:54 [INFO]: Epoch 019 - training loss: 0.5614, validation loss: 0.1656
2024-06-02 11:08:55 [INFO]: Epoch 020 - training loss: 0.5694, validation loss: 0.1664
2024-06-02 11:08:57 [INFO]: Epoch 021 - training loss: 0.5533, validation loss: 0.1634
2024-06-02 11:08:58 [INFO]: Epoch 022 - training loss: 0.5512, validation loss: 0.1633
2024-06-02 11:08:59 [INFO]: Epoch 023 - training loss: 0.5394, validation loss: 0.1572
2024-06-02 11:09:01 [INFO]: Epoch 024 - training loss: 0.5431, validation loss: 0.1584
2024-06-02 11:09:02 [INFO]: Epoch 025 - training loss: 0.5401, validation loss: 0.1540
2024-06-02 11:09:04 [INFO]: Epoch 026 - training loss: 0.5446, validation loss: 0.1538
2024-06-02 11:09:05 [INFO]: Epoch 027 - training loss: 0.5348, validation loss: 0.1483
2024-06-02 11:09:07 [INFO]: Epoch 028 - training loss: 0.5401, validation loss: 0.1467
2024-06-02 11:09:08 [INFO]: Epoch 029 - training loss: 0.5261, validation loss: 0.1486
2024-06-02 11:09:10 [INFO]: Epoch 030 - training loss: 0.5255, validation loss: 0.1500
2024-06-02 11:09:11 [INFO]: Epoch 031 - training loss: 0.5377, validation loss: 0.1475
2024-06-02 11:09:13 [INFO]: Epoch 032 - training loss: 0.5316, validation loss: 0.1553
2024-06-02 11:09:14 [INFO]: Epoch 033 - training loss: 0.5165, validation loss: 0.1445
2024-06-02 11:09:16 [INFO]: Epoch 034 - training loss: 0.5206, validation loss: 0.1467
2024-06-02 11:09:17 [INFO]: Epoch 035 - training loss: 0.5108, validation loss: 0.1425
2024-06-02 11:09:18 [INFO]: Epoch 036 - training loss: 0.5185, validation loss: 0.1337
2024-06-02 11:09:20 [INFO]: Epoch 037 - training loss: 0.5143, validation loss: 0.1331
2024-06-02 11:09:21 [INFO]: Epoch 038 - training loss: 0.4956, validation loss: 0.1334
2024-06-02 11:09:23 [INFO]: Epoch 039 - training loss: 0.5027, validation loss: 0.1329
2024-06-02 11:09:24 [INFO]: Epoch 040 - training loss: 0.4937, validation loss: 0.1323
2024-06-02 11:09:25 [INFO]: Epoch 041 - training loss: 0.4902, validation loss: 0.1408
2024-06-02 11:09:27 [INFO]: Epoch 042 - training loss: 0.4855, validation loss: 0.1348
2024-06-02 11:09:28 [INFO]: Epoch 043 - training loss: 0.4879, validation loss: 0.1285
2024-06-02 11:09:30 [INFO]: Epoch 044 - training loss: 0.4878, validation loss: 0.1296
2024-06-02 11:09:31 [INFO]: Epoch 045 - training loss: 0.4741, validation loss: 0.1376
2024-06-02 11:09:33 [INFO]: Epoch 046 - training loss: 0.4776, validation loss: 0.1357
2024-06-02 11:09:34 [INFO]: Epoch 047 - training loss: 0.4660, validation loss: 0.1261
2024-06-02 11:09:36 [INFO]: Epoch 048 - training loss: 0.4800, validation loss: 0.1214
2024-06-02 11:09:37 [INFO]: Epoch 049 - training loss: 0.4773, validation loss: 0.1391
2024-06-02 11:09:38 [INFO]: Epoch 050 - training loss: 0.4722, validation loss: 0.1344
2024-06-02 11:09:40 [INFO]: Epoch 051 - training loss: 0.4729, validation loss: 0.1214
2024-06-02 11:09:41 [INFO]: Epoch 052 - training loss: 0.4713, validation loss: 0.1287
2024-06-02 11:09:43 [INFO]: Epoch 053 - training loss: 0.4732, validation loss: 0.1263
2024-06-02 11:09:44 [INFO]: Epoch 054 - training loss: 0.4623, validation loss: 0.1265
2024-06-02 11:09:46 [INFO]: Epoch 055 - training loss: 0.4619, validation loss: 0.1191
2024-06-02 11:09:47 [INFO]: Epoch 056 - training loss: 0.4720, validation loss: 0.1200
2024-06-02 11:09:49 [INFO]: Epoch 057 - training loss: 0.4531, validation loss: 0.1166
2024-06-02 11:09:50 [INFO]: Epoch 058 - training loss: 0.4580, validation loss: 0.1178
2024-06-02 11:09:52 [INFO]: Epoch 059 - training loss: 0.4541, validation loss: 0.1123
2024-06-02 11:09:53 [INFO]: Epoch 060 - training loss: 0.4460, validation loss: 0.1249
2024-06-02 11:09:54 [INFO]: Epoch 061 - training loss: 0.4545, validation loss: 0.1144
2024-06-02 11:09:56 [INFO]: Epoch 062 - training loss: 0.4530, validation loss: 0.1131
2024-06-02 11:09:57 [INFO]: Epoch 063 - training loss: 0.4415, validation loss: 0.1208
2024-06-02 11:09:58 [INFO]: Epoch 064 - training loss: 0.4471, validation loss: 0.1254
2024-06-02 11:10:00 [INFO]: Epoch 065 - training loss: 0.4552, validation loss: 0.1244
2024-06-02 11:10:01 [INFO]: Epoch 066 - training loss: 0.4496, validation loss: 0.1294
2024-06-02 11:10:02 [INFO]: Epoch 067 - training loss: 0.4459, validation loss: 0.1167
2024-06-02 11:10:03 [INFO]: Epoch 068 - training loss: 0.4401, validation loss: 0.1100
2024-06-02 11:10:05 [INFO]: Epoch 069 - training loss: 0.4465, validation loss: 0.1142
2024-06-02 11:10:06 [INFO]: Epoch 070 - training loss: 0.4450, validation loss: 0.1127
2024-06-02 11:10:08 [INFO]: Epoch 071 - training loss: 0.4402, validation loss: 0.1094
2024-06-02 11:10:09 [INFO]: Epoch 072 - training loss: 0.4405, validation loss: 0.1096
2024-06-02 11:10:11 [INFO]: Epoch 073 - training loss: 0.4427, validation loss: 0.1114
2024-06-02 11:10:12 [INFO]: Epoch 074 - training loss: 0.4427, validation loss: 0.1209
2024-06-02 11:10:14 [INFO]: Epoch 075 - training loss: 0.4374, validation loss: 0.1207
2024-06-02 11:10:15 [INFO]: Epoch 076 - training loss: 0.4369, validation loss: 0.1094
2024-06-02 11:10:17 [INFO]: Epoch 077 - training loss: 0.4360, validation loss: 0.1078
2024-06-02 11:10:18 [INFO]: Epoch 078 - training loss: 0.4352, validation loss: 0.1040
2024-06-02 11:10:20 [INFO]: Epoch 079 - training loss: 0.4358, validation loss: 0.1147
2024-06-02 11:10:21 [INFO]: Epoch 080 - training loss: 0.4372, validation loss: 0.1155
2024-06-02 11:10:23 [INFO]: Epoch 081 - training loss: 0.4350, validation loss: 0.1123
2024-06-02 11:10:24 [INFO]: Epoch 082 - training loss: 0.4334, validation loss: 0.1139
2024-06-02 11:10:26 [INFO]: Epoch 083 - training loss: 0.4356, validation loss: 0.1055
2024-06-02 11:10:27 [INFO]: Epoch 084 - training loss: 0.4331, validation loss: 0.1072
2024-06-02 11:10:28 [INFO]: Epoch 085 - training loss: 0.4258, validation loss: 0.1119
2024-06-02 11:10:30 [INFO]: Epoch 086 - training loss: 0.4271, validation loss: 0.1128
2024-06-02 11:10:31 [INFO]: Epoch 087 - training loss: 0.4257, validation loss: 0.1055
2024-06-02 11:10:33 [INFO]: Epoch 088 - training loss: 0.4222, validation loss: 0.1069
2024-06-02 11:10:33 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:10:33 [INFO]: Finished training. The best model is from epoch#78.
2024-06-02 11:10:33 [INFO]: Saved the model to results_point_rate01/ETT_h1/MICN_ETT_h1/round_2/20240602_T110827/MICN.pypots
2024-06-02 11:10:33 [INFO]: Successfully saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_2/imputation.pkl
2024-06-02 11:10:33 [INFO]: Round2 - MICN on ETT_h1: MAE=0.2598, MSE=0.1412, MRE=0.3066
2024-06-02 11:10:33 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 11:10:33 [INFO]: Using the given device: cuda:0
2024-06-02 11:10:33 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_3/20240602_T111033
2024-06-02 11:10:33 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_3/20240602_T111033/tensorboard
2024-06-02 11:10:33 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-02 11:10:35 [INFO]: Epoch 001 - training loss: 1.0334, validation loss: 0.2278
2024-06-02 11:10:36 [INFO]: Epoch 002 - training loss: 0.6685, validation loss: 0.2041
2024-06-02 11:10:38 [INFO]: Epoch 003 - training loss: 0.6310, validation loss: 0.2174
2024-06-02 11:10:39 [INFO]: Epoch 004 - training loss: 0.6179, validation loss: 0.2037
2024-06-02 11:10:41 [INFO]: Epoch 005 - training loss: 0.6210, validation loss: 0.1977
2024-06-02 11:10:42 [INFO]: Epoch 006 - training loss: 0.6153, validation loss: 0.1925
2024-06-02 11:10:44 [INFO]: Epoch 007 - training loss: 0.6068, validation loss: 0.1937
2024-06-02 11:10:45 [INFO]: Epoch 008 - training loss: 0.5982, validation loss: 0.2051
2024-06-02 11:10:47 [INFO]: Epoch 009 - training loss: 0.5939, validation loss: 0.1978
2024-06-02 11:10:48 [INFO]: Epoch 010 - training loss: 0.5925, validation loss: 0.1976
2024-06-02 11:10:50 [INFO]: Epoch 011 - training loss: 0.5984, validation loss: 0.1872
2024-06-02 11:10:51 [INFO]: Epoch 012 - training loss: 0.5846, validation loss: 0.1840
2024-06-02 11:10:53 [INFO]: Epoch 013 - training loss: 0.5827, validation loss: 0.1863
2024-06-02 11:10:54 [INFO]: Epoch 014 - training loss: 0.5826, validation loss: 0.1898
2024-06-02 11:10:56 [INFO]: Epoch 015 - training loss: 0.5727, validation loss: 0.1838
2024-06-02 11:10:57 [INFO]: Epoch 016 - training loss: 0.5671, validation loss: 0.1803
2024-06-02 11:10:58 [INFO]: Epoch 017 - training loss: 0.5733, validation loss: 0.1813
2024-06-02 11:10:59 [INFO]: Epoch 018 - training loss: 0.5645, validation loss: 0.1839
2024-06-02 11:11:00 [INFO]: Epoch 019 - training loss: 0.5678, validation loss: 0.1650
2024-06-02 11:11:02 [INFO]: Epoch 020 - training loss: 0.5585, validation loss: 0.1891
2024-06-02 11:11:03 [INFO]: Epoch 021 - training loss: 0.5591, validation loss: 0.1698
2024-06-02 11:11:04 [INFO]: Epoch 022 - training loss: 0.5552, validation loss: 0.1618
2024-06-02 11:11:06 [INFO]: Epoch 023 - training loss: 0.5608, validation loss: 0.1619
2024-06-02 11:11:07 [INFO]: Epoch 024 - training loss: 0.5543, validation loss: 0.1703
2024-06-02 11:11:08 [INFO]: Epoch 025 - training loss: 0.5329, validation loss: 0.1587
2024-06-02 11:11:10 [INFO]: Epoch 026 - training loss: 0.5402, validation loss: 0.1730
2024-06-02 11:11:11 [INFO]: Epoch 027 - training loss: 0.5360, validation loss: 0.1591
2024-06-02 11:11:13 [INFO]: Epoch 028 - training loss: 0.5293, validation loss: 0.1478
2024-06-02 11:11:14 [INFO]: Epoch 029 - training loss: 0.5316, validation loss: 0.1578
2024-06-02 11:11:16 [INFO]: Epoch 030 - training loss: 0.5347, validation loss: 0.1532
2024-06-02 11:11:17 [INFO]: Epoch 031 - training loss: 0.5249, validation loss: 0.1448
2024-06-02 11:11:19 [INFO]: Epoch 032 - training loss: 0.5237, validation loss: 0.1464
2024-06-02 11:11:20 [INFO]: Epoch 033 - training loss: 0.5214, validation loss: 0.1405
2024-06-02 11:11:21 [INFO]: Epoch 034 - training loss: 0.5175, validation loss: 0.1456
2024-06-02 11:11:23 [INFO]: Epoch 035 - training loss: 0.5156, validation loss: 0.1403
2024-06-02 11:11:24 [INFO]: Epoch 036 - training loss: 0.5110, validation loss: 0.1430
2024-06-02 11:11:26 [INFO]: Epoch 037 - training loss: 0.5099, validation loss: 0.1450
2024-06-02 11:11:27 [INFO]: Epoch 038 - training loss: 0.5084, validation loss: 0.1475
2024-06-02 11:11:29 [INFO]: Epoch 039 - training loss: 0.5107, validation loss: 0.1397
2024-06-02 11:11:30 [INFO]: Epoch 040 - training loss: 0.5039, validation loss: 0.1350
2024-06-02 11:11:31 [INFO]: Epoch 041 - training loss: 0.5090, validation loss: 0.1380
2024-06-02 11:11:33 [INFO]: Epoch 042 - training loss: 0.4958, validation loss: 0.1348
2024-06-02 11:11:34 [INFO]: Epoch 043 - training loss: 0.4976, validation loss: 0.1371
2024-06-02 11:11:36 [INFO]: Epoch 044 - training loss: 0.4925, validation loss: 0.1322
2024-06-02 11:11:37 [INFO]: Epoch 045 - training loss: 0.5013, validation loss: 0.1306
2024-06-02 11:11:39 [INFO]: Epoch 046 - training loss: 0.4946, validation loss: 0.1189
2024-06-02 11:11:40 [INFO]: Epoch 047 - training loss: 0.4917, validation loss: 0.1378
2024-06-02 11:11:42 [INFO]: Epoch 048 - training loss: 0.4805, validation loss: 0.1308
2024-06-02 11:11:43 [INFO]: Epoch 049 - training loss: 0.4851, validation loss: 0.1173
2024-06-02 11:11:45 [INFO]: Epoch 050 - training loss: 0.4800, validation loss: 0.1221
2024-06-02 11:11:46 [INFO]: Epoch 051 - training loss: 0.4814, validation loss: 0.1225
2024-06-02 11:11:48 [INFO]: Epoch 052 - training loss: 0.4817, validation loss: 0.1267
2024-06-02 11:11:49 [INFO]: Epoch 053 - training loss: 0.4759, validation loss: 0.1299
2024-06-02 11:11:51 [INFO]: Epoch 054 - training loss: 0.4724, validation loss: 0.1292
2024-06-02 11:11:52 [INFO]: Epoch 055 - training loss: 0.4657, validation loss: 0.1228
2024-06-02 11:11:54 [INFO]: Epoch 056 - training loss: 0.4714, validation loss: 0.1225
2024-06-02 11:11:55 [INFO]: Epoch 057 - training loss: 0.4584, validation loss: 0.1297
2024-06-02 11:11:56 [INFO]: Epoch 058 - training loss: 0.4554, validation loss: 0.1198
2024-06-02 11:11:58 [INFO]: Epoch 059 - training loss: 0.4656, validation loss: 0.1212
2024-06-02 11:11:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:11:58 [INFO]: Finished training. The best model is from epoch#49.
2024-06-02 11:11:58 [INFO]: Saved the model to results_point_rate01/ETT_h1/MICN_ETT_h1/round_3/20240602_T111033/MICN.pypots
2024-06-02 11:11:58 [INFO]: Successfully saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_3/imputation.pkl
2024-06-02 11:11:58 [INFO]: Round3 - MICN on ETT_h1: MAE=0.2852, MSE=0.1686, MRE=0.3366
2024-06-02 11:11:58 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 11:11:58 [INFO]: Using the given device: cuda:0
2024-06-02 11:11:58 [INFO]: Model files will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_4/20240602_T111158
2024-06-02 11:11:58 [INFO]: Tensorboard file will be saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_4/20240602_T111158/tensorboard
2024-06-02 11:11:58 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-02 11:12:00 [INFO]: Epoch 001 - training loss: 0.9834, validation loss: 0.2197
2024-06-02 11:12:01 [INFO]: Epoch 002 - training loss: 0.6715, validation loss: 0.2127
2024-06-02 11:12:03 [INFO]: Epoch 003 - training loss: 0.6362, validation loss: 0.1955
2024-06-02 11:12:04 [INFO]: Epoch 004 - training loss: 0.6184, validation loss: 0.1939
2024-06-02 11:12:05 [INFO]: Epoch 005 - training loss: 0.6053, validation loss: 0.1833
2024-06-02 11:12:07 [INFO]: Epoch 006 - training loss: 0.6021, validation loss: 0.1759
2024-06-02 11:12:08 [INFO]: Epoch 007 - training loss: 0.5963, validation loss: 0.1746
2024-06-02 11:12:09 [INFO]: Epoch 008 - training loss: 0.5933, validation loss: 0.1760
2024-06-02 11:12:10 [INFO]: Epoch 009 - training loss: 0.5846, validation loss: 0.1690
2024-06-02 11:12:12 [INFO]: Epoch 010 - training loss: 0.5826, validation loss: 0.1686
2024-06-02 11:12:13 [INFO]: Epoch 011 - training loss: 0.5765, validation loss: 0.1771
2024-06-02 11:12:14 [INFO]: Epoch 012 - training loss: 0.5741, validation loss: 0.1714
2024-06-02 11:12:16 [INFO]: Epoch 013 - training loss: 0.5725, validation loss: 0.1656
2024-06-02 11:12:17 [INFO]: Epoch 014 - training loss: 0.5631, validation loss: 0.1615
2024-06-02 11:12:19 [INFO]: Epoch 015 - training loss: 0.5583, validation loss: 0.1581
2024-06-02 11:12:20 [INFO]: Epoch 016 - training loss: 0.5510, validation loss: 0.1620
2024-06-02 11:12:21 [INFO]: Epoch 017 - training loss: 0.5506, validation loss: 0.1553
2024-06-02 11:12:23 [INFO]: Epoch 018 - training loss: 0.5499, validation loss: 0.1607
2024-06-02 11:12:24 [INFO]: Epoch 019 - training loss: 0.5550, validation loss: 0.1615
2024-06-02 11:12:26 [INFO]: Epoch 020 - training loss: 0.5439, validation loss: 0.1586
2024-06-02 11:12:27 [INFO]: Epoch 021 - training loss: 0.5345, validation loss: 0.1608
2024-06-02 11:12:29 [INFO]: Epoch 022 - training loss: 0.5328, validation loss: 0.1529
2024-06-02 11:12:30 [INFO]: Epoch 023 - training loss: 0.5316, validation loss: 0.1561
2024-06-02 11:12:32 [INFO]: Epoch 024 - training loss: 0.5291, validation loss: 0.1523
2024-06-02 11:12:33 [INFO]: Epoch 025 - training loss: 0.5269, validation loss: 0.1512
2024-06-02 11:12:35 [INFO]: Epoch 026 - training loss: 0.5309, validation loss: 0.1490
2024-06-02 11:12:36 [INFO]: Epoch 027 - training loss: 0.5244, validation loss: 0.1397
2024-06-02 11:12:38 [INFO]: Epoch 028 - training loss: 0.5260, validation loss: 0.1485
2024-06-02 11:12:39 [INFO]: Epoch 029 - training loss: 0.5215, validation loss: 0.1525
2024-06-02 11:12:41 [INFO]: Epoch 030 - training loss: 0.5228, validation loss: 0.1483
2024-06-02 11:12:42 [INFO]: Epoch 031 - training loss: 0.5100, validation loss: 0.1438
2024-06-02 11:12:44 [INFO]: Epoch 032 - training loss: 0.5128, validation loss: 0.1420
2024-06-02 11:12:45 [INFO]: Epoch 033 - training loss: 0.5123, validation loss: 0.1379
2024-06-02 11:12:47 [INFO]: Epoch 034 - training loss: 0.4980, validation loss: 0.1409
2024-06-02 11:12:48 [INFO]: Epoch 035 - training loss: 0.5032, validation loss: 0.1369
2024-06-02 11:12:50 [INFO]: Epoch 036 - training loss: 0.5012, validation loss: 0.1345
2024-06-02 11:12:51 [INFO]: Epoch 037 - training loss: 0.4905, validation loss: 0.1323
2024-06-02 11:12:52 [INFO]: Epoch 038 - training loss: 0.4918, validation loss: 0.1331
2024-06-02 11:12:54 [INFO]: Epoch 039 - training loss: 0.5018, validation loss: 0.1403
2024-06-02 11:12:55 [INFO]: Epoch 040 - training loss: 0.4915, validation loss: 0.1288
2024-06-02 11:12:57 [INFO]: Epoch 041 - training loss: 0.4922, validation loss: 0.1275
2024-06-02 11:12:58 [INFO]: Epoch 042 - training loss: 0.4891, validation loss: 0.1354
2024-06-02 11:13:00 [INFO]: Epoch 043 - training loss: 0.4842, validation loss: 0.1284
2024-06-02 11:13:01 [INFO]: Epoch 044 - training loss: 0.4773, validation loss: 0.1160
2024-06-02 11:13:03 [INFO]: Epoch 045 - training loss: 0.4784, validation loss: 0.1337
2024-06-02 11:13:04 [INFO]: Epoch 046 - training loss: 0.4830, validation loss: 0.1205
2024-06-02 11:13:06 [INFO]: Epoch 047 - training loss: 0.4738, validation loss: 0.1207
2024-06-02 11:13:07 [INFO]: Epoch 048 - training loss: 0.4770, validation loss: 0.1201
2024-06-02 11:13:09 [INFO]: Epoch 049 - training loss: 0.4713, validation loss: 0.1240
2024-06-02 11:13:10 [INFO]: Epoch 050 - training loss: 0.4737, validation loss: 0.1328
2024-06-02 11:13:11 [INFO]: Epoch 051 - training loss: 0.4690, validation loss: 0.1230
2024-06-02 11:13:12 [INFO]: Epoch 052 - training loss: 0.4590, validation loss: 0.1215
2024-06-02 11:13:14 [INFO]: Epoch 053 - training loss: 0.4594, validation loss: 0.1112
2024-06-02 11:13:15 [INFO]: Epoch 054 - training loss: 0.4594, validation loss: 0.1110
2024-06-02 11:13:16 [INFO]: Epoch 055 - training loss: 0.4555, validation loss: 0.1146
2024-06-02 11:13:18 [INFO]: Epoch 056 - training loss: 0.4530, validation loss: 0.1175
2024-06-02 11:13:19 [INFO]: Epoch 057 - training loss: 0.4571, validation loss: 0.1149
2024-06-02 11:13:20 [INFO]: Epoch 058 - training loss: 0.4604, validation loss: 0.1155
2024-06-02 11:13:22 [INFO]: Epoch 059 - training loss: 0.4586, validation loss: 0.1132
2024-06-02 11:13:23 [INFO]: Epoch 060 - training loss: 0.4462, validation loss: 0.1155
2024-06-02 11:13:25 [INFO]: Epoch 061 - training loss: 0.4503, validation loss: 0.1120
2024-06-02 11:13:26 [INFO]: Epoch 062 - training loss: 0.4437, validation loss: 0.1091
2024-06-02 11:13:28 [INFO]: Epoch 063 - training loss: 0.4461, validation loss: 0.0999
2024-06-02 11:13:30 [INFO]: Epoch 064 - training loss: 0.4457, validation loss: 0.1089
2024-06-02 11:13:31 [INFO]: Epoch 065 - training loss: 0.4393, validation loss: 0.1116
2024-06-02 11:13:33 [INFO]: Epoch 066 - training loss: 0.4389, validation loss: 0.1117
2024-06-02 11:13:34 [INFO]: Epoch 067 - training loss: 0.4428, validation loss: 0.1156
2024-06-02 11:13:35 [INFO]: Epoch 068 - training loss: 0.4355, validation loss: 0.1134
2024-06-02 11:13:37 [INFO]: Epoch 069 - training loss: 0.4420, validation loss: 0.1164
2024-06-02 11:13:38 [INFO]: Epoch 070 - training loss: 0.4379, validation loss: 0.1111
2024-06-02 11:13:40 [INFO]: Epoch 071 - training loss: 0.4279, validation loss: 0.1044
2024-06-02 11:13:41 [INFO]: Epoch 072 - training loss: 0.4342, validation loss: 0.1036
2024-06-02 11:13:43 [INFO]: Epoch 073 - training loss: 0.4328, validation loss: 0.1060
2024-06-02 11:13:43 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:13:43 [INFO]: Finished training. The best model is from epoch#63.
2024-06-02 11:13:43 [INFO]: Saved the model to results_point_rate01/ETT_h1/MICN_ETT_h1/round_4/20240602_T111158/MICN.pypots
2024-06-02 11:13:43 [INFO]: Successfully saved to results_point_rate01/ETT_h1/MICN_ETT_h1/round_4/imputation.pkl
2024-06-02 11:13:43 [INFO]: Round4 - MICN on ETT_h1: MAE=0.2597, MSE=0.1457, MRE=0.3065
2024-06-02 11:13:43 [INFO]: Done! Final results:
Averaged MICN (3,153,163 params) on ETT_h1: MAE=0.2673 ± 0.01041316797634605, MSE=0.1496 ± 0.010791712608181198, MRE=0.3154 ± 0.01228848649202904, average inference time=0.14
