2024-06-02 01:50:20 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 01:50:20 [INFO]: Using the given device: cuda:0
2024-06-02 01:50:20 [INFO]: Model files will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_0/20240602_T015020
2024-06-02 01:50:20 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_0/20240602_T015020/tensorboard
2024-06-02 01:50:20 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-02 01:50:24 [INFO]: Epoch 001 - training loss: 1.0230, validation loss: 0.6294
2024-06-02 01:50:25 [INFO]: Epoch 002 - training loss: 0.7108, validation loss: 0.5061
2024-06-02 01:50:26 [INFO]: Epoch 003 - training loss: 0.6376, validation loss: 0.5113
2024-06-02 01:50:28 [INFO]: Epoch 004 - training loss: 0.6063, validation loss: 0.4637
2024-06-02 01:50:29 [INFO]: Epoch 005 - training loss: 0.5810, validation loss: 0.4887
2024-06-02 01:50:30 [INFO]: Epoch 006 - training loss: 0.5537, validation loss: 0.4666
2024-06-02 01:50:32 [INFO]: Epoch 007 - training loss: 0.5389, validation loss: 0.4655
2024-06-02 01:50:33 [INFO]: Epoch 008 - training loss: 0.5261, validation loss: 0.4566
2024-06-02 01:50:34 [INFO]: Epoch 009 - training loss: 0.5133, validation loss: 0.4555
2024-06-02 01:50:36 [INFO]: Epoch 010 - training loss: 0.5035, validation loss: 0.4506
2024-06-02 01:50:37 [INFO]: Epoch 011 - training loss: 0.5042, validation loss: 0.4465
2024-06-02 01:50:38 [INFO]: Epoch 012 - training loss: 0.4924, validation loss: 0.4455
2024-06-02 01:50:40 [INFO]: Epoch 013 - training loss: 0.4879, validation loss: 0.4308
2024-06-02 01:50:41 [INFO]: Epoch 014 - training loss: 0.4808, validation loss: 0.4368
2024-06-02 01:50:42 [INFO]: Epoch 015 - training loss: 0.4845, validation loss: 0.4254
2024-06-02 01:50:43 [INFO]: Epoch 016 - training loss: 0.4821, validation loss: 0.4387
2024-06-02 01:50:45 [INFO]: Epoch 017 - training loss: 0.4798, validation loss: 0.4180
2024-06-02 01:50:46 [INFO]: Epoch 018 - training loss: 0.4697, validation loss: 0.4350
2024-06-02 01:50:47 [INFO]: Epoch 019 - training loss: 0.4737, validation loss: 0.4138
2024-06-02 01:50:48 [INFO]: Epoch 020 - training loss: 0.4708, validation loss: 0.4158
2024-06-02 01:50:50 [INFO]: Epoch 021 - training loss: 0.4696, validation loss: 0.4135
2024-06-02 01:50:51 [INFO]: Epoch 022 - training loss: 0.4675, validation loss: 0.4090
2024-06-02 01:50:52 [INFO]: Epoch 023 - training loss: 0.4642, validation loss: 0.4252
2024-06-02 01:50:54 [INFO]: Epoch 024 - training loss: 0.4633, validation loss: 0.4064
2024-06-02 01:50:55 [INFO]: Epoch 025 - training loss: 0.4608, validation loss: 0.4192
2024-06-02 01:50:56 [INFO]: Epoch 026 - training loss: 0.4591, validation loss: 0.4186
2024-06-02 01:50:57 [INFO]: Epoch 027 - training loss: 0.4578, validation loss: 0.4153
2024-06-02 01:50:59 [INFO]: Epoch 028 - training loss: 0.4558, validation loss: 0.4244
2024-06-02 01:51:00 [INFO]: Epoch 029 - training loss: 0.4605, validation loss: 0.4024
2024-06-02 01:51:01 [INFO]: Epoch 030 - training loss: 0.4566, validation loss: 0.3975
2024-06-02 01:51:03 [INFO]: Epoch 031 - training loss: 0.4569, validation loss: 0.4021
2024-06-02 01:51:04 [INFO]: Epoch 032 - training loss: 0.4543, validation loss: 0.4035
2024-06-02 01:51:05 [INFO]: Epoch 033 - training loss: 0.4504, validation loss: 0.4028
2024-06-02 01:51:07 [INFO]: Epoch 034 - training loss: 0.4485, validation loss: 0.4199
2024-06-02 01:51:08 [INFO]: Epoch 035 - training loss: 0.4532, validation loss: 0.4043
2024-06-02 01:51:09 [INFO]: Epoch 036 - training loss: 0.4528, validation loss: 0.3973
2024-06-02 01:51:11 [INFO]: Epoch 037 - training loss: 0.4458, validation loss: 0.4159
2024-06-02 01:51:12 [INFO]: Epoch 038 - training loss: 0.4514, validation loss: 0.4003
2024-06-02 01:51:13 [INFO]: Epoch 039 - training loss: 0.4422, validation loss: 0.4047
2024-06-02 01:51:15 [INFO]: Epoch 040 - training loss: 0.4448, validation loss: 0.4254
2024-06-02 01:51:16 [INFO]: Epoch 041 - training loss: 0.4494, validation loss: 0.4069
2024-06-02 01:51:18 [INFO]: Epoch 042 - training loss: 0.4472, validation loss: 0.4053
2024-06-02 01:51:19 [INFO]: Epoch 043 - training loss: 0.4447, validation loss: 0.4087
2024-06-02 01:51:20 [INFO]: Epoch 044 - training loss: 0.4451, validation loss: 0.4033
2024-06-02 01:51:21 [INFO]: Epoch 045 - training loss: 0.4446, validation loss: 0.4043
2024-06-02 01:51:22 [INFO]: Epoch 046 - training loss: 0.4429, validation loss: 0.4033
2024-06-02 01:51:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:51:22 [INFO]: Finished training. The best model is from epoch#36.
2024-06-02 01:51:22 [INFO]: Saved the model to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_0/20240602_T015020/NonstationaryTransformer.pypots
2024-06-02 01:51:22 [INFO]: Successfully saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_0/imputation.pkl
2024-06-02 01:51:22 [INFO]: Round0 - NonstationaryTransformer on PeMS: MAE=0.3429, MSE=0.5544, MRE=0.4250
2024-06-02 01:51:22 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 01:51:22 [INFO]: Using the given device: cuda:0
2024-06-02 01:51:22 [INFO]: Model files will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_1/20240602_T015122
2024-06-02 01:51:22 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_1/20240602_T015122/tensorboard
2024-06-02 01:51:22 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-02 01:51:24 [INFO]: Epoch 001 - training loss: 1.0303, validation loss: 0.6034
2024-06-02 01:51:25 [INFO]: Epoch 002 - training loss: 0.6979, validation loss: 0.4852
2024-06-02 01:51:27 [INFO]: Epoch 003 - training loss: 0.6411, validation loss: 0.4694
2024-06-02 01:51:28 [INFO]: Epoch 004 - training loss: 0.6034, validation loss: 0.4575
2024-06-02 01:51:29 [INFO]: Epoch 005 - training loss: 0.5805, validation loss: 0.4515
2024-06-02 01:51:30 [INFO]: Epoch 006 - training loss: 0.5554, validation loss: 0.4364
2024-06-02 01:51:31 [INFO]: Epoch 007 - training loss: 0.5406, validation loss: 0.4496
2024-06-02 01:51:33 [INFO]: Epoch 008 - training loss: 0.5273, validation loss: 0.4400
2024-06-02 01:51:34 [INFO]: Epoch 009 - training loss: 0.5153, validation loss: 0.4249
2024-06-02 01:51:36 [INFO]: Epoch 010 - training loss: 0.5038, validation loss: 0.4379
2024-06-02 01:51:37 [INFO]: Epoch 011 - training loss: 0.4938, validation loss: 0.4363
2024-06-02 01:51:38 [INFO]: Epoch 012 - training loss: 0.4883, validation loss: 0.4202
2024-06-02 01:51:39 [INFO]: Epoch 013 - training loss: 0.4858, validation loss: 0.4294
2024-06-02 01:51:41 [INFO]: Epoch 014 - training loss: 0.4804, validation loss: 0.4159
2024-06-02 01:51:42 [INFO]: Epoch 015 - training loss: 0.4797, validation loss: 0.4166
2024-06-02 01:51:43 [INFO]: Epoch 016 - training loss: 0.4782, validation loss: 0.4122
2024-06-02 01:51:44 [INFO]: Epoch 017 - training loss: 0.4779, validation loss: 0.4107
2024-06-02 01:51:46 [INFO]: Epoch 018 - training loss: 0.4760, validation loss: 0.4081
2024-06-02 01:51:47 [INFO]: Epoch 019 - training loss: 0.4723, validation loss: 0.4015
2024-06-02 01:51:48 [INFO]: Epoch 020 - training loss: 0.4681, validation loss: 0.4108
2024-06-02 01:51:50 [INFO]: Epoch 021 - training loss: 0.4660, validation loss: 0.3987
2024-06-02 01:51:51 [INFO]: Epoch 022 - training loss: 0.4653, validation loss: 0.4299
2024-06-02 01:51:53 [INFO]: Epoch 023 - training loss: 0.4664, validation loss: 0.4043
2024-06-02 01:51:54 [INFO]: Epoch 024 - training loss: 0.4610, validation loss: 0.3895
2024-06-02 01:51:55 [INFO]: Epoch 025 - training loss: 0.4571, validation loss: 0.4244
2024-06-02 01:51:56 [INFO]: Epoch 026 - training loss: 0.4630, validation loss: 0.4009
2024-06-02 01:51:58 [INFO]: Epoch 027 - training loss: 0.4544, validation loss: 0.4165
2024-06-02 01:51:59 [INFO]: Epoch 028 - training loss: 0.4549, validation loss: 0.4049
2024-06-02 01:52:00 [INFO]: Epoch 029 - training loss: 0.4583, validation loss: 0.4015
2024-06-02 01:52:02 [INFO]: Epoch 030 - training loss: 0.4582, validation loss: 0.3927
2024-06-02 01:52:03 [INFO]: Epoch 031 - training loss: 0.4522, validation loss: 0.3932
2024-06-02 01:52:04 [INFO]: Epoch 032 - training loss: 0.4485, validation loss: 0.4152
2024-06-02 01:52:05 [INFO]: Epoch 033 - training loss: 0.4510, validation loss: 0.4076
2024-06-02 01:52:07 [INFO]: Epoch 034 - training loss: 0.4569, validation loss: 0.3969
2024-06-02 01:52:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:52:07 [INFO]: Finished training. The best model is from epoch#24.
2024-06-02 01:52:07 [INFO]: Saved the model to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_1/20240602_T015122/NonstationaryTransformer.pypots
2024-06-02 01:52:07 [INFO]: Successfully saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_1/imputation.pkl
2024-06-02 01:52:07 [INFO]: Round1 - NonstationaryTransformer on PeMS: MAE=0.3321, MSE=0.5400, MRE=0.4117
2024-06-02 01:52:07 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 01:52:07 [INFO]: Using the given device: cuda:0
2024-06-02 01:52:07 [INFO]: Model files will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_2/20240602_T015207
2024-06-02 01:52:07 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_2/20240602_T015207/tensorboard
2024-06-02 01:52:07 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-02 01:52:09 [INFO]: Epoch 001 - training loss: 1.0242, validation loss: 0.6313
2024-06-02 01:52:10 [INFO]: Epoch 002 - training loss: 0.7111, validation loss: 0.4782
2024-06-02 01:52:11 [INFO]: Epoch 003 - training loss: 0.6582, validation loss: 0.5033
2024-06-02 01:52:12 [INFO]: Epoch 004 - training loss: 0.6072, validation loss: 0.4654
2024-06-02 01:52:14 [INFO]: Epoch 005 - training loss: 0.5818, validation loss: 0.4430
2024-06-02 01:52:15 [INFO]: Epoch 006 - training loss: 0.5520, validation loss: 0.4527
2024-06-02 01:52:16 [INFO]: Epoch 007 - training loss: 0.5408, validation loss: 0.4369
2024-06-02 01:52:17 [INFO]: Epoch 008 - training loss: 0.5247, validation loss: 0.4293
2024-06-02 01:52:19 [INFO]: Epoch 009 - training loss: 0.5129, validation loss: 0.4136
2024-06-02 01:52:20 [INFO]: Epoch 010 - training loss: 0.5040, validation loss: 0.4053
2024-06-02 01:52:21 [INFO]: Epoch 011 - training loss: 0.4941, validation loss: 0.4006
2024-06-02 01:52:22 [INFO]: Epoch 012 - training loss: 0.4930, validation loss: 0.3995
2024-06-02 01:52:23 [INFO]: Epoch 013 - training loss: 0.4885, validation loss: 0.4392
2024-06-02 01:52:25 [INFO]: Epoch 014 - training loss: 0.5046, validation loss: 0.4014
2024-06-02 01:52:26 [INFO]: Epoch 015 - training loss: 0.4808, validation loss: 0.3946
2024-06-02 01:52:27 [INFO]: Epoch 016 - training loss: 0.4762, validation loss: 0.3907
2024-06-02 01:52:29 [INFO]: Epoch 017 - training loss: 0.4761, validation loss: 0.3895
2024-06-02 01:52:30 [INFO]: Epoch 018 - training loss: 0.4749, validation loss: 0.4043
2024-06-02 01:52:31 [INFO]: Epoch 019 - training loss: 0.4706, validation loss: 0.4104
2024-06-02 01:52:33 [INFO]: Epoch 020 - training loss: 0.4697, validation loss: 0.3962
2024-06-02 01:52:34 [INFO]: Epoch 021 - training loss: 0.4631, validation loss: 0.3947
2024-06-02 01:52:36 [INFO]: Epoch 022 - training loss: 0.4625, validation loss: 0.3909
2024-06-02 01:52:37 [INFO]: Epoch 023 - training loss: 0.4599, validation loss: 0.3866
2024-06-02 01:52:38 [INFO]: Epoch 024 - training loss: 0.4607, validation loss: 0.3896
2024-06-02 01:52:39 [INFO]: Epoch 025 - training loss: 0.4607, validation loss: 0.3885
2024-06-02 01:52:41 [INFO]: Epoch 026 - training loss: 0.4560, validation loss: 0.3795
2024-06-02 01:52:42 [INFO]: Epoch 027 - training loss: 0.4583, validation loss: 0.3899
2024-06-02 01:52:43 [INFO]: Epoch 028 - training loss: 0.4557, validation loss: 0.3777
2024-06-02 01:52:44 [INFO]: Epoch 029 - training loss: 0.4536, validation loss: 0.3938
2024-06-02 01:52:46 [INFO]: Epoch 030 - training loss: 0.4530, validation loss: 0.3878
2024-06-02 01:52:47 [INFO]: Epoch 031 - training loss: 0.4567, validation loss: 0.4081
2024-06-02 01:52:48 [INFO]: Epoch 032 - training loss: 0.4559, validation loss: 0.3895
2024-06-02 01:52:49 [INFO]: Epoch 033 - training loss: 0.4547, validation loss: 0.3793
2024-06-02 01:52:50 [INFO]: Epoch 034 - training loss: 0.4560, validation loss: 0.3815
2024-06-02 01:52:52 [INFO]: Epoch 035 - training loss: 0.4494, validation loss: 0.3975
2024-06-02 01:52:53 [INFO]: Epoch 036 - training loss: 0.4527, validation loss: 0.3916
2024-06-02 01:52:55 [INFO]: Epoch 037 - training loss: 0.4502, validation loss: 0.3816
2024-06-02 01:52:56 [INFO]: Epoch 038 - training loss: 0.4512, validation loss: 0.3978
2024-06-02 01:52:56 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:52:56 [INFO]: Finished training. The best model is from epoch#28.
2024-06-02 01:52:56 [INFO]: Saved the model to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_2/20240602_T015207/NonstationaryTransformer.pypots
2024-06-02 01:52:56 [INFO]: Successfully saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_2/imputation.pkl
2024-06-02 01:52:56 [INFO]: Round2 - NonstationaryTransformer on PeMS: MAE=0.3528, MSE=0.5493, MRE=0.4374
2024-06-02 01:52:56 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 01:52:56 [INFO]: Using the given device: cuda:0
2024-06-02 01:52:56 [INFO]: Model files will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_3/20240602_T015256
2024-06-02 01:52:56 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_3/20240602_T015256/tensorboard
2024-06-02 01:52:56 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-02 01:52:58 [INFO]: Epoch 001 - training loss: 1.0382, validation loss: 0.6104
2024-06-02 01:52:59 [INFO]: Epoch 002 - training loss: 0.7077, validation loss: 0.4728
2024-06-02 01:53:00 [INFO]: Epoch 003 - training loss: 0.6457, validation loss: 0.4743
2024-06-02 01:53:02 [INFO]: Epoch 004 - training loss: 0.6066, validation loss: 0.4488
2024-06-02 01:53:03 [INFO]: Epoch 005 - training loss: 0.5744, validation loss: 0.4367
2024-06-02 01:53:04 [INFO]: Epoch 006 - training loss: 0.5524, validation loss: 0.4323
2024-06-02 01:53:05 [INFO]: Epoch 007 - training loss: 0.5410, validation loss: 0.4371
2024-06-02 01:53:07 [INFO]: Epoch 008 - training loss: 0.5277, validation loss: 0.4360
2024-06-02 01:53:08 [INFO]: Epoch 009 - training loss: 0.5161, validation loss: 0.4475
2024-06-02 01:53:09 [INFO]: Epoch 010 - training loss: 0.5054, validation loss: 0.4212
2024-06-02 01:53:11 [INFO]: Epoch 011 - training loss: 0.4984, validation loss: 0.4185
2024-06-02 01:53:12 [INFO]: Epoch 012 - training loss: 0.4886, validation loss: 0.4070
2024-06-02 01:53:13 [INFO]: Epoch 013 - training loss: 0.4876, validation loss: 0.4025
2024-06-02 01:53:14 [INFO]: Epoch 014 - training loss: 0.4861, validation loss: 0.4028
2024-06-02 01:53:15 [INFO]: Epoch 015 - training loss: 0.4758, validation loss: 0.3894
2024-06-02 01:53:17 [INFO]: Epoch 016 - training loss: 0.4774, validation loss: 0.3987
2024-06-02 01:53:18 [INFO]: Epoch 017 - training loss: 0.4783, validation loss: 0.3907
2024-06-02 01:53:19 [INFO]: Epoch 018 - training loss: 0.4809, validation loss: 0.3862
2024-06-02 01:53:20 [INFO]: Epoch 019 - training loss: 0.4674, validation loss: 0.3807
2024-06-02 01:53:22 [INFO]: Epoch 020 - training loss: 0.4694, validation loss: 0.3900
2024-06-02 01:53:23 [INFO]: Epoch 021 - training loss: 0.4663, validation loss: 0.3816
2024-06-02 01:53:24 [INFO]: Epoch 022 - training loss: 0.4639, validation loss: 0.3754
2024-06-02 01:53:26 [INFO]: Epoch 023 - training loss: 0.4647, validation loss: 0.3856
2024-06-02 01:53:27 [INFO]: Epoch 024 - training loss: 0.4624, validation loss: 0.3830
2024-06-02 01:53:28 [INFO]: Epoch 025 - training loss: 0.4556, validation loss: 0.3848
2024-06-02 01:53:30 [INFO]: Epoch 026 - training loss: 0.4590, validation loss: 0.3823
2024-06-02 01:53:31 [INFO]: Epoch 027 - training loss: 0.4578, validation loss: 0.3729
2024-06-02 01:53:32 [INFO]: Epoch 028 - training loss: 0.4517, validation loss: 0.3858
2024-06-02 01:53:33 [INFO]: Epoch 029 - training loss: 0.4585, validation loss: 0.3758
2024-06-02 01:53:34 [INFO]: Epoch 030 - training loss: 0.4598, validation loss: 0.3799
2024-06-02 01:53:36 [INFO]: Epoch 031 - training loss: 0.4547, validation loss: 0.3860
2024-06-02 01:53:37 [INFO]: Epoch 032 - training loss: 0.4526, validation loss: 0.3845
2024-06-02 01:53:39 [INFO]: Epoch 033 - training loss: 0.4548, validation loss: 0.3816
2024-06-02 01:53:40 [INFO]: Epoch 034 - training loss: 0.4600, validation loss: 0.3722
2024-06-02 01:53:41 [INFO]: Epoch 035 - training loss: 0.4552, validation loss: 0.3750
2024-06-02 01:53:43 [INFO]: Epoch 036 - training loss: 0.4540, validation loss: 0.3727
2024-06-02 01:53:44 [INFO]: Epoch 037 - training loss: 0.4459, validation loss: 0.3724
2024-06-02 01:53:46 [INFO]: Epoch 038 - training loss: 0.4486, validation loss: 0.3712
2024-06-02 01:53:47 [INFO]: Epoch 039 - training loss: 0.4448, validation loss: 0.3803
2024-06-02 01:53:48 [INFO]: Epoch 040 - training loss: 0.4430, validation loss: 0.3780
2024-06-02 01:53:49 [INFO]: Epoch 041 - training loss: 0.4469, validation loss: 0.3741
2024-06-02 01:53:50 [INFO]: Epoch 042 - training loss: 0.4442, validation loss: 0.3784
2024-06-02 01:53:51 [INFO]: Epoch 043 - training loss: 0.4487, validation loss: 0.3767
2024-06-02 01:53:53 [INFO]: Epoch 044 - training loss: 0.4423, validation loss: 0.3743
2024-06-02 01:53:54 [INFO]: Epoch 045 - training loss: 0.4399, validation loss: 0.3749
2024-06-02 01:53:55 [INFO]: Epoch 046 - training loss: 0.4446, validation loss: 0.3782
2024-06-02 01:53:57 [INFO]: Epoch 047 - training loss: 0.4437, validation loss: 0.3766
2024-06-02 01:53:58 [INFO]: Epoch 048 - training loss: 0.4402, validation loss: 0.3780
2024-06-02 01:53:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:53:58 [INFO]: Finished training. The best model is from epoch#38.
2024-06-02 01:53:58 [INFO]: Saved the model to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_3/20240602_T015256/NonstationaryTransformer.pypots
2024-06-02 01:53:58 [INFO]: Successfully saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_3/imputation.pkl
2024-06-02 01:53:58 [INFO]: Round3 - NonstationaryTransformer on PeMS: MAE=0.3034, MSE=0.5258, MRE=0.3761
2024-06-02 01:53:58 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 01:53:58 [INFO]: Using the given device: cuda:0
2024-06-02 01:53:58 [INFO]: Model files will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_4/20240602_T015358
2024-06-02 01:53:58 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_4/20240602_T015358/tensorboard
2024-06-02 01:53:58 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-02 01:54:00 [INFO]: Epoch 001 - training loss: 1.0348, validation loss: 0.6254
2024-06-02 01:54:01 [INFO]: Epoch 002 - training loss: 0.7429, validation loss: 0.4863
2024-06-02 01:54:02 [INFO]: Epoch 003 - training loss: 0.6486, validation loss: 0.4528
2024-06-02 01:54:03 [INFO]: Epoch 004 - training loss: 0.6137, validation loss: 0.4408
2024-06-02 01:54:04 [INFO]: Epoch 005 - training loss: 0.5772, validation loss: 0.4302
2024-06-02 01:54:06 [INFO]: Epoch 006 - training loss: 0.5525, validation loss: 0.4345
2024-06-02 01:54:07 [INFO]: Epoch 007 - training loss: 0.5377, validation loss: 0.4223
2024-06-02 01:54:08 [INFO]: Epoch 008 - training loss: 0.5200, validation loss: 0.4255
2024-06-02 01:54:09 [INFO]: Epoch 009 - training loss: 0.5192, validation loss: 0.4126
2024-06-02 01:54:11 [INFO]: Epoch 010 - training loss: 0.5135, validation loss: 0.4034
2024-06-02 01:54:12 [INFO]: Epoch 011 - training loss: 0.5033, validation loss: 0.4071
2024-06-02 01:54:13 [INFO]: Epoch 012 - training loss: 0.4963, validation loss: 0.3975
2024-06-02 01:54:14 [INFO]: Epoch 013 - training loss: 0.4880, validation loss: 0.4176
2024-06-02 01:54:15 [INFO]: Epoch 014 - training loss: 0.4866, validation loss: 0.3997
2024-06-02 01:54:17 [INFO]: Epoch 015 - training loss: 0.4815, validation loss: 0.3989
2024-06-02 01:54:18 [INFO]: Epoch 016 - training loss: 0.4763, validation loss: 0.3971
2024-06-02 01:54:19 [INFO]: Epoch 017 - training loss: 0.4719, validation loss: 0.3830
2024-06-02 01:54:20 [INFO]: Epoch 018 - training loss: 0.4717, validation loss: 0.3926
2024-06-02 01:54:22 [INFO]: Epoch 019 - training loss: 0.4707, validation loss: 0.3884
2024-06-02 01:54:23 [INFO]: Epoch 020 - training loss: 0.4690, validation loss: 0.3803
2024-06-02 01:54:24 [INFO]: Epoch 021 - training loss: 0.4688, validation loss: 0.3764
2024-06-02 01:54:25 [INFO]: Epoch 022 - training loss: 0.4655, validation loss: 0.3930
2024-06-02 01:54:26 [INFO]: Epoch 023 - training loss: 0.4707, validation loss: 0.3719
2024-06-02 01:54:27 [INFO]: Epoch 024 - training loss: 0.4656, validation loss: 0.3863
2024-06-02 01:54:29 [INFO]: Epoch 025 - training loss: 0.4644, validation loss: 0.3806
2024-06-02 01:54:30 [INFO]: Epoch 026 - training loss: 0.4629, validation loss: 0.3809
2024-06-02 01:54:31 [INFO]: Epoch 027 - training loss: 0.4605, validation loss: 0.3770
2024-06-02 01:54:32 [INFO]: Epoch 028 - training loss: 0.4616, validation loss: 0.3823
2024-06-02 01:54:34 [INFO]: Epoch 029 - training loss: 0.4592, validation loss: 0.3798
2024-06-02 01:54:35 [INFO]: Epoch 030 - training loss: 0.4542, validation loss: 0.3959
2024-06-02 01:54:36 [INFO]: Epoch 031 - training loss: 0.4570, validation loss: 0.3772
2024-06-02 01:54:38 [INFO]: Epoch 032 - training loss: 0.4571, validation loss: 0.3764
2024-06-02 01:54:39 [INFO]: Epoch 033 - training loss: 0.4541, validation loss: 0.3877
2024-06-02 01:54:39 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:54:39 [INFO]: Finished training. The best model is from epoch#23.
2024-06-02 01:54:39 [INFO]: Saved the model to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_4/20240602_T015358/NonstationaryTransformer.pypots
2024-06-02 01:54:39 [INFO]: Successfully saved to results_point_rate01/PeMS/NonstationaryTransformer_PeMS/round_4/imputation.pkl
2024-06-02 01:54:39 [INFO]: Round4 - NonstationaryTransformer on PeMS: MAE=0.3223, MSE=0.5492, MRE=0.3996
2024-06-02 01:54:39 [INFO]: Done! Final results:
Averaged NonstationaryTransformer (n params: 346,318) on PeMS: MAE=0.3307 ± 0.01705389426419485, MSE=0.5437 ± 0.010112904910772052, MRE=0.4100 ± 0.02114000104010697, average inference time=0.12
