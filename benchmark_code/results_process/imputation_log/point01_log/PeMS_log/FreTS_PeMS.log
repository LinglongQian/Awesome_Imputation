2024-06-02 01:21:15 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 01:21:15 [INFO]: Using the given device: cuda:0
2024-06-02 01:21:16 [INFO]: Model files will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_0/20240602_T012116
2024-06-02 01:21:16 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_0/20240602_T012116/tensorboard
2024-06-02 01:21:16 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-02 01:21:26 [INFO]: Epoch 001 - training loss: 1.3327, validation loss: 0.8841
2024-06-02 01:21:27 [INFO]: Epoch 002 - training loss: 0.7982, validation loss: 0.7459
2024-06-02 01:21:28 [INFO]: Epoch 003 - training loss: 0.7046, validation loss: 0.7374
2024-06-02 01:21:29 [INFO]: Epoch 004 - training loss: 0.6381, validation loss: 0.6210
2024-06-02 01:21:30 [INFO]: Epoch 005 - training loss: 0.5858, validation loss: 0.6063
2024-06-02 01:21:32 [INFO]: Epoch 006 - training loss: 0.5608, validation loss: 0.5879
2024-06-02 01:21:33 [INFO]: Epoch 007 - training loss: 0.5417, validation loss: 0.5975
2024-06-02 01:21:34 [INFO]: Epoch 008 - training loss: 0.5284, validation loss: 0.5641
2024-06-02 01:21:35 [INFO]: Epoch 009 - training loss: 0.5118, validation loss: 0.5590
2024-06-02 01:21:36 [INFO]: Epoch 010 - training loss: 0.4990, validation loss: 0.5696
2024-06-02 01:21:37 [INFO]: Epoch 011 - training loss: 0.5077, validation loss: 0.5354
2024-06-02 01:21:38 [INFO]: Epoch 012 - training loss: 0.4800, validation loss: 0.5172
2024-06-02 01:21:39 [INFO]: Epoch 013 - training loss: 0.4713, validation loss: 0.5364
2024-06-02 01:21:40 [INFO]: Epoch 014 - training loss: 0.4644, validation loss: 0.5184
2024-06-02 01:21:41 [INFO]: Epoch 015 - training loss: 0.4519, validation loss: 0.5091
2024-06-02 01:21:43 [INFO]: Epoch 016 - training loss: 0.4651, validation loss: 0.5188
2024-06-02 01:21:44 [INFO]: Epoch 017 - training loss: 0.4526, validation loss: 0.5126
2024-06-02 01:21:45 [INFO]: Epoch 018 - training loss: 0.4363, validation loss: 0.4958
2024-06-02 01:21:46 [INFO]: Epoch 019 - training loss: 0.4302, validation loss: 0.4825
2024-06-02 01:21:47 [INFO]: Epoch 020 - training loss: 0.4377, validation loss: 0.4916
2024-06-02 01:21:48 [INFO]: Epoch 021 - training loss: 0.4276, validation loss: 0.5062
2024-06-02 01:21:49 [INFO]: Epoch 022 - training loss: 0.4269, validation loss: 0.4792
2024-06-02 01:21:50 [INFO]: Epoch 023 - training loss: 0.4317, validation loss: 0.5038
2024-06-02 01:21:52 [INFO]: Epoch 024 - training loss: 0.4163, validation loss: 0.4665
2024-06-02 01:21:53 [INFO]: Epoch 025 - training loss: 0.4080, validation loss: 0.4776
2024-06-02 01:21:54 [INFO]: Epoch 026 - training loss: 0.4065, validation loss: 0.4788
2024-06-02 01:21:55 [INFO]: Epoch 027 - training loss: 0.4001, validation loss: 0.4835
2024-06-02 01:21:56 [INFO]: Epoch 028 - training loss: 0.3956, validation loss: 0.4852
2024-06-02 01:21:58 [INFO]: Epoch 029 - training loss: 0.4018, validation loss: 0.4639
2024-06-02 01:21:59 [INFO]: Epoch 030 - training loss: 0.3955, validation loss: 0.4560
2024-06-02 01:22:00 [INFO]: Epoch 031 - training loss: 0.3979, validation loss: 0.4682
2024-06-02 01:22:01 [INFO]: Epoch 032 - training loss: 0.3965, validation loss: 0.4663
2024-06-02 01:22:02 [INFO]: Epoch 033 - training loss: 0.3964, validation loss: 0.4565
2024-06-02 01:22:04 [INFO]: Epoch 034 - training loss: 0.3836, validation loss: 0.4674
2024-06-02 01:22:05 [INFO]: Epoch 035 - training loss: 0.3818, validation loss: 0.4633
2024-06-02 01:22:06 [INFO]: Epoch 036 - training loss: 0.3870, validation loss: 0.4579
2024-06-02 01:22:07 [INFO]: Epoch 037 - training loss: 0.3848, validation loss: 0.4354
2024-06-02 01:22:08 [INFO]: Epoch 038 - training loss: 0.3850, validation loss: 0.4653
2024-06-02 01:22:09 [INFO]: Epoch 039 - training loss: 0.3748, validation loss: 0.4512
2024-06-02 01:22:10 [INFO]: Epoch 040 - training loss: 0.3740, validation loss: 0.4462
2024-06-02 01:22:11 [INFO]: Epoch 041 - training loss: 0.3706, validation loss: 0.4305
2024-06-02 01:22:12 [INFO]: Epoch 042 - training loss: 0.3837, validation loss: 0.4282
2024-06-02 01:22:13 [INFO]: Epoch 043 - training loss: 0.3989, validation loss: 0.4430
2024-06-02 01:22:14 [INFO]: Epoch 044 - training loss: 0.3760, validation loss: 0.4371
2024-06-02 01:22:15 [INFO]: Epoch 045 - training loss: 0.3692, validation loss: 0.4337
2024-06-02 01:22:16 [INFO]: Epoch 046 - training loss: 0.3629, validation loss: 0.4451
2024-06-02 01:22:18 [INFO]: Epoch 047 - training loss: 0.3569, validation loss: 0.4360
2024-06-02 01:22:19 [INFO]: Epoch 048 - training loss: 0.3600, validation loss: 0.4298
2024-06-02 01:22:20 [INFO]: Epoch 049 - training loss: 0.3598, validation loss: 0.4344
2024-06-02 01:22:21 [INFO]: Epoch 050 - training loss: 0.3602, validation loss: 0.4484
2024-06-02 01:22:22 [INFO]: Epoch 051 - training loss: 0.3609, validation loss: 0.4393
2024-06-02 01:22:23 [INFO]: Epoch 052 - training loss: 0.3520, validation loss: 0.4357
2024-06-02 01:22:23 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:22:23 [INFO]: Finished training. The best model is from epoch#42.
2024-06-02 01:22:23 [INFO]: Saved the model to results_point_rate01/PeMS/FreTS_PeMS/round_0/20240602_T012116/FreTS.pypots
2024-06-02 01:22:23 [INFO]: Successfully saved to results_point_rate01/PeMS/FreTS_PeMS/round_0/imputation.pkl
2024-06-02 01:22:23 [INFO]: Round0 - FreTS on PeMS: MAE=0.3965, MSE=0.6299, MRE=0.4916
2024-06-02 01:22:23 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 01:22:23 [INFO]: Using the given device: cuda:0
2024-06-02 01:22:23 [INFO]: Model files will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_1/20240602_T012223
2024-06-02 01:22:23 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_1/20240602_T012223/tensorboard
2024-06-02 01:22:23 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-02 01:22:24 [INFO]: Epoch 001 - training loss: 1.2949, validation loss: 0.9744
2024-06-02 01:22:25 [INFO]: Epoch 002 - training loss: 0.7624, validation loss: 0.7119
2024-06-02 01:22:27 [INFO]: Epoch 003 - training loss: 0.6384, validation loss: 0.6667
2024-06-02 01:22:28 [INFO]: Epoch 004 - training loss: 0.6053, validation loss: 0.6076
2024-06-02 01:22:29 [INFO]: Epoch 005 - training loss: 0.5770, validation loss: 0.6090
2024-06-02 01:22:30 [INFO]: Epoch 006 - training loss: 0.5675, validation loss: 0.5658
2024-06-02 01:22:31 [INFO]: Epoch 007 - training loss: 0.5449, validation loss: 0.5716
2024-06-02 01:22:33 [INFO]: Epoch 008 - training loss: 0.5182, validation loss: 0.5716
2024-06-02 01:22:34 [INFO]: Epoch 009 - training loss: 0.5078, validation loss: 0.5583
2024-06-02 01:22:35 [INFO]: Epoch 010 - training loss: 0.4933, validation loss: 0.5546
2024-06-02 01:22:36 [INFO]: Epoch 011 - training loss: 0.4862, validation loss: 0.5420
2024-06-02 01:22:37 [INFO]: Epoch 012 - training loss: 0.4832, validation loss: 0.5265
2024-06-02 01:22:39 [INFO]: Epoch 013 - training loss: 0.4818, validation loss: 0.5288
2024-06-02 01:22:40 [INFO]: Epoch 014 - training loss: 0.4688, validation loss: 0.5387
2024-06-02 01:22:41 [INFO]: Epoch 015 - training loss: 0.4664, validation loss: 0.5403
2024-06-02 01:22:42 [INFO]: Epoch 016 - training loss: 0.4555, validation loss: 0.5347
2024-06-02 01:22:43 [INFO]: Epoch 017 - training loss: 0.4433, validation loss: 0.5268
2024-06-02 01:22:44 [INFO]: Epoch 018 - training loss: 0.4493, validation loss: 0.5251
2024-06-02 01:22:45 [INFO]: Epoch 019 - training loss: 0.4364, validation loss: 0.5035
2024-06-02 01:22:46 [INFO]: Epoch 020 - training loss: 0.4321, validation loss: 0.5334
2024-06-02 01:22:47 [INFO]: Epoch 021 - training loss: 0.4348, validation loss: 0.5586
2024-06-02 01:22:48 [INFO]: Epoch 022 - training loss: 0.4225, validation loss: 0.5296
2024-06-02 01:22:49 [INFO]: Epoch 023 - training loss: 0.4128, validation loss: 0.4822
2024-06-02 01:22:50 [INFO]: Epoch 024 - training loss: 0.4337, validation loss: 0.4893
2024-06-02 01:22:51 [INFO]: Epoch 025 - training loss: 0.4160, validation loss: 0.4971
2024-06-02 01:22:51 [INFO]: Epoch 026 - training loss: 0.4069, validation loss: 0.4965
2024-06-02 01:22:52 [INFO]: Epoch 027 - training loss: 0.4023, validation loss: 0.4921
2024-06-02 01:22:53 [INFO]: Epoch 028 - training loss: 0.4089, validation loss: 0.4884
2024-06-02 01:22:54 [INFO]: Epoch 029 - training loss: 0.4069, validation loss: 0.5002
2024-06-02 01:22:55 [INFO]: Epoch 030 - training loss: 0.4097, validation loss: 0.4843
2024-06-02 01:22:56 [INFO]: Epoch 031 - training loss: 0.3932, validation loss: 0.4883
2024-06-02 01:22:57 [INFO]: Epoch 032 - training loss: 0.3990, validation loss: 0.4892
2024-06-02 01:22:57 [INFO]: Epoch 033 - training loss: 0.3934, validation loss: 0.4874
2024-06-02 01:22:57 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:22:57 [INFO]: Finished training. The best model is from epoch#23.
2024-06-02 01:22:57 [INFO]: Saved the model to results_point_rate01/PeMS/FreTS_PeMS/round_1/20240602_T012223/FreTS.pypots
2024-06-02 01:22:57 [INFO]: Successfully saved to results_point_rate01/PeMS/FreTS_PeMS/round_1/imputation.pkl
2024-06-02 01:22:57 [INFO]: Round1 - FreTS on PeMS: MAE=0.4472, MSE=0.7026, MRE=0.5543
2024-06-02 01:22:57 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 01:22:57 [INFO]: Using the given device: cuda:0
2024-06-02 01:22:57 [INFO]: Model files will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_2/20240602_T012257
2024-06-02 01:22:57 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_2/20240602_T012257/tensorboard
2024-06-02 01:22:57 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-02 01:22:59 [INFO]: Epoch 001 - training loss: 1.2155, validation loss: 0.9568
2024-06-02 01:22:59 [INFO]: Epoch 002 - training loss: 0.7780, validation loss: 0.8519
2024-06-02 01:23:00 [INFO]: Epoch 003 - training loss: 0.6649, validation loss: 0.7142
2024-06-02 01:23:01 [INFO]: Epoch 004 - training loss: 0.6341, validation loss: 0.7426
2024-06-02 01:23:02 [INFO]: Epoch 005 - training loss: 0.5936, validation loss: 0.6188
2024-06-02 01:23:02 [INFO]: Epoch 006 - training loss: 0.5891, validation loss: 0.6779
2024-06-02 01:23:03 [INFO]: Epoch 007 - training loss: 0.5660, validation loss: 0.6242
2024-06-02 01:23:04 [INFO]: Epoch 008 - training loss: 0.5469, validation loss: 0.5969
2024-06-02 01:23:05 [INFO]: Epoch 009 - training loss: 0.5385, validation loss: 0.6116
2024-06-02 01:23:06 [INFO]: Epoch 010 - training loss: 0.5076, validation loss: 0.5688
2024-06-02 01:23:07 [INFO]: Epoch 011 - training loss: 0.5008, validation loss: 0.5865
2024-06-02 01:23:07 [INFO]: Epoch 012 - training loss: 0.4886, validation loss: 0.5398
2024-06-02 01:23:08 [INFO]: Epoch 013 - training loss: 0.4947, validation loss: 0.5472
2024-06-02 01:23:09 [INFO]: Epoch 014 - training loss: 0.4934, validation loss: 0.5624
2024-06-02 01:23:10 [INFO]: Epoch 015 - training loss: 0.4716, validation loss: 0.5335
2024-06-02 01:23:11 [INFO]: Epoch 016 - training loss: 0.4555, validation loss: 0.5309
2024-06-02 01:23:11 [INFO]: Epoch 017 - training loss: 0.4603, validation loss: 0.5323
2024-06-02 01:23:12 [INFO]: Epoch 018 - training loss: 0.4472, validation loss: 0.5212
2024-06-02 01:23:13 [INFO]: Epoch 019 - training loss: 0.4717, validation loss: 0.5471
2024-06-02 01:23:14 [INFO]: Epoch 020 - training loss: 0.4513, validation loss: 0.5263
2024-06-02 01:23:15 [INFO]: Epoch 021 - training loss: 0.4395, validation loss: 0.4986
2024-06-02 01:23:16 [INFO]: Epoch 022 - training loss: 0.4337, validation loss: 0.5217
2024-06-02 01:23:17 [INFO]: Epoch 023 - training loss: 0.4240, validation loss: 0.4987
2024-06-02 01:23:18 [INFO]: Epoch 024 - training loss: 0.4232, validation loss: 0.4866
2024-06-02 01:23:19 [INFO]: Epoch 025 - training loss: 0.4297, validation loss: 0.5005
2024-06-02 01:23:20 [INFO]: Epoch 026 - training loss: 0.4080, validation loss: 0.5042
2024-06-02 01:23:21 [INFO]: Epoch 027 - training loss: 0.4212, validation loss: 0.4944
2024-06-02 01:23:22 [INFO]: Epoch 028 - training loss: 0.4187, validation loss: 0.4902
2024-06-02 01:23:23 [INFO]: Epoch 029 - training loss: 0.4168, validation loss: 0.4850
2024-06-02 01:23:24 [INFO]: Epoch 030 - training loss: 0.4061, validation loss: 0.4837
2024-06-02 01:23:25 [INFO]: Epoch 031 - training loss: 0.3910, validation loss: 0.4875
2024-06-02 01:23:25 [INFO]: Epoch 032 - training loss: 0.3938, validation loss: 0.4873
2024-06-02 01:23:26 [INFO]: Epoch 033 - training loss: 0.3875, validation loss: 0.4607
2024-06-02 01:23:27 [INFO]: Epoch 034 - training loss: 0.3997, validation loss: 0.4703
2024-06-02 01:23:28 [INFO]: Epoch 035 - training loss: 0.3950, validation loss: 0.4740
2024-06-02 01:23:29 [INFO]: Epoch 036 - training loss: 0.3871, validation loss: 0.4709
2024-06-02 01:23:30 [INFO]: Epoch 037 - training loss: 0.3785, validation loss: 0.4574
2024-06-02 01:23:30 [INFO]: Epoch 038 - training loss: 0.3734, validation loss: 0.4506
2024-06-02 01:23:31 [INFO]: Epoch 039 - training loss: 0.3830, validation loss: 0.4582
2024-06-02 01:23:32 [INFO]: Epoch 040 - training loss: 0.3785, validation loss: 0.4483
2024-06-02 01:23:33 [INFO]: Epoch 041 - training loss: 0.3748, validation loss: 0.4598
2024-06-02 01:23:34 [INFO]: Epoch 042 - training loss: 0.3661, validation loss: 0.4596
2024-06-02 01:23:35 [INFO]: Epoch 043 - training loss: 0.3719, validation loss: 0.4710
2024-06-02 01:23:35 [INFO]: Epoch 044 - training loss: 0.3702, validation loss: 0.4555
2024-06-02 01:23:36 [INFO]: Epoch 045 - training loss: 0.3675, validation loss: 0.4499
2024-06-02 01:23:37 [INFO]: Epoch 046 - training loss: 0.3673, validation loss: 0.4580
2024-06-02 01:23:38 [INFO]: Epoch 047 - training loss: 0.3601, validation loss: 0.4596
2024-06-02 01:23:38 [INFO]: Epoch 048 - training loss: 0.3647, validation loss: 0.4474
2024-06-02 01:23:39 [INFO]: Epoch 049 - training loss: 0.3719, validation loss: 0.4671
2024-06-02 01:23:40 [INFO]: Epoch 050 - training loss: 0.3693, validation loss: 0.4645
2024-06-02 01:23:41 [INFO]: Epoch 051 - training loss: 0.3610, validation loss: 0.4591
2024-06-02 01:23:42 [INFO]: Epoch 052 - training loss: 0.3507, validation loss: 0.4628
2024-06-02 01:23:43 [INFO]: Epoch 053 - training loss: 0.3558, validation loss: 0.4470
2024-06-02 01:23:43 [INFO]: Epoch 054 - training loss: 0.3549, validation loss: 0.4399
2024-06-02 01:23:44 [INFO]: Epoch 055 - training loss: 0.3580, validation loss: 0.4703
2024-06-02 01:23:45 [INFO]: Epoch 056 - training loss: 0.3669, validation loss: 0.4495
2024-06-02 01:23:46 [INFO]: Epoch 057 - training loss: 0.3525, validation loss: 0.4474
2024-06-02 01:23:47 [INFO]: Epoch 058 - training loss: 0.3494, validation loss: 0.4377
2024-06-02 01:23:48 [INFO]: Epoch 059 - training loss: 0.3618, validation loss: 0.4610
2024-06-02 01:23:49 [INFO]: Epoch 060 - training loss: 0.3555, validation loss: 0.4384
2024-06-02 01:23:50 [INFO]: Epoch 061 - training loss: 0.3544, validation loss: 0.4347
2024-06-02 01:23:50 [INFO]: Epoch 062 - training loss: 0.3408, validation loss: 0.4359
2024-06-02 01:23:51 [INFO]: Epoch 063 - training loss: 0.3391, validation loss: 0.4425
2024-06-02 01:23:52 [INFO]: Epoch 064 - training loss: 0.3571, validation loss: 0.4334
2024-06-02 01:23:53 [INFO]: Epoch 065 - training loss: 0.3513, validation loss: 0.4388
2024-06-02 01:23:53 [INFO]: Epoch 066 - training loss: 0.3388, validation loss: 0.4382
2024-06-02 01:23:54 [INFO]: Epoch 067 - training loss: 0.3345, validation loss: 0.4255
2024-06-02 01:23:55 [INFO]: Epoch 068 - training loss: 0.3365, validation loss: 0.4339
2024-06-02 01:23:56 [INFO]: Epoch 069 - training loss: 0.3368, validation loss: 0.4235
2024-06-02 01:23:56 [INFO]: Epoch 070 - training loss: 0.3336, validation loss: 0.4255
2024-06-02 01:23:57 [INFO]: Epoch 071 - training loss: 0.3367, validation loss: 0.4252
2024-06-02 01:23:58 [INFO]: Epoch 072 - training loss: 0.3348, validation loss: 0.4414
2024-06-02 01:23:59 [INFO]: Epoch 073 - training loss: 0.3351, validation loss: 0.4234
2024-06-02 01:23:59 [INFO]: Epoch 074 - training loss: 0.3383, validation loss: 0.4175
2024-06-02 01:24:00 [INFO]: Epoch 075 - training loss: 0.3349, validation loss: 0.4328
2024-06-02 01:24:01 [INFO]: Epoch 076 - training loss: 0.3346, validation loss: 0.4296
2024-06-02 01:24:02 [INFO]: Epoch 077 - training loss: 0.3284, validation loss: 0.4147
2024-06-02 01:24:03 [INFO]: Epoch 078 - training loss: 0.3303, validation loss: 0.4174
2024-06-02 01:24:04 [INFO]: Epoch 079 - training loss: 0.3278, validation loss: 0.4204
2024-06-02 01:24:05 [INFO]: Epoch 080 - training loss: 0.3241, validation loss: 0.4342
2024-06-02 01:24:06 [INFO]: Epoch 081 - training loss: 0.3397, validation loss: 0.4163
2024-06-02 01:24:07 [INFO]: Epoch 082 - training loss: 0.3459, validation loss: 0.4358
2024-06-02 01:24:08 [INFO]: Epoch 083 - training loss: 0.3333, validation loss: 0.4147
2024-06-02 01:24:09 [INFO]: Epoch 084 - training loss: 0.3277, validation loss: 0.4170
2024-06-02 01:24:10 [INFO]: Epoch 085 - training loss: 0.3277, validation loss: 0.4128
2024-06-02 01:24:11 [INFO]: Epoch 086 - training loss: 0.3202, validation loss: 0.4224
2024-06-02 01:24:11 [INFO]: Epoch 087 - training loss: 0.3225, validation loss: 0.4199
2024-06-02 01:24:12 [INFO]: Epoch 088 - training loss: 0.3183, validation loss: 0.4146
2024-06-02 01:24:13 [INFO]: Epoch 089 - training loss: 0.3238, validation loss: 0.4197
2024-06-02 01:24:14 [INFO]: Epoch 090 - training loss: 0.3205, validation loss: 0.4102
2024-06-02 01:24:15 [INFO]: Epoch 091 - training loss: 0.3207, validation loss: 0.4166
2024-06-02 01:24:16 [INFO]: Epoch 092 - training loss: 0.3201, validation loss: 0.4069
2024-06-02 01:24:17 [INFO]: Epoch 093 - training loss: 0.3205, validation loss: 0.4235
2024-06-02 01:24:18 [INFO]: Epoch 094 - training loss: 0.3198, validation loss: 0.4066
2024-06-02 01:24:19 [INFO]: Epoch 095 - training loss: 0.3224, validation loss: 0.4118
2024-06-02 01:24:20 [INFO]: Epoch 096 - training loss: 0.3168, validation loss: 0.4090
2024-06-02 01:24:20 [INFO]: Epoch 097 - training loss: 0.3132, validation loss: 0.4155
2024-06-02 01:24:21 [INFO]: Epoch 098 - training loss: 0.3119, validation loss: 0.4183
2024-06-02 01:24:22 [INFO]: Epoch 099 - training loss: 0.3149, validation loss: 0.4148
2024-06-02 01:24:23 [INFO]: Epoch 100 - training loss: 0.3143, validation loss: 0.4090
2024-06-02 01:24:23 [INFO]: Finished training. The best model is from epoch#94.
2024-06-02 01:24:23 [INFO]: Saved the model to results_point_rate01/PeMS/FreTS_PeMS/round_2/20240602_T012257/FreTS.pypots
2024-06-02 01:24:23 [INFO]: Successfully saved to results_point_rate01/PeMS/FreTS_PeMS/round_2/imputation.pkl
2024-06-02 01:24:23 [INFO]: Round2 - FreTS on PeMS: MAE=0.3856, MSE=0.6359, MRE=0.4780
2024-06-02 01:24:23 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 01:24:23 [INFO]: Using the given device: cuda:0
2024-06-02 01:24:23 [INFO]: Model files will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_3/20240602_T012423
2024-06-02 01:24:23 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_3/20240602_T012423/tensorboard
2024-06-02 01:24:23 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-02 01:24:24 [INFO]: Epoch 001 - training loss: 1.2541, validation loss: 0.9326
2024-06-02 01:24:25 [INFO]: Epoch 002 - training loss: 0.7844, validation loss: 0.6704
2024-06-02 01:24:26 [INFO]: Epoch 003 - training loss: 0.6380, validation loss: 0.6133
2024-06-02 01:24:27 [INFO]: Epoch 004 - training loss: 0.6220, validation loss: 0.5948
2024-06-02 01:24:28 [INFO]: Epoch 005 - training loss: 0.5602, validation loss: 0.5500
2024-06-02 01:24:29 [INFO]: Epoch 006 - training loss: 0.5783, validation loss: 0.5702
2024-06-02 01:24:30 [INFO]: Epoch 007 - training loss: 0.5415, validation loss: 0.5628
2024-06-02 01:24:31 [INFO]: Epoch 008 - training loss: 0.5154, validation loss: 0.5227
2024-06-02 01:24:32 [INFO]: Epoch 009 - training loss: 0.4960, validation loss: 0.5057
2024-06-02 01:24:32 [INFO]: Epoch 010 - training loss: 0.4945, validation loss: 0.5147
2024-06-02 01:24:33 [INFO]: Epoch 011 - training loss: 0.4723, validation loss: 0.4955
2024-06-02 01:24:34 [INFO]: Epoch 012 - training loss: 0.4702, validation loss: 0.5047
2024-06-02 01:24:35 [INFO]: Epoch 013 - training loss: 0.4710, validation loss: 0.5059
2024-06-02 01:24:36 [INFO]: Epoch 014 - training loss: 0.4551, validation loss: 0.4956
2024-06-02 01:24:37 [INFO]: Epoch 015 - training loss: 0.4454, validation loss: 0.4843
2024-06-02 01:24:38 [INFO]: Epoch 016 - training loss: 0.4427, validation loss: 0.4752
2024-06-02 01:24:39 [INFO]: Epoch 017 - training loss: 0.4617, validation loss: 0.5004
2024-06-02 01:24:40 [INFO]: Epoch 018 - training loss: 0.4447, validation loss: 0.4757
2024-06-02 01:24:41 [INFO]: Epoch 019 - training loss: 0.4274, validation loss: 0.4828
2024-06-02 01:24:42 [INFO]: Epoch 020 - training loss: 0.4289, validation loss: 0.4744
2024-06-02 01:24:43 [INFO]: Epoch 021 - training loss: 0.4211, validation loss: 0.4678
2024-06-02 01:24:43 [INFO]: Epoch 022 - training loss: 0.4134, validation loss: 0.4687
2024-06-02 01:24:44 [INFO]: Epoch 023 - training loss: 0.4162, validation loss: 0.4810
2024-06-02 01:24:45 [INFO]: Epoch 024 - training loss: 0.4169, validation loss: 0.4827
2024-06-02 01:24:46 [INFO]: Epoch 025 - training loss: 0.4304, validation loss: 0.4769
2024-06-02 01:24:47 [INFO]: Epoch 026 - training loss: 0.4110, validation loss: 0.4719
2024-06-02 01:24:48 [INFO]: Epoch 027 - training loss: 0.4024, validation loss: 0.4492
2024-06-02 01:24:48 [INFO]: Epoch 028 - training loss: 0.3992, validation loss: 0.4543
2024-06-02 01:24:49 [INFO]: Epoch 029 - training loss: 0.3922, validation loss: 0.4615
2024-06-02 01:24:50 [INFO]: Epoch 030 - training loss: 0.3864, validation loss: 0.4416
2024-06-02 01:24:51 [INFO]: Epoch 031 - training loss: 0.3899, validation loss: 0.4356
2024-06-02 01:24:52 [INFO]: Epoch 032 - training loss: 0.3880, validation loss: 0.4566
2024-06-02 01:24:53 [INFO]: Epoch 033 - training loss: 0.3835, validation loss: 0.4427
2024-06-02 01:24:54 [INFO]: Epoch 034 - training loss: 0.3881, validation loss: 0.4508
2024-06-02 01:24:55 [INFO]: Epoch 035 - training loss: 0.3858, validation loss: 0.4434
2024-06-02 01:24:55 [INFO]: Epoch 036 - training loss: 0.3781, validation loss: 0.4411
2024-06-02 01:24:56 [INFO]: Epoch 037 - training loss: 0.3804, validation loss: 0.4541
2024-06-02 01:24:57 [INFO]: Epoch 038 - training loss: 0.3767, validation loss: 0.4279
2024-06-02 01:24:58 [INFO]: Epoch 039 - training loss: 0.3735, validation loss: 0.4401
2024-06-02 01:24:58 [INFO]: Epoch 040 - training loss: 0.3733, validation loss: 0.4455
2024-06-02 01:24:59 [INFO]: Epoch 041 - training loss: 0.3748, validation loss: 0.4299
2024-06-02 01:25:00 [INFO]: Epoch 042 - training loss: 0.3712, validation loss: 0.4392
2024-06-02 01:25:01 [INFO]: Epoch 043 - training loss: 0.3650, validation loss: 0.4269
2024-06-02 01:25:02 [INFO]: Epoch 044 - training loss: 0.3657, validation loss: 0.4368
2024-06-02 01:25:03 [INFO]: Epoch 045 - training loss: 0.3666, validation loss: 0.4360
2024-06-02 01:25:04 [INFO]: Epoch 046 - training loss: 0.3650, validation loss: 0.4278
2024-06-02 01:25:04 [INFO]: Epoch 047 - training loss: 0.3609, validation loss: 0.4393
2024-06-02 01:25:05 [INFO]: Epoch 048 - training loss: 0.3715, validation loss: 0.4477
2024-06-02 01:25:06 [INFO]: Epoch 049 - training loss: 0.3791, validation loss: 0.4272
2024-06-02 01:25:07 [INFO]: Epoch 050 - training loss: 0.3640, validation loss: 0.4392
2024-06-02 01:25:08 [INFO]: Epoch 051 - training loss: 0.3637, validation loss: 0.4505
2024-06-02 01:25:09 [INFO]: Epoch 052 - training loss: 0.3630, validation loss: 0.4252
2024-06-02 01:25:10 [INFO]: Epoch 053 - training loss: 0.3561, validation loss: 0.4293
2024-06-02 01:25:11 [INFO]: Epoch 054 - training loss: 0.3509, validation loss: 0.4198
2024-06-02 01:25:12 [INFO]: Epoch 055 - training loss: 0.3506, validation loss: 0.4232
2024-06-02 01:25:13 [INFO]: Epoch 056 - training loss: 0.3430, validation loss: 0.4181
2024-06-02 01:25:14 [INFO]: Epoch 057 - training loss: 0.3427, validation loss: 0.4248
2024-06-02 01:25:15 [INFO]: Epoch 058 - training loss: 0.3464, validation loss: 0.4157
2024-06-02 01:25:16 [INFO]: Epoch 059 - training loss: 0.3427, validation loss: 0.4196
2024-06-02 01:25:17 [INFO]: Epoch 060 - training loss: 0.3406, validation loss: 0.4166
2024-06-02 01:25:18 [INFO]: Epoch 061 - training loss: 0.3458, validation loss: 0.4106
2024-06-02 01:25:18 [INFO]: Epoch 062 - training loss: 0.3442, validation loss: 0.4186
2024-06-02 01:25:19 [INFO]: Epoch 063 - training loss: 0.3469, validation loss: 0.4138
2024-06-02 01:25:20 [INFO]: Epoch 064 - training loss: 0.3449, validation loss: 0.4241
2024-06-02 01:25:21 [INFO]: Epoch 065 - training loss: 0.3426, validation loss: 0.4158
2024-06-02 01:25:22 [INFO]: Epoch 066 - training loss: 0.3334, validation loss: 0.4114
2024-06-02 01:25:23 [INFO]: Epoch 067 - training loss: 0.3382, validation loss: 0.4029
2024-06-02 01:25:24 [INFO]: Epoch 068 - training loss: 0.3375, validation loss: 0.4172
2024-06-02 01:25:25 [INFO]: Epoch 069 - training loss: 0.3409, validation loss: 0.4189
2024-06-02 01:25:26 [INFO]: Epoch 070 - training loss: 0.3375, validation loss: 0.4193
2024-06-02 01:25:27 [INFO]: Epoch 071 - training loss: 0.3380, validation loss: 0.4179
2024-06-02 01:25:27 [INFO]: Epoch 072 - training loss: 0.3352, validation loss: 0.4205
2024-06-02 01:25:28 [INFO]: Epoch 073 - training loss: 0.3293, validation loss: 0.4188
2024-06-02 01:25:29 [INFO]: Epoch 074 - training loss: 0.3307, validation loss: 0.4035
2024-06-02 01:25:30 [INFO]: Epoch 075 - training loss: 0.3320, validation loss: 0.4158
2024-06-02 01:25:30 [INFO]: Epoch 076 - training loss: 0.3339, validation loss: 0.4134
2024-06-02 01:25:31 [INFO]: Epoch 077 - training loss: 0.3293, validation loss: 0.4141
2024-06-02 01:25:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:25:31 [INFO]: Finished training. The best model is from epoch#67.
2024-06-02 01:25:31 [INFO]: Saved the model to results_point_rate01/PeMS/FreTS_PeMS/round_3/20240602_T012423/FreTS.pypots
2024-06-02 01:25:32 [INFO]: Successfully saved to results_point_rate01/PeMS/FreTS_PeMS/round_3/imputation.pkl
2024-06-02 01:25:32 [INFO]: Round3 - FreTS on PeMS: MAE=0.3696, MSE=0.6067, MRE=0.4581
2024-06-02 01:25:32 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 01:25:32 [INFO]: Using the given device: cuda:0
2024-06-02 01:25:32 [INFO]: Model files will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_4/20240602_T012532
2024-06-02 01:25:32 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/FreTS_PeMS/round_4/20240602_T012532/tensorboard
2024-06-02 01:25:32 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-02 01:25:33 [INFO]: Epoch 001 - training loss: 1.2730, validation loss: 0.8475
2024-06-02 01:25:33 [INFO]: Epoch 002 - training loss: 0.7621, validation loss: 0.6493
2024-06-02 01:25:34 [INFO]: Epoch 003 - training loss: 0.6834, validation loss: 0.6606
2024-06-02 01:25:34 [INFO]: Epoch 004 - training loss: 0.6129, validation loss: 0.6178
2024-06-02 01:25:35 [INFO]: Epoch 005 - training loss: 0.5809, validation loss: 0.5891
2024-06-02 01:25:35 [INFO]: Epoch 006 - training loss: 0.5605, validation loss: 0.5851
2024-06-02 01:25:36 [INFO]: Epoch 007 - training loss: 0.5502, validation loss: 0.5840
2024-06-02 01:25:36 [INFO]: Epoch 008 - training loss: 0.5338, validation loss: 0.5905
2024-06-02 01:25:37 [INFO]: Epoch 009 - training loss: 0.5274, validation loss: 0.5536
2024-06-02 01:25:38 [INFO]: Epoch 010 - training loss: 0.5078, validation loss: 0.5448
2024-06-02 01:25:38 [INFO]: Epoch 011 - training loss: 0.5015, validation loss: 0.5529
2024-06-02 01:25:39 [INFO]: Epoch 012 - training loss: 0.4873, validation loss: 0.5411
2024-06-02 01:25:39 [INFO]: Epoch 013 - training loss: 0.4928, validation loss: 0.5456
2024-06-02 01:25:40 [INFO]: Epoch 014 - training loss: 0.4881, validation loss: 0.5203
2024-06-02 01:25:40 [INFO]: Epoch 015 - training loss: 0.4810, validation loss: 0.5203
2024-06-02 01:25:41 [INFO]: Epoch 016 - training loss: 0.4563, validation loss: 0.5438
2024-06-02 01:25:41 [INFO]: Epoch 017 - training loss: 0.4723, validation loss: 0.5336
2024-06-02 01:25:42 [INFO]: Epoch 018 - training loss: 0.4494, validation loss: 0.5168
2024-06-02 01:25:42 [INFO]: Epoch 019 - training loss: 0.4429, validation loss: 0.5066
2024-06-02 01:25:43 [INFO]: Epoch 020 - training loss: 0.4413, validation loss: 0.5369
2024-06-02 01:25:43 [INFO]: Epoch 021 - training loss: 0.4370, validation loss: 0.5037
2024-06-02 01:25:44 [INFO]: Epoch 022 - training loss: 0.4336, validation loss: 0.4991
2024-06-02 01:25:44 [INFO]: Epoch 023 - training loss: 0.4391, validation loss: 0.5042
2024-06-02 01:25:45 [INFO]: Epoch 024 - training loss: 0.4393, validation loss: 0.5079
2024-06-02 01:25:46 [INFO]: Epoch 025 - training loss: 0.4325, validation loss: 0.4818
2024-06-02 01:25:46 [INFO]: Epoch 026 - training loss: 0.4154, validation loss: 0.4809
2024-06-02 01:25:47 [INFO]: Epoch 027 - training loss: 0.4127, validation loss: 0.4913
2024-06-02 01:25:47 [INFO]: Epoch 028 - training loss: 0.4059, validation loss: 0.4752
2024-06-02 01:25:48 [INFO]: Epoch 029 - training loss: 0.4058, validation loss: 0.4951
2024-06-02 01:25:48 [INFO]: Epoch 030 - training loss: 0.4101, validation loss: 0.4878
2024-06-02 01:25:49 [INFO]: Epoch 031 - training loss: 0.3985, validation loss: 0.4789
2024-06-02 01:25:49 [INFO]: Epoch 032 - training loss: 0.3928, validation loss: 0.4756
2024-06-02 01:25:50 [INFO]: Epoch 033 - training loss: 0.3923, validation loss: 0.4761
2024-06-02 01:25:50 [INFO]: Epoch 034 - training loss: 0.3975, validation loss: 0.4754
2024-06-02 01:25:51 [INFO]: Epoch 035 - training loss: 0.3970, validation loss: 0.4700
2024-06-02 01:25:51 [INFO]: Epoch 036 - training loss: 0.3862, validation loss: 0.4609
2024-06-02 01:25:52 [INFO]: Epoch 037 - training loss: 0.3865, validation loss: 0.4719
2024-06-02 01:25:53 [INFO]: Epoch 038 - training loss: 0.3849, validation loss: 0.4734
2024-06-02 01:25:53 [INFO]: Epoch 039 - training loss: 0.3842, validation loss: 0.4694
2024-06-02 01:25:54 [INFO]: Epoch 040 - training loss: 0.3848, validation loss: 0.4659
2024-06-02 01:25:54 [INFO]: Epoch 041 - training loss: 0.3824, validation loss: 0.4763
2024-06-02 01:25:55 [INFO]: Epoch 042 - training loss: 0.3797, validation loss: 0.4492
2024-06-02 01:25:55 [INFO]: Epoch 043 - training loss: 0.3729, validation loss: 0.4526
2024-06-02 01:25:56 [INFO]: Epoch 044 - training loss: 0.3848, validation loss: 0.4734
2024-06-02 01:25:56 [INFO]: Epoch 045 - training loss: 0.3746, validation loss: 0.4505
2024-06-02 01:25:57 [INFO]: Epoch 046 - training loss: 0.3698, validation loss: 0.4391
2024-06-02 01:25:57 [INFO]: Epoch 047 - training loss: 0.3693, validation loss: 0.4537
2024-06-02 01:25:58 [INFO]: Epoch 048 - training loss: 0.3639, validation loss: 0.4477
2024-06-02 01:25:58 [INFO]: Epoch 049 - training loss: 0.3582, validation loss: 0.4401
2024-06-02 01:25:59 [INFO]: Epoch 050 - training loss: 0.3627, validation loss: 0.4497
2024-06-02 01:26:00 [INFO]: Epoch 051 - training loss: 0.3599, validation loss: 0.4469
2024-06-02 01:26:00 [INFO]: Epoch 052 - training loss: 0.3624, validation loss: 0.4602
2024-06-02 01:26:01 [INFO]: Epoch 053 - training loss: 0.3572, validation loss: 0.4441
2024-06-02 01:26:01 [INFO]: Epoch 054 - training loss: 0.3554, validation loss: 0.4459
2024-06-02 01:26:02 [INFO]: Epoch 055 - training loss: 0.3548, validation loss: 0.4367
2024-06-02 01:26:02 [INFO]: Epoch 056 - training loss: 0.3555, validation loss: 0.4504
2024-06-02 01:26:03 [INFO]: Epoch 057 - training loss: 0.3552, validation loss: 0.4452
2024-06-02 01:26:03 [INFO]: Epoch 058 - training loss: 0.3592, validation loss: 0.4640
2024-06-02 01:26:04 [INFO]: Epoch 059 - training loss: 0.3600, validation loss: 0.4421
2024-06-02 01:26:04 [INFO]: Epoch 060 - training loss: 0.3536, validation loss: 0.4317
2024-06-02 01:26:05 [INFO]: Epoch 061 - training loss: 0.3482, validation loss: 0.4444
2024-06-02 01:26:05 [INFO]: Epoch 062 - training loss: 0.3570, validation loss: 0.4392
2024-06-02 01:26:06 [INFO]: Epoch 063 - training loss: 0.3446, validation loss: 0.4396
2024-06-02 01:26:06 [INFO]: Epoch 064 - training loss: 0.3414, validation loss: 0.4383
2024-06-02 01:26:07 [INFO]: Epoch 065 - training loss: 0.3441, validation loss: 0.4461
2024-06-02 01:26:08 [INFO]: Epoch 066 - training loss: 0.3441, validation loss: 0.4440
2024-06-02 01:26:08 [INFO]: Epoch 067 - training loss: 0.3360, validation loss: 0.4437
2024-06-02 01:26:09 [INFO]: Epoch 068 - training loss: 0.3446, validation loss: 0.4339
2024-06-02 01:26:09 [INFO]: Epoch 069 - training loss: 0.3415, validation loss: 0.4307
2024-06-02 01:26:10 [INFO]: Epoch 070 - training loss: 0.3375, validation loss: 0.4305
2024-06-02 01:26:10 [INFO]: Epoch 071 - training loss: 0.3368, validation loss: 0.4317
2024-06-02 01:26:11 [INFO]: Epoch 072 - training loss: 0.3358, validation loss: 0.4302
2024-06-02 01:26:11 [INFO]: Epoch 073 - training loss: 0.3373, validation loss: 0.4331
2024-06-02 01:26:12 [INFO]: Epoch 074 - training loss: 0.3320, validation loss: 0.4247
2024-06-02 01:26:12 [INFO]: Epoch 075 - training loss: 0.3290, validation loss: 0.4339
2024-06-02 01:26:13 [INFO]: Epoch 076 - training loss: 0.3344, validation loss: 0.4314
2024-06-02 01:26:13 [INFO]: Epoch 077 - training loss: 0.3400, validation loss: 0.4387
2024-06-02 01:26:14 [INFO]: Epoch 078 - training loss: 0.3330, validation loss: 0.4301
2024-06-02 01:26:14 [INFO]: Epoch 079 - training loss: 0.3293, validation loss: 0.4446
2024-06-02 01:26:15 [INFO]: Epoch 080 - training loss: 0.3417, validation loss: 0.4287
2024-06-02 01:26:16 [INFO]: Epoch 081 - training loss: 0.3336, validation loss: 0.4279
2024-06-02 01:26:16 [INFO]: Epoch 082 - training loss: 0.3315, validation loss: 0.4450
2024-06-02 01:26:17 [INFO]: Epoch 083 - training loss: 0.3287, validation loss: 0.4302
2024-06-02 01:26:17 [INFO]: Epoch 084 - training loss: 0.3311, validation loss: 0.4256
2024-06-02 01:26:17 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:26:17 [INFO]: Finished training. The best model is from epoch#74.
2024-06-02 01:26:17 [INFO]: Saved the model to results_point_rate01/PeMS/FreTS_PeMS/round_4/20240602_T012532/FreTS.pypots
2024-06-02 01:26:17 [INFO]: Successfully saved to results_point_rate01/PeMS/FreTS_PeMS/round_4/imputation.pkl
2024-06-02 01:26:17 [INFO]: Round4 - FreTS on PeMS: MAE=0.3829, MSE=0.6583, MRE=0.4746
2024-06-02 01:26:17 [INFO]: Done! Final results:
Averaged FreTS (n params: 1,715,958) on PeMS: MAE=0.3964 ± 0.026831917252501596, MSE=0.6467 ± 0.03243441444458059, MRE=0.4913 ± 0.03326083473009783, average inference time=0.05
