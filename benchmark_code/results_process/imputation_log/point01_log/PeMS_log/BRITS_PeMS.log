2024-06-01 21:52:20 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-01 21:52:20 [INFO]: Using the given device: cuda:0
2024-06-01 21:52:20 [INFO]: Model files will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_0/20240601_T215220
2024-06-01 21:52:20 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_0/20240601_T215220/tensorboard
2024-06-01 21:52:21 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-01 21:53:03 [INFO]: Epoch 001 - training loss: 0.9467, validation loss: 0.5660
2024-06-01 21:53:43 [INFO]: Epoch 002 - training loss: 0.6496, validation loss: 0.5028
2024-06-01 21:54:21 [INFO]: Epoch 003 - training loss: 0.5749, validation loss: 0.4793
2024-06-01 21:55:00 [INFO]: Epoch 004 - training loss: 0.5421, validation loss: 0.4644
2024-06-01 21:55:38 [INFO]: Epoch 005 - training loss: 0.5157, validation loss: 0.4541
2024-06-01 21:56:19 [INFO]: Epoch 006 - training loss: 0.5006, validation loss: 0.4473
2024-06-01 21:56:58 [INFO]: Epoch 007 - training loss: 0.4816, validation loss: 0.4424
2024-06-01 21:57:37 [INFO]: Epoch 008 - training loss: 0.4703, validation loss: 0.4366
2024-06-01 21:58:16 [INFO]: Epoch 009 - training loss: 0.4574, validation loss: 0.4321
2024-06-01 21:58:56 [INFO]: Epoch 010 - training loss: 0.4449, validation loss: 0.4287
2024-06-01 21:59:34 [INFO]: Epoch 011 - training loss: 0.4393, validation loss: 0.4238
2024-06-01 22:00:13 [INFO]: Epoch 012 - training loss: 0.4302, validation loss: 0.4200
2024-06-01 22:00:55 [INFO]: Epoch 013 - training loss: 0.4225, validation loss: 0.4169
2024-06-01 22:01:37 [INFO]: Epoch 014 - training loss: 0.4131, validation loss: 0.4127
2024-06-01 22:02:16 [INFO]: Epoch 015 - training loss: 0.4069, validation loss: 0.4103
2024-06-01 22:02:53 [INFO]: Epoch 016 - training loss: 0.4029, validation loss: 0.4070
2024-06-01 22:03:34 [INFO]: Epoch 017 - training loss: 0.3946, validation loss: 0.4040
2024-06-01 22:04:11 [INFO]: Epoch 018 - training loss: 0.3923, validation loss: 0.4014
2024-06-01 22:04:50 [INFO]: Epoch 019 - training loss: 0.3829, validation loss: 0.3999
2024-06-01 22:05:32 [INFO]: Epoch 020 - training loss: 0.3767, validation loss: 0.3975
2024-06-01 22:06:12 [INFO]: Epoch 021 - training loss: 0.3740, validation loss: 0.3961
2024-06-01 22:06:48 [INFO]: Epoch 022 - training loss: 0.3716, validation loss: 0.3928
2024-06-01 22:07:27 [INFO]: Epoch 023 - training loss: 0.3653, validation loss: 0.3916
2024-06-01 22:08:02 [INFO]: Epoch 024 - training loss: 0.3618, validation loss: 0.3892
2024-06-01 22:08:38 [INFO]: Epoch 025 - training loss: 0.3580, validation loss: 0.3876
2024-06-01 22:09:11 [INFO]: Epoch 026 - training loss: 0.3519, validation loss: 0.3858
2024-06-01 22:09:43 [INFO]: Epoch 027 - training loss: 0.3512, validation loss: 0.3832
2024-06-01 22:10:16 [INFO]: Epoch 028 - training loss: 0.3466, validation loss: 0.3831
2024-06-01 22:10:55 [INFO]: Epoch 029 - training loss: 0.3447, validation loss: 0.3811
2024-06-01 22:11:30 [INFO]: Epoch 030 - training loss: 0.3408, validation loss: 0.3789
2024-06-01 22:12:02 [INFO]: Epoch 031 - training loss: 0.3394, validation loss: 0.3779
2024-06-01 22:12:35 [INFO]: Epoch 032 - training loss: 0.3328, validation loss: 0.3766
2024-06-01 22:13:14 [INFO]: Epoch 033 - training loss: 0.3349, validation loss: 0.3753
2024-06-01 22:13:48 [INFO]: Epoch 034 - training loss: 0.3304, validation loss: 0.3738
2024-06-01 22:14:22 [INFO]: Epoch 035 - training loss: 0.3290, validation loss: 0.3728
2024-06-01 22:14:56 [INFO]: Epoch 036 - training loss: 0.3251, validation loss: 0.3713
2024-06-01 22:15:34 [INFO]: Epoch 037 - training loss: 0.3237, validation loss: 0.3704
2024-06-01 22:16:12 [INFO]: Epoch 038 - training loss: 0.3200, validation loss: 0.3691
2024-06-01 22:16:46 [INFO]: Epoch 039 - training loss: 0.3180, validation loss: 0.3678
2024-06-01 22:17:19 [INFO]: Epoch 040 - training loss: 0.3187, validation loss: 0.3674
2024-06-01 22:17:53 [INFO]: Epoch 041 - training loss: 0.3122, validation loss: 0.3654
2024-06-01 22:18:29 [INFO]: Epoch 042 - training loss: 0.3147, validation loss: 0.3642
2024-06-01 22:19:02 [INFO]: Epoch 043 - training loss: 0.3113, validation loss: 0.3635
2024-06-01 22:19:33 [INFO]: Epoch 044 - training loss: 0.3112, validation loss: 0.3626
2024-06-01 22:20:01 [INFO]: Epoch 045 - training loss: 0.3074, validation loss: 0.3625
2024-06-01 22:20:11 [INFO]: Epoch 046 - training loss: 0.3075, validation loss: 0.3599
2024-06-01 22:20:32 [INFO]: Epoch 047 - training loss: 0.3052, validation loss: 0.3615
2024-06-01 22:20:44 [INFO]: Epoch 048 - training loss: 0.3054, validation loss: 0.3600
2024-06-01 22:21:03 [INFO]: Epoch 049 - training loss: 0.3027, validation loss: 0.3577
2024-06-01 22:21:16 [INFO]: Epoch 050 - training loss: 0.3015, validation loss: 0.3583
2024-06-01 22:21:34 [INFO]: Epoch 051 - training loss: 0.2997, validation loss: 0.3555
2024-06-01 22:21:49 [INFO]: Epoch 052 - training loss: 0.2993, validation loss: 0.3559
2024-06-01 22:22:05 [INFO]: Epoch 053 - training loss: 0.2966, validation loss: 0.3555
2024-06-01 22:22:22 [INFO]: Epoch 054 - training loss: 0.2974, validation loss: 0.3543
2024-06-01 22:22:36 [INFO]: Epoch 055 - training loss: 0.2944, validation loss: 0.3540
2024-06-01 22:22:54 [INFO]: Epoch 056 - training loss: 0.2944, validation loss: 0.3527
2024-06-01 22:23:08 [INFO]: Epoch 057 - training loss: 0.2912, validation loss: 0.3520
2024-06-01 22:23:27 [INFO]: Epoch 058 - training loss: 0.2920, validation loss: 0.3509
2024-06-01 22:23:39 [INFO]: Epoch 059 - training loss: 0.2898, validation loss: 0.3508
2024-06-01 22:23:59 [INFO]: Epoch 060 - training loss: 0.2884, validation loss: 0.3508
2024-06-01 22:24:11 [INFO]: Epoch 061 - training loss: 0.2886, validation loss: 0.3502
2024-06-01 22:24:32 [INFO]: Epoch 062 - training loss: 0.2853, validation loss: 0.3490
2024-06-01 22:24:44 [INFO]: Epoch 063 - training loss: 0.2863, validation loss: 0.3477
2024-06-01 22:25:05 [INFO]: Epoch 064 - training loss: 0.2858, validation loss: 0.3482
2024-06-01 22:25:16 [INFO]: Epoch 065 - training loss: 0.2860, validation loss: 0.3468
2024-06-01 22:25:36 [INFO]: Epoch 066 - training loss: 0.2837, validation loss: 0.3463
2024-06-01 22:25:49 [INFO]: Epoch 067 - training loss: 0.2814, validation loss: 0.3461
2024-06-01 22:26:07 [INFO]: Epoch 068 - training loss: 0.2777, validation loss: 0.3456
2024-06-01 22:26:21 [INFO]: Epoch 069 - training loss: 0.2814, validation loss: 0.3452
2024-06-01 22:26:38 [INFO]: Epoch 070 - training loss: 0.2766, validation loss: 0.3445
2024-06-01 22:26:54 [INFO]: Epoch 071 - training loss: 0.2772, validation loss: 0.3437
2024-06-01 22:27:09 [INFO]: Epoch 072 - training loss: 0.2763, validation loss: 0.3432
2024-06-01 22:27:25 [INFO]: Epoch 073 - training loss: 0.2758, validation loss: 0.3423
2024-06-01 22:27:40 [INFO]: Epoch 074 - training loss: 0.2781, validation loss: 0.3422
2024-06-01 22:27:58 [INFO]: Epoch 075 - training loss: 0.2749, validation loss: 0.3422
2024-06-01 22:28:11 [INFO]: Epoch 076 - training loss: 0.2735, validation loss: 0.3407
2024-06-01 22:28:31 [INFO]: Epoch 077 - training loss: 0.2733, validation loss: 0.3416
2024-06-01 22:28:43 [INFO]: Epoch 078 - training loss: 0.2723, validation loss: 0.3412
2024-06-01 22:29:04 [INFO]: Epoch 079 - training loss: 0.2720, validation loss: 0.3395
2024-06-01 22:29:15 [INFO]: Epoch 080 - training loss: 0.2692, validation loss: 0.3397
2024-06-01 22:29:36 [INFO]: Epoch 081 - training loss: 0.2698, validation loss: 0.3395
2024-06-01 22:29:47 [INFO]: Epoch 082 - training loss: 0.2665, validation loss: 0.3398
2024-06-01 22:30:08 [INFO]: Epoch 083 - training loss: 0.2662, validation loss: 0.3383
2024-06-01 22:30:19 [INFO]: Epoch 084 - training loss: 0.2678, validation loss: 0.3378
2024-06-01 22:30:39 [INFO]: Epoch 085 - training loss: 0.2654, validation loss: 0.3375
2024-06-01 22:30:52 [INFO]: Epoch 086 - training loss: 0.2639, validation loss: 0.3381
2024-06-01 22:31:10 [INFO]: Epoch 087 - training loss: 0.2649, validation loss: 0.3376
2024-06-01 22:31:24 [INFO]: Epoch 088 - training loss: 0.2654, validation loss: 0.3368
2024-06-01 22:31:45 [INFO]: Epoch 089 - training loss: 0.2642, validation loss: 0.3360
2024-06-01 22:32:07 [INFO]: Epoch 090 - training loss: 0.2640, validation loss: 0.3358
2024-06-01 22:32:29 [INFO]: Epoch 091 - training loss: 0.2643, validation loss: 0.3363
2024-06-01 22:32:50 [INFO]: Epoch 092 - training loss: 0.2602, validation loss: 0.3356
2024-06-01 22:33:12 [INFO]: Epoch 093 - training loss: 0.2591, validation loss: 0.3354
2024-06-01 22:33:34 [INFO]: Epoch 094 - training loss: 0.2621, validation loss: 0.3349
2024-06-01 22:33:55 [INFO]: Epoch 095 - training loss: 0.2590, validation loss: 0.3335
2024-06-01 22:34:17 [INFO]: Epoch 096 - training loss: 0.2601, validation loss: 0.3337
2024-06-01 22:34:39 [INFO]: Epoch 097 - training loss: 0.2594, validation loss: 0.3332
2024-06-01 22:35:01 [INFO]: Epoch 098 - training loss: 0.2603, validation loss: 0.3325
2024-06-01 22:35:22 [INFO]: Epoch 099 - training loss: 0.2593, validation loss: 0.3340
2024-06-01 22:35:44 [INFO]: Epoch 100 - training loss: 0.2573, validation loss: 0.3324
2024-06-01 22:35:44 [INFO]: Finished training. The best model is from epoch#100.
2024-06-01 22:35:44 [INFO]: Saved the model to results_point_rate01/PeMS/BRITS_PeMS/round_0/20240601_T215220/BRITS.pypots
2024-06-01 22:35:50 [INFO]: Successfully saved to results_point_rate01/PeMS/BRITS_PeMS/round_0/imputation.pkl
2024-06-01 22:35:50 [INFO]: Round0 - BRITS on PeMS: MAE=0.2712, MSE=0.5178, MRE=0.3362
2024-06-01 22:35:50 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-01 22:35:50 [INFO]: Using the given device: cuda:0
2024-06-01 22:35:50 [INFO]: Model files will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_1/20240601_T223550
2024-06-01 22:35:50 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_1/20240601_T223550/tensorboard
2024-06-01 22:35:50 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-01 22:36:12 [INFO]: Epoch 001 - training loss: 0.9457, validation loss: 0.5597
2024-06-01 22:36:33 [INFO]: Epoch 002 - training loss: 0.6476, validation loss: 0.5020
2024-06-01 22:36:45 [INFO]: Epoch 003 - training loss: 0.5801, validation loss: 0.4788
2024-06-01 22:37:04 [INFO]: Epoch 004 - training loss: 0.5453, validation loss: 0.4633
2024-06-01 22:37:16 [INFO]: Epoch 005 - training loss: 0.5232, validation loss: 0.4541
2024-06-01 22:37:36 [INFO]: Epoch 006 - training loss: 0.4983, validation loss: 0.4464
2024-06-01 22:37:47 [INFO]: Epoch 007 - training loss: 0.4852, validation loss: 0.4425
2024-06-01 22:38:09 [INFO]: Epoch 008 - training loss: 0.4687, validation loss: 0.4385
2024-06-01 22:38:20 [INFO]: Epoch 009 - training loss: 0.4584, validation loss: 0.4332
2024-06-01 22:38:41 [INFO]: Epoch 010 - training loss: 0.4434, validation loss: 0.4300
2024-06-01 22:38:53 [INFO]: Epoch 011 - training loss: 0.4398, validation loss: 0.4244
2024-06-01 22:39:13 [INFO]: Epoch 012 - training loss: 0.4293, validation loss: 0.4218
2024-06-01 22:39:25 [INFO]: Epoch 013 - training loss: 0.4229, validation loss: 0.4188
2024-06-01 22:39:44 [INFO]: Epoch 014 - training loss: 0.4171, validation loss: 0.4151
2024-06-01 22:39:58 [INFO]: Epoch 015 - training loss: 0.4081, validation loss: 0.4117
2024-06-01 22:40:15 [INFO]: Epoch 016 - training loss: 0.4063, validation loss: 0.4086
2024-06-01 22:40:31 [INFO]: Epoch 017 - training loss: 0.3958, validation loss: 0.4058
2024-06-01 22:40:46 [INFO]: Epoch 018 - training loss: 0.3920, validation loss: 0.4027
2024-06-01 22:41:03 [INFO]: Epoch 019 - training loss: 0.3863, validation loss: 0.4010
2024-06-01 22:41:18 [INFO]: Epoch 020 - training loss: 0.3781, validation loss: 0.3991
2024-06-01 22:41:36 [INFO]: Epoch 021 - training loss: 0.3729, validation loss: 0.3960
2024-06-01 22:41:49 [INFO]: Epoch 022 - training loss: 0.3705, validation loss: 0.3943
2024-06-01 22:42:08 [INFO]: Epoch 023 - training loss: 0.3669, validation loss: 0.3931
2024-06-01 22:42:20 [INFO]: Epoch 024 - training loss: 0.3579, validation loss: 0.3904
2024-06-01 22:42:41 [INFO]: Epoch 025 - training loss: 0.3587, validation loss: 0.3895
2024-06-01 22:42:52 [INFO]: Epoch 026 - training loss: 0.3533, validation loss: 0.3874
2024-06-01 22:43:13 [INFO]: Epoch 027 - training loss: 0.3487, validation loss: 0.3853
2024-06-01 22:43:25 [INFO]: Epoch 028 - training loss: 0.3501, validation loss: 0.3831
2024-06-01 22:43:46 [INFO]: Epoch 029 - training loss: 0.3451, validation loss: 0.3825
2024-06-01 22:43:57 [INFO]: Epoch 030 - training loss: 0.3393, validation loss: 0.3807
2024-06-01 22:44:17 [INFO]: Epoch 031 - training loss: 0.3400, validation loss: 0.3789
2024-06-01 22:44:30 [INFO]: Epoch 032 - training loss: 0.3366, validation loss: 0.3775
2024-06-01 22:44:48 [INFO]: Epoch 033 - training loss: 0.3350, validation loss: 0.3761
2024-06-01 22:45:03 [INFO]: Epoch 034 - training loss: 0.3290, validation loss: 0.3750
2024-06-01 22:45:19 [INFO]: Epoch 035 - training loss: 0.3281, validation loss: 0.3740
2024-06-01 22:45:35 [INFO]: Epoch 036 - training loss: 0.3265, validation loss: 0.3729
2024-06-01 22:45:50 [INFO]: Epoch 037 - training loss: 0.3255, validation loss: 0.3709
2024-06-01 22:46:08 [INFO]: Epoch 038 - training loss: 0.3205, validation loss: 0.3698
2024-06-01 22:46:22 [INFO]: Epoch 039 - training loss: 0.3183, validation loss: 0.3695
2024-06-01 22:46:40 [INFO]: Epoch 040 - training loss: 0.3177, validation loss: 0.3680
2024-06-01 22:46:53 [INFO]: Epoch 041 - training loss: 0.3168, validation loss: 0.3671
2024-06-01 22:47:13 [INFO]: Epoch 042 - training loss: 0.3120, validation loss: 0.3667
2024-06-01 22:47:24 [INFO]: Epoch 043 - training loss: 0.3115, validation loss: 0.3643
2024-06-01 22:47:45 [INFO]: Epoch 044 - training loss: 0.3120, validation loss: 0.3641
2024-06-01 22:47:57 [INFO]: Epoch 045 - training loss: 0.3095, validation loss: 0.3641
2024-06-01 22:48:18 [INFO]: Epoch 046 - training loss: 0.3087, validation loss: 0.3623
2024-06-01 22:48:29 [INFO]: Epoch 047 - training loss: 0.3078, validation loss: 0.3611
2024-06-01 22:48:50 [INFO]: Epoch 048 - training loss: 0.3049, validation loss: 0.3600
2024-06-01 22:49:02 [INFO]: Epoch 049 - training loss: 0.3013, validation loss: 0.3593
2024-06-01 22:49:21 [INFO]: Epoch 050 - training loss: 0.3011, validation loss: 0.3579
2024-06-01 22:49:35 [INFO]: Epoch 051 - training loss: 0.2999, validation loss: 0.3579
2024-06-01 22:49:52 [INFO]: Epoch 052 - training loss: 0.2991, validation loss: 0.3560
2024-06-01 22:50:07 [INFO]: Epoch 053 - training loss: 0.2998, validation loss: 0.3560
2024-06-01 22:50:23 [INFO]: Epoch 054 - training loss: 0.2956, validation loss: 0.3566
2024-06-01 22:50:40 [INFO]: Epoch 055 - training loss: 0.2947, validation loss: 0.3535
2024-06-01 22:50:54 [INFO]: Epoch 056 - training loss: 0.2966, validation loss: 0.3544
2024-06-01 22:51:12 [INFO]: Epoch 057 - training loss: 0.2926, validation loss: 0.3529
2024-06-01 22:51:25 [INFO]: Epoch 058 - training loss: 0.2912, validation loss: 0.3530
2024-06-01 22:51:45 [INFO]: Epoch 059 - training loss: 0.2912, validation loss: 0.3516
2024-06-01 22:51:57 [INFO]: Epoch 060 - training loss: 0.2890, validation loss: 0.3513
2024-06-01 22:52:18 [INFO]: Epoch 061 - training loss: 0.2869, validation loss: 0.3502
2024-06-01 22:52:29 [INFO]: Epoch 062 - training loss: 0.2871, validation loss: 0.3491
2024-06-01 22:52:50 [INFO]: Epoch 063 - training loss: 0.2854, validation loss: 0.3497
2024-06-01 22:53:01 [INFO]: Epoch 064 - training loss: 0.2851, validation loss: 0.3488
2024-06-01 22:53:22 [INFO]: Epoch 065 - training loss: 0.2838, validation loss: 0.3481
2024-06-01 22:53:34 [INFO]: Epoch 066 - training loss: 0.2847, validation loss: 0.3476
2024-06-01 22:53:53 [INFO]: Epoch 067 - training loss: 0.2808, validation loss: 0.3471
2024-06-01 22:54:07 [INFO]: Epoch 068 - training loss: 0.2814, validation loss: 0.3457
2024-06-01 22:54:25 [INFO]: Epoch 069 - training loss: 0.2820, validation loss: 0.3455
2024-06-01 22:54:39 [INFO]: Epoch 070 - training loss: 0.2802, validation loss: 0.3445
2024-06-01 22:54:56 [INFO]: Epoch 071 - training loss: 0.2772, validation loss: 0.3455
2024-06-01 22:55:12 [INFO]: Epoch 072 - training loss: 0.2767, validation loss: 0.3437
2024-06-01 22:55:27 [INFO]: Epoch 073 - training loss: 0.2762, validation loss: 0.3439
2024-06-01 22:55:44 [INFO]: Epoch 074 - training loss: 0.2730, validation loss: 0.3426
2024-06-01 22:55:58 [INFO]: Epoch 075 - training loss: 0.2751, validation loss: 0.3424
2024-06-01 22:56:17 [INFO]: Epoch 076 - training loss: 0.2741, validation loss: 0.3421
2024-06-01 22:56:29 [INFO]: Epoch 077 - training loss: 0.2720, validation loss: 0.3409
2024-06-01 22:56:49 [INFO]: Epoch 078 - training loss: 0.2714, validation loss: 0.3415
2024-06-01 22:57:01 [INFO]: Epoch 079 - training loss: 0.2724, validation loss: 0.3408
2024-06-01 22:57:22 [INFO]: Epoch 080 - training loss: 0.2679, validation loss: 0.3400
2024-06-01 22:57:33 [INFO]: Epoch 081 - training loss: 0.2715, validation loss: 0.3395
2024-06-01 22:57:54 [INFO]: Epoch 082 - training loss: 0.2690, validation loss: 0.3393
2024-06-01 22:58:07 [INFO]: Epoch 083 - training loss: 0.2712, validation loss: 0.3386
2024-06-01 22:58:26 [INFO]: Epoch 084 - training loss: 0.2687, validation loss: 0.3381
2024-06-01 22:58:39 [INFO]: Epoch 085 - training loss: 0.2669, validation loss: 0.3380
2024-06-01 22:58:58 [INFO]: Epoch 086 - training loss: 0.2665, validation loss: 0.3376
2024-06-01 22:59:12 [INFO]: Epoch 087 - training loss: 0.2639, validation loss: 0.3368
2024-06-01 22:59:29 [INFO]: Epoch 088 - training loss: 0.2647, validation loss: 0.3376
2024-06-01 22:59:45 [INFO]: Epoch 089 - training loss: 0.2639, validation loss: 0.3367
2024-06-01 23:00:00 [INFO]: Epoch 090 - training loss: 0.2653, validation loss: 0.3362
2024-06-01 23:00:17 [INFO]: Epoch 091 - training loss: 0.2630, validation loss: 0.3356
2024-06-01 23:00:31 [INFO]: Epoch 092 - training loss: 0.2600, validation loss: 0.3352
2024-06-01 23:00:50 [INFO]: Epoch 093 - training loss: 0.2609, validation loss: 0.3352
2024-06-01 23:01:02 [INFO]: Epoch 094 - training loss: 0.2611, validation loss: 0.3352
2024-06-01 23:01:22 [INFO]: Epoch 095 - training loss: 0.2587, validation loss: 0.3332
2024-06-01 23:01:44 [INFO]: Epoch 096 - training loss: 0.2618, validation loss: 0.3337
2024-06-01 23:02:05 [INFO]: Epoch 097 - training loss: 0.2633, validation loss: 0.3340
2024-06-01 23:02:27 [INFO]: Epoch 098 - training loss: 0.2588, validation loss: 0.3328
2024-06-01 23:02:49 [INFO]: Epoch 099 - training loss: 0.2604, validation loss: 0.3333
2024-06-01 23:03:10 [INFO]: Epoch 100 - training loss: 0.2599, validation loss: 0.3331
2024-06-01 23:03:11 [INFO]: Finished training. The best model is from epoch#98.
2024-06-01 23:03:11 [INFO]: Saved the model to results_point_rate01/PeMS/BRITS_PeMS/round_1/20240601_T223550/BRITS.pypots
2024-06-01 23:03:16 [INFO]: Successfully saved to results_point_rate01/PeMS/BRITS_PeMS/round_1/imputation.pkl
2024-06-01 23:03:16 [INFO]: Round1 - BRITS on PeMS: MAE=0.2706, MSE=0.5165, MRE=0.3354
2024-06-01 23:03:16 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-01 23:03:16 [INFO]: Using the given device: cuda:0
2024-06-01 23:03:16 [INFO]: Model files will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_2/20240601_T230316
2024-06-01 23:03:16 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_2/20240601_T230316/tensorboard
2024-06-01 23:03:17 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-01 23:03:39 [INFO]: Epoch 001 - training loss: 0.9505, validation loss: 0.5658
2024-06-01 23:04:00 [INFO]: Epoch 002 - training loss: 0.6514, validation loss: 0.5020
2024-06-01 23:04:22 [INFO]: Epoch 003 - training loss: 0.5827, validation loss: 0.4780
2024-06-01 23:04:44 [INFO]: Epoch 004 - training loss: 0.5420, validation loss: 0.4625
2024-06-01 23:05:05 [INFO]: Epoch 005 - training loss: 0.5201, validation loss: 0.4550
2024-06-01 23:05:27 [INFO]: Epoch 006 - training loss: 0.4970, validation loss: 0.4477
2024-06-01 23:05:49 [INFO]: Epoch 007 - training loss: 0.4823, validation loss: 0.4424
2024-06-01 23:06:10 [INFO]: Epoch 008 - training loss: 0.4743, validation loss: 0.4380
2024-06-01 23:06:28 [INFO]: Epoch 009 - training loss: 0.4555, validation loss: 0.4329
2024-06-01 23:06:40 [INFO]: Epoch 010 - training loss: 0.4487, validation loss: 0.4286
2024-06-01 23:06:59 [INFO]: Epoch 011 - training loss: 0.4381, validation loss: 0.4244
2024-06-01 23:07:12 [INFO]: Epoch 012 - training loss: 0.4288, validation loss: 0.4207
2024-06-01 23:07:31 [INFO]: Epoch 013 - training loss: 0.4223, validation loss: 0.4167
2024-06-01 23:07:45 [INFO]: Epoch 014 - training loss: 0.4143, validation loss: 0.4136
2024-06-01 23:08:02 [INFO]: Epoch 015 - training loss: 0.4064, validation loss: 0.4118
2024-06-01 23:08:17 [INFO]: Epoch 016 - training loss: 0.4046, validation loss: 0.4073
2024-06-01 23:08:33 [INFO]: Epoch 017 - training loss: 0.3942, validation loss: 0.4059
2024-06-01 23:08:50 [INFO]: Epoch 018 - training loss: 0.3876, validation loss: 0.4030
2024-06-01 23:09:04 [INFO]: Epoch 019 - training loss: 0.3848, validation loss: 0.4006
2024-06-01 23:09:23 [INFO]: Epoch 020 - training loss: 0.3789, validation loss: 0.3983
2024-06-01 23:09:35 [INFO]: Epoch 021 - training loss: 0.3758, validation loss: 0.3953
2024-06-01 23:09:55 [INFO]: Epoch 022 - training loss: 0.3701, validation loss: 0.3936
2024-06-01 23:10:06 [INFO]: Epoch 023 - training loss: 0.3676, validation loss: 0.3921
2024-06-01 23:10:27 [INFO]: Epoch 024 - training loss: 0.3635, validation loss: 0.3897
2024-06-01 23:10:39 [INFO]: Epoch 025 - training loss: 0.3601, validation loss: 0.3880
2024-06-01 23:11:00 [INFO]: Epoch 026 - training loss: 0.3540, validation loss: 0.3866
2024-06-01 23:11:12 [INFO]: Epoch 027 - training loss: 0.3477, validation loss: 0.3848
2024-06-01 23:11:32 [INFO]: Epoch 028 - training loss: 0.3495, validation loss: 0.3832
2024-06-01 23:11:44 [INFO]: Epoch 029 - training loss: 0.3424, validation loss: 0.3811
2024-06-01 23:12:03 [INFO]: Epoch 030 - training loss: 0.3393, validation loss: 0.3799
2024-06-01 23:12:17 [INFO]: Epoch 031 - training loss: 0.3383, validation loss: 0.3784
2024-06-01 23:12:34 [INFO]: Epoch 032 - training loss: 0.3361, validation loss: 0.3768
2024-06-01 23:12:49 [INFO]: Epoch 033 - training loss: 0.3338, validation loss: 0.3748
2024-06-01 23:13:05 [INFO]: Epoch 034 - training loss: 0.3313, validation loss: 0.3750
2024-06-01 23:13:22 [INFO]: Epoch 035 - training loss: 0.3273, validation loss: 0.3726
2024-06-01 23:13:36 [INFO]: Epoch 036 - training loss: 0.3249, validation loss: 0.3726
2024-06-01 23:13:54 [INFO]: Epoch 037 - training loss: 0.3251, validation loss: 0.3704
2024-06-01 23:14:07 [INFO]: Epoch 038 - training loss: 0.3211, validation loss: 0.3698
2024-06-01 23:14:27 [INFO]: Epoch 039 - training loss: 0.3199, validation loss: 0.3681
2024-06-01 23:14:38 [INFO]: Epoch 040 - training loss: 0.3175, validation loss: 0.3681
2024-06-01 23:14:59 [INFO]: Epoch 041 - training loss: 0.3187, validation loss: 0.3661
2024-06-01 23:15:11 [INFO]: Epoch 042 - training loss: 0.3141, validation loss: 0.3648
2024-06-01 23:15:32 [INFO]: Epoch 043 - training loss: 0.3103, validation loss: 0.3644
2024-06-01 23:15:43 [INFO]: Epoch 044 - training loss: 0.3109, validation loss: 0.3642
2024-06-01 23:16:04 [INFO]: Epoch 045 - training loss: 0.3060, validation loss: 0.3627
2024-06-01 23:16:16 [INFO]: Epoch 046 - training loss: 0.3056, validation loss: 0.3612
2024-06-01 23:16:35 [INFO]: Epoch 047 - training loss: 0.3048, validation loss: 0.3605
2024-06-01 23:16:48 [INFO]: Epoch 048 - training loss: 0.3071, validation loss: 0.3601
2024-06-01 23:17:06 [INFO]: Epoch 049 - training loss: 0.3012, validation loss: 0.3594
2024-06-01 23:17:21 [INFO]: Epoch 050 - training loss: 0.3011, validation loss: 0.3584
2024-06-01 23:17:38 [INFO]: Epoch 051 - training loss: 0.3009, validation loss: 0.3575
2024-06-01 23:17:53 [INFO]: Epoch 052 - training loss: 0.2989, validation loss: 0.3569
2024-06-01 23:18:09 [INFO]: Epoch 053 - training loss: 0.3002, validation loss: 0.3566
2024-06-01 23:18:26 [INFO]: Epoch 054 - training loss: 0.2956, validation loss: 0.3543
2024-06-01 23:18:40 [INFO]: Epoch 055 - training loss: 0.2937, validation loss: 0.3546
2024-06-01 23:18:58 [INFO]: Epoch 056 - training loss: 0.2928, validation loss: 0.3527
2024-06-01 23:19:11 [INFO]: Epoch 057 - training loss: 0.2929, validation loss: 0.3532
2024-06-01 23:19:31 [INFO]: Epoch 058 - training loss: 0.2914, validation loss: 0.3520
2024-06-01 23:19:42 [INFO]: Epoch 059 - training loss: 0.2863, validation loss: 0.3508
2024-06-01 23:20:03 [INFO]: Epoch 060 - training loss: 0.2902, validation loss: 0.3496
2024-06-01 23:20:16 [INFO]: Epoch 061 - training loss: 0.2869, validation loss: 0.3498
2024-06-01 23:20:37 [INFO]: Epoch 062 - training loss: 0.2871, validation loss: 0.3491
2024-06-01 23:20:47 [INFO]: Epoch 063 - training loss: 0.2864, validation loss: 0.3486
2024-06-01 23:21:08 [INFO]: Epoch 064 - training loss: 0.2840, validation loss: 0.3476
2024-06-01 23:21:19 [INFO]: Epoch 065 - training loss: 0.2826, validation loss: 0.3474
2024-06-01 23:21:38 [INFO]: Epoch 066 - training loss: 0.2796, validation loss: 0.3467
2024-06-01 23:21:51 [INFO]: Epoch 067 - training loss: 0.2809, validation loss: 0.3467
2024-06-01 23:22:10 [INFO]: Epoch 068 - training loss: 0.2792, validation loss: 0.3456
2024-06-01 23:22:24 [INFO]: Epoch 069 - training loss: 0.2785, validation loss: 0.3449
2024-06-01 23:22:41 [INFO]: Epoch 070 - training loss: 0.2786, validation loss: 0.3455
2024-06-01 23:22:57 [INFO]: Epoch 071 - training loss: 0.2781, validation loss: 0.3437
2024-06-01 23:23:12 [INFO]: Epoch 072 - training loss: 0.2780, validation loss: 0.3437
2024-06-01 23:23:29 [INFO]: Epoch 073 - training loss: 0.2759, validation loss: 0.3424
2024-06-01 23:23:43 [INFO]: Epoch 074 - training loss: 0.2749, validation loss: 0.3445
2024-06-01 23:24:02 [INFO]: Epoch 075 - training loss: 0.2746, validation loss: 0.3414
2024-06-01 23:24:14 [INFO]: Epoch 076 - training loss: 0.2719, validation loss: 0.3407
2024-06-01 23:24:34 [INFO]: Epoch 077 - training loss: 0.2755, validation loss: 0.3408
2024-06-01 23:24:45 [INFO]: Epoch 078 - training loss: 0.2723, validation loss: 0.3401
2024-06-01 23:25:06 [INFO]: Epoch 079 - training loss: 0.2734, validation loss: 0.3397
2024-06-01 23:25:17 [INFO]: Epoch 080 - training loss: 0.2711, validation loss: 0.3399
2024-06-01 23:25:38 [INFO]: Epoch 081 - training loss: 0.2699, validation loss: 0.3390
2024-06-01 23:25:50 [INFO]: Epoch 082 - training loss: 0.2686, validation loss: 0.3386
2024-06-01 23:26:11 [INFO]: Epoch 083 - training loss: 0.2688, validation loss: 0.3377
2024-06-01 23:26:22 [INFO]: Epoch 084 - training loss: 0.2668, validation loss: 0.3379
2024-06-01 23:26:42 [INFO]: Epoch 085 - training loss: 0.2654, validation loss: 0.3375
2024-06-01 23:26:55 [INFO]: Epoch 086 - training loss: 0.2670, validation loss: 0.3368
2024-06-01 23:27:13 [INFO]: Epoch 087 - training loss: 0.2645, validation loss: 0.3365
2024-06-01 23:27:27 [INFO]: Epoch 088 - training loss: 0.2652, validation loss: 0.3352
2024-06-01 23:27:44 [INFO]: Epoch 089 - training loss: 0.2650, validation loss: 0.3353
2024-06-01 23:28:00 [INFO]: Epoch 090 - training loss: 0.2632, validation loss: 0.3347
2024-06-01 23:28:15 [INFO]: Epoch 091 - training loss: 0.2631, validation loss: 0.3354
2024-06-01 23:28:32 [INFO]: Epoch 092 - training loss: 0.2620, validation loss: 0.3344
2024-06-01 23:28:46 [INFO]: Epoch 093 - training loss: 0.2611, validation loss: 0.3341
2024-06-01 23:29:05 [INFO]: Epoch 094 - training loss: 0.2616, validation loss: 0.3345
2024-06-01 23:29:17 [INFO]: Epoch 095 - training loss: 0.2600, validation loss: 0.3340
2024-06-01 23:29:37 [INFO]: Epoch 096 - training loss: 0.2606, validation loss: 0.3333
2024-06-01 23:29:49 [INFO]: Epoch 097 - training loss: 0.2599, validation loss: 0.3322
2024-06-01 23:30:10 [INFO]: Epoch 098 - training loss: 0.2593, validation loss: 0.3329
2024-06-01 23:30:21 [INFO]: Epoch 099 - training loss: 0.2572, validation loss: 0.3327
2024-06-01 23:30:43 [INFO]: Epoch 100 - training loss: 0.2582, validation loss: 0.3330
2024-06-01 23:30:43 [INFO]: Finished training. The best model is from epoch#97.
2024-06-01 23:30:43 [INFO]: Saved the model to results_point_rate01/PeMS/BRITS_PeMS/round_2/20240601_T230316/BRITS.pypots
2024-06-01 23:30:45 [INFO]: Successfully saved to results_point_rate01/PeMS/BRITS_PeMS/round_2/imputation.pkl
2024-06-01 23:30:45 [INFO]: Round2 - BRITS on PeMS: MAE=0.2711, MSE=0.5152, MRE=0.3360
2024-06-01 23:30:45 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-01 23:30:45 [INFO]: Using the given device: cuda:0
2024-06-01 23:30:45 [INFO]: Model files will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_3/20240601_T233045
2024-06-01 23:30:45 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_3/20240601_T233045/tensorboard
2024-06-01 23:30:45 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-01 23:31:02 [INFO]: Epoch 001 - training loss: 0.9516, validation loss: 0.5669
2024-06-01 23:31:17 [INFO]: Epoch 002 - training loss: 0.6506, validation loss: 0.5061
2024-06-01 23:31:34 [INFO]: Epoch 003 - training loss: 0.5836, validation loss: 0.4766
2024-06-01 23:31:48 [INFO]: Epoch 004 - training loss: 0.5424, validation loss: 0.4630
2024-06-01 23:32:07 [INFO]: Epoch 005 - training loss: 0.5178, validation loss: 0.4526
2024-06-01 23:32:19 [INFO]: Epoch 006 - training loss: 0.4979, validation loss: 0.4478
2024-06-01 23:32:39 [INFO]: Epoch 007 - training loss: 0.4835, validation loss: 0.4406
2024-06-01 23:32:50 [INFO]: Epoch 008 - training loss: 0.4706, validation loss: 0.4370
2024-06-01 23:33:11 [INFO]: Epoch 009 - training loss: 0.4626, validation loss: 0.4317
2024-06-01 23:33:22 [INFO]: Epoch 010 - training loss: 0.4499, validation loss: 0.4270
2024-06-01 23:33:43 [INFO]: Epoch 011 - training loss: 0.4382, validation loss: 0.4243
2024-06-01 23:33:53 [INFO]: Epoch 012 - training loss: 0.4286, validation loss: 0.4209
2024-06-01 23:34:14 [INFO]: Epoch 013 - training loss: 0.4220, validation loss: 0.4158
2024-06-01 23:34:25 [INFO]: Epoch 014 - training loss: 0.4158, validation loss: 0.4127
2024-06-01 23:34:46 [INFO]: Epoch 015 - training loss: 0.4075, validation loss: 0.4104
2024-06-01 23:34:56 [INFO]: Epoch 016 - training loss: 0.4017, validation loss: 0.4070
2024-06-01 23:35:17 [INFO]: Epoch 017 - training loss: 0.3956, validation loss: 0.4039
2024-06-01 23:35:28 [INFO]: Epoch 018 - training loss: 0.3904, validation loss: 0.4016
2024-06-01 23:35:47 [INFO]: Epoch 019 - training loss: 0.3896, validation loss: 0.3990
2024-06-01 23:35:59 [INFO]: Epoch 020 - training loss: 0.3791, validation loss: 0.3973
2024-06-01 23:36:18 [INFO]: Epoch 021 - training loss: 0.3744, validation loss: 0.3946
2024-06-01 23:36:31 [INFO]: Epoch 022 - training loss: 0.3725, validation loss: 0.3924
2024-06-01 23:36:49 [INFO]: Epoch 023 - training loss: 0.3649, validation loss: 0.3912
2024-06-01 23:37:04 [INFO]: Epoch 024 - training loss: 0.3624, validation loss: 0.3892
2024-06-01 23:37:20 [INFO]: Epoch 025 - training loss: 0.3578, validation loss: 0.3867
2024-06-01 23:37:36 [INFO]: Epoch 026 - training loss: 0.3571, validation loss: 0.3848
2024-06-01 23:37:51 [INFO]: Epoch 027 - training loss: 0.3510, validation loss: 0.3831
2024-06-01 23:38:09 [INFO]: Epoch 028 - training loss: 0.3484, validation loss: 0.3819
2024-06-01 23:38:30 [INFO]: Epoch 029 - training loss: 0.3433, validation loss: 0.3799
2024-06-01 23:38:51 [INFO]: Epoch 030 - training loss: 0.3436, validation loss: 0.3783
2024-06-01 23:39:13 [INFO]: Epoch 031 - training loss: 0.3379, validation loss: 0.3785
2024-06-01 23:39:35 [INFO]: Epoch 032 - training loss: 0.3387, validation loss: 0.3759
2024-06-01 23:39:56 [INFO]: Epoch 033 - training loss: 0.3345, validation loss: 0.3747
2024-06-01 23:40:18 [INFO]: Epoch 034 - training loss: 0.3313, validation loss: 0.3727
2024-06-01 23:40:39 [INFO]: Epoch 035 - training loss: 0.3296, validation loss: 0.3725
2024-06-01 23:41:01 [INFO]: Epoch 036 - training loss: 0.3254, validation loss: 0.3709
2024-06-01 23:41:23 [INFO]: Epoch 037 - training loss: 0.3256, validation loss: 0.3698
2024-06-01 23:41:44 [INFO]: Epoch 038 - training loss: 0.3235, validation loss: 0.3682
2024-06-01 23:42:06 [INFO]: Epoch 039 - training loss: 0.3206, validation loss: 0.3668
2024-06-01 23:42:28 [INFO]: Epoch 040 - training loss: 0.3195, validation loss: 0.3663
2024-06-01 23:42:49 [INFO]: Epoch 041 - training loss: 0.3178, validation loss: 0.3652
2024-06-01 23:43:10 [INFO]: Epoch 042 - training loss: 0.3140, validation loss: 0.3644
2024-06-01 23:43:23 [INFO]: Epoch 043 - training loss: 0.3128, validation loss: 0.3630
2024-06-01 23:43:41 [INFO]: Epoch 044 - training loss: 0.3123, validation loss: 0.3626
2024-06-01 23:43:54 [INFO]: Epoch 045 - training loss: 0.3070, validation loss: 0.3618
2024-06-01 23:44:13 [INFO]: Epoch 046 - training loss: 0.3060, validation loss: 0.3598
2024-06-01 23:44:25 [INFO]: Epoch 047 - training loss: 0.3064, validation loss: 0.3603
2024-06-01 23:44:46 [INFO]: Epoch 048 - training loss: 0.3068, validation loss: 0.3586
2024-06-01 23:44:57 [INFO]: Epoch 049 - training loss: 0.3042, validation loss: 0.3577
2024-06-01 23:45:18 [INFO]: Epoch 050 - training loss: 0.3016, validation loss: 0.3575
2024-06-01 23:45:30 [INFO]: Epoch 051 - training loss: 0.3005, validation loss: 0.3559
2024-06-01 23:45:51 [INFO]: Epoch 052 - training loss: 0.2994, validation loss: 0.3552
2024-06-01 23:46:02 [INFO]: Epoch 053 - training loss: 0.2986, validation loss: 0.3539
2024-06-01 23:46:22 [INFO]: Epoch 054 - training loss: 0.2975, validation loss: 0.3528
2024-06-01 23:46:33 [INFO]: Epoch 055 - training loss: 0.2950, validation loss: 0.3526
2024-06-01 23:46:53 [INFO]: Epoch 056 - training loss: 0.2942, validation loss: 0.3526
2024-06-01 23:47:05 [INFO]: Epoch 057 - training loss: 0.2923, validation loss: 0.3517
2024-06-01 23:47:23 [INFO]: Epoch 058 - training loss: 0.2893, validation loss: 0.3508
2024-06-01 23:47:36 [INFO]: Epoch 059 - training loss: 0.2918, validation loss: 0.3504
2024-06-01 23:47:54 [INFO]: Epoch 060 - training loss: 0.2895, validation loss: 0.3493
2024-06-01 23:48:09 [INFO]: Epoch 061 - training loss: 0.2900, validation loss: 0.3486
2024-06-01 23:48:26 [INFO]: Epoch 062 - training loss: 0.2867, validation loss: 0.3473
2024-06-01 23:48:42 [INFO]: Epoch 063 - training loss: 0.2866, validation loss: 0.3473
2024-06-01 23:48:57 [INFO]: Epoch 064 - training loss: 0.2850, validation loss: 0.3467
2024-06-01 23:49:14 [INFO]: Epoch 065 - training loss: 0.2842, validation loss: 0.3457
2024-06-01 23:49:28 [INFO]: Epoch 066 - training loss: 0.2831, validation loss: 0.3459
2024-06-01 23:49:47 [INFO]: Epoch 067 - training loss: 0.2819, validation loss: 0.3443
2024-06-01 23:49:59 [INFO]: Epoch 068 - training loss: 0.2810, validation loss: 0.3446
2024-06-01 23:50:19 [INFO]: Epoch 069 - training loss: 0.2800, validation loss: 0.3436
2024-06-01 23:50:31 [INFO]: Epoch 070 - training loss: 0.2799, validation loss: 0.3431
2024-06-01 23:50:52 [INFO]: Epoch 071 - training loss: 0.2777, validation loss: 0.3426
2024-06-01 23:51:02 [INFO]: Epoch 072 - training loss: 0.2791, validation loss: 0.3419
2024-06-01 23:51:24 [INFO]: Epoch 073 - training loss: 0.2784, validation loss: 0.3420
2024-06-01 23:51:35 [INFO]: Epoch 074 - training loss: 0.2765, validation loss: 0.3411
2024-06-01 23:51:56 [INFO]: Epoch 075 - training loss: 0.2741, validation loss: 0.3411
2024-06-01 23:52:07 [INFO]: Epoch 076 - training loss: 0.2748, validation loss: 0.3392
2024-06-01 23:52:27 [INFO]: Epoch 077 - training loss: 0.2731, validation loss: 0.3404
2024-06-01 23:52:38 [INFO]: Epoch 078 - training loss: 0.2743, validation loss: 0.3384
2024-06-01 23:52:57 [INFO]: Epoch 079 - training loss: 0.2702, validation loss: 0.3383
2024-06-01 23:53:10 [INFO]: Epoch 080 - training loss: 0.2715, validation loss: 0.3378
2024-06-01 23:53:28 [INFO]: Epoch 081 - training loss: 0.2702, validation loss: 0.3382
2024-06-01 23:53:41 [INFO]: Epoch 082 - training loss: 0.2701, validation loss: 0.3376
2024-06-01 23:54:00 [INFO]: Epoch 083 - training loss: 0.2682, validation loss: 0.3371
2024-06-01 23:54:14 [INFO]: Epoch 084 - training loss: 0.2654, validation loss: 0.3362
2024-06-01 23:54:31 [INFO]: Epoch 085 - training loss: 0.2665, validation loss: 0.3365
2024-06-01 23:54:47 [INFO]: Epoch 086 - training loss: 0.2652, validation loss: 0.3355
2024-06-01 23:55:02 [INFO]: Epoch 087 - training loss: 0.2641, validation loss: 0.3349
2024-06-01 23:55:19 [INFO]: Epoch 088 - training loss: 0.2635, validation loss: 0.3354
2024-06-01 23:55:33 [INFO]: Epoch 089 - training loss: 0.2661, validation loss: 0.3348
2024-06-01 23:55:52 [INFO]: Epoch 090 - training loss: 0.2637, validation loss: 0.3337
2024-06-01 23:56:04 [INFO]: Epoch 091 - training loss: 0.2622, validation loss: 0.3343
2024-06-01 23:56:25 [INFO]: Epoch 092 - training loss: 0.2641, validation loss: 0.3328
2024-06-01 23:56:36 [INFO]: Epoch 093 - training loss: 0.2595, validation loss: 0.3332
2024-06-01 23:56:57 [INFO]: Epoch 094 - training loss: 0.2617, validation loss: 0.3320
2024-06-01 23:57:09 [INFO]: Epoch 095 - training loss: 0.2580, validation loss: 0.3328
2024-06-01 23:57:30 [INFO]: Epoch 096 - training loss: 0.2605, validation loss: 0.3317
2024-06-01 23:57:40 [INFO]: Epoch 097 - training loss: 0.2594, validation loss: 0.3316
2024-06-01 23:58:01 [INFO]: Epoch 098 - training loss: 0.2587, validation loss: 0.3321
2024-06-01 23:58:12 [INFO]: Epoch 099 - training loss: 0.2595, validation loss: 0.3302
2024-06-01 23:58:32 [INFO]: Epoch 100 - training loss: 0.2556, validation loss: 0.3314
2024-06-01 23:58:32 [INFO]: Finished training. The best model is from epoch#99.
2024-06-01 23:58:32 [INFO]: Saved the model to results_point_rate01/PeMS/BRITS_PeMS/round_3/20240601_T233045/BRITS.pypots
2024-06-01 23:58:34 [INFO]: Successfully saved to results_point_rate01/PeMS/BRITS_PeMS/round_3/imputation.pkl
2024-06-01 23:58:34 [INFO]: Round3 - BRITS on PeMS: MAE=0.2710, MSE=0.5153, MRE=0.3360
2024-06-01 23:58:34 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-01 23:58:34 [INFO]: Using the given device: cuda:0
2024-06-01 23:58:34 [INFO]: Model files will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_4/20240601_T235834
2024-06-01 23:58:34 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/BRITS_PeMS/round_4/20240601_T235834/tensorboard
2024-06-01 23:58:34 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-01 23:58:52 [INFO]: Epoch 001 - training loss: 0.9450, validation loss: 0.5617
2024-06-01 23:59:06 [INFO]: Epoch 002 - training loss: 0.6522, validation loss: 0.5006
2024-06-01 23:59:25 [INFO]: Epoch 003 - training loss: 0.5860, validation loss: 0.4781
2024-06-01 23:59:37 [INFO]: Epoch 004 - training loss: 0.5426, validation loss: 0.4643
2024-06-01 23:59:57 [INFO]: Epoch 005 - training loss: 0.5125, validation loss: 0.4553
2024-06-02 00:00:08 [INFO]: Epoch 006 - training loss: 0.4997, validation loss: 0.4485
2024-06-02 00:00:30 [INFO]: Epoch 007 - training loss: 0.4789, validation loss: 0.4443
2024-06-02 00:00:41 [INFO]: Epoch 008 - training loss: 0.4691, validation loss: 0.4375
2024-06-02 00:01:02 [INFO]: Epoch 009 - training loss: 0.4582, validation loss: 0.4327
2024-06-02 00:01:14 [INFO]: Epoch 010 - training loss: 0.4491, validation loss: 0.4283
2024-06-02 00:01:34 [INFO]: Epoch 011 - training loss: 0.4395, validation loss: 0.4245
2024-06-02 00:01:45 [INFO]: Epoch 012 - training loss: 0.4306, validation loss: 0.4204
2024-06-02 00:02:05 [INFO]: Epoch 013 - training loss: 0.4242, validation loss: 0.4174
2024-06-02 00:02:18 [INFO]: Epoch 014 - training loss: 0.4139, validation loss: 0.4143
2024-06-02 00:02:36 [INFO]: Epoch 015 - training loss: 0.4069, validation loss: 0.4123
2024-06-02 00:02:51 [INFO]: Epoch 016 - training loss: 0.3996, validation loss: 0.4084
2024-06-02 00:03:07 [INFO]: Epoch 017 - training loss: 0.3959, validation loss: 0.4058
2024-06-02 00:03:23 [INFO]: Epoch 018 - training loss: 0.3888, validation loss: 0.4029
2024-06-02 00:03:39 [INFO]: Epoch 019 - training loss: 0.3827, validation loss: 0.4004
2024-06-02 00:03:56 [INFO]: Epoch 020 - training loss: 0.3786, validation loss: 0.3982
2024-06-02 00:04:10 [INFO]: Epoch 021 - training loss: 0.3763, validation loss: 0.3963
2024-06-02 00:04:29 [INFO]: Epoch 022 - training loss: 0.3702, validation loss: 0.3945
2024-06-02 00:04:41 [INFO]: Epoch 023 - training loss: 0.3671, validation loss: 0.3919
2024-06-02 00:05:01 [INFO]: Epoch 024 - training loss: 0.3624, validation loss: 0.3911
2024-06-02 00:05:13 [INFO]: Epoch 025 - training loss: 0.3591, validation loss: 0.3879
2024-06-02 00:05:34 [INFO]: Epoch 026 - training loss: 0.3551, validation loss: 0.3869
2024-06-02 00:05:44 [INFO]: Epoch 027 - training loss: 0.3523, validation loss: 0.3850
2024-06-02 00:06:05 [INFO]: Epoch 028 - training loss: 0.3466, validation loss: 0.3828
2024-06-02 00:06:17 [INFO]: Epoch 029 - training loss: 0.3460, validation loss: 0.3817
2024-06-02 00:06:38 [INFO]: Epoch 030 - training loss: 0.3436, validation loss: 0.3805
2024-06-02 00:06:50 [INFO]: Epoch 031 - training loss: 0.3402, validation loss: 0.3783
2024-06-02 00:07:09 [INFO]: Epoch 032 - training loss: 0.3342, validation loss: 0.3765
2024-06-02 00:07:22 [INFO]: Epoch 033 - training loss: 0.3327, validation loss: 0.3767
2024-06-02 00:07:40 [INFO]: Epoch 034 - training loss: 0.3304, validation loss: 0.3753
2024-06-02 00:07:55 [INFO]: Epoch 035 - training loss: 0.3275, validation loss: 0.3739
2024-06-02 00:08:12 [INFO]: Epoch 036 - training loss: 0.3262, validation loss: 0.3713
2024-06-02 00:08:27 [INFO]: Epoch 037 - training loss: 0.3257, validation loss: 0.3710
2024-06-02 00:08:43 [INFO]: Epoch 038 - training loss: 0.3231, validation loss: 0.3696
2024-06-02 00:09:00 [INFO]: Epoch 039 - training loss: 0.3171, validation loss: 0.3687
2024-06-02 00:09:14 [INFO]: Epoch 040 - training loss: 0.3152, validation loss: 0.3678
2024-06-02 00:09:33 [INFO]: Epoch 041 - training loss: 0.3179, validation loss: 0.3665
2024-06-02 00:09:45 [INFO]: Epoch 042 - training loss: 0.3156, validation loss: 0.3662
2024-06-02 00:10:05 [INFO]: Epoch 043 - training loss: 0.3113, validation loss: 0.3649
2024-06-02 00:10:17 [INFO]: Epoch 044 - training loss: 0.3114, validation loss: 0.3630
2024-06-02 00:10:38 [INFO]: Epoch 045 - training loss: 0.3071, validation loss: 0.3620
2024-06-02 00:10:48 [INFO]: Epoch 046 - training loss: 0.3068, validation loss: 0.3617
2024-06-02 00:11:09 [INFO]: Epoch 047 - training loss: 0.3052, validation loss: 0.3609
2024-06-02 00:11:20 [INFO]: Epoch 048 - training loss: 0.3028, validation loss: 0.3593
2024-06-02 00:11:41 [INFO]: Epoch 049 - training loss: 0.3033, validation loss: 0.3593
2024-06-02 00:11:51 [INFO]: Epoch 050 - training loss: 0.3023, validation loss: 0.3578
2024-06-02 00:12:12 [INFO]: Epoch 051 - training loss: 0.2999, validation loss: 0.3580
2024-06-02 00:12:24 [INFO]: Epoch 052 - training loss: 0.2962, validation loss: 0.3561
2024-06-02 00:12:44 [INFO]: Epoch 053 - training loss: 0.2973, validation loss: 0.3557
2024-06-02 00:12:57 [INFO]: Epoch 054 - training loss: 0.2950, validation loss: 0.3546
2024-06-02 00:13:15 [INFO]: Epoch 055 - training loss: 0.2924, validation loss: 0.3535
2024-06-02 00:13:29 [INFO]: Epoch 056 - training loss: 0.2938, validation loss: 0.3530
2024-06-02 00:13:46 [INFO]: Epoch 057 - training loss: 0.2928, validation loss: 0.3534
2024-06-02 00:14:02 [INFO]: Epoch 058 - training loss: 0.2933, validation loss: 0.3507
2024-06-02 00:14:17 [INFO]: Epoch 059 - training loss: 0.2890, validation loss: 0.3515
2024-06-02 00:14:35 [INFO]: Epoch 060 - training loss: 0.2901, validation loss: 0.3507
2024-06-02 00:14:49 [INFO]: Epoch 061 - training loss: 0.2860, validation loss: 0.3501
2024-06-02 00:15:07 [INFO]: Epoch 062 - training loss: 0.2883, validation loss: 0.3499
2024-06-02 00:15:20 [INFO]: Epoch 063 - training loss: 0.2856, validation loss: 0.3483
2024-06-02 00:15:40 [INFO]: Epoch 064 - training loss: 0.2835, validation loss: 0.3494
2024-06-02 00:15:51 [INFO]: Epoch 065 - training loss: 0.2833, validation loss: 0.3471
2024-06-02 00:16:12 [INFO]: Epoch 066 - training loss: 0.2816, validation loss: 0.3471
2024-06-02 00:16:23 [INFO]: Epoch 067 - training loss: 0.2820, validation loss: 0.3471
2024-06-02 00:16:44 [INFO]: Epoch 068 - training loss: 0.2812, validation loss: 0.3454
2024-06-02 00:16:54 [INFO]: Epoch 069 - training loss: 0.2795, validation loss: 0.3458
2024-06-02 00:17:16 [INFO]: Epoch 070 - training loss: 0.2794, validation loss: 0.3444
2024-06-02 00:17:27 [INFO]: Epoch 071 - training loss: 0.2769, validation loss: 0.3446
2024-06-02 00:17:47 [INFO]: Epoch 072 - training loss: 0.2776, validation loss: 0.3438
2024-06-02 00:18:00 [INFO]: Epoch 073 - training loss: 0.2755, validation loss: 0.3442
2024-06-02 00:18:18 [INFO]: Epoch 074 - training loss: 0.2738, validation loss: 0.3422
2024-06-02 00:18:31 [INFO]: Epoch 075 - training loss: 0.2755, validation loss: 0.3422
2024-06-02 00:18:50 [INFO]: Epoch 076 - training loss: 0.2749, validation loss: 0.3419
2024-06-02 00:19:04 [INFO]: Epoch 077 - training loss: 0.2713, validation loss: 0.3411
2024-06-02 00:19:21 [INFO]: Epoch 078 - training loss: 0.2724, validation loss: 0.3404
2024-06-02 00:19:37 [INFO]: Epoch 079 - training loss: 0.2724, validation loss: 0.3405
2024-06-02 00:19:52 [INFO]: Epoch 080 - training loss: 0.2708, validation loss: 0.3398
2024-06-02 00:20:09 [INFO]: Epoch 081 - training loss: 0.2705, validation loss: 0.3401
2024-06-02 00:20:23 [INFO]: Epoch 082 - training loss: 0.2679, validation loss: 0.3386
2024-06-02 00:20:42 [INFO]: Epoch 083 - training loss: 0.2682, validation loss: 0.3380
2024-06-02 00:20:54 [INFO]: Epoch 084 - training loss: 0.2683, validation loss: 0.3382
2024-06-02 00:21:15 [INFO]: Epoch 085 - training loss: 0.2669, validation loss: 0.3380
2024-06-02 00:21:26 [INFO]: Epoch 086 - training loss: 0.2683, validation loss: 0.3377
2024-06-02 00:21:47 [INFO]: Epoch 087 - training loss: 0.2646, validation loss: 0.3376
2024-06-02 00:21:57 [INFO]: Epoch 088 - training loss: 0.2638, validation loss: 0.3356
2024-06-02 00:22:19 [INFO]: Epoch 089 - training loss: 0.2647, validation loss: 0.3361
2024-06-02 00:22:30 [INFO]: Epoch 090 - training loss: 0.2654, validation loss: 0.3359
2024-06-02 00:22:51 [INFO]: Epoch 091 - training loss: 0.2621, validation loss: 0.3348
2024-06-02 00:23:03 [INFO]: Epoch 092 - training loss: 0.2614, validation loss: 0.3362
2024-06-02 00:23:22 [INFO]: Epoch 093 - training loss: 0.2617, validation loss: 0.3354
2024-06-02 00:23:34 [INFO]: Epoch 094 - training loss: 0.2617, validation loss: 0.3344
2024-06-02 00:23:53 [INFO]: Epoch 095 - training loss: 0.2595, validation loss: 0.3341
2024-06-02 00:24:07 [INFO]: Epoch 096 - training loss: 0.2575, validation loss: 0.3341
2024-06-02 00:24:25 [INFO]: Epoch 097 - training loss: 0.2577, validation loss: 0.3335
2024-06-02 00:24:40 [INFO]: Epoch 098 - training loss: 0.2625, validation loss: 0.3324
2024-06-02 00:24:56 [INFO]: Epoch 099 - training loss: 0.2593, validation loss: 0.3321
2024-06-02 00:25:13 [INFO]: Epoch 100 - training loss: 0.2585, validation loss: 0.3325
2024-06-02 00:25:14 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 00:25:14 [INFO]: Saved the model to results_point_rate01/PeMS/BRITS_PeMS/round_4/20240601_T235834/BRITS.pypots
2024-06-02 00:25:19 [INFO]: Successfully saved to results_point_rate01/PeMS/BRITS_PeMS/round_4/imputation.pkl
2024-06-02 00:25:19 [INFO]: Round4 - BRITS on PeMS: MAE=0.2716, MSE=0.5181, MRE=0.3367
2024-06-02 00:25:19 [INFO]: Done! Final results:
Averaged BRITS (n params: 32,012,048) on PeMS: MAE=0.2711 ± 0.00033555362770184286, MSE=0.5166 ± 0.0012181747238967234, MRE=0.3361 ± 0.00041595215314087827, average inference time=4.06
