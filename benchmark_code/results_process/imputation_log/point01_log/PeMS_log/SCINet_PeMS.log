2024-06-02 03:23:55 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 03:23:55 [INFO]: Using the given device: cuda:0
2024-06-02 03:23:56 [INFO]: Model files will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_0/20240602_T032356
2024-06-02 03:23:56 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_0/20240602_T032356/tensorboard
2024-06-02 03:24:05 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-02 03:24:14 [INFO]: Epoch 001 - training loss: 2.1549, validation loss: 1.4216
2024-06-02 03:24:20 [INFO]: Epoch 002 - training loss: 1.3278, validation loss: 1.0954
2024-06-02 03:24:27 [INFO]: Epoch 003 - training loss: 1.1003, validation loss: 0.9950
2024-06-02 03:24:34 [INFO]: Epoch 004 - training loss: 1.0171, validation loss: 0.9438
2024-06-02 03:24:41 [INFO]: Epoch 005 - training loss: 0.9527, validation loss: 0.9034
2024-06-02 03:24:48 [INFO]: Epoch 006 - training loss: 0.9008, validation loss: 0.8546
2024-06-02 03:24:55 [INFO]: Epoch 007 - training loss: 0.8636, validation loss: 0.8685
2024-06-02 03:25:02 [INFO]: Epoch 008 - training loss: 0.8221, validation loss: 0.8040
2024-06-02 03:25:09 [INFO]: Epoch 009 - training loss: 0.7809, validation loss: 0.7695
2024-06-02 03:25:16 [INFO]: Epoch 010 - training loss: 0.7397, validation loss: 0.6915
2024-06-02 03:25:23 [INFO]: Epoch 011 - training loss: 0.7025, validation loss: 0.6361
2024-06-02 03:25:30 [INFO]: Epoch 012 - training loss: 0.6655, validation loss: 0.5804
2024-06-02 03:25:37 [INFO]: Epoch 013 - training loss: 0.6298, validation loss: 0.5144
2024-06-02 03:25:44 [INFO]: Epoch 014 - training loss: 0.6052, validation loss: 0.4949
2024-06-02 03:25:51 [INFO]: Epoch 015 - training loss: 0.5868, validation loss: 0.4797
2024-06-02 03:25:58 [INFO]: Epoch 016 - training loss: 0.5731, validation loss: 0.4692
2024-06-02 03:26:05 [INFO]: Epoch 017 - training loss: 0.5651, validation loss: 0.4621
2024-06-02 03:26:12 [INFO]: Epoch 018 - training loss: 0.5620, validation loss: 0.4564
2024-06-02 03:26:19 [INFO]: Epoch 019 - training loss: 0.5514, validation loss: 0.4533
2024-06-02 03:26:26 [INFO]: Epoch 020 - training loss: 0.5516, validation loss: 0.4490
2024-06-02 03:26:33 [INFO]: Epoch 021 - training loss: 0.5502, validation loss: 0.4476
2024-06-02 03:26:40 [INFO]: Epoch 022 - training loss: 0.5448, validation loss: 0.4410
2024-06-02 03:26:46 [INFO]: Epoch 023 - training loss: 0.5428, validation loss: 0.4343
2024-06-02 03:26:53 [INFO]: Epoch 024 - training loss: 0.5408, validation loss: 0.4374
2024-06-02 03:27:00 [INFO]: Epoch 025 - training loss: 0.5378, validation loss: 0.4340
2024-06-02 03:27:07 [INFO]: Epoch 026 - training loss: 0.5385, validation loss: 0.4351
2024-06-02 03:27:14 [INFO]: Epoch 027 - training loss: 0.5344, validation loss: 0.4219
2024-06-02 03:27:20 [INFO]: Epoch 028 - training loss: 0.5361, validation loss: 0.4262
2024-06-02 03:27:27 [INFO]: Epoch 029 - training loss: 0.5375, validation loss: 0.4202
2024-06-02 03:27:34 [INFO]: Epoch 030 - training loss: 0.5320, validation loss: 0.4243
2024-06-02 03:27:41 [INFO]: Epoch 031 - training loss: 0.5308, validation loss: 0.4198
2024-06-02 03:27:48 [INFO]: Epoch 032 - training loss: 0.5316, validation loss: 0.4174
2024-06-02 03:27:55 [INFO]: Epoch 033 - training loss: 0.5272, validation loss: 0.4173
2024-06-02 03:28:02 [INFO]: Epoch 034 - training loss: 0.5284, validation loss: 0.4198
2024-06-02 03:28:09 [INFO]: Epoch 035 - training loss: 0.5258, validation loss: 0.4136
2024-06-02 03:28:15 [INFO]: Epoch 036 - training loss: 0.5288, validation loss: 0.4116
2024-06-02 03:28:22 [INFO]: Epoch 037 - training loss: 0.5262, validation loss: 0.4104
2024-06-02 03:28:29 [INFO]: Epoch 038 - training loss: 0.5207, validation loss: 0.4053
2024-06-02 03:28:36 [INFO]: Epoch 039 - training loss: 0.5254, validation loss: 0.4154
2024-06-02 03:28:43 [INFO]: Epoch 040 - training loss: 0.5240, validation loss: 0.4079
2024-06-02 03:28:49 [INFO]: Epoch 041 - training loss: 0.5229, validation loss: 0.4041
2024-06-02 03:28:56 [INFO]: Epoch 042 - training loss: 0.5192, validation loss: 0.4021
2024-06-02 03:29:03 [INFO]: Epoch 043 - training loss: 0.5224, validation loss: 0.4088
2024-06-02 03:29:10 [INFO]: Epoch 044 - training loss: 0.5230, validation loss: 0.4029
2024-06-02 03:29:17 [INFO]: Epoch 045 - training loss: 0.5170, validation loss: 0.4032
2024-06-02 03:29:24 [INFO]: Epoch 046 - training loss: 0.5246, validation loss: 0.4038
2024-06-02 03:29:30 [INFO]: Epoch 047 - training loss: 0.5204, validation loss: 0.3946
2024-06-02 03:29:37 [INFO]: Epoch 048 - training loss: 0.5180, validation loss: 0.3951
2024-06-02 03:29:44 [INFO]: Epoch 049 - training loss: 0.5192, validation loss: 0.3993
2024-06-02 03:29:51 [INFO]: Epoch 050 - training loss: 0.5167, validation loss: 0.3934
2024-06-02 03:29:58 [INFO]: Epoch 051 - training loss: 0.5199, validation loss: 0.3941
2024-06-02 03:30:05 [INFO]: Epoch 052 - training loss: 0.5173, validation loss: 0.3927
2024-06-02 03:30:11 [INFO]: Epoch 053 - training loss: 0.5162, validation loss: 0.3956
2024-06-02 03:30:18 [INFO]: Epoch 054 - training loss: 0.5167, validation loss: 0.3932
2024-06-02 03:30:25 [INFO]: Epoch 055 - training loss: 0.5146, validation loss: 0.3907
2024-06-02 03:30:32 [INFO]: Epoch 056 - training loss: 0.5159, validation loss: 0.3916
2024-06-02 03:30:38 [INFO]: Epoch 057 - training loss: 0.5156, validation loss: 0.3856
2024-06-02 03:30:45 [INFO]: Epoch 058 - training loss: 0.5193, validation loss: 0.3922
2024-06-02 03:30:52 [INFO]: Epoch 059 - training loss: 0.5152, validation loss: 0.3881
2024-06-02 03:30:59 [INFO]: Epoch 060 - training loss: 0.5173, validation loss: 0.3838
2024-06-02 03:31:06 [INFO]: Epoch 061 - training loss: 0.5139, validation loss: 0.3925
2024-06-02 03:31:13 [INFO]: Epoch 062 - training loss: 0.5126, validation loss: 0.3832
2024-06-02 03:31:20 [INFO]: Epoch 063 - training loss: 0.5162, validation loss: 0.3832
2024-06-02 03:31:27 [INFO]: Epoch 064 - training loss: 0.5135, validation loss: 0.3847
2024-06-02 03:31:34 [INFO]: Epoch 065 - training loss: 0.5086, validation loss: 0.3866
2024-06-02 03:31:41 [INFO]: Epoch 066 - training loss: 0.5099, validation loss: 0.3797
2024-06-02 03:31:48 [INFO]: Epoch 067 - training loss: 0.5147, validation loss: 0.3917
2024-06-02 03:31:55 [INFO]: Epoch 068 - training loss: 0.5131, validation loss: 0.3823
2024-06-02 03:32:02 [INFO]: Epoch 069 - training loss: 0.5142, validation loss: 0.3783
2024-06-02 03:32:09 [INFO]: Epoch 070 - training loss: 0.5109, validation loss: 0.3859
2024-06-02 03:32:14 [INFO]: Epoch 071 - training loss: 0.5063, validation loss: 0.3787
2024-06-02 03:32:20 [INFO]: Epoch 072 - training loss: 0.5119, validation loss: 0.3789
2024-06-02 03:32:25 [INFO]: Epoch 073 - training loss: 0.5092, validation loss: 0.3772
2024-06-02 03:32:31 [INFO]: Epoch 074 - training loss: 0.5096, validation loss: 0.3824
2024-06-02 03:32:36 [INFO]: Epoch 075 - training loss: 0.5084, validation loss: 0.3788
2024-06-02 03:32:42 [INFO]: Epoch 076 - training loss: 0.5086, validation loss: 0.3835
2024-06-02 03:32:47 [INFO]: Epoch 077 - training loss: 0.5130, validation loss: 0.3796
2024-06-02 03:32:52 [INFO]: Epoch 078 - training loss: 0.5095, validation loss: 0.3791
2024-06-02 03:32:58 [INFO]: Epoch 079 - training loss: 0.5118, validation loss: 0.3749
2024-06-02 03:33:03 [INFO]: Epoch 080 - training loss: 0.5110, validation loss: 0.3829
2024-06-02 03:33:09 [INFO]: Epoch 081 - training loss: 0.5087, validation loss: 0.3758
2024-06-02 03:33:14 [INFO]: Epoch 082 - training loss: 0.5088, validation loss: 0.3767
2024-06-02 03:33:19 [INFO]: Epoch 083 - training loss: 0.5116, validation loss: 0.3728
2024-06-02 03:33:25 [INFO]: Epoch 084 - training loss: 0.5086, validation loss: 0.3751
2024-06-02 03:33:30 [INFO]: Epoch 085 - training loss: 0.5093, validation loss: 0.3751
2024-06-02 03:33:36 [INFO]: Epoch 086 - training loss: 0.5101, validation loss: 0.3778
2024-06-02 03:33:41 [INFO]: Epoch 087 - training loss: 0.5022, validation loss: 0.3717
2024-06-02 03:33:47 [INFO]: Epoch 088 - training loss: 0.5115, validation loss: 0.3724
2024-06-02 03:33:52 [INFO]: Epoch 089 - training loss: 0.5064, validation loss: 0.3718
2024-06-02 03:33:58 [INFO]: Epoch 090 - training loss: 0.5086, validation loss: 0.3716
2024-06-02 03:34:03 [INFO]: Epoch 091 - training loss: 0.5083, validation loss: 0.3716
2024-06-02 03:34:09 [INFO]: Epoch 092 - training loss: 0.5093, validation loss: 0.3772
2024-06-02 03:34:14 [INFO]: Epoch 093 - training loss: 0.5031, validation loss: 0.3672
2024-06-02 03:34:20 [INFO]: Epoch 094 - training loss: 0.5072, validation loss: 0.3719
2024-06-02 03:34:25 [INFO]: Epoch 095 - training loss: 0.5057, validation loss: 0.3720
2024-06-02 03:34:31 [INFO]: Epoch 096 - training loss: 0.5053, validation loss: 0.3668
2024-06-02 03:34:36 [INFO]: Epoch 097 - training loss: 0.5065, validation loss: 0.3724
2024-06-02 03:34:42 [INFO]: Epoch 098 - training loss: 0.5062, validation loss: 0.3685
2024-06-02 03:34:47 [INFO]: Epoch 099 - training loss: 0.5074, validation loss: 0.3720
2024-06-02 03:34:53 [INFO]: Epoch 100 - training loss: 0.5105, validation loss: 0.3730
2024-06-02 03:34:53 [INFO]: Finished training. The best model is from epoch#96.
2024-06-02 03:35:06 [INFO]: Saved the model to results_point_rate01/PeMS/SCINet_PeMS/round_0/20240602_T032356/SCINet.pypots
2024-06-02 03:35:06 [INFO]: Successfully saved to results_point_rate01/PeMS/SCINet_PeMS/round_0/imputation.pkl
2024-06-02 03:35:06 [INFO]: Round0 - SCINet on PeMS: MAE=0.3688, MSE=0.5514, MRE=0.4572
2024-06-02 03:35:06 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 03:35:06 [INFO]: Using the given device: cuda:0
2024-06-02 03:35:06 [INFO]: Model files will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_1/20240602_T033506
2024-06-02 03:35:06 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_1/20240602_T033506/tensorboard
2024-06-02 03:35:21 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-02 03:35:27 [INFO]: Epoch 001 - training loss: 2.0389, validation loss: 1.2723
2024-06-02 03:35:32 [INFO]: Epoch 002 - training loss: 1.3084, validation loss: 1.0326
2024-06-02 03:35:37 [INFO]: Epoch 003 - training loss: 1.0844, validation loss: 0.9820
2024-06-02 03:35:43 [INFO]: Epoch 004 - training loss: 0.9931, validation loss: 0.9465
2024-06-02 03:35:48 [INFO]: Epoch 005 - training loss: 0.9378, validation loss: 0.9191
2024-06-02 03:35:53 [INFO]: Epoch 006 - training loss: 0.8887, validation loss: 0.8796
2024-06-02 03:35:58 [INFO]: Epoch 007 - training loss: 0.8399, validation loss: 0.8020
2024-06-02 03:36:03 [INFO]: Epoch 008 - training loss: 0.7901, validation loss: 0.7581
2024-06-02 03:36:09 [INFO]: Epoch 009 - training loss: 0.7544, validation loss: 0.7231
2024-06-02 03:36:13 [INFO]: Epoch 010 - training loss: 0.7250, validation loss: 0.6870
2024-06-02 03:36:18 [INFO]: Epoch 011 - training loss: 0.7036, validation loss: 0.6700
2024-06-02 03:36:24 [INFO]: Epoch 012 - training loss: 0.6850, validation loss: 0.6457
2024-06-02 03:36:29 [INFO]: Epoch 013 - training loss: 0.6614, validation loss: 0.6123
2024-06-02 03:36:34 [INFO]: Epoch 014 - training loss: 0.6517, validation loss: 0.5761
2024-06-02 03:36:39 [INFO]: Epoch 015 - training loss: 0.6336, validation loss: 0.5570
2024-06-02 03:36:44 [INFO]: Epoch 016 - training loss: 0.6132, validation loss: 0.5247
2024-06-02 03:36:50 [INFO]: Epoch 017 - training loss: 0.5930, validation loss: 0.4943
2024-06-02 03:36:55 [INFO]: Epoch 018 - training loss: 0.5810, validation loss: 0.4680
2024-06-02 03:37:00 [INFO]: Epoch 019 - training loss: 0.5663, validation loss: 0.4625
2024-06-02 03:37:05 [INFO]: Epoch 020 - training loss: 0.5569, validation loss: 0.4488
2024-06-02 03:37:10 [INFO]: Epoch 021 - training loss: 0.5544, validation loss: 0.4407
2024-06-02 03:37:16 [INFO]: Epoch 022 - training loss: 0.5472, validation loss: 0.4376
2024-06-02 03:37:21 [INFO]: Epoch 023 - training loss: 0.5421, validation loss: 0.4373
2024-06-02 03:37:26 [INFO]: Epoch 024 - training loss: 0.5386, validation loss: 0.4267
2024-06-02 03:37:31 [INFO]: Epoch 025 - training loss: 0.5334, validation loss: 0.4244
2024-06-02 03:37:37 [INFO]: Epoch 026 - training loss: 0.5398, validation loss: 0.4240
2024-06-02 03:37:42 [INFO]: Epoch 027 - training loss: 0.5357, validation loss: 0.4186
2024-06-02 03:37:47 [INFO]: Epoch 028 - training loss: 0.5285, validation loss: 0.4146
2024-06-02 03:37:52 [INFO]: Epoch 029 - training loss: 0.5279, validation loss: 0.4128
2024-06-02 03:37:57 [INFO]: Epoch 030 - training loss: 0.5276, validation loss: 0.4132
2024-06-02 03:38:02 [INFO]: Epoch 031 - training loss: 0.5214, validation loss: 0.4058
2024-06-02 03:38:08 [INFO]: Epoch 032 - training loss: 0.5249, validation loss: 0.4018
2024-06-02 03:38:13 [INFO]: Epoch 033 - training loss: 0.5232, validation loss: 0.4059
2024-06-02 03:38:18 [INFO]: Epoch 034 - training loss: 0.5237, validation loss: 0.4045
2024-06-02 03:38:23 [INFO]: Epoch 035 - training loss: 0.5246, validation loss: 0.4041
2024-06-02 03:38:28 [INFO]: Epoch 036 - training loss: 0.5219, validation loss: 0.4054
2024-06-02 03:38:34 [INFO]: Epoch 037 - training loss: 0.5208, validation loss: 0.4022
2024-06-02 03:38:39 [INFO]: Epoch 038 - training loss: 0.5195, validation loss: 0.4021
2024-06-02 03:38:44 [INFO]: Epoch 039 - training loss: 0.5189, validation loss: 0.3985
2024-06-02 03:38:49 [INFO]: Epoch 040 - training loss: 0.5151, validation loss: 0.3956
2024-06-02 03:38:55 [INFO]: Epoch 041 - training loss: 0.5195, validation loss: 0.4016
2024-06-02 03:39:00 [INFO]: Epoch 042 - training loss: 0.5165, validation loss: 0.3955
2024-06-02 03:39:04 [INFO]: Epoch 043 - training loss: 0.5176, validation loss: 0.3891
2024-06-02 03:39:10 [INFO]: Epoch 044 - training loss: 0.5169, validation loss: 0.3936
2024-06-02 03:39:15 [INFO]: Epoch 045 - training loss: 0.5141, validation loss: 0.3948
2024-06-02 03:39:20 [INFO]: Epoch 046 - training loss: 0.5143, validation loss: 0.3965
2024-06-02 03:39:25 [INFO]: Epoch 047 - training loss: 0.5151, validation loss: 0.3941
2024-06-02 03:39:31 [INFO]: Epoch 048 - training loss: 0.5113, validation loss: 0.3883
2024-06-02 03:39:36 [INFO]: Epoch 049 - training loss: 0.5137, validation loss: 0.3912
2024-06-02 03:39:41 [INFO]: Epoch 050 - training loss: 0.5132, validation loss: 0.3883
2024-06-02 03:39:46 [INFO]: Epoch 051 - training loss: 0.5125, validation loss: 0.3890
2024-06-02 03:39:52 [INFO]: Epoch 052 - training loss: 0.5099, validation loss: 0.3868
2024-06-02 03:39:57 [INFO]: Epoch 053 - training loss: 0.5094, validation loss: 0.3824
2024-06-02 03:40:02 [INFO]: Epoch 054 - training loss: 0.5103, validation loss: 0.3837
2024-06-02 03:40:07 [INFO]: Epoch 055 - training loss: 0.5043, validation loss: 0.3855
2024-06-02 03:40:12 [INFO]: Epoch 056 - training loss: 0.5133, validation loss: 0.3809
2024-06-02 03:40:18 [INFO]: Epoch 057 - training loss: 0.5088, validation loss: 0.3825
2024-06-02 03:40:23 [INFO]: Epoch 058 - training loss: 0.5119, validation loss: 0.3819
2024-06-02 03:40:28 [INFO]: Epoch 059 - training loss: 0.5059, validation loss: 0.3760
2024-06-02 03:40:33 [INFO]: Epoch 060 - training loss: 0.5081, validation loss: 0.3838
2024-06-02 03:40:39 [INFO]: Epoch 061 - training loss: 0.5098, validation loss: 0.3795
2024-06-02 03:40:44 [INFO]: Epoch 062 - training loss: 0.5066, validation loss: 0.3778
2024-06-02 03:40:49 [INFO]: Epoch 063 - training loss: 0.5062, validation loss: 0.3808
2024-06-02 03:40:54 [INFO]: Epoch 064 - training loss: 0.5064, validation loss: 0.3787
2024-06-02 03:40:59 [INFO]: Epoch 065 - training loss: 0.5100, validation loss: 0.3771
2024-06-02 03:41:05 [INFO]: Epoch 066 - training loss: 0.5106, validation loss: 0.3776
2024-06-02 03:41:10 [INFO]: Epoch 067 - training loss: 0.5086, validation loss: 0.3772
2024-06-02 03:41:15 [INFO]: Epoch 068 - training loss: 0.5071, validation loss: 0.3767
2024-06-02 03:41:20 [INFO]: Epoch 069 - training loss: 0.5063, validation loss: 0.3738
2024-06-02 03:41:26 [INFO]: Epoch 070 - training loss: 0.5066, validation loss: 0.3792
2024-06-02 03:41:31 [INFO]: Epoch 071 - training loss: 0.5046, validation loss: 0.3740
2024-06-02 03:41:36 [INFO]: Epoch 072 - training loss: 0.5070, validation loss: 0.3754
2024-06-02 03:41:41 [INFO]: Epoch 073 - training loss: 0.5048, validation loss: 0.3742
2024-06-02 03:41:46 [INFO]: Epoch 074 - training loss: 0.5072, validation loss: 0.3735
2024-06-02 03:41:52 [INFO]: Epoch 075 - training loss: 0.5044, validation loss: 0.3803
2024-06-02 03:41:56 [INFO]: Epoch 076 - training loss: 0.5071, validation loss: 0.3732
2024-06-02 03:41:59 [INFO]: Epoch 077 - training loss: 0.5055, validation loss: 0.3733
2024-06-02 03:42:02 [INFO]: Epoch 078 - training loss: 0.5025, validation loss: 0.3714
2024-06-02 03:42:06 [INFO]: Epoch 079 - training loss: 0.5028, validation loss: 0.3771
2024-06-02 03:42:09 [INFO]: Epoch 080 - training loss: 0.5072, validation loss: 0.3690
2024-06-02 03:42:12 [INFO]: Epoch 081 - training loss: 0.5086, validation loss: 0.3666
2024-06-02 03:42:15 [INFO]: Epoch 082 - training loss: 0.5104, validation loss: 0.3666
2024-06-02 03:42:18 [INFO]: Epoch 083 - training loss: 0.5056, validation loss: 0.3697
2024-06-02 03:42:22 [INFO]: Epoch 084 - training loss: 0.5102, validation loss: 0.3690
2024-06-02 03:42:25 [INFO]: Epoch 085 - training loss: 0.5025, validation loss: 0.3644
2024-06-02 03:42:28 [INFO]: Epoch 086 - training loss: 0.5046, validation loss: 0.3646
2024-06-02 03:42:31 [INFO]: Epoch 087 - training loss: 0.5035, validation loss: 0.3740
2024-06-02 03:42:34 [INFO]: Epoch 088 - training loss: 0.5074, validation loss: 0.3619
2024-06-02 03:42:38 [INFO]: Epoch 089 - training loss: 0.5073, validation loss: 0.3660
2024-06-02 03:42:41 [INFO]: Epoch 090 - training loss: 0.5040, validation loss: 0.3681
2024-06-02 03:42:44 [INFO]: Epoch 091 - training loss: 0.5022, validation loss: 0.3629
2024-06-02 03:42:47 [INFO]: Epoch 092 - training loss: 0.5049, validation loss: 0.3715
2024-06-02 03:42:50 [INFO]: Epoch 093 - training loss: 0.5032, validation loss: 0.3645
2024-06-02 03:42:54 [INFO]: Epoch 094 - training loss: 0.5025, validation loss: 0.3638
2024-06-02 03:42:57 [INFO]: Epoch 095 - training loss: 0.5053, validation loss: 0.3735
2024-06-02 03:43:00 [INFO]: Epoch 096 - training loss: 0.5011, validation loss: 0.3672
2024-06-02 03:43:03 [INFO]: Epoch 097 - training loss: 0.5029, validation loss: 0.3666
2024-06-02 03:43:07 [INFO]: Epoch 098 - training loss: 0.5016, validation loss: 0.3685
2024-06-02 03:43:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 03:43:07 [INFO]: Finished training. The best model is from epoch#88.
2024-06-02 03:43:12 [INFO]: Saved the model to results_point_rate01/PeMS/SCINet_PeMS/round_1/20240602_T033506/SCINet.pypots
2024-06-02 03:43:13 [INFO]: Successfully saved to results_point_rate01/PeMS/SCINet_PeMS/round_1/imputation.pkl
2024-06-02 03:43:13 [INFO]: Round1 - SCINet on PeMS: MAE=0.3715, MSE=0.5490, MRE=0.4605
2024-06-02 03:43:13 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 03:43:13 [INFO]: Using the given device: cuda:0
2024-06-02 03:43:13 [INFO]: Model files will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_2/20240602_T034313
2024-06-02 03:43:13 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_2/20240602_T034313/tensorboard
2024-06-02 03:43:19 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-02 03:43:22 [INFO]: Epoch 001 - training loss: 2.1557, validation loss: 1.3071
2024-06-02 03:43:25 [INFO]: Epoch 002 - training loss: 1.3497, validation loss: 1.0573
2024-06-02 03:43:28 [INFO]: Epoch 003 - training loss: 1.0987, validation loss: 0.9821
2024-06-02 03:43:32 [INFO]: Epoch 004 - training loss: 0.9933, validation loss: 0.9501
2024-06-02 03:43:35 [INFO]: Epoch 005 - training loss: 0.9371, validation loss: 0.9277
2024-06-02 03:43:38 [INFO]: Epoch 006 - training loss: 0.8848, validation loss: 0.8847
2024-06-02 03:43:41 [INFO]: Epoch 007 - training loss: 0.8422, validation loss: 0.8849
2024-06-02 03:43:44 [INFO]: Epoch 008 - training loss: 0.8047, validation loss: 0.8616
2024-06-02 03:43:48 [INFO]: Epoch 009 - training loss: 0.7722, validation loss: 0.8500
2024-06-02 03:43:51 [INFO]: Epoch 010 - training loss: 0.7442, validation loss: 0.8413
2024-06-02 03:43:54 [INFO]: Epoch 011 - training loss: 0.7242, validation loss: 0.8136
2024-06-02 03:43:57 [INFO]: Epoch 012 - training loss: 0.7009, validation loss: 0.7908
2024-06-02 03:44:00 [INFO]: Epoch 013 - training loss: 0.6813, validation loss: 0.7619
2024-06-02 03:44:04 [INFO]: Epoch 014 - training loss: 0.6627, validation loss: 0.7534
2024-06-02 03:44:07 [INFO]: Epoch 015 - training loss: 0.6604, validation loss: 0.7074
2024-06-02 03:44:10 [INFO]: Epoch 016 - training loss: 0.6496, validation loss: 0.6921
2024-06-02 03:44:13 [INFO]: Epoch 017 - training loss: 0.6368, validation loss: 0.6881
2024-06-02 03:44:17 [INFO]: Epoch 018 - training loss: 0.6303, validation loss: 0.6538
2024-06-02 03:44:20 [INFO]: Epoch 019 - training loss: 0.6224, validation loss: 0.6547
2024-06-02 03:44:23 [INFO]: Epoch 020 - training loss: 0.6164, validation loss: 0.6541
2024-06-02 03:44:26 [INFO]: Epoch 021 - training loss: 0.6123, validation loss: 0.6524
2024-06-02 03:44:29 [INFO]: Epoch 022 - training loss: 0.6122, validation loss: 0.6385
2024-06-02 03:44:33 [INFO]: Epoch 023 - training loss: 0.6079, validation loss: 0.6186
2024-06-02 03:44:36 [INFO]: Epoch 024 - training loss: 0.6014, validation loss: 0.6427
2024-06-02 03:44:39 [INFO]: Epoch 025 - training loss: 0.6037, validation loss: 0.6145
2024-06-02 03:44:42 [INFO]: Epoch 026 - training loss: 0.6004, validation loss: 0.6308
2024-06-02 03:44:45 [INFO]: Epoch 027 - training loss: 0.5955, validation loss: 0.6252
2024-06-02 03:44:49 [INFO]: Epoch 028 - training loss: 0.5969, validation loss: 0.6043
2024-06-02 03:44:52 [INFO]: Epoch 029 - training loss: 0.5917, validation loss: 0.6110
2024-06-02 03:44:55 [INFO]: Epoch 030 - training loss: 0.5904, validation loss: 0.6065
2024-06-02 03:44:58 [INFO]: Epoch 031 - training loss: 0.5911, validation loss: 0.6014
2024-06-02 03:45:01 [INFO]: Epoch 032 - training loss: 0.5939, validation loss: 0.6032
2024-06-02 03:45:05 [INFO]: Epoch 033 - training loss: 0.5882, validation loss: 0.6039
2024-06-02 03:45:08 [INFO]: Epoch 034 - training loss: 0.5883, validation loss: 0.6012
2024-06-02 03:45:11 [INFO]: Epoch 035 - training loss: 0.5879, validation loss: 0.5897
2024-06-02 03:45:14 [INFO]: Epoch 036 - training loss: 0.5865, validation loss: 0.5914
2024-06-02 03:45:17 [INFO]: Epoch 037 - training loss: 0.5864, validation loss: 0.5879
2024-06-02 03:45:21 [INFO]: Epoch 038 - training loss: 0.5843, validation loss: 0.5850
2024-06-02 03:45:24 [INFO]: Epoch 039 - training loss: 0.5809, validation loss: 0.5849
2024-06-02 03:45:27 [INFO]: Epoch 040 - training loss: 0.5839, validation loss: 0.5814
2024-06-02 03:45:30 [INFO]: Epoch 041 - training loss: 0.5799, validation loss: 0.5732
2024-06-02 03:45:34 [INFO]: Epoch 042 - training loss: 0.5808, validation loss: 0.5690
2024-06-02 03:45:37 [INFO]: Epoch 043 - training loss: 0.5788, validation loss: 0.5695
2024-06-02 03:45:40 [INFO]: Epoch 044 - training loss: 0.5758, validation loss: 0.5691
2024-06-02 03:45:43 [INFO]: Epoch 045 - training loss: 0.5731, validation loss: 0.5681
2024-06-02 03:45:46 [INFO]: Epoch 046 - training loss: 0.5755, validation loss: 0.5731
2024-06-02 03:45:50 [INFO]: Epoch 047 - training loss: 0.5755, validation loss: 0.5645
2024-06-02 03:45:53 [INFO]: Epoch 048 - training loss: 0.5768, validation loss: 0.5581
2024-06-02 03:45:56 [INFO]: Epoch 049 - training loss: 0.5726, validation loss: 0.5580
2024-06-02 03:45:59 [INFO]: Epoch 050 - training loss: 0.5729, validation loss: 0.5542
2024-06-02 03:46:02 [INFO]: Epoch 051 - training loss: 0.5702, validation loss: 0.5730
2024-06-02 03:46:06 [INFO]: Epoch 052 - training loss: 0.5691, validation loss: 0.5778
2024-06-02 03:46:09 [INFO]: Epoch 053 - training loss: 0.5723, validation loss: 0.5549
2024-06-02 03:46:12 [INFO]: Epoch 054 - training loss: 0.5744, validation loss: 0.5404
2024-06-02 03:46:15 [INFO]: Epoch 055 - training loss: 0.5786, validation loss: 0.5505
2024-06-02 03:46:19 [INFO]: Epoch 056 - training loss: 0.5753, validation loss: 0.5533
2024-06-02 03:46:22 [INFO]: Epoch 057 - training loss: 0.5701, validation loss: 0.5493
2024-06-02 03:46:25 [INFO]: Epoch 058 - training loss: 0.5645, validation loss: 0.5569
2024-06-02 03:46:28 [INFO]: Epoch 059 - training loss: 0.5679, validation loss: 0.5626
2024-06-02 03:46:31 [INFO]: Epoch 060 - training loss: 0.5677, validation loss: 0.5563
2024-06-02 03:46:35 [INFO]: Epoch 061 - training loss: 0.5669, validation loss: 0.5646
2024-06-02 03:46:38 [INFO]: Epoch 062 - training loss: 0.5669, validation loss: 0.5527
2024-06-02 03:46:41 [INFO]: Epoch 063 - training loss: 0.5665, validation loss: 0.5581
2024-06-02 03:46:44 [INFO]: Epoch 064 - training loss: 0.5644, validation loss: 0.5564
2024-06-02 03:46:44 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 03:46:44 [INFO]: Finished training. The best model is from epoch#54.
2024-06-02 03:46:50 [INFO]: Saved the model to results_point_rate01/PeMS/SCINet_PeMS/round_2/20240602_T034313/SCINet.pypots
2024-06-02 03:46:50 [INFO]: Successfully saved to results_point_rate01/PeMS/SCINet_PeMS/round_2/imputation.pkl
2024-06-02 03:46:50 [INFO]: Round2 - SCINet on PeMS: MAE=0.5149, MSE=0.8950, MRE=0.6383
2024-06-02 03:46:50 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 03:46:50 [INFO]: Using the given device: cuda:0
2024-06-02 03:46:50 [INFO]: Model files will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_3/20240602_T034650
2024-06-02 03:46:50 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_3/20240602_T034650/tensorboard
2024-06-02 03:46:56 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-02 03:47:00 [INFO]: Epoch 001 - training loss: 2.0404, validation loss: 1.1575
2024-06-02 03:47:03 [INFO]: Epoch 002 - training loss: 1.2572, validation loss: 0.9943
2024-06-02 03:47:06 [INFO]: Epoch 003 - training loss: 1.0288, validation loss: 0.9247
2024-06-02 03:47:09 [INFO]: Epoch 004 - training loss: 0.9421, validation loss: 0.8700
2024-06-02 03:47:12 [INFO]: Epoch 005 - training loss: 0.8936, validation loss: 0.8369
2024-06-02 03:47:16 [INFO]: Epoch 006 - training loss: 0.8510, validation loss: 0.8285
2024-06-02 03:47:19 [INFO]: Epoch 007 - training loss: 0.8152, validation loss: 0.8074
2024-06-02 03:47:22 [INFO]: Epoch 008 - training loss: 0.7913, validation loss: 0.7956
2024-06-02 03:47:25 [INFO]: Epoch 009 - training loss: 0.7640, validation loss: 0.7728
2024-06-02 03:47:28 [INFO]: Epoch 010 - training loss: 0.7401, validation loss: 0.7759
2024-06-02 03:47:32 [INFO]: Epoch 011 - training loss: 0.7141, validation loss: 0.7827
2024-06-02 03:47:35 [INFO]: Epoch 012 - training loss: 0.7026, validation loss: 0.7713
2024-06-02 03:47:38 [INFO]: Epoch 013 - training loss: 0.6909, validation loss: 0.7706
2024-06-02 03:47:41 [INFO]: Epoch 014 - training loss: 0.6769, validation loss: 0.7988
2024-06-02 03:47:44 [INFO]: Epoch 015 - training loss: 0.6658, validation loss: 0.7665
2024-06-02 03:47:48 [INFO]: Epoch 016 - training loss: 0.6644, validation loss: 0.7385
2024-06-02 03:47:51 [INFO]: Epoch 017 - training loss: 0.6566, validation loss: 0.7577
2024-06-02 03:47:54 [INFO]: Epoch 018 - training loss: 0.6514, validation loss: 0.7171
2024-06-02 03:47:57 [INFO]: Epoch 019 - training loss: 0.6426, validation loss: 0.7023
2024-06-02 03:48:00 [INFO]: Epoch 020 - training loss: 0.6444, validation loss: 0.6978
2024-06-02 03:48:04 [INFO]: Epoch 021 - training loss: 0.6400, validation loss: 0.6993
2024-06-02 03:48:07 [INFO]: Epoch 022 - training loss: 0.6333, validation loss: 0.6709
2024-06-02 03:48:10 [INFO]: Epoch 023 - training loss: 0.6287, validation loss: 0.6691
2024-06-02 03:48:13 [INFO]: Epoch 024 - training loss: 0.6285, validation loss: 0.6333
2024-06-02 03:48:16 [INFO]: Epoch 025 - training loss: 0.6275, validation loss: 0.6566
2024-06-02 03:48:20 [INFO]: Epoch 026 - training loss: 0.6262, validation loss: 0.6480
2024-06-02 03:48:23 [INFO]: Epoch 027 - training loss: 0.6292, validation loss: 0.6490
2024-06-02 03:48:26 [INFO]: Epoch 028 - training loss: 0.6207, validation loss: 0.6257
2024-06-02 03:48:29 [INFO]: Epoch 029 - training loss: 0.6167, validation loss: 0.6308
2024-06-02 03:48:32 [INFO]: Epoch 030 - training loss: 0.6221, validation loss: 0.6223
2024-06-02 03:48:36 [INFO]: Epoch 031 - training loss: 0.6169, validation loss: 0.6057
2024-06-02 03:48:39 [INFO]: Epoch 032 - training loss: 0.6132, validation loss: 0.6133
2024-06-02 03:48:42 [INFO]: Epoch 033 - training loss: 0.6114, validation loss: 0.6163
2024-06-02 03:48:45 [INFO]: Epoch 034 - training loss: 0.6125, validation loss: 0.6248
2024-06-02 03:48:48 [INFO]: Epoch 035 - training loss: 0.6163, validation loss: 0.6083
2024-06-02 03:48:52 [INFO]: Epoch 036 - training loss: 0.6105, validation loss: 0.6063
2024-06-02 03:48:55 [INFO]: Epoch 037 - training loss: 0.6136, validation loss: 0.6188
2024-06-02 03:48:58 [INFO]: Epoch 038 - training loss: 0.6133, validation loss: 0.6198
2024-06-02 03:49:01 [INFO]: Epoch 039 - training loss: 0.6137, validation loss: 0.6440
2024-06-02 03:49:04 [INFO]: Epoch 040 - training loss: 0.6075, validation loss: 0.6487
2024-06-02 03:49:08 [INFO]: Epoch 041 - training loss: 0.6067, validation loss: 0.6218
2024-06-02 03:49:08 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 03:49:08 [INFO]: Finished training. The best model is from epoch#31.
2024-06-02 03:49:13 [INFO]: Saved the model to results_point_rate01/PeMS/SCINet_PeMS/round_3/20240602_T034650/SCINet.pypots
2024-06-02 03:49:14 [INFO]: Successfully saved to results_point_rate01/PeMS/SCINet_PeMS/round_3/imputation.pkl
2024-06-02 03:49:14 [INFO]: Round3 - SCINet on PeMS: MAE=0.5629, MSE=0.9808, MRE=0.6978
2024-06-02 03:49:14 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 03:49:14 [INFO]: Using the given device: cuda:0
2024-06-02 03:49:14 [INFO]: Model files will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_4/20240602_T034914
2024-06-02 03:49:14 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/SCINet_PeMS/round_4/20240602_T034914/tensorboard
2024-06-02 03:49:20 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-02 03:49:23 [INFO]: Epoch 001 - training loss: 2.2128, validation loss: 1.3575
2024-06-02 03:49:26 [INFO]: Epoch 002 - training loss: 1.3578, validation loss: 1.0122
2024-06-02 03:49:29 [INFO]: Epoch 003 - training loss: 1.0951, validation loss: 0.9435
2024-06-02 03:49:33 [INFO]: Epoch 004 - training loss: 0.9952, validation loss: 0.8672
2024-06-02 03:49:36 [INFO]: Epoch 005 - training loss: 0.9335, validation loss: 0.8536
2024-06-02 03:49:39 [INFO]: Epoch 006 - training loss: 0.8785, validation loss: 0.8247
2024-06-02 03:49:42 [INFO]: Epoch 007 - training loss: 0.8465, validation loss: 0.8014
2024-06-02 03:49:45 [INFO]: Epoch 008 - training loss: 0.8078, validation loss: 0.7632
2024-06-02 03:49:49 [INFO]: Epoch 009 - training loss: 0.7787, validation loss: 0.7661
2024-06-02 03:49:52 [INFO]: Epoch 010 - training loss: 0.7485, validation loss: 0.7084
2024-06-02 03:49:55 [INFO]: Epoch 011 - training loss: 0.7174, validation loss: 0.7205
2024-06-02 03:49:58 [INFO]: Epoch 012 - training loss: 0.6966, validation loss: 0.7102
2024-06-02 03:50:01 [INFO]: Epoch 013 - training loss: 0.6805, validation loss: 0.6677
2024-06-02 03:50:05 [INFO]: Epoch 014 - training loss: 0.6683, validation loss: 0.6453
2024-06-02 03:50:08 [INFO]: Epoch 015 - training loss: 0.6619, validation loss: 0.7089
2024-06-02 03:50:11 [INFO]: Epoch 016 - training loss: 0.6478, validation loss: 0.6581
2024-06-02 03:50:14 [INFO]: Epoch 017 - training loss: 0.6431, validation loss: 0.6275
2024-06-02 03:50:17 [INFO]: Epoch 018 - training loss: 0.6454, validation loss: 0.6932
2024-06-02 03:50:21 [INFO]: Epoch 019 - training loss: 0.6451, validation loss: 0.6297
2024-06-02 03:50:24 [INFO]: Epoch 020 - training loss: 0.6465, validation loss: 0.7017
2024-06-02 03:50:27 [INFO]: Epoch 021 - training loss: 0.6357, validation loss: 0.7282
2024-06-02 03:50:30 [INFO]: Epoch 022 - training loss: 0.6362, validation loss: 0.7037
2024-06-02 03:50:34 [INFO]: Epoch 023 - training loss: 0.6270, validation loss: 0.6785
2024-06-02 03:50:37 [INFO]: Epoch 024 - training loss: 0.6283, validation loss: 0.7171
2024-06-02 03:50:40 [INFO]: Epoch 025 - training loss: 0.6284, validation loss: 0.7051
2024-06-02 03:50:43 [INFO]: Epoch 026 - training loss: 0.6236, validation loss: 0.7313
2024-06-02 03:50:46 [INFO]: Epoch 027 - training loss: 0.6213, validation loss: 0.6752
2024-06-02 03:50:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 03:50:46 [INFO]: Finished training. The best model is from epoch#17.
2024-06-02 03:50:52 [INFO]: Saved the model to results_point_rate01/PeMS/SCINet_PeMS/round_4/20240602_T034914/SCINet.pypots
2024-06-02 03:50:52 [INFO]: Successfully saved to results_point_rate01/PeMS/SCINet_PeMS/round_4/imputation.pkl
2024-06-02 03:50:52 [INFO]: Round4 - SCINet on PeMS: MAE=0.6176, MSE=1.1430, MRE=0.7656
2024-06-02 03:50:52 [INFO]: Done! Final results:
Averaged SCINet (n params: 1,143,027,230) on PeMS: MAE=0.4872 ± 0.10091641250339314, MSE=0.8238 ± 0.23721182662492213, MRE=0.6039 ± 0.1250959477193825, average inference time=0.30
