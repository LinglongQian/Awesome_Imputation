2024-06-02 01:26:44 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 01:26:44 [INFO]: Using the given device: cuda:0
2024-06-02 01:26:45 [INFO]: Model files will be saved to results_point_rate01/PeMS/Informer_PeMS/round_0/20240602_T012645
2024-06-02 01:26:45 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Informer_PeMS/round_0/20240602_T012645/tensorboard
2024-06-02 01:26:45 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 13,149,022
2024-06-02 01:26:50 [INFO]: Epoch 001 - training loss: 0.9265, validation loss: 0.5896
2024-06-02 01:26:53 [INFO]: Epoch 002 - training loss: 0.6156, validation loss: 0.5199
2024-06-02 01:26:56 [INFO]: Epoch 003 - training loss: 0.5602, validation loss: 0.4961
2024-06-02 01:26:58 [INFO]: Epoch 004 - training loss: 0.5267, validation loss: 0.4877
2024-06-02 01:27:00 [INFO]: Epoch 005 - training loss: 0.5145, validation loss: 0.4792
2024-06-02 01:27:03 [INFO]: Epoch 006 - training loss: 0.4976, validation loss: 0.4763
2024-06-02 01:27:05 [INFO]: Epoch 007 - training loss: 0.4906, validation loss: 0.4697
2024-06-02 01:27:08 [INFO]: Epoch 008 - training loss: 0.4797, validation loss: 0.4645
2024-06-02 01:27:10 [INFO]: Epoch 009 - training loss: 0.4691, validation loss: 0.4610
2024-06-02 01:27:13 [INFO]: Epoch 010 - training loss: 0.4582, validation loss: 0.4669
2024-06-02 01:27:15 [INFO]: Epoch 011 - training loss: 0.4540, validation loss: 0.4655
2024-06-02 01:27:18 [INFO]: Epoch 012 - training loss: 0.4452, validation loss: 0.4547
2024-06-02 01:27:21 [INFO]: Epoch 013 - training loss: 0.4383, validation loss: 0.4498
2024-06-02 01:27:23 [INFO]: Epoch 014 - training loss: 0.4341, validation loss: 0.4502
2024-06-02 01:27:25 [INFO]: Epoch 015 - training loss: 0.4255, validation loss: 0.4457
2024-06-02 01:27:28 [INFO]: Epoch 016 - training loss: 0.4185, validation loss: 0.4462
2024-06-02 01:27:31 [INFO]: Epoch 017 - training loss: 0.4143, validation loss: 0.4417
2024-06-02 01:27:33 [INFO]: Epoch 018 - training loss: 0.4184, validation loss: 0.4413
2024-06-02 01:27:35 [INFO]: Epoch 019 - training loss: 0.4052, validation loss: 0.4388
2024-06-02 01:27:38 [INFO]: Epoch 020 - training loss: 0.3987, validation loss: 0.4379
2024-06-02 01:27:40 [INFO]: Epoch 021 - training loss: 0.3986, validation loss: 0.4334
2024-06-02 01:27:42 [INFO]: Epoch 022 - training loss: 0.3951, validation loss: 0.4379
2024-06-02 01:27:45 [INFO]: Epoch 023 - training loss: 0.3877, validation loss: 0.4281
2024-06-02 01:27:48 [INFO]: Epoch 024 - training loss: 0.3909, validation loss: 0.4289
2024-06-02 01:27:50 [INFO]: Epoch 025 - training loss: 0.3833, validation loss: 0.4287
2024-06-02 01:27:53 [INFO]: Epoch 026 - training loss: 0.3752, validation loss: 0.4243
2024-06-02 01:27:55 [INFO]: Epoch 027 - training loss: 0.3708, validation loss: 0.4225
2024-06-02 01:27:57 [INFO]: Epoch 028 - training loss: 0.3684, validation loss: 0.4241
2024-06-02 01:27:59 [INFO]: Epoch 029 - training loss: 0.3714, validation loss: 0.4330
2024-06-02 01:28:02 [INFO]: Epoch 030 - training loss: 0.3728, validation loss: 0.4213
2024-06-02 01:28:05 [INFO]: Epoch 031 - training loss: 0.3593, validation loss: 0.4205
2024-06-02 01:28:07 [INFO]: Epoch 032 - training loss: 0.3553, validation loss: 0.4224
2024-06-02 01:28:09 [INFO]: Epoch 033 - training loss: 0.3528, validation loss: 0.4176
2024-06-02 01:28:12 [INFO]: Epoch 034 - training loss: 0.3529, validation loss: 0.4160
2024-06-02 01:28:15 [INFO]: Epoch 035 - training loss: 0.3511, validation loss: 0.4179
2024-06-02 01:28:17 [INFO]: Epoch 036 - training loss: 0.3481, validation loss: 0.4154
2024-06-02 01:28:20 [INFO]: Epoch 037 - training loss: 0.3434, validation loss: 0.4148
2024-06-02 01:28:22 [INFO]: Epoch 038 - training loss: 0.3440, validation loss: 0.4171
2024-06-02 01:28:25 [INFO]: Epoch 039 - training loss: 0.3399, validation loss: 0.4165
2024-06-02 01:28:27 [INFO]: Epoch 040 - training loss: 0.3364, validation loss: 0.4133
2024-06-02 01:28:30 [INFO]: Epoch 041 - training loss: 0.3377, validation loss: 0.4101
2024-06-02 01:28:32 [INFO]: Epoch 042 - training loss: 0.3395, validation loss: 0.4132
2024-06-02 01:28:34 [INFO]: Epoch 043 - training loss: 0.3349, validation loss: 0.4112
2024-06-02 01:28:37 [INFO]: Epoch 044 - training loss: 0.3335, validation loss: 0.4135
2024-06-02 01:28:39 [INFO]: Epoch 045 - training loss: 0.3300, validation loss: 0.4064
2024-06-02 01:28:42 [INFO]: Epoch 046 - training loss: 0.3270, validation loss: 0.4081
2024-06-02 01:28:44 [INFO]: Epoch 047 - training loss: 0.3279, validation loss: 0.4057
2024-06-02 01:28:47 [INFO]: Epoch 048 - training loss: 0.3219, validation loss: 0.4043
2024-06-02 01:28:50 [INFO]: Epoch 049 - training loss: 0.3298, validation loss: 0.4125
2024-06-02 01:28:52 [INFO]: Epoch 050 - training loss: 0.3235, validation loss: 0.4062
2024-06-02 01:28:55 [INFO]: Epoch 051 - training loss: 0.3218, validation loss: 0.4121
2024-06-02 01:28:57 [INFO]: Epoch 052 - training loss: 0.3234, validation loss: 0.4089
2024-06-02 01:28:59 [INFO]: Epoch 053 - training loss: 0.3181, validation loss: 0.4017
2024-06-02 01:29:02 [INFO]: Epoch 054 - training loss: 0.3190, validation loss: 0.4059
2024-06-02 01:29:04 [INFO]: Epoch 055 - training loss: 0.3166, validation loss: 0.4009
2024-06-02 01:29:07 [INFO]: Epoch 056 - training loss: 0.3197, validation loss: 0.3993
2024-06-02 01:29:09 [INFO]: Epoch 057 - training loss: 0.3151, validation loss: 0.4033
2024-06-02 01:29:11 [INFO]: Epoch 058 - training loss: 0.3114, validation loss: 0.4033
2024-06-02 01:29:13 [INFO]: Epoch 059 - training loss: 0.3153, validation loss: 0.4013
2024-06-02 01:29:16 [INFO]: Epoch 060 - training loss: 0.3123, validation loss: 0.4032
2024-06-02 01:29:19 [INFO]: Epoch 061 - training loss: 0.3117, validation loss: 0.4003
2024-06-02 01:29:21 [INFO]: Epoch 062 - training loss: 0.3076, validation loss: 0.4029
2024-06-02 01:29:23 [INFO]: Epoch 063 - training loss: 0.3044, validation loss: 0.3988
2024-06-02 01:29:26 [INFO]: Epoch 064 - training loss: 0.3067, validation loss: 0.3997
2024-06-02 01:29:28 [INFO]: Epoch 065 - training loss: 0.3081, validation loss: 0.3992
2024-06-02 01:29:31 [INFO]: Epoch 066 - training loss: 0.3088, validation loss: 0.3987
2024-06-02 01:29:33 [INFO]: Epoch 067 - training loss: 0.3049, validation loss: 0.3948
2024-06-02 01:29:35 [INFO]: Epoch 068 - training loss: 0.3024, validation loss: 0.3965
2024-06-02 01:29:38 [INFO]: Epoch 069 - training loss: 0.2996, validation loss: 0.3941
2024-06-02 01:29:40 [INFO]: Epoch 070 - training loss: 0.3043, validation loss: 0.3952
2024-06-02 01:29:43 [INFO]: Epoch 071 - training loss: 0.3013, validation loss: 0.3952
2024-06-02 01:29:44 [INFO]: Epoch 072 - training loss: 0.2976, validation loss: 0.3936
2024-06-02 01:29:47 [INFO]: Epoch 073 - training loss: 0.2982, validation loss: 0.3929
2024-06-02 01:29:50 [INFO]: Epoch 074 - training loss: 0.3009, validation loss: 0.3963
2024-06-02 01:29:52 [INFO]: Epoch 075 - training loss: 0.2969, validation loss: 0.3950
2024-06-02 01:29:54 [INFO]: Epoch 076 - training loss: 0.2987, validation loss: 0.3893
2024-06-02 01:29:57 [INFO]: Epoch 077 - training loss: 0.2959, validation loss: 0.3929
2024-06-02 01:29:59 [INFO]: Epoch 078 - training loss: 0.2936, validation loss: 0.3898
2024-06-02 01:30:01 [INFO]: Epoch 079 - training loss: 0.2943, validation loss: 0.3919
2024-06-02 01:30:04 [INFO]: Epoch 080 - training loss: 0.2963, validation loss: 0.3952
2024-06-02 01:30:07 [INFO]: Epoch 081 - training loss: 0.2963, validation loss: 0.3906
2024-06-02 01:30:08 [INFO]: Epoch 082 - training loss: 0.2966, validation loss: 0.3934
2024-06-02 01:30:10 [INFO]: Epoch 083 - training loss: 0.2940, validation loss: 0.3934
2024-06-02 01:30:12 [INFO]: Epoch 084 - training loss: 0.2917, validation loss: 0.3885
2024-06-02 01:30:15 [INFO]: Epoch 085 - training loss: 0.2927, validation loss: 0.3927
2024-06-02 01:30:17 [INFO]: Epoch 086 - training loss: 0.2895, validation loss: 0.3892
2024-06-02 01:30:19 [INFO]: Epoch 087 - training loss: 0.2885, validation loss: 0.3903
2024-06-02 01:30:21 [INFO]: Epoch 088 - training loss: 0.2898, validation loss: 0.3886
2024-06-02 01:30:23 [INFO]: Epoch 089 - training loss: 0.2899, validation loss: 0.3901
2024-06-02 01:30:25 [INFO]: Epoch 090 - training loss: 0.2937, validation loss: 0.3869
2024-06-02 01:30:28 [INFO]: Epoch 091 - training loss: 0.2951, validation loss: 0.3914
2024-06-02 01:30:30 [INFO]: Epoch 092 - training loss: 0.2879, validation loss: 0.3896
2024-06-02 01:30:33 [INFO]: Epoch 093 - training loss: 0.2871, validation loss: 0.3878
2024-06-02 01:30:35 [INFO]: Epoch 094 - training loss: 0.2872, validation loss: 0.3909
2024-06-02 01:30:37 [INFO]: Epoch 095 - training loss: 0.2868, validation loss: 0.3848
2024-06-02 01:30:40 [INFO]: Epoch 096 - training loss: 0.2855, validation loss: 0.3899
2024-06-02 01:30:42 [INFO]: Epoch 097 - training loss: 0.2851, validation loss: 0.3867
2024-06-02 01:30:44 [INFO]: Epoch 098 - training loss: 0.2816, validation loss: 0.3893
2024-06-02 01:30:47 [INFO]: Epoch 099 - training loss: 0.2831, validation loss: 0.3845
2024-06-02 01:30:49 [INFO]: Epoch 100 - training loss: 0.2799, validation loss: 0.3855
2024-06-02 01:30:49 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 01:30:49 [INFO]: Saved the model to results_point_rate01/PeMS/Informer_PeMS/round_0/20240602_T012645/Informer.pypots
2024-06-02 01:30:49 [INFO]: Successfully saved to results_point_rate01/PeMS/Informer_PeMS/round_0/imputation.pkl
2024-06-02 01:30:49 [INFO]: Round0 - Informer on PeMS: MAE=0.3004, MSE=0.5691, MRE=0.3724
2024-06-02 01:30:49 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 01:30:49 [INFO]: Using the given device: cuda:0
2024-06-02 01:30:49 [INFO]: Model files will be saved to results_point_rate01/PeMS/Informer_PeMS/round_1/20240602_T013049
2024-06-02 01:30:49 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Informer_PeMS/round_1/20240602_T013049/tensorboard
2024-06-02 01:30:49 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 13,149,022
2024-06-02 01:30:52 [INFO]: Epoch 001 - training loss: 0.9093, validation loss: 0.5768
2024-06-02 01:30:54 [INFO]: Epoch 002 - training loss: 0.6171, validation loss: 0.5244
2024-06-02 01:30:57 [INFO]: Epoch 003 - training loss: 0.5548, validation loss: 0.4976
2024-06-02 01:30:59 [INFO]: Epoch 004 - training loss: 0.5347, validation loss: 0.4932
2024-06-02 01:31:01 [INFO]: Epoch 005 - training loss: 0.5097, validation loss: 0.4770
2024-06-02 01:31:04 [INFO]: Epoch 006 - training loss: 0.5009, validation loss: 0.4809
2024-06-02 01:31:06 [INFO]: Epoch 007 - training loss: 0.4906, validation loss: 0.4700
2024-06-02 01:31:08 [INFO]: Epoch 008 - training loss: 0.4789, validation loss: 0.4659
2024-06-02 01:31:10 [INFO]: Epoch 009 - training loss: 0.4695, validation loss: 0.4635
2024-06-02 01:31:13 [INFO]: Epoch 010 - training loss: 0.4562, validation loss: 0.4584
2024-06-02 01:31:15 [INFO]: Epoch 011 - training loss: 0.4497, validation loss: 0.4626
2024-06-02 01:31:17 [INFO]: Epoch 012 - training loss: 0.4459, validation loss: 0.4633
2024-06-02 01:31:19 [INFO]: Epoch 013 - training loss: 0.4379, validation loss: 0.4540
2024-06-02 01:31:21 [INFO]: Epoch 014 - training loss: 0.4320, validation loss: 0.4470
2024-06-02 01:31:24 [INFO]: Epoch 015 - training loss: 0.4269, validation loss: 0.4543
2024-06-02 01:31:26 [INFO]: Epoch 016 - training loss: 0.4251, validation loss: 0.4613
2024-06-02 01:31:28 [INFO]: Epoch 017 - training loss: 0.4195, validation loss: 0.4463
2024-06-02 01:31:31 [INFO]: Epoch 018 - training loss: 0.4128, validation loss: 0.4426
2024-06-02 01:31:33 [INFO]: Epoch 019 - training loss: 0.4074, validation loss: 0.4395
2024-06-02 01:31:35 [INFO]: Epoch 020 - training loss: 0.4004, validation loss: 0.4383
2024-06-02 01:31:37 [INFO]: Epoch 021 - training loss: 0.3992, validation loss: 0.4386
2024-06-02 01:31:40 [INFO]: Epoch 022 - training loss: 0.3903, validation loss: 0.4362
2024-06-02 01:31:42 [INFO]: Epoch 023 - training loss: 0.3868, validation loss: 0.4301
2024-06-02 01:31:44 [INFO]: Epoch 024 - training loss: 0.3798, validation loss: 0.4306
2024-06-02 01:31:46 [INFO]: Epoch 025 - training loss: 0.3773, validation loss: 0.4252
2024-06-02 01:31:48 [INFO]: Epoch 026 - training loss: 0.3745, validation loss: 0.4283
2024-06-02 01:31:51 [INFO]: Epoch 027 - training loss: 0.3699, validation loss: 0.4273
2024-06-02 01:31:52 [INFO]: Epoch 028 - training loss: 0.3666, validation loss: 0.4229
2024-06-02 01:31:54 [INFO]: Epoch 029 - training loss: 0.3672, validation loss: 0.4205
2024-06-02 01:31:55 [INFO]: Epoch 030 - training loss: 0.3672, validation loss: 0.4214
2024-06-02 01:31:57 [INFO]: Epoch 031 - training loss: 0.3608, validation loss: 0.4278
2024-06-02 01:31:58 [INFO]: Epoch 032 - training loss: 0.3589, validation loss: 0.4196
2024-06-02 01:32:00 [INFO]: Epoch 033 - training loss: 0.3541, validation loss: 0.4166
2024-06-02 01:32:01 [INFO]: Epoch 034 - training loss: 0.3534, validation loss: 0.4151
2024-06-02 01:32:03 [INFO]: Epoch 035 - training loss: 0.3521, validation loss: 0.4154
2024-06-02 01:32:04 [INFO]: Epoch 036 - training loss: 0.3518, validation loss: 0.4144
2024-06-02 01:32:06 [INFO]: Epoch 037 - training loss: 0.3441, validation loss: 0.4149
2024-06-02 01:32:07 [INFO]: Epoch 038 - training loss: 0.3414, validation loss: 0.4128
2024-06-02 01:32:09 [INFO]: Epoch 039 - training loss: 0.3419, validation loss: 0.4191
2024-06-02 01:32:10 [INFO]: Epoch 040 - training loss: 0.3394, validation loss: 0.4120
2024-06-02 01:32:12 [INFO]: Epoch 041 - training loss: 0.3361, validation loss: 0.4129
2024-06-02 01:32:13 [INFO]: Epoch 042 - training loss: 0.3336, validation loss: 0.4114
2024-06-02 01:32:15 [INFO]: Epoch 043 - training loss: 0.3328, validation loss: 0.4116
2024-06-02 01:32:16 [INFO]: Epoch 044 - training loss: 0.3300, validation loss: 0.4085
2024-06-02 01:32:18 [INFO]: Epoch 045 - training loss: 0.3326, validation loss: 0.4091
2024-06-02 01:32:19 [INFO]: Epoch 046 - training loss: 0.3311, validation loss: 0.4082
2024-06-02 01:32:21 [INFO]: Epoch 047 - training loss: 0.3272, validation loss: 0.4114
2024-06-02 01:32:22 [INFO]: Epoch 048 - training loss: 0.3271, validation loss: 0.4054
2024-06-02 01:32:24 [INFO]: Epoch 049 - training loss: 0.3325, validation loss: 0.4055
2024-06-02 01:32:25 [INFO]: Epoch 050 - training loss: 0.3250, validation loss: 0.4065
2024-06-02 01:32:27 [INFO]: Epoch 051 - training loss: 0.3228, validation loss: 0.4058
2024-06-02 01:32:28 [INFO]: Epoch 052 - training loss: 0.3260, validation loss: 0.4060
2024-06-02 01:32:30 [INFO]: Epoch 053 - training loss: 0.3209, validation loss: 0.4074
2024-06-02 01:32:31 [INFO]: Epoch 054 - training loss: 0.3172, validation loss: 0.4037
2024-06-02 01:32:33 [INFO]: Epoch 055 - training loss: 0.3169, validation loss: 0.4001
2024-06-02 01:32:34 [INFO]: Epoch 056 - training loss: 0.3133, validation loss: 0.4027
2024-06-02 01:32:36 [INFO]: Epoch 057 - training loss: 0.3124, validation loss: 0.3999
2024-06-02 01:32:37 [INFO]: Epoch 058 - training loss: 0.3134, validation loss: 0.4013
2024-06-02 01:32:39 [INFO]: Epoch 059 - training loss: 0.3141, validation loss: 0.3994
2024-06-02 01:32:40 [INFO]: Epoch 060 - training loss: 0.3133, validation loss: 0.3994
2024-06-02 01:32:41 [INFO]: Epoch 061 - training loss: 0.3112, validation loss: 0.4002
2024-06-02 01:32:43 [INFO]: Epoch 062 - training loss: 0.3090, validation loss: 0.3961
2024-06-02 01:32:45 [INFO]: Epoch 063 - training loss: 0.3062, validation loss: 0.3969
2024-06-02 01:32:46 [INFO]: Epoch 064 - training loss: 0.3061, validation loss: 0.3962
2024-06-02 01:32:48 [INFO]: Epoch 065 - training loss: 0.3063, validation loss: 0.3948
2024-06-02 01:32:49 [INFO]: Epoch 066 - training loss: 0.3054, validation loss: 0.3989
2024-06-02 01:32:51 [INFO]: Epoch 067 - training loss: 0.3055, validation loss: 0.3955
2024-06-02 01:32:52 [INFO]: Epoch 068 - training loss: 0.3037, validation loss: 0.3961
2024-06-02 01:32:54 [INFO]: Epoch 069 - training loss: 0.3012, validation loss: 0.3978
2024-06-02 01:32:55 [INFO]: Epoch 070 - training loss: 0.3024, validation loss: 0.3945
2024-06-02 01:32:57 [INFO]: Epoch 071 - training loss: 0.2996, validation loss: 0.3956
2024-06-02 01:32:58 [INFO]: Epoch 072 - training loss: 0.3024, validation loss: 0.3930
2024-06-02 01:33:00 [INFO]: Epoch 073 - training loss: 0.2998, validation loss: 0.3945
2024-06-02 01:33:01 [INFO]: Epoch 074 - training loss: 0.2999, validation loss: 0.3917
2024-06-02 01:33:03 [INFO]: Epoch 075 - training loss: 0.2943, validation loss: 0.3926
2024-06-02 01:33:04 [INFO]: Epoch 076 - training loss: 0.2934, validation loss: 0.3895
2024-06-02 01:33:06 [INFO]: Epoch 077 - training loss: 0.2941, validation loss: 0.3899
2024-06-02 01:33:07 [INFO]: Epoch 078 - training loss: 0.3008, validation loss: 0.3915
2024-06-02 01:33:09 [INFO]: Epoch 079 - training loss: 0.2949, validation loss: 0.3906
2024-06-02 01:33:10 [INFO]: Epoch 080 - training loss: 0.2948, validation loss: 0.3889
2024-06-02 01:33:12 [INFO]: Epoch 081 - training loss: 0.2972, validation loss: 0.3938
2024-06-02 01:33:13 [INFO]: Epoch 082 - training loss: 0.2946, validation loss: 0.3929
2024-06-02 01:33:15 [INFO]: Epoch 083 - training loss: 0.2953, validation loss: 0.3900
2024-06-02 01:33:16 [INFO]: Epoch 084 - training loss: 0.2886, validation loss: 0.3895
2024-06-02 01:33:17 [INFO]: Epoch 085 - training loss: 0.2911, validation loss: 0.3883
2024-06-02 01:33:19 [INFO]: Epoch 086 - training loss: 0.2878, validation loss: 0.3858
2024-06-02 01:33:20 [INFO]: Epoch 087 - training loss: 0.2904, validation loss: 0.3862
2024-06-02 01:33:22 [INFO]: Epoch 088 - training loss: 0.2909, validation loss: 0.3876
2024-06-02 01:33:23 [INFO]: Epoch 089 - training loss: 0.2912, validation loss: 0.3867
2024-06-02 01:33:25 [INFO]: Epoch 090 - training loss: 0.2879, validation loss: 0.3890
2024-06-02 01:33:26 [INFO]: Epoch 091 - training loss: 0.2889, validation loss: 0.3870
2024-06-02 01:33:28 [INFO]: Epoch 092 - training loss: 0.2872, validation loss: 0.3837
2024-06-02 01:33:29 [INFO]: Epoch 093 - training loss: 0.2846, validation loss: 0.3858
2024-06-02 01:33:31 [INFO]: Epoch 094 - training loss: 0.2822, validation loss: 0.3839
2024-06-02 01:33:32 [INFO]: Epoch 095 - training loss: 0.2839, validation loss: 0.3860
2024-06-02 01:33:34 [INFO]: Epoch 096 - training loss: 0.2825, validation loss: 0.3866
2024-06-02 01:33:35 [INFO]: Epoch 097 - training loss: 0.2858, validation loss: 0.3885
2024-06-02 01:33:37 [INFO]: Epoch 098 - training loss: 0.2892, validation loss: 0.3848
2024-06-02 01:33:38 [INFO]: Epoch 099 - training loss: 0.2866, validation loss: 0.3852
2024-06-02 01:33:40 [INFO]: Epoch 100 - training loss: 0.2830, validation loss: 0.3825
2024-06-02 01:33:40 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 01:33:40 [INFO]: Saved the model to results_point_rate01/PeMS/Informer_PeMS/round_1/20240602_T013049/Informer.pypots
2024-06-02 01:33:40 [INFO]: Successfully saved to results_point_rate01/PeMS/Informer_PeMS/round_1/imputation.pkl
2024-06-02 01:33:40 [INFO]: Round1 - Informer on PeMS: MAE=0.2996, MSE=0.5681, MRE=0.3714
2024-06-02 01:33:40 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 01:33:40 [INFO]: Using the given device: cuda:0
2024-06-02 01:33:40 [INFO]: Model files will be saved to results_point_rate01/PeMS/Informer_PeMS/round_2/20240602_T013340
2024-06-02 01:33:40 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Informer_PeMS/round_2/20240602_T013340/tensorboard
2024-06-02 01:33:40 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 13,149,022
2024-06-02 01:33:42 [INFO]: Epoch 001 - training loss: 0.9193, validation loss: 0.5753
2024-06-02 01:33:43 [INFO]: Epoch 002 - training loss: 0.6179, validation loss: 0.5276
2024-06-02 01:33:45 [INFO]: Epoch 003 - training loss: 0.5537, validation loss: 0.4995
2024-06-02 01:33:46 [INFO]: Epoch 004 - training loss: 0.5297, validation loss: 0.4912
2024-06-02 01:33:48 [INFO]: Epoch 005 - training loss: 0.5127, validation loss: 0.4824
2024-06-02 01:33:49 [INFO]: Epoch 006 - training loss: 0.5031, validation loss: 0.4810
2024-06-02 01:33:51 [INFO]: Epoch 007 - training loss: 0.4866, validation loss: 0.4722
2024-06-02 01:33:52 [INFO]: Epoch 008 - training loss: 0.4755, validation loss: 0.4656
2024-06-02 01:33:54 [INFO]: Epoch 009 - training loss: 0.4645, validation loss: 0.4658
2024-06-02 01:33:55 [INFO]: Epoch 010 - training loss: 0.4598, validation loss: 0.4651
2024-06-02 01:33:57 [INFO]: Epoch 011 - training loss: 0.4512, validation loss: 0.4534
2024-06-02 01:33:58 [INFO]: Epoch 012 - training loss: 0.4460, validation loss: 0.4551
2024-06-02 01:34:00 [INFO]: Epoch 013 - training loss: 0.4471, validation loss: 0.4531
2024-06-02 01:34:02 [INFO]: Epoch 014 - training loss: 0.4327, validation loss: 0.4526
2024-06-02 01:34:03 [INFO]: Epoch 015 - training loss: 0.4217, validation loss: 0.4545
2024-06-02 01:34:04 [INFO]: Epoch 016 - training loss: 0.4197, validation loss: 0.4471
2024-06-02 01:34:06 [INFO]: Epoch 017 - training loss: 0.4125, validation loss: 0.4441
2024-06-02 01:34:07 [INFO]: Epoch 018 - training loss: 0.4069, validation loss: 0.4409
2024-06-02 01:34:09 [INFO]: Epoch 019 - training loss: 0.4069, validation loss: 0.4392
2024-06-02 01:34:10 [INFO]: Epoch 020 - training loss: 0.3977, validation loss: 0.4354
2024-06-02 01:34:12 [INFO]: Epoch 021 - training loss: 0.3906, validation loss: 0.4350
2024-06-02 01:34:13 [INFO]: Epoch 022 - training loss: 0.3881, validation loss: 0.4321
2024-06-02 01:34:15 [INFO]: Epoch 023 - training loss: 0.3862, validation loss: 0.4325
2024-06-02 01:34:16 [INFO]: Epoch 024 - training loss: 0.3826, validation loss: 0.4283
2024-06-02 01:34:18 [INFO]: Epoch 025 - training loss: 0.3777, validation loss: 0.4329
2024-06-02 01:34:19 [INFO]: Epoch 026 - training loss: 0.3712, validation loss: 0.4224
2024-06-02 01:34:21 [INFO]: Epoch 027 - training loss: 0.3707, validation loss: 0.4264
2024-06-02 01:34:22 [INFO]: Epoch 028 - training loss: 0.3693, validation loss: 0.4218
2024-06-02 01:34:24 [INFO]: Epoch 029 - training loss: 0.3644, validation loss: 0.4212
2024-06-02 01:34:25 [INFO]: Epoch 030 - training loss: 0.3556, validation loss: 0.4199
2024-06-02 01:34:27 [INFO]: Epoch 031 - training loss: 0.3611, validation loss: 0.4207
2024-06-02 01:34:28 [INFO]: Epoch 032 - training loss: 0.3548, validation loss: 0.4198
2024-06-02 01:34:30 [INFO]: Epoch 033 - training loss: 0.3500, validation loss: 0.4189
2024-06-02 01:34:31 [INFO]: Epoch 034 - training loss: 0.3485, validation loss: 0.4136
2024-06-02 01:34:33 [INFO]: Epoch 035 - training loss: 0.3501, validation loss: 0.4190
2024-06-02 01:34:34 [INFO]: Epoch 036 - training loss: 0.3452, validation loss: 0.4153
2024-06-02 01:34:36 [INFO]: Epoch 037 - training loss: 0.3426, validation loss: 0.4181
2024-06-02 01:34:37 [INFO]: Epoch 038 - training loss: 0.3419, validation loss: 0.4152
2024-06-02 01:34:39 [INFO]: Epoch 039 - training loss: 0.3402, validation loss: 0.4128
2024-06-02 01:34:40 [INFO]: Epoch 040 - training loss: 0.3382, validation loss: 0.4096
2024-06-02 01:34:42 [INFO]: Epoch 041 - training loss: 0.3325, validation loss: 0.4111
2024-06-02 01:34:43 [INFO]: Epoch 042 - training loss: 0.3316, validation loss: 0.4089
2024-06-02 01:34:45 [INFO]: Epoch 043 - training loss: 0.3341, validation loss: 0.4109
2024-06-02 01:34:46 [INFO]: Epoch 044 - training loss: 0.3326, validation loss: 0.4076
2024-06-02 01:34:48 [INFO]: Epoch 045 - training loss: 0.3287, validation loss: 0.4085
2024-06-02 01:34:49 [INFO]: Epoch 046 - training loss: 0.3304, validation loss: 0.4051
2024-06-02 01:34:51 [INFO]: Epoch 047 - training loss: 0.3240, validation loss: 0.4094
2024-06-02 01:34:52 [INFO]: Epoch 048 - training loss: 0.3229, validation loss: 0.4096
2024-06-02 01:34:54 [INFO]: Epoch 049 - training loss: 0.3245, validation loss: 0.4043
2024-06-02 01:34:55 [INFO]: Epoch 050 - training loss: 0.3190, validation loss: 0.4036
2024-06-02 01:34:57 [INFO]: Epoch 051 - training loss: 0.3241, validation loss: 0.4029
2024-06-02 01:34:58 [INFO]: Epoch 052 - training loss: 0.3204, validation loss: 0.4043
2024-06-02 01:35:00 [INFO]: Epoch 053 - training loss: 0.3165, validation loss: 0.4020
2024-06-02 01:35:01 [INFO]: Epoch 054 - training loss: 0.3176, validation loss: 0.4042
2024-06-02 01:35:03 [INFO]: Epoch 055 - training loss: 0.3162, validation loss: 0.4001
2024-06-02 01:35:04 [INFO]: Epoch 056 - training loss: 0.3168, validation loss: 0.4000
2024-06-02 01:35:06 [INFO]: Epoch 057 - training loss: 0.3130, validation loss: 0.4026
2024-06-02 01:35:07 [INFO]: Epoch 058 - training loss: 0.3113, validation loss: 0.4012
2024-06-02 01:35:09 [INFO]: Epoch 059 - training loss: 0.3107, validation loss: 0.3998
2024-06-02 01:35:10 [INFO]: Epoch 060 - training loss: 0.3117, validation loss: 0.3991
2024-06-02 01:35:12 [INFO]: Epoch 061 - training loss: 0.3085, validation loss: 0.3986
2024-06-02 01:35:13 [INFO]: Epoch 062 - training loss: 0.3082, validation loss: 0.3992
2024-06-02 01:35:15 [INFO]: Epoch 063 - training loss: 0.3076, validation loss: 0.3963
2024-06-02 01:35:16 [INFO]: Epoch 064 - training loss: 0.3085, validation loss: 0.3980
2024-06-02 01:35:18 [INFO]: Epoch 065 - training loss: 0.3066, validation loss: 0.3981
2024-06-02 01:35:19 [INFO]: Epoch 066 - training loss: 0.3070, validation loss: 0.3985
2024-06-02 01:35:21 [INFO]: Epoch 067 - training loss: 0.3024, validation loss: 0.3952
2024-06-02 01:35:22 [INFO]: Epoch 068 - training loss: 0.3008, validation loss: 0.3994
2024-06-02 01:35:24 [INFO]: Epoch 069 - training loss: 0.3042, validation loss: 0.3965
2024-06-02 01:35:25 [INFO]: Epoch 070 - training loss: 0.3013, validation loss: 0.3951
2024-06-02 01:35:27 [INFO]: Epoch 071 - training loss: 0.3003, validation loss: 0.3929
2024-06-02 01:35:28 [INFO]: Epoch 072 - training loss: 0.3011, validation loss: 0.3923
2024-06-02 01:35:30 [INFO]: Epoch 073 - training loss: 0.2964, validation loss: 0.3953
2024-06-02 01:35:31 [INFO]: Epoch 074 - training loss: 0.2946, validation loss: 0.3944
2024-06-02 01:35:33 [INFO]: Epoch 075 - training loss: 0.2985, validation loss: 0.3889
2024-06-02 01:35:34 [INFO]: Epoch 076 - training loss: 0.2960, validation loss: 0.3923
2024-06-02 01:35:36 [INFO]: Epoch 077 - training loss: 0.2975, validation loss: 0.3894
2024-06-02 01:35:37 [INFO]: Epoch 078 - training loss: 0.2987, validation loss: 0.3878
2024-06-02 01:35:39 [INFO]: Epoch 079 - training loss: 0.2979, validation loss: 0.3926
2024-06-02 01:35:40 [INFO]: Epoch 080 - training loss: 0.2977, validation loss: 0.3926
2024-06-02 01:35:42 [INFO]: Epoch 081 - training loss: 0.2930, validation loss: 0.3923
2024-06-02 01:35:43 [INFO]: Epoch 082 - training loss: 0.2913, validation loss: 0.3892
2024-06-02 01:35:45 [INFO]: Epoch 083 - training loss: 0.2891, validation loss: 0.3870
2024-06-02 01:35:46 [INFO]: Epoch 084 - training loss: 0.2899, validation loss: 0.3899
2024-06-02 01:35:48 [INFO]: Epoch 085 - training loss: 0.2913, validation loss: 0.3865
2024-06-02 01:35:49 [INFO]: Epoch 086 - training loss: 0.2912, validation loss: 0.3855
2024-06-02 01:35:51 [INFO]: Epoch 087 - training loss: 0.2926, validation loss: 0.3863
2024-06-02 01:35:52 [INFO]: Epoch 088 - training loss: 0.2882, validation loss: 0.3830
2024-06-02 01:35:54 [INFO]: Epoch 089 - training loss: 0.2915, validation loss: 0.3855
2024-06-02 01:35:55 [INFO]: Epoch 090 - training loss: 0.2905, validation loss: 0.3872
2024-06-02 01:35:57 [INFO]: Epoch 091 - training loss: 0.2871, validation loss: 0.3877
2024-06-02 01:35:58 [INFO]: Epoch 092 - training loss: 0.2851, validation loss: 0.3864
2024-06-02 01:36:00 [INFO]: Epoch 093 - training loss: 0.2825, validation loss: 0.3845
2024-06-02 01:36:01 [INFO]: Epoch 094 - training loss: 0.2840, validation loss: 0.3867
2024-06-02 01:36:03 [INFO]: Epoch 095 - training loss: 0.2823, validation loss: 0.3842
2024-06-02 01:36:04 [INFO]: Epoch 096 - training loss: 0.2832, validation loss: 0.3869
2024-06-02 01:36:06 [INFO]: Epoch 097 - training loss: 0.2808, validation loss: 0.3870
2024-06-02 01:36:07 [INFO]: Epoch 098 - training loss: 0.2846, validation loss: 0.3849
2024-06-02 01:36:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:36:07 [INFO]: Finished training. The best model is from epoch#88.
2024-06-02 01:36:08 [INFO]: Saved the model to results_point_rate01/PeMS/Informer_PeMS/round_2/20240602_T013340/Informer.pypots
2024-06-02 01:36:08 [INFO]: Successfully saved to results_point_rate01/PeMS/Informer_PeMS/round_2/imputation.pkl
2024-06-02 01:36:08 [INFO]: Round2 - Informer on PeMS: MAE=0.2998, MSE=0.5730, MRE=0.3717
2024-06-02 01:36:08 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 01:36:08 [INFO]: Using the given device: cuda:0
2024-06-02 01:36:08 [INFO]: Model files will be saved to results_point_rate01/PeMS/Informer_PeMS/round_3/20240602_T013608
2024-06-02 01:36:08 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Informer_PeMS/round_3/20240602_T013608/tensorboard
2024-06-02 01:36:08 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 13,149,022
2024-06-02 01:36:09 [INFO]: Epoch 001 - training loss: 0.9134, validation loss: 0.6102
2024-06-02 01:36:11 [INFO]: Epoch 002 - training loss: 0.6077, validation loss: 0.5210
2024-06-02 01:36:12 [INFO]: Epoch 003 - training loss: 0.5540, validation loss: 0.4927
2024-06-02 01:36:14 [INFO]: Epoch 004 - training loss: 0.5292, validation loss: 0.4947
2024-06-02 01:36:15 [INFO]: Epoch 005 - training loss: 0.5187, validation loss: 0.4829
2024-06-02 01:36:17 [INFO]: Epoch 006 - training loss: 0.5045, validation loss: 0.4807
2024-06-02 01:36:18 [INFO]: Epoch 007 - training loss: 0.4883, validation loss: 0.4734
2024-06-02 01:36:20 [INFO]: Epoch 008 - training loss: 0.4786, validation loss: 0.4659
2024-06-02 01:36:21 [INFO]: Epoch 009 - training loss: 0.4644, validation loss: 0.4613
2024-06-02 01:36:23 [INFO]: Epoch 010 - training loss: 0.4584, validation loss: 0.4597
2024-06-02 01:36:24 [INFO]: Epoch 011 - training loss: 0.4538, validation loss: 0.4608
2024-06-02 01:36:26 [INFO]: Epoch 012 - training loss: 0.4500, validation loss: 0.4592
2024-06-02 01:36:27 [INFO]: Epoch 013 - training loss: 0.4368, validation loss: 0.4521
2024-06-02 01:36:29 [INFO]: Epoch 014 - training loss: 0.4302, validation loss: 0.4509
2024-06-02 01:36:30 [INFO]: Epoch 015 - training loss: 0.4213, validation loss: 0.4491
2024-06-02 01:36:32 [INFO]: Epoch 016 - training loss: 0.4200, validation loss: 0.4494
2024-06-02 01:36:33 [INFO]: Epoch 017 - training loss: 0.4151, validation loss: 0.4435
2024-06-02 01:36:35 [INFO]: Epoch 018 - training loss: 0.4065, validation loss: 0.4425
2024-06-02 01:36:36 [INFO]: Epoch 019 - training loss: 0.4099, validation loss: 0.4458
2024-06-02 01:36:38 [INFO]: Epoch 020 - training loss: 0.4024, validation loss: 0.4358
2024-06-02 01:36:39 [INFO]: Epoch 021 - training loss: 0.3936, validation loss: 0.4351
2024-06-02 01:36:41 [INFO]: Epoch 022 - training loss: 0.3909, validation loss: 0.4357
2024-06-02 01:36:42 [INFO]: Epoch 023 - training loss: 0.3859, validation loss: 0.4398
2024-06-02 01:36:44 [INFO]: Epoch 024 - training loss: 0.3839, validation loss: 0.4286
2024-06-02 01:36:45 [INFO]: Epoch 025 - training loss: 0.3803, validation loss: 0.4339
2024-06-02 01:36:47 [INFO]: Epoch 026 - training loss: 0.3776, validation loss: 0.4299
2024-06-02 01:36:48 [INFO]: Epoch 027 - training loss: 0.3700, validation loss: 0.4249
2024-06-02 01:36:50 [INFO]: Epoch 028 - training loss: 0.3627, validation loss: 0.4232
2024-06-02 01:36:51 [INFO]: Epoch 029 - training loss: 0.3615, validation loss: 0.4237
2024-06-02 01:36:53 [INFO]: Epoch 030 - training loss: 0.3626, validation loss: 0.4213
2024-06-02 01:36:54 [INFO]: Epoch 031 - training loss: 0.3620, validation loss: 0.4222
2024-06-02 01:36:56 [INFO]: Epoch 032 - training loss: 0.3560, validation loss: 0.4180
2024-06-02 01:36:57 [INFO]: Epoch 033 - training loss: 0.3549, validation loss: 0.4251
2024-06-02 01:36:59 [INFO]: Epoch 034 - training loss: 0.3517, validation loss: 0.4234
2024-06-02 01:37:00 [INFO]: Epoch 035 - training loss: 0.3555, validation loss: 0.4179
2024-06-02 01:37:02 [INFO]: Epoch 036 - training loss: 0.3523, validation loss: 0.4147
2024-06-02 01:37:03 [INFO]: Epoch 037 - training loss: 0.3450, validation loss: 0.4176
2024-06-02 01:37:05 [INFO]: Epoch 038 - training loss: 0.3424, validation loss: 0.4155
2024-06-02 01:37:06 [INFO]: Epoch 039 - training loss: 0.3409, validation loss: 0.4142
2024-06-02 01:37:08 [INFO]: Epoch 040 - training loss: 0.3340, validation loss: 0.4126
2024-06-02 01:37:09 [INFO]: Epoch 041 - training loss: 0.3316, validation loss: 0.4107
2024-06-02 01:37:11 [INFO]: Epoch 042 - training loss: 0.3323, validation loss: 0.4128
2024-06-02 01:37:12 [INFO]: Epoch 043 - training loss: 0.3307, validation loss: 0.4094
2024-06-02 01:37:14 [INFO]: Epoch 044 - training loss: 0.3292, validation loss: 0.4095
2024-06-02 01:37:15 [INFO]: Epoch 045 - training loss: 0.3279, validation loss: 0.4072
2024-06-02 01:37:17 [INFO]: Epoch 046 - training loss: 0.3254, validation loss: 0.4121
2024-06-02 01:37:18 [INFO]: Epoch 047 - training loss: 0.3270, validation loss: 0.4082
2024-06-02 01:37:20 [INFO]: Epoch 048 - training loss: 0.3274, validation loss: 0.4072
2024-06-02 01:37:21 [INFO]: Epoch 049 - training loss: 0.3270, validation loss: 0.4098
2024-06-02 01:37:23 [INFO]: Epoch 050 - training loss: 0.3199, validation loss: 0.4075
2024-06-02 01:37:24 [INFO]: Epoch 051 - training loss: 0.3199, validation loss: 0.4051
2024-06-02 01:37:26 [INFO]: Epoch 052 - training loss: 0.3193, validation loss: 0.4027
2024-06-02 01:37:27 [INFO]: Epoch 053 - training loss: 0.3175, validation loss: 0.4017
2024-06-02 01:37:29 [INFO]: Epoch 054 - training loss: 0.3159, validation loss: 0.4093
2024-06-02 01:37:30 [INFO]: Epoch 055 - training loss: 0.3157, validation loss: 0.4014
2024-06-02 01:37:32 [INFO]: Epoch 056 - training loss: 0.3166, validation loss: 0.4031
2024-06-02 01:37:33 [INFO]: Epoch 057 - training loss: 0.3142, validation loss: 0.4005
2024-06-02 01:37:35 [INFO]: Epoch 058 - training loss: 0.3108, validation loss: 0.4011
2024-06-02 01:37:36 [INFO]: Epoch 059 - training loss: 0.3115, validation loss: 0.3983
2024-06-02 01:37:38 [INFO]: Epoch 060 - training loss: 0.3110, validation loss: 0.3972
2024-06-02 01:37:39 [INFO]: Epoch 061 - training loss: 0.3072, validation loss: 0.3969
2024-06-02 01:37:41 [INFO]: Epoch 062 - training loss: 0.3060, validation loss: 0.4019
2024-06-02 01:37:42 [INFO]: Epoch 063 - training loss: 0.3059, validation loss: 0.4074
2024-06-02 01:37:44 [INFO]: Epoch 064 - training loss: 0.3134, validation loss: 0.4009
2024-06-02 01:37:45 [INFO]: Epoch 065 - training loss: 0.3083, validation loss: 0.3947
2024-06-02 01:37:47 [INFO]: Epoch 066 - training loss: 0.3077, validation loss: 0.3947
2024-06-02 01:37:48 [INFO]: Epoch 067 - training loss: 0.3069, validation loss: 0.3949
2024-06-02 01:37:50 [INFO]: Epoch 068 - training loss: 0.3054, validation loss: 0.3976
2024-06-02 01:37:51 [INFO]: Epoch 069 - training loss: 0.3015, validation loss: 0.3950
2024-06-02 01:37:53 [INFO]: Epoch 070 - training loss: 0.2991, validation loss: 0.3970
2024-06-02 01:37:54 [INFO]: Epoch 071 - training loss: 0.3010, validation loss: 0.3943
2024-06-02 01:37:56 [INFO]: Epoch 072 - training loss: 0.3001, validation loss: 0.3949
2024-06-02 01:37:57 [INFO]: Epoch 073 - training loss: 0.2992, validation loss: 0.3926
2024-06-02 01:37:59 [INFO]: Epoch 074 - training loss: 0.2979, validation loss: 0.3945
2024-06-02 01:38:00 [INFO]: Epoch 075 - training loss: 0.2983, validation loss: 0.3888
2024-06-02 01:38:02 [INFO]: Epoch 076 - training loss: 0.2994, validation loss: 0.3911
2024-06-02 01:38:03 [INFO]: Epoch 077 - training loss: 0.2973, validation loss: 0.3921
2024-06-02 01:38:05 [INFO]: Epoch 078 - training loss: 0.2996, validation loss: 0.3933
2024-06-02 01:38:06 [INFO]: Epoch 079 - training loss: 0.2957, validation loss: 0.3930
2024-06-02 01:38:08 [INFO]: Epoch 080 - training loss: 0.2938, validation loss: 0.3914
2024-06-02 01:38:09 [INFO]: Epoch 081 - training loss: 0.2915, validation loss: 0.3896
2024-06-02 01:38:11 [INFO]: Epoch 082 - training loss: 0.2937, validation loss: 0.3936
2024-06-02 01:38:12 [INFO]: Epoch 083 - training loss: 0.2902, validation loss: 0.3907
2024-06-02 01:38:14 [INFO]: Epoch 084 - training loss: 0.2913, validation loss: 0.3922
2024-06-02 01:38:15 [INFO]: Epoch 085 - training loss: 0.2917, validation loss: 0.3938
2024-06-02 01:38:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:38:15 [INFO]: Finished training. The best model is from epoch#75.
2024-06-02 01:38:15 [INFO]: Saved the model to results_point_rate01/PeMS/Informer_PeMS/round_3/20240602_T013608/Informer.pypots
2024-06-02 01:38:15 [INFO]: Successfully saved to results_point_rate01/PeMS/Informer_PeMS/round_3/imputation.pkl
2024-06-02 01:38:15 [INFO]: Round3 - Informer on PeMS: MAE=0.3073, MSE=0.5777, MRE=0.3809
2024-06-02 01:38:15 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 01:38:15 [INFO]: Using the given device: cuda:0
2024-06-02 01:38:15 [INFO]: Model files will be saved to results_point_rate01/PeMS/Informer_PeMS/round_4/20240602_T013815
2024-06-02 01:38:15 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Informer_PeMS/round_4/20240602_T013815/tensorboard
2024-06-02 01:38:16 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 13,149,022
2024-06-02 01:38:17 [INFO]: Epoch 001 - training loss: 0.9210, validation loss: 0.5841
2024-06-02 01:38:19 [INFO]: Epoch 002 - training loss: 0.6337, validation loss: 0.5286
2024-06-02 01:38:20 [INFO]: Epoch 003 - training loss: 0.5658, validation loss: 0.4914
2024-06-02 01:38:22 [INFO]: Epoch 004 - training loss: 0.5270, validation loss: 0.4864
2024-06-02 01:38:23 [INFO]: Epoch 005 - training loss: 0.5124, validation loss: 0.4779
2024-06-02 01:38:25 [INFO]: Epoch 006 - training loss: 0.5001, validation loss: 0.4762
2024-06-02 01:38:26 [INFO]: Epoch 007 - training loss: 0.4870, validation loss: 0.4791
2024-06-02 01:38:28 [INFO]: Epoch 008 - training loss: 0.4763, validation loss: 0.4760
2024-06-02 01:38:29 [INFO]: Epoch 009 - training loss: 0.4659, validation loss: 0.4646
2024-06-02 01:38:31 [INFO]: Epoch 010 - training loss: 0.4550, validation loss: 0.4608
2024-06-02 01:38:32 [INFO]: Epoch 011 - training loss: 0.4490, validation loss: 0.4636
2024-06-02 01:38:34 [INFO]: Epoch 012 - training loss: 0.4423, validation loss: 0.4568
2024-06-02 01:38:35 [INFO]: Epoch 013 - training loss: 0.4370, validation loss: 0.4588
2024-06-02 01:38:37 [INFO]: Epoch 014 - training loss: 0.4322, validation loss: 0.4491
2024-06-02 01:38:38 [INFO]: Epoch 015 - training loss: 0.4258, validation loss: 0.4512
2024-06-02 01:38:40 [INFO]: Epoch 016 - training loss: 0.4169, validation loss: 0.4470
2024-06-02 01:38:41 [INFO]: Epoch 017 - training loss: 0.4140, validation loss: 0.4468
2024-06-02 01:38:43 [INFO]: Epoch 018 - training loss: 0.4123, validation loss: 0.4446
2024-06-02 01:38:44 [INFO]: Epoch 019 - training loss: 0.4049, validation loss: 0.4409
2024-06-02 01:38:46 [INFO]: Epoch 020 - training loss: 0.3996, validation loss: 0.4356
2024-06-02 01:38:47 [INFO]: Epoch 021 - training loss: 0.3974, validation loss: 0.4327
2024-06-02 01:38:49 [INFO]: Epoch 022 - training loss: 0.3904, validation loss: 0.4366
2024-06-02 01:38:50 [INFO]: Epoch 023 - training loss: 0.3902, validation loss: 0.4316
2024-06-02 01:38:52 [INFO]: Epoch 024 - training loss: 0.3828, validation loss: 0.4296
2024-06-02 01:38:53 [INFO]: Epoch 025 - training loss: 0.3807, validation loss: 0.4282
2024-06-02 01:38:55 [INFO]: Epoch 026 - training loss: 0.3780, validation loss: 0.4272
2024-06-02 01:38:56 [INFO]: Epoch 027 - training loss: 0.3703, validation loss: 0.4234
2024-06-02 01:38:58 [INFO]: Epoch 028 - training loss: 0.3681, validation loss: 0.4261
2024-06-02 01:38:59 [INFO]: Epoch 029 - training loss: 0.3685, validation loss: 0.4199
2024-06-02 01:39:01 [INFO]: Epoch 030 - training loss: 0.3619, validation loss: 0.4245
2024-06-02 01:39:02 [INFO]: Epoch 031 - training loss: 0.3632, validation loss: 0.4161
2024-06-02 01:39:04 [INFO]: Epoch 032 - training loss: 0.3553, validation loss: 0.4167
2024-06-02 01:39:05 [INFO]: Epoch 033 - training loss: 0.3559, validation loss: 0.4197
2024-06-02 01:39:07 [INFO]: Epoch 034 - training loss: 0.3535, validation loss: 0.4138
2024-06-02 01:39:08 [INFO]: Epoch 035 - training loss: 0.3478, validation loss: 0.4181
2024-06-02 01:39:10 [INFO]: Epoch 036 - training loss: 0.3420, validation loss: 0.4156
2024-06-02 01:39:11 [INFO]: Epoch 037 - training loss: 0.3434, validation loss: 0.4128
2024-06-02 01:39:13 [INFO]: Epoch 038 - training loss: 0.3402, validation loss: 0.4192
2024-06-02 01:39:14 [INFO]: Epoch 039 - training loss: 0.3417, validation loss: 0.4124
2024-06-02 01:39:16 [INFO]: Epoch 040 - training loss: 0.3384, validation loss: 0.4095
2024-06-02 01:39:17 [INFO]: Epoch 041 - training loss: 0.3407, validation loss: 0.4121
2024-06-02 01:39:19 [INFO]: Epoch 042 - training loss: 0.3371, validation loss: 0.4095
2024-06-02 01:39:20 [INFO]: Epoch 043 - training loss: 0.3385, validation loss: 0.4077
2024-06-02 01:39:22 [INFO]: Epoch 044 - training loss: 0.3326, validation loss: 0.4084
2024-06-02 01:39:23 [INFO]: Epoch 045 - training loss: 0.3270, validation loss: 0.4116
2024-06-02 01:39:25 [INFO]: Epoch 046 - training loss: 0.3286, validation loss: 0.4096
2024-06-02 01:39:26 [INFO]: Epoch 047 - training loss: 0.3283, validation loss: 0.4029
2024-06-02 01:39:28 [INFO]: Epoch 048 - training loss: 0.3278, validation loss: 0.4081
2024-06-02 01:39:29 [INFO]: Epoch 049 - training loss: 0.3282, validation loss: 0.4117
2024-06-02 01:39:31 [INFO]: Epoch 050 - training loss: 0.3236, validation loss: 0.4036
2024-06-02 01:39:32 [INFO]: Epoch 051 - training loss: 0.3187, validation loss: 0.4065
2024-06-02 01:39:34 [INFO]: Epoch 052 - training loss: 0.3202, validation loss: 0.4043
2024-06-02 01:39:35 [INFO]: Epoch 053 - training loss: 0.3193, validation loss: 0.4061
2024-06-02 01:39:37 [INFO]: Epoch 054 - training loss: 0.3158, validation loss: 0.4035
2024-06-02 01:39:38 [INFO]: Epoch 055 - training loss: 0.3147, validation loss: 0.4003
2024-06-02 01:39:40 [INFO]: Epoch 056 - training loss: 0.3121, validation loss: 0.4017
2024-06-02 01:39:41 [INFO]: Epoch 057 - training loss: 0.3129, validation loss: 0.4031
2024-06-02 01:39:43 [INFO]: Epoch 058 - training loss: 0.3103, validation loss: 0.4011
2024-06-02 01:39:44 [INFO]: Epoch 059 - training loss: 0.3109, validation loss: 0.3987
2024-06-02 01:39:46 [INFO]: Epoch 060 - training loss: 0.3101, validation loss: 0.3998
2024-06-02 01:39:47 [INFO]: Epoch 061 - training loss: 0.3108, validation loss: 0.4052
2024-06-02 01:39:49 [INFO]: Epoch 062 - training loss: 0.3104, validation loss: 0.3999
2024-06-02 01:39:50 [INFO]: Epoch 063 - training loss: 0.3104, validation loss: 0.3954
2024-06-02 01:39:52 [INFO]: Epoch 064 - training loss: 0.3060, validation loss: 0.3979
2024-06-02 01:39:53 [INFO]: Epoch 065 - training loss: 0.3069, validation loss: 0.3997
2024-06-02 01:39:55 [INFO]: Epoch 066 - training loss: 0.3068, validation loss: 0.3982
2024-06-02 01:39:56 [INFO]: Epoch 067 - training loss: 0.3036, validation loss: 0.3932
2024-06-02 01:39:58 [INFO]: Epoch 068 - training loss: 0.3024, validation loss: 0.3951
2024-06-02 01:39:59 [INFO]: Epoch 069 - training loss: 0.3037, validation loss: 0.3944
2024-06-02 01:40:01 [INFO]: Epoch 070 - training loss: 0.3013, validation loss: 0.3985
2024-06-02 01:40:02 [INFO]: Epoch 071 - training loss: 0.3012, validation loss: 0.3966
2024-06-02 01:40:04 [INFO]: Epoch 072 - training loss: 0.2988, validation loss: 0.3946
2024-06-02 01:40:05 [INFO]: Epoch 073 - training loss: 0.2954, validation loss: 0.3936
2024-06-02 01:40:07 [INFO]: Epoch 074 - training loss: 0.2968, validation loss: 0.3927
2024-06-02 01:40:08 [INFO]: Epoch 075 - training loss: 0.3002, validation loss: 0.3943
2024-06-02 01:40:10 [INFO]: Epoch 076 - training loss: 0.2981, validation loss: 0.3964
2024-06-02 01:40:11 [INFO]: Epoch 077 - training loss: 0.2983, validation loss: 0.3968
2024-06-02 01:40:13 [INFO]: Epoch 078 - training loss: 0.2988, validation loss: 0.3896
2024-06-02 01:40:14 [INFO]: Epoch 079 - training loss: 0.2968, validation loss: 0.3936
2024-06-02 01:40:16 [INFO]: Epoch 080 - training loss: 0.2972, validation loss: 0.3927
2024-06-02 01:40:17 [INFO]: Epoch 081 - training loss: 0.2957, validation loss: 0.3930
2024-06-02 01:40:19 [INFO]: Epoch 082 - training loss: 0.2958, validation loss: 0.3926
2024-06-02 01:40:20 [INFO]: Epoch 083 - training loss: 0.2947, validation loss: 0.3914
2024-06-02 01:40:22 [INFO]: Epoch 084 - training loss: 0.2923, validation loss: 0.3947
2024-06-02 01:40:23 [INFO]: Epoch 085 - training loss: 0.2930, validation loss: 0.3918
2024-06-02 01:40:25 [INFO]: Epoch 086 - training loss: 0.2890, validation loss: 0.3923
2024-06-02 01:40:26 [INFO]: Epoch 087 - training loss: 0.2921, validation loss: 0.3884
2024-06-02 01:40:28 [INFO]: Epoch 088 - training loss: 0.2889, validation loss: 0.3911
2024-06-02 01:40:29 [INFO]: Epoch 089 - training loss: 0.2876, validation loss: 0.3895
2024-06-02 01:40:31 [INFO]: Epoch 090 - training loss: 0.2868, validation loss: 0.3879
2024-06-02 01:40:33 [INFO]: Epoch 091 - training loss: 0.2846, validation loss: 0.3871
2024-06-02 01:40:34 [INFO]: Epoch 092 - training loss: 0.2873, validation loss: 0.3933
2024-06-02 01:40:36 [INFO]: Epoch 093 - training loss: 0.2884, validation loss: 0.3889
2024-06-02 01:40:37 [INFO]: Epoch 094 - training loss: 0.2858, validation loss: 0.3857
2024-06-02 01:40:39 [INFO]: Epoch 095 - training loss: 0.2878, validation loss: 0.3874
2024-06-02 01:40:40 [INFO]: Epoch 096 - training loss: 0.2846, validation loss: 0.3864
2024-06-02 01:40:42 [INFO]: Epoch 097 - training loss: 0.2835, validation loss: 0.3874
2024-06-02 01:40:43 [INFO]: Epoch 098 - training loss: 0.2812, validation loss: 0.3875
2024-06-02 01:40:45 [INFO]: Epoch 099 - training loss: 0.2804, validation loss: 0.3882
2024-06-02 01:40:46 [INFO]: Epoch 100 - training loss: 0.2780, validation loss: 0.3861
2024-06-02 01:40:46 [INFO]: Finished training. The best model is from epoch#94.
2024-06-02 01:40:46 [INFO]: Saved the model to results_point_rate01/PeMS/Informer_PeMS/round_4/20240602_T013815/Informer.pypots
2024-06-02 01:40:46 [INFO]: Successfully saved to results_point_rate01/PeMS/Informer_PeMS/round_4/imputation.pkl
2024-06-02 01:40:46 [INFO]: Round4 - Informer on PeMS: MAE=0.3019, MSE=0.5718, MRE=0.3742
2024-06-02 01:40:46 [INFO]: Done! Final results:
Averaged Informer (n params: 13,149,022) on PeMS: MAE=0.3018 ± 0.002847921448917297, MSE=0.5719 ± 0.0033778498034071134, MRE=0.3741 ± 0.003530282377712222, average inference time=0.17
