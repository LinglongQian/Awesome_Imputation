2024-06-02 03:23:55 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 03:23:55 [INFO]: Using the given device: cuda:0
2024-06-02 03:23:56 [INFO]: Model files will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_0/20240602_T032356
2024-06-02 03:23:56 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_0/20240602_T032356/tensorboard
2024-06-02 03:23:56 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-02 03:24:05 [INFO]: Epoch 001 - training loss: 0.9551, validation loss: 0.5830
2024-06-02 03:24:06 [INFO]: Epoch 002 - training loss: 0.5817, validation loss: 0.5116
2024-06-02 03:24:07 [INFO]: Epoch 003 - training loss: 0.5190, validation loss: 0.5012
2024-06-02 03:24:09 [INFO]: Epoch 004 - training loss: 0.4883, validation loss: 0.4880
2024-06-02 03:24:10 [INFO]: Epoch 005 - training loss: 0.4649, validation loss: 0.4835
2024-06-02 03:24:12 [INFO]: Epoch 006 - training loss: 0.4546, validation loss: 0.4767
2024-06-02 03:24:14 [INFO]: Epoch 007 - training loss: 0.4410, validation loss: 0.4672
2024-06-02 03:24:16 [INFO]: Epoch 008 - training loss: 0.4289, validation loss: 0.4623
2024-06-02 03:24:17 [INFO]: Epoch 009 - training loss: 0.4221, validation loss: 0.4637
2024-06-02 03:24:19 [INFO]: Epoch 010 - training loss: 0.4140, validation loss: 0.4610
2024-06-02 03:24:21 [INFO]: Epoch 011 - training loss: 0.4080, validation loss: 0.4499
2024-06-02 03:24:23 [INFO]: Epoch 012 - training loss: 0.3969, validation loss: 0.4483
2024-06-02 03:24:25 [INFO]: Epoch 013 - training loss: 0.3879, validation loss: 0.4467
2024-06-02 03:24:27 [INFO]: Epoch 014 - training loss: 0.3862, validation loss: 0.4473
2024-06-02 03:24:28 [INFO]: Epoch 015 - training loss: 0.3746, validation loss: 0.4430
2024-06-02 03:24:30 [INFO]: Epoch 016 - training loss: 0.3743, validation loss: 0.4401
2024-06-02 03:24:32 [INFO]: Epoch 017 - training loss: 0.3739, validation loss: 0.4477
2024-06-02 03:24:34 [INFO]: Epoch 018 - training loss: 0.3777, validation loss: 0.4376
2024-06-02 03:24:35 [INFO]: Epoch 019 - training loss: 0.3694, validation loss: 0.4327
2024-06-02 03:24:37 [INFO]: Epoch 020 - training loss: 0.3634, validation loss: 0.4348
2024-06-02 03:24:39 [INFO]: Epoch 021 - training loss: 0.3494, validation loss: 0.4300
2024-06-02 03:24:41 [INFO]: Epoch 022 - training loss: 0.3515, validation loss: 0.4299
2024-06-02 03:24:42 [INFO]: Epoch 023 - training loss: 0.3481, validation loss: 0.4267
2024-06-02 03:24:44 [INFO]: Epoch 024 - training loss: 0.3454, validation loss: 0.4292
2024-06-02 03:24:46 [INFO]: Epoch 025 - training loss: 0.3456, validation loss: 0.4235
2024-06-02 03:24:48 [INFO]: Epoch 026 - training loss: 0.3425, validation loss: 0.4262
2024-06-02 03:24:49 [INFO]: Epoch 027 - training loss: 0.3451, validation loss: 0.4239
2024-06-02 03:24:51 [INFO]: Epoch 028 - training loss: 0.3409, validation loss: 0.4230
2024-06-02 03:24:53 [INFO]: Epoch 029 - training loss: 0.3318, validation loss: 0.4186
2024-06-02 03:24:55 [INFO]: Epoch 030 - training loss: 0.3303, validation loss: 0.4156
2024-06-02 03:24:56 [INFO]: Epoch 031 - training loss: 0.3278, validation loss: 0.4173
2024-06-02 03:24:58 [INFO]: Epoch 032 - training loss: 0.3296, validation loss: 0.4141
2024-06-02 03:25:00 [INFO]: Epoch 033 - training loss: 0.3256, validation loss: 0.4129
2024-06-02 03:25:01 [INFO]: Epoch 034 - training loss: 0.3228, validation loss: 0.4138
2024-06-02 03:25:03 [INFO]: Epoch 035 - training loss: 0.3218, validation loss: 0.4140
2024-06-02 03:25:05 [INFO]: Epoch 036 - training loss: 0.3203, validation loss: 0.4112
2024-06-02 03:25:07 [INFO]: Epoch 037 - training loss: 0.3163, validation loss: 0.4077
2024-06-02 03:25:08 [INFO]: Epoch 038 - training loss: 0.3168, validation loss: 0.4075
2024-06-02 03:25:10 [INFO]: Epoch 039 - training loss: 0.3200, validation loss: 0.4117
2024-06-02 03:25:12 [INFO]: Epoch 040 - training loss: 0.3189, validation loss: 0.4087
2024-06-02 03:25:13 [INFO]: Epoch 041 - training loss: 0.3154, validation loss: 0.4143
2024-06-02 03:25:15 [INFO]: Epoch 042 - training loss: 0.3140, validation loss: 0.4045
2024-06-02 03:25:17 [INFO]: Epoch 043 - training loss: 0.3050, validation loss: 0.4080
2024-06-02 03:25:19 [INFO]: Epoch 044 - training loss: 0.3111, validation loss: 0.4039
2024-06-02 03:25:20 [INFO]: Epoch 045 - training loss: 0.3066, validation loss: 0.4036
2024-06-02 03:25:22 [INFO]: Epoch 046 - training loss: 0.3108, validation loss: 0.4007
2024-06-02 03:25:24 [INFO]: Epoch 047 - training loss: 0.3051, validation loss: 0.4077
2024-06-02 03:25:26 [INFO]: Epoch 048 - training loss: 0.3008, validation loss: 0.4022
2024-06-02 03:25:27 [INFO]: Epoch 049 - training loss: 0.3029, validation loss: 0.4009
2024-06-02 03:25:29 [INFO]: Epoch 050 - training loss: 0.2973, validation loss: 0.3989
2024-06-02 03:25:31 [INFO]: Epoch 051 - training loss: 0.2956, validation loss: 0.3994
2024-06-02 03:25:32 [INFO]: Epoch 052 - training loss: 0.2963, validation loss: 0.3976
2024-06-02 03:25:34 [INFO]: Epoch 053 - training loss: 0.2962, validation loss: 0.3965
2024-06-02 03:25:36 [INFO]: Epoch 054 - training loss: 0.2984, validation loss: 0.3965
2024-06-02 03:25:38 [INFO]: Epoch 055 - training loss: 0.3014, validation loss: 0.3934
2024-06-02 03:25:39 [INFO]: Epoch 056 - training loss: 0.2932, validation loss: 0.3959
2024-06-02 03:25:41 [INFO]: Epoch 057 - training loss: 0.2921, validation loss: 0.3958
2024-06-02 03:25:43 [INFO]: Epoch 058 - training loss: 0.2932, validation loss: 0.3954
2024-06-02 03:25:45 [INFO]: Epoch 059 - training loss: 0.2942, validation loss: 0.3923
2024-06-02 03:25:47 [INFO]: Epoch 060 - training loss: 0.2907, validation loss: 0.3954
2024-06-02 03:25:49 [INFO]: Epoch 061 - training loss: 0.2855, validation loss: 0.3901
2024-06-02 03:25:50 [INFO]: Epoch 062 - training loss: 0.2875, validation loss: 0.3930
2024-06-02 03:25:52 [INFO]: Epoch 063 - training loss: 0.2927, validation loss: 0.3936
2024-06-02 03:25:54 [INFO]: Epoch 064 - training loss: 0.2874, validation loss: 0.3942
2024-06-02 03:25:56 [INFO]: Epoch 065 - training loss: 0.2830, validation loss: 0.3903
2024-06-02 03:25:57 [INFO]: Epoch 066 - training loss: 0.2831, validation loss: 0.3905
2024-06-02 03:25:59 [INFO]: Epoch 067 - training loss: 0.2846, validation loss: 0.3888
2024-06-02 03:26:01 [INFO]: Epoch 068 - training loss: 0.2800, validation loss: 0.3871
2024-06-02 03:26:02 [INFO]: Epoch 069 - training loss: 0.2801, validation loss: 0.3881
2024-06-02 03:26:04 [INFO]: Epoch 070 - training loss: 0.2797, validation loss: 0.3902
2024-06-02 03:26:05 [INFO]: Epoch 071 - training loss: 0.2780, validation loss: 0.3864
2024-06-02 03:26:07 [INFO]: Epoch 072 - training loss: 0.2781, validation loss: 0.3885
2024-06-02 03:26:09 [INFO]: Epoch 073 - training loss: 0.2759, validation loss: 0.3852
2024-06-02 03:26:11 [INFO]: Epoch 074 - training loss: 0.2771, validation loss: 0.3846
2024-06-02 03:26:12 [INFO]: Epoch 075 - training loss: 0.2787, validation loss: 0.3846
2024-06-02 03:26:14 [INFO]: Epoch 076 - training loss: 0.2776, validation loss: 0.3840
2024-06-02 03:26:16 [INFO]: Epoch 077 - training loss: 0.2722, validation loss: 0.3839
2024-06-02 03:26:17 [INFO]: Epoch 078 - training loss: 0.2758, validation loss: 0.3837
2024-06-02 03:26:19 [INFO]: Epoch 079 - training loss: 0.2754, validation loss: 0.3860
2024-06-02 03:26:21 [INFO]: Epoch 080 - training loss: 0.2762, validation loss: 0.3839
2024-06-02 03:26:22 [INFO]: Epoch 081 - training loss: 0.2731, validation loss: 0.3828
2024-06-02 03:26:24 [INFO]: Epoch 082 - training loss: 0.2716, validation loss: 0.3815
2024-06-02 03:26:26 [INFO]: Epoch 083 - training loss: 0.2712, validation loss: 0.3828
2024-06-02 03:26:27 [INFO]: Epoch 084 - training loss: 0.2750, validation loss: 0.3831
2024-06-02 03:26:29 [INFO]: Epoch 085 - training loss: 0.2696, validation loss: 0.3825
2024-06-02 03:26:31 [INFO]: Epoch 086 - training loss: 0.2716, validation loss: 0.3791
2024-06-02 03:26:33 [INFO]: Epoch 087 - training loss: 0.2696, validation loss: 0.3781
2024-06-02 03:26:35 [INFO]: Epoch 088 - training loss: 0.2704, validation loss: 0.3809
2024-06-02 03:26:36 [INFO]: Epoch 089 - training loss: 0.2694, validation loss: 0.3834
2024-06-02 03:26:38 [INFO]: Epoch 090 - training loss: 0.2673, validation loss: 0.3809
2024-06-02 03:26:40 [INFO]: Epoch 091 - training loss: 0.2665, validation loss: 0.3803
2024-06-02 03:26:41 [INFO]: Epoch 092 - training loss: 0.2651, validation loss: 0.3822
2024-06-02 03:26:43 [INFO]: Epoch 093 - training loss: 0.2666, validation loss: 0.3792
2024-06-02 03:26:45 [INFO]: Epoch 094 - training loss: 0.2651, validation loss: 0.3777
2024-06-02 03:26:47 [INFO]: Epoch 095 - training loss: 0.2663, validation loss: 0.3806
2024-06-02 03:26:48 [INFO]: Epoch 096 - training loss: 0.2663, validation loss: 0.3795
2024-06-02 03:26:50 [INFO]: Epoch 097 - training loss: 0.2637, validation loss: 0.3774
2024-06-02 03:26:52 [INFO]: Epoch 098 - training loss: 0.2609, validation loss: 0.3792
2024-06-02 03:26:53 [INFO]: Epoch 099 - training loss: 0.2594, validation loss: 0.3807
2024-06-02 03:26:55 [INFO]: Epoch 100 - training loss: 0.2637, validation loss: 0.3805
2024-06-02 03:26:55 [INFO]: Finished training. The best model is from epoch#97.
2024-06-02 03:26:55 [INFO]: Saved the model to results_point_rate01/PeMS/Pyraformer_PeMS/round_0/20240602_T032356/Pyraformer.pypots
2024-06-02 03:26:55 [INFO]: Successfully saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_0/imputation.pkl
2024-06-02 03:26:55 [INFO]: Round0 - Pyraformer on PeMS: MAE=0.2851, MSE=0.5674, MRE=0.3534
2024-06-02 03:26:55 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 03:26:55 [INFO]: Using the given device: cuda:0
2024-06-02 03:26:55 [INFO]: Model files will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_1/20240602_T032655
2024-06-02 03:26:55 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_1/20240602_T032655/tensorboard
2024-06-02 03:26:56 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-02 03:26:57 [INFO]: Epoch 001 - training loss: 0.9587, validation loss: 0.6081
2024-06-02 03:26:59 [INFO]: Epoch 002 - training loss: 0.5956, validation loss: 0.5211
2024-06-02 03:27:01 [INFO]: Epoch 003 - training loss: 0.5273, validation loss: 0.4920
2024-06-02 03:27:03 [INFO]: Epoch 004 - training loss: 0.4872, validation loss: 0.4868
2024-06-02 03:27:05 [INFO]: Epoch 005 - training loss: 0.4694, validation loss: 0.4791
2024-06-02 03:27:06 [INFO]: Epoch 006 - training loss: 0.4609, validation loss: 0.4822
2024-06-02 03:27:08 [INFO]: Epoch 007 - training loss: 0.4398, validation loss: 0.4720
2024-06-02 03:27:10 [INFO]: Epoch 008 - training loss: 0.4209, validation loss: 0.4696
2024-06-02 03:27:12 [INFO]: Epoch 009 - training loss: 0.4180, validation loss: 0.4646
2024-06-02 03:27:14 [INFO]: Epoch 010 - training loss: 0.4122, validation loss: 0.4559
2024-06-02 03:27:16 [INFO]: Epoch 011 - training loss: 0.4030, validation loss: 0.4570
2024-06-02 03:27:17 [INFO]: Epoch 012 - training loss: 0.3985, validation loss: 0.4533
2024-06-02 03:27:19 [INFO]: Epoch 013 - training loss: 0.3972, validation loss: 0.4452
2024-06-02 03:27:21 [INFO]: Epoch 014 - training loss: 0.3845, validation loss: 0.4454
2024-06-02 03:27:23 [INFO]: Epoch 015 - training loss: 0.3775, validation loss: 0.4459
2024-06-02 03:27:25 [INFO]: Epoch 016 - training loss: 0.3717, validation loss: 0.4456
2024-06-02 03:27:26 [INFO]: Epoch 017 - training loss: 0.3783, validation loss: 0.4372
2024-06-02 03:27:28 [INFO]: Epoch 018 - training loss: 0.3652, validation loss: 0.4367
2024-06-02 03:27:30 [INFO]: Epoch 019 - training loss: 0.3574, validation loss: 0.4329
2024-06-02 03:27:32 [INFO]: Epoch 020 - training loss: 0.3574, validation loss: 0.4342
2024-06-02 03:27:34 [INFO]: Epoch 021 - training loss: 0.3554, validation loss: 0.4309
2024-06-02 03:27:35 [INFO]: Epoch 022 - training loss: 0.3528, validation loss: 0.4277
2024-06-02 03:27:37 [INFO]: Epoch 023 - training loss: 0.3472, validation loss: 0.4248
2024-06-02 03:27:39 [INFO]: Epoch 024 - training loss: 0.3516, validation loss: 0.4302
2024-06-02 03:27:41 [INFO]: Epoch 025 - training loss: 0.3487, validation loss: 0.4260
2024-06-02 03:27:42 [INFO]: Epoch 026 - training loss: 0.3461, validation loss: 0.4237
2024-06-02 03:27:44 [INFO]: Epoch 027 - training loss: 0.3422, validation loss: 0.4258
2024-06-02 03:27:46 [INFO]: Epoch 028 - training loss: 0.3334, validation loss: 0.4181
2024-06-02 03:27:48 [INFO]: Epoch 029 - training loss: 0.3327, validation loss: 0.4166
2024-06-02 03:27:50 [INFO]: Epoch 030 - training loss: 0.3334, validation loss: 0.4161
2024-06-02 03:27:52 [INFO]: Epoch 031 - training loss: 0.3284, validation loss: 0.4226
2024-06-02 03:27:53 [INFO]: Epoch 032 - training loss: 0.3320, validation loss: 0.4204
2024-06-02 03:27:55 [INFO]: Epoch 033 - training loss: 0.3265, validation loss: 0.4185
2024-06-02 03:27:57 [INFO]: Epoch 034 - training loss: 0.3222, validation loss: 0.4162
2024-06-02 03:27:58 [INFO]: Epoch 035 - training loss: 0.3238, validation loss: 0.4132
2024-06-02 03:28:00 [INFO]: Epoch 036 - training loss: 0.3199, validation loss: 0.4110
2024-06-02 03:28:02 [INFO]: Epoch 037 - training loss: 0.3192, validation loss: 0.4167
2024-06-02 03:28:04 [INFO]: Epoch 038 - training loss: 0.3188, validation loss: 0.4121
2024-06-02 03:28:05 [INFO]: Epoch 039 - training loss: 0.3130, validation loss: 0.4092
2024-06-02 03:28:07 [INFO]: Epoch 040 - training loss: 0.3148, validation loss: 0.4113
2024-06-02 03:28:09 [INFO]: Epoch 041 - training loss: 0.3142, validation loss: 0.4080
2024-06-02 03:28:11 [INFO]: Epoch 042 - training loss: 0.3105, validation loss: 0.4065
2024-06-02 03:28:13 [INFO]: Epoch 043 - training loss: 0.3121, validation loss: 0.4090
2024-06-02 03:28:14 [INFO]: Epoch 044 - training loss: 0.3070, validation loss: 0.4041
2024-06-02 03:28:16 [INFO]: Epoch 045 - training loss: 0.3038, validation loss: 0.4083
2024-06-02 03:28:18 [INFO]: Epoch 046 - training loss: 0.3070, validation loss: 0.4104
2024-06-02 03:28:20 [INFO]: Epoch 047 - training loss: 0.3064, validation loss: 0.4012
2024-06-02 03:28:22 [INFO]: Epoch 048 - training loss: 0.3018, validation loss: 0.4045
2024-06-02 03:28:23 [INFO]: Epoch 049 - training loss: 0.3001, validation loss: 0.4028
2024-06-02 03:28:25 [INFO]: Epoch 050 - training loss: 0.2993, validation loss: 0.3992
2024-06-02 03:28:27 [INFO]: Epoch 051 - training loss: 0.2968, validation loss: 0.3981
2024-06-02 03:28:29 [INFO]: Epoch 052 - training loss: 0.2997, validation loss: 0.3989
2024-06-02 03:28:30 [INFO]: Epoch 053 - training loss: 0.2991, validation loss: 0.3983
2024-06-02 03:28:32 [INFO]: Epoch 054 - training loss: 0.2967, validation loss: 0.3964
2024-06-02 03:28:34 [INFO]: Epoch 055 - training loss: 0.2927, validation loss: 0.3966
2024-06-02 03:28:35 [INFO]: Epoch 056 - training loss: 0.2963, validation loss: 0.3967
2024-06-02 03:28:37 [INFO]: Epoch 057 - training loss: 0.2903, validation loss: 0.3944
2024-06-02 03:28:39 [INFO]: Epoch 058 - training loss: 0.2922, validation loss: 0.3946
2024-06-02 03:28:40 [INFO]: Epoch 059 - training loss: 0.2901, validation loss: 0.3958
2024-06-02 03:28:42 [INFO]: Epoch 060 - training loss: 0.2896, validation loss: 0.3960
2024-06-02 03:28:44 [INFO]: Epoch 061 - training loss: 0.2917, validation loss: 0.3943
2024-06-02 03:28:45 [INFO]: Epoch 062 - training loss: 0.2868, validation loss: 0.3937
2024-06-02 03:28:47 [INFO]: Epoch 063 - training loss: 0.2896, validation loss: 0.3934
2024-06-02 03:28:48 [INFO]: Epoch 064 - training loss: 0.2883, validation loss: 0.3919
2024-06-02 03:28:50 [INFO]: Epoch 065 - training loss: 0.2837, validation loss: 0.3898
2024-06-02 03:28:52 [INFO]: Epoch 066 - training loss: 0.2820, validation loss: 0.3907
2024-06-02 03:28:54 [INFO]: Epoch 067 - training loss: 0.2823, validation loss: 0.3882
2024-06-02 03:28:55 [INFO]: Epoch 068 - training loss: 0.2803, validation loss: 0.3882
2024-06-02 03:28:57 [INFO]: Epoch 069 - training loss: 0.2838, validation loss: 0.3861
2024-06-02 03:28:59 [INFO]: Epoch 070 - training loss: 0.2820, validation loss: 0.3880
2024-06-02 03:29:00 [INFO]: Epoch 071 - training loss: 0.2800, validation loss: 0.3937
2024-06-02 03:29:02 [INFO]: Epoch 072 - training loss: 0.2855, validation loss: 0.3863
2024-06-02 03:29:04 [INFO]: Epoch 073 - training loss: 0.2829, validation loss: 0.3871
2024-06-02 03:29:05 [INFO]: Epoch 074 - training loss: 0.2806, validation loss: 0.3851
2024-06-02 03:29:07 [INFO]: Epoch 075 - training loss: 0.2770, validation loss: 0.3812
2024-06-02 03:29:09 [INFO]: Epoch 076 - training loss: 0.2745, validation loss: 0.3848
2024-06-02 03:29:10 [INFO]: Epoch 077 - training loss: 0.2746, validation loss: 0.3835
2024-06-02 03:29:12 [INFO]: Epoch 078 - training loss: 0.2758, validation loss: 0.3827
2024-06-02 03:29:14 [INFO]: Epoch 079 - training loss: 0.2751, validation loss: 0.3831
2024-06-02 03:29:15 [INFO]: Epoch 080 - training loss: 0.2721, validation loss: 0.3824
2024-06-02 03:29:17 [INFO]: Epoch 081 - training loss: 0.2708, validation loss: 0.3798
2024-06-02 03:29:19 [INFO]: Epoch 082 - training loss: 0.2703, validation loss: 0.3798
2024-06-02 03:29:20 [INFO]: Epoch 083 - training loss: 0.2694, validation loss: 0.3780
2024-06-02 03:29:22 [INFO]: Epoch 084 - training loss: 0.2717, validation loss: 0.3795
2024-06-02 03:29:24 [INFO]: Epoch 085 - training loss: 0.2736, validation loss: 0.3838
2024-06-02 03:29:26 [INFO]: Epoch 086 - training loss: 0.2811, validation loss: 0.3802
2024-06-02 03:29:28 [INFO]: Epoch 087 - training loss: 0.2721, validation loss: 0.3818
2024-06-02 03:29:29 [INFO]: Epoch 088 - training loss: 0.2687, validation loss: 0.3768
2024-06-02 03:29:31 [INFO]: Epoch 089 - training loss: 0.2686, validation loss: 0.3797
2024-06-02 03:29:33 [INFO]: Epoch 090 - training loss: 0.2672, validation loss: 0.3798
2024-06-02 03:29:35 [INFO]: Epoch 091 - training loss: 0.2679, validation loss: 0.3790
2024-06-02 03:29:36 [INFO]: Epoch 092 - training loss: 0.2669, validation loss: 0.3767
2024-06-02 03:29:38 [INFO]: Epoch 093 - training loss: 0.2664, validation loss: 0.3770
2024-06-02 03:29:40 [INFO]: Epoch 094 - training loss: 0.2617, validation loss: 0.3777
2024-06-02 03:29:42 [INFO]: Epoch 095 - training loss: 0.2626, validation loss: 0.3773
2024-06-02 03:29:43 [INFO]: Epoch 096 - training loss: 0.2626, validation loss: 0.3765
2024-06-02 03:29:45 [INFO]: Epoch 097 - training loss: 0.2632, validation loss: 0.3787
2024-06-02 03:29:47 [INFO]: Epoch 098 - training loss: 0.2620, validation loss: 0.3775
2024-06-02 03:29:49 [INFO]: Epoch 099 - training loss: 0.2615, validation loss: 0.3779
2024-06-02 03:29:50 [INFO]: Epoch 100 - training loss: 0.2649, validation loss: 0.3798
2024-06-02 03:29:51 [INFO]: Finished training. The best model is from epoch#96.
2024-06-02 03:29:51 [INFO]: Saved the model to results_point_rate01/PeMS/Pyraformer_PeMS/round_1/20240602_T032655/Pyraformer.pypots
2024-06-02 03:29:51 [INFO]: Successfully saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_1/imputation.pkl
2024-06-02 03:29:51 [INFO]: Round1 - Pyraformer on PeMS: MAE=0.2834, MSE=0.5657, MRE=0.3512
2024-06-02 03:29:51 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 03:29:51 [INFO]: Using the given device: cuda:0
2024-06-02 03:29:51 [INFO]: Model files will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_2/20240602_T032951
2024-06-02 03:29:51 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_2/20240602_T032951/tensorboard
2024-06-02 03:29:51 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-02 03:29:53 [INFO]: Epoch 001 - training loss: 0.9360, validation loss: 0.5735
2024-06-02 03:29:54 [INFO]: Epoch 002 - training loss: 0.5844, validation loss: 0.5184
2024-06-02 03:29:56 [INFO]: Epoch 003 - training loss: 0.5176, validation loss: 0.5000
2024-06-02 03:29:58 [INFO]: Epoch 004 - training loss: 0.4809, validation loss: 0.4856
2024-06-02 03:30:00 [INFO]: Epoch 005 - training loss: 0.4643, validation loss: 0.4801
2024-06-02 03:30:01 [INFO]: Epoch 006 - training loss: 0.4544, validation loss: 0.4758
2024-06-02 03:30:03 [INFO]: Epoch 007 - training loss: 0.4445, validation loss: 0.4728
2024-06-02 03:30:04 [INFO]: Epoch 008 - training loss: 0.4259, validation loss: 0.4740
2024-06-02 03:30:06 [INFO]: Epoch 009 - training loss: 0.4210, validation loss: 0.4645
2024-06-02 03:30:08 [INFO]: Epoch 010 - training loss: 0.4185, validation loss: 0.4617
2024-06-02 03:30:09 [INFO]: Epoch 011 - training loss: 0.4052, validation loss: 0.4527
2024-06-02 03:30:11 [INFO]: Epoch 012 - training loss: 0.3945, validation loss: 0.4515
2024-06-02 03:30:13 [INFO]: Epoch 013 - training loss: 0.3879, validation loss: 0.4518
2024-06-02 03:30:14 [INFO]: Epoch 014 - training loss: 0.3824, validation loss: 0.4431
2024-06-02 03:30:16 [INFO]: Epoch 015 - training loss: 0.3826, validation loss: 0.4405
2024-06-02 03:30:18 [INFO]: Epoch 016 - training loss: 0.3788, validation loss: 0.4377
2024-06-02 03:30:20 [INFO]: Epoch 017 - training loss: 0.3702, validation loss: 0.4416
2024-06-02 03:30:21 [INFO]: Epoch 018 - training loss: 0.3652, validation loss: 0.4327
2024-06-02 03:30:23 [INFO]: Epoch 019 - training loss: 0.3618, validation loss: 0.4275
2024-06-02 03:30:25 [INFO]: Epoch 020 - training loss: 0.3546, validation loss: 0.4279
2024-06-02 03:30:26 [INFO]: Epoch 021 - training loss: 0.3564, validation loss: 0.4299
2024-06-02 03:30:28 [INFO]: Epoch 022 - training loss: 0.3441, validation loss: 0.4207
2024-06-02 03:30:30 [INFO]: Epoch 023 - training loss: 0.3453, validation loss: 0.4231
2024-06-02 03:30:32 [INFO]: Epoch 024 - training loss: 0.3436, validation loss: 0.4221
2024-06-02 03:30:33 [INFO]: Epoch 025 - training loss: 0.3463, validation loss: 0.4209
2024-06-02 03:30:35 [INFO]: Epoch 026 - training loss: 0.3449, validation loss: 0.4182
2024-06-02 03:30:37 [INFO]: Epoch 027 - training loss: 0.3378, validation loss: 0.4191
2024-06-02 03:30:39 [INFO]: Epoch 028 - training loss: 0.3330, validation loss: 0.4159
2024-06-02 03:30:40 [INFO]: Epoch 029 - training loss: 0.3288, validation loss: 0.4123
2024-06-02 03:30:42 [INFO]: Epoch 030 - training loss: 0.3303, validation loss: 0.4150
2024-06-02 03:30:44 [INFO]: Epoch 031 - training loss: 0.3275, validation loss: 0.4155
2024-06-02 03:30:46 [INFO]: Epoch 032 - training loss: 0.3276, validation loss: 0.4099
2024-06-02 03:30:48 [INFO]: Epoch 033 - training loss: 0.3289, validation loss: 0.4117
2024-06-02 03:30:49 [INFO]: Epoch 034 - training loss: 0.3234, validation loss: 0.4106
2024-06-02 03:30:51 [INFO]: Epoch 035 - training loss: 0.3219, validation loss: 0.4086
2024-06-02 03:30:53 [INFO]: Epoch 036 - training loss: 0.3189, validation loss: 0.4062
2024-06-02 03:30:55 [INFO]: Epoch 037 - training loss: 0.3165, validation loss: 0.4044
2024-06-02 03:30:57 [INFO]: Epoch 038 - training loss: 0.3142, validation loss: 0.4039
2024-06-02 03:30:58 [INFO]: Epoch 039 - training loss: 0.3092, validation loss: 0.4024
2024-06-02 03:31:00 [INFO]: Epoch 040 - training loss: 0.3084, validation loss: 0.4038
2024-06-02 03:31:02 [INFO]: Epoch 041 - training loss: 0.3093, validation loss: 0.3981
2024-06-02 03:31:04 [INFO]: Epoch 042 - training loss: 0.3070, validation loss: 0.4030
2024-06-02 03:31:05 [INFO]: Epoch 043 - training loss: 0.3074, validation loss: 0.3967
2024-06-02 03:31:07 [INFO]: Epoch 044 - training loss: 0.3060, validation loss: 0.3990
2024-06-02 03:31:08 [INFO]: Epoch 045 - training loss: 0.3059, validation loss: 0.3967
2024-06-02 03:31:10 [INFO]: Epoch 046 - training loss: 0.3076, validation loss: 0.3990
2024-06-02 03:31:12 [INFO]: Epoch 047 - training loss: 0.3018, validation loss: 0.3984
2024-06-02 03:31:13 [INFO]: Epoch 048 - training loss: 0.3009, validation loss: 0.3990
2024-06-02 03:31:15 [INFO]: Epoch 049 - training loss: 0.2990, validation loss: 0.3968
2024-06-02 03:31:16 [INFO]: Epoch 050 - training loss: 0.2961, validation loss: 0.3945
2024-06-02 03:31:18 [INFO]: Epoch 051 - training loss: 0.2974, validation loss: 0.3966
2024-06-02 03:31:20 [INFO]: Epoch 052 - training loss: 0.2968, validation loss: 0.3927
2024-06-02 03:31:22 [INFO]: Epoch 053 - training loss: 0.2925, validation loss: 0.3937
2024-06-02 03:31:23 [INFO]: Epoch 054 - training loss: 0.2934, validation loss: 0.3923
2024-06-02 03:31:25 [INFO]: Epoch 055 - training loss: 0.2910, validation loss: 0.3931
2024-06-02 03:31:27 [INFO]: Epoch 056 - training loss: 0.2891, validation loss: 0.3914
2024-06-02 03:31:28 [INFO]: Epoch 057 - training loss: 0.2903, validation loss: 0.3945
2024-06-02 03:31:30 [INFO]: Epoch 058 - training loss: 0.2943, validation loss: 0.3912
2024-06-02 03:31:32 [INFO]: Epoch 059 - training loss: 0.2930, validation loss: 0.3911
2024-06-02 03:31:34 [INFO]: Epoch 060 - training loss: 0.2893, validation loss: 0.3872
2024-06-02 03:31:35 [INFO]: Epoch 061 - training loss: 0.2866, validation loss: 0.3876
2024-06-02 03:31:37 [INFO]: Epoch 062 - training loss: 0.2830, validation loss: 0.3894
2024-06-02 03:31:38 [INFO]: Epoch 063 - training loss: 0.2833, validation loss: 0.3898
2024-06-02 03:31:40 [INFO]: Epoch 064 - training loss: 0.2817, validation loss: 0.3894
2024-06-02 03:31:42 [INFO]: Epoch 065 - training loss: 0.2833, validation loss: 0.3872
2024-06-02 03:31:43 [INFO]: Epoch 066 - training loss: 0.2825, validation loss: 0.3862
2024-06-02 03:31:45 [INFO]: Epoch 067 - training loss: 0.2843, validation loss: 0.3873
2024-06-02 03:31:47 [INFO]: Epoch 068 - training loss: 0.2812, validation loss: 0.3849
2024-06-02 03:31:48 [INFO]: Epoch 069 - training loss: 0.2815, validation loss: 0.3852
2024-06-02 03:31:50 [INFO]: Epoch 070 - training loss: 0.2810, validation loss: 0.3848
2024-06-02 03:31:52 [INFO]: Epoch 071 - training loss: 0.2763, validation loss: 0.3893
2024-06-02 03:31:54 [INFO]: Epoch 072 - training loss: 0.2892, validation loss: 0.3887
2024-06-02 03:31:55 [INFO]: Epoch 073 - training loss: 0.2876, validation loss: 0.3847
2024-06-02 03:31:57 [INFO]: Epoch 074 - training loss: 0.2793, validation loss: 0.3875
2024-06-02 03:31:59 [INFO]: Epoch 075 - training loss: 0.2776, validation loss: 0.3862
2024-06-02 03:32:01 [INFO]: Epoch 076 - training loss: 0.2721, validation loss: 0.3842
2024-06-02 03:32:03 [INFO]: Epoch 077 - training loss: 0.2737, validation loss: 0.3820
2024-06-02 03:32:05 [INFO]: Epoch 078 - training loss: 0.2727, validation loss: 0.3835
2024-06-02 03:32:06 [INFO]: Epoch 079 - training loss: 0.2728, validation loss: 0.3808
2024-06-02 03:32:08 [INFO]: Epoch 080 - training loss: 0.2734, validation loss: 0.3813
2024-06-02 03:32:10 [INFO]: Epoch 081 - training loss: 0.2748, validation loss: 0.3788
2024-06-02 03:32:11 [INFO]: Epoch 082 - training loss: 0.2693, validation loss: 0.3806
2024-06-02 03:32:12 [INFO]: Epoch 083 - training loss: 0.2693, validation loss: 0.3817
2024-06-02 03:32:13 [INFO]: Epoch 084 - training loss: 0.2711, validation loss: 0.3807
2024-06-02 03:32:14 [INFO]: Epoch 085 - training loss: 0.2721, validation loss: 0.3805
2024-06-02 03:32:15 [INFO]: Epoch 086 - training loss: 0.2680, validation loss: 0.3803
2024-06-02 03:32:16 [INFO]: Epoch 087 - training loss: 0.2682, validation loss: 0.3811
2024-06-02 03:32:18 [INFO]: Epoch 088 - training loss: 0.2681, validation loss: 0.3799
2024-06-02 03:32:18 [INFO]: Epoch 089 - training loss: 0.2674, validation loss: 0.3780
2024-06-02 03:32:20 [INFO]: Epoch 090 - training loss: 0.2658, validation loss: 0.3781
2024-06-02 03:32:21 [INFO]: Epoch 091 - training loss: 0.2666, validation loss: 0.3782
2024-06-02 03:32:22 [INFO]: Epoch 092 - training loss: 0.2692, validation loss: 0.3812
2024-06-02 03:32:23 [INFO]: Epoch 093 - training loss: 0.2643, validation loss: 0.3822
2024-06-02 03:32:24 [INFO]: Epoch 094 - training loss: 0.2656, validation loss: 0.3801
2024-06-02 03:32:25 [INFO]: Epoch 095 - training loss: 0.2642, validation loss: 0.3798
2024-06-02 03:32:27 [INFO]: Epoch 096 - training loss: 0.2629, validation loss: 0.3785
2024-06-02 03:32:28 [INFO]: Epoch 097 - training loss: 0.2653, validation loss: 0.3780
2024-06-02 03:32:29 [INFO]: Epoch 098 - training loss: 0.2628, validation loss: 0.3775
2024-06-02 03:32:30 [INFO]: Epoch 099 - training loss: 0.2594, validation loss: 0.3759
2024-06-02 03:32:31 [INFO]: Epoch 100 - training loss: 0.2601, validation loss: 0.3762
2024-06-02 03:32:31 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 03:32:31 [INFO]: Saved the model to results_point_rate01/PeMS/Pyraformer_PeMS/round_2/20240602_T032951/Pyraformer.pypots
2024-06-02 03:32:31 [INFO]: Successfully saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_2/imputation.pkl
2024-06-02 03:32:31 [INFO]: Round2 - Pyraformer on PeMS: MAE=0.2808, MSE=0.5595, MRE=0.3481
2024-06-02 03:32:31 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 03:32:31 [INFO]: Using the given device: cuda:0
2024-06-02 03:32:31 [INFO]: Model files will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_3/20240602_T033231
2024-06-02 03:32:31 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_3/20240602_T033231/tensorboard
2024-06-02 03:32:31 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-02 03:32:32 [INFO]: Epoch 001 - training loss: 0.9565, validation loss: 0.5842
2024-06-02 03:32:33 [INFO]: Epoch 002 - training loss: 0.5952, validation loss: 0.5145
2024-06-02 03:32:35 [INFO]: Epoch 003 - training loss: 0.5192, validation loss: 0.5047
2024-06-02 03:32:36 [INFO]: Epoch 004 - training loss: 0.4867, validation loss: 0.4980
2024-06-02 03:32:37 [INFO]: Epoch 005 - training loss: 0.4691, validation loss: 0.4854
2024-06-02 03:32:38 [INFO]: Epoch 006 - training loss: 0.4564, validation loss: 0.4805
2024-06-02 03:32:39 [INFO]: Epoch 007 - training loss: 0.4469, validation loss: 0.4777
2024-06-02 03:32:41 [INFO]: Epoch 008 - training loss: 0.4337, validation loss: 0.4661
2024-06-02 03:32:42 [INFO]: Epoch 009 - training loss: 0.4193, validation loss: 0.4672
2024-06-02 03:32:43 [INFO]: Epoch 010 - training loss: 0.4142, validation loss: 0.4576
2024-06-02 03:32:44 [INFO]: Epoch 011 - training loss: 0.3994, validation loss: 0.4571
2024-06-02 03:32:45 [INFO]: Epoch 012 - training loss: 0.3998, validation loss: 0.4531
2024-06-02 03:32:46 [INFO]: Epoch 013 - training loss: 0.3905, validation loss: 0.4510
2024-06-02 03:32:47 [INFO]: Epoch 014 - training loss: 0.3835, validation loss: 0.4528
2024-06-02 03:32:49 [INFO]: Epoch 015 - training loss: 0.3791, validation loss: 0.4427
2024-06-02 03:32:50 [INFO]: Epoch 016 - training loss: 0.3749, validation loss: 0.4480
2024-06-02 03:32:51 [INFO]: Epoch 017 - training loss: 0.3745, validation loss: 0.4339
2024-06-02 03:32:52 [INFO]: Epoch 018 - training loss: 0.3688, validation loss: 0.4464
2024-06-02 03:32:53 [INFO]: Epoch 019 - training loss: 0.3651, validation loss: 0.4377
2024-06-02 03:32:54 [INFO]: Epoch 020 - training loss: 0.3574, validation loss: 0.4333
2024-06-02 03:32:55 [INFO]: Epoch 021 - training loss: 0.3533, validation loss: 0.4322
2024-06-02 03:32:56 [INFO]: Epoch 022 - training loss: 0.3533, validation loss: 0.4306
2024-06-02 03:32:57 [INFO]: Epoch 023 - training loss: 0.3432, validation loss: 0.4265
2024-06-02 03:32:58 [INFO]: Epoch 024 - training loss: 0.3444, validation loss: 0.4208
2024-06-02 03:32:59 [INFO]: Epoch 025 - training loss: 0.3436, validation loss: 0.4273
2024-06-02 03:33:01 [INFO]: Epoch 026 - training loss: 0.3441, validation loss: 0.4249
2024-06-02 03:33:02 [INFO]: Epoch 027 - training loss: 0.3421, validation loss: 0.4206
2024-06-02 03:33:03 [INFO]: Epoch 028 - training loss: 0.3362, validation loss: 0.4183
2024-06-02 03:33:04 [INFO]: Epoch 029 - training loss: 0.3346, validation loss: 0.4154
2024-06-02 03:33:05 [INFO]: Epoch 030 - training loss: 0.3314, validation loss: 0.4160
2024-06-02 03:33:06 [INFO]: Epoch 031 - training loss: 0.3278, validation loss: 0.4131
2024-06-02 03:33:07 [INFO]: Epoch 032 - training loss: 0.3281, validation loss: 0.4166
2024-06-02 03:33:08 [INFO]: Epoch 033 - training loss: 0.3247, validation loss: 0.4117
2024-06-02 03:33:09 [INFO]: Epoch 034 - training loss: 0.3212, validation loss: 0.4090
2024-06-02 03:33:10 [INFO]: Epoch 035 - training loss: 0.3259, validation loss: 0.4102
2024-06-02 03:33:11 [INFO]: Epoch 036 - training loss: 0.3207, validation loss: 0.4163
2024-06-02 03:33:12 [INFO]: Epoch 037 - training loss: 0.3175, validation loss: 0.4050
2024-06-02 03:33:13 [INFO]: Epoch 038 - training loss: 0.3187, validation loss: 0.4069
2024-06-02 03:33:14 [INFO]: Epoch 039 - training loss: 0.3140, validation loss: 0.4064
2024-06-02 03:33:15 [INFO]: Epoch 040 - training loss: 0.3136, validation loss: 0.4052
2024-06-02 03:33:16 [INFO]: Epoch 041 - training loss: 0.3156, validation loss: 0.4016
2024-06-02 03:33:17 [INFO]: Epoch 042 - training loss: 0.3157, validation loss: 0.4044
2024-06-02 03:33:18 [INFO]: Epoch 043 - training loss: 0.3060, validation loss: 0.4020
2024-06-02 03:33:20 [INFO]: Epoch 044 - training loss: 0.3103, validation loss: 0.4018
2024-06-02 03:33:21 [INFO]: Epoch 045 - training loss: 0.3083, validation loss: 0.4017
2024-06-02 03:33:22 [INFO]: Epoch 046 - training loss: 0.3060, validation loss: 0.4023
2024-06-02 03:33:23 [INFO]: Epoch 047 - training loss: 0.3056, validation loss: 0.3952
2024-06-02 03:33:24 [INFO]: Epoch 048 - training loss: 0.3004, validation loss: 0.4006
2024-06-02 03:33:25 [INFO]: Epoch 049 - training loss: 0.3016, validation loss: 0.3969
2024-06-02 03:33:26 [INFO]: Epoch 050 - training loss: 0.2985, validation loss: 0.3973
2024-06-02 03:33:27 [INFO]: Epoch 051 - training loss: 0.3009, validation loss: 0.3953
2024-06-02 03:33:28 [INFO]: Epoch 052 - training loss: 0.2977, validation loss: 0.3980
2024-06-02 03:33:29 [INFO]: Epoch 053 - training loss: 0.2947, validation loss: 0.3946
2024-06-02 03:33:31 [INFO]: Epoch 054 - training loss: 0.2961, validation loss: 0.3976
2024-06-02 03:33:32 [INFO]: Epoch 055 - training loss: 0.2957, validation loss: 0.3946
2024-06-02 03:33:33 [INFO]: Epoch 056 - training loss: 0.2961, validation loss: 0.3939
2024-06-02 03:33:34 [INFO]: Epoch 057 - training loss: 0.2973, validation loss: 0.3947
2024-06-02 03:33:35 [INFO]: Epoch 058 - training loss: 0.2980, validation loss: 0.3927
2024-06-02 03:33:36 [INFO]: Epoch 059 - training loss: 0.2918, validation loss: 0.3903
2024-06-02 03:33:37 [INFO]: Epoch 060 - training loss: 0.2870, validation loss: 0.3922
2024-06-02 03:33:38 [INFO]: Epoch 061 - training loss: 0.2865, validation loss: 0.3920
2024-06-02 03:33:39 [INFO]: Epoch 062 - training loss: 0.2930, validation loss: 0.3900
2024-06-02 03:33:40 [INFO]: Epoch 063 - training loss: 0.2887, validation loss: 0.3933
2024-06-02 03:33:42 [INFO]: Epoch 064 - training loss: 0.2864, validation loss: 0.3899
2024-06-02 03:33:43 [INFO]: Epoch 065 - training loss: 0.2845, validation loss: 0.3906
2024-06-02 03:33:44 [INFO]: Epoch 066 - training loss: 0.2813, validation loss: 0.3855
2024-06-02 03:33:45 [INFO]: Epoch 067 - training loss: 0.2824, validation loss: 0.3871
2024-06-02 03:33:47 [INFO]: Epoch 068 - training loss: 0.2798, validation loss: 0.3888
2024-06-02 03:33:48 [INFO]: Epoch 069 - training loss: 0.2817, validation loss: 0.3850
2024-06-02 03:33:49 [INFO]: Epoch 070 - training loss: 0.2808, validation loss: 0.3918
2024-06-02 03:33:50 [INFO]: Epoch 071 - training loss: 0.2912, validation loss: 0.3862
2024-06-02 03:33:51 [INFO]: Epoch 072 - training loss: 0.2856, validation loss: 0.3877
2024-06-02 03:33:53 [INFO]: Epoch 073 - training loss: 0.2811, validation loss: 0.3849
2024-06-02 03:33:54 [INFO]: Epoch 074 - training loss: 0.2827, validation loss: 0.3891
2024-06-02 03:33:55 [INFO]: Epoch 075 - training loss: 0.2773, validation loss: 0.3866
2024-06-02 03:33:56 [INFO]: Epoch 076 - training loss: 0.2741, validation loss: 0.3857
2024-06-02 03:33:57 [INFO]: Epoch 077 - training loss: 0.2755, validation loss: 0.3823
2024-06-02 03:33:58 [INFO]: Epoch 078 - training loss: 0.2716, validation loss: 0.3839
2024-06-02 03:33:59 [INFO]: Epoch 079 - training loss: 0.2741, validation loss: 0.3825
2024-06-02 03:34:00 [INFO]: Epoch 080 - training loss: 0.2731, validation loss: 0.3783
2024-06-02 03:34:01 [INFO]: Epoch 081 - training loss: 0.2700, validation loss: 0.3818
2024-06-02 03:34:03 [INFO]: Epoch 082 - training loss: 0.2719, validation loss: 0.3833
2024-06-02 03:34:04 [INFO]: Epoch 083 - training loss: 0.2724, validation loss: 0.3787
2024-06-02 03:34:05 [INFO]: Epoch 084 - training loss: 0.2715, validation loss: 0.3803
2024-06-02 03:34:06 [INFO]: Epoch 085 - training loss: 0.2706, validation loss: 0.3800
2024-06-02 03:34:07 [INFO]: Epoch 086 - training loss: 0.2707, validation loss: 0.3791
2024-06-02 03:34:08 [INFO]: Epoch 087 - training loss: 0.2728, validation loss: 0.3802
2024-06-02 03:34:09 [INFO]: Epoch 088 - training loss: 0.2682, validation loss: 0.3787
2024-06-02 03:34:11 [INFO]: Epoch 089 - training loss: 0.2682, validation loss: 0.3812
2024-06-02 03:34:12 [INFO]: Epoch 090 - training loss: 0.2695, validation loss: 0.3793
2024-06-02 03:34:12 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 03:34:12 [INFO]: Finished training. The best model is from epoch#80.
2024-06-02 03:34:12 [INFO]: Saved the model to results_point_rate01/PeMS/Pyraformer_PeMS/round_3/20240602_T033231/Pyraformer.pypots
2024-06-02 03:34:12 [INFO]: Successfully saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_3/imputation.pkl
2024-06-02 03:34:12 [INFO]: Round3 - Pyraformer on PeMS: MAE=0.2852, MSE=0.5625, MRE=0.3535
2024-06-02 03:34:12 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 03:34:12 [INFO]: Using the given device: cuda:0
2024-06-02 03:34:12 [INFO]: Model files will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_4/20240602_T033412
2024-06-02 03:34:12 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_4/20240602_T033412/tensorboard
2024-06-02 03:34:12 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-02 03:34:13 [INFO]: Epoch 001 - training loss: 0.9294, validation loss: 0.6002
2024-06-02 03:34:14 [INFO]: Epoch 002 - training loss: 0.5852, validation loss: 0.5179
2024-06-02 03:34:15 [INFO]: Epoch 003 - training loss: 0.5238, validation loss: 0.5037
2024-06-02 03:34:16 [INFO]: Epoch 004 - training loss: 0.4833, validation loss: 0.4896
2024-06-02 03:34:18 [INFO]: Epoch 005 - training loss: 0.4610, validation loss: 0.4935
2024-06-02 03:34:19 [INFO]: Epoch 006 - training loss: 0.4530, validation loss: 0.4773
2024-06-02 03:34:20 [INFO]: Epoch 007 - training loss: 0.4378, validation loss: 0.4720
2024-06-02 03:34:21 [INFO]: Epoch 008 - training loss: 0.4293, validation loss: 0.4664
2024-06-02 03:34:22 [INFO]: Epoch 009 - training loss: 0.4217, validation loss: 0.4679
2024-06-02 03:34:23 [INFO]: Epoch 010 - training loss: 0.4061, validation loss: 0.4630
2024-06-02 03:34:25 [INFO]: Epoch 011 - training loss: 0.4075, validation loss: 0.4555
2024-06-02 03:34:26 [INFO]: Epoch 012 - training loss: 0.3963, validation loss: 0.4506
2024-06-02 03:34:27 [INFO]: Epoch 013 - training loss: 0.3867, validation loss: 0.4514
2024-06-02 03:34:28 [INFO]: Epoch 014 - training loss: 0.3787, validation loss: 0.4420
2024-06-02 03:34:29 [INFO]: Epoch 015 - training loss: 0.3789, validation loss: 0.4367
2024-06-02 03:34:30 [INFO]: Epoch 016 - training loss: 0.3704, validation loss: 0.4431
2024-06-02 03:34:31 [INFO]: Epoch 017 - training loss: 0.3702, validation loss: 0.4347
2024-06-02 03:34:32 [INFO]: Epoch 018 - training loss: 0.3623, validation loss: 0.4331
2024-06-02 03:34:33 [INFO]: Epoch 019 - training loss: 0.3605, validation loss: 0.4280
2024-06-02 03:34:34 [INFO]: Epoch 020 - training loss: 0.3573, validation loss: 0.4260
2024-06-02 03:34:36 [INFO]: Epoch 021 - training loss: 0.3512, validation loss: 0.4280
2024-06-02 03:34:37 [INFO]: Epoch 022 - training loss: 0.3493, validation loss: 0.4227
2024-06-02 03:34:38 [INFO]: Epoch 023 - training loss: 0.3483, validation loss: 0.4211
2024-06-02 03:34:39 [INFO]: Epoch 024 - training loss: 0.3413, validation loss: 0.4205
2024-06-02 03:34:40 [INFO]: Epoch 025 - training loss: 0.3425, validation loss: 0.4214
2024-06-02 03:34:41 [INFO]: Epoch 026 - training loss: 0.3376, validation loss: 0.4166
2024-06-02 03:34:42 [INFO]: Epoch 027 - training loss: 0.3359, validation loss: 0.4143
2024-06-02 03:34:43 [INFO]: Epoch 028 - training loss: 0.3319, validation loss: 0.4176
2024-06-02 03:34:45 [INFO]: Epoch 029 - training loss: 0.3345, validation loss: 0.4146
2024-06-02 03:34:45 [INFO]: Epoch 030 - training loss: 0.3275, validation loss: 0.4174
2024-06-02 03:34:47 [INFO]: Epoch 031 - training loss: 0.3323, validation loss: 0.4063
2024-06-02 03:34:48 [INFO]: Epoch 032 - training loss: 0.3244, validation loss: 0.4100
2024-06-02 03:34:49 [INFO]: Epoch 033 - training loss: 0.3217, validation loss: 0.4076
2024-06-02 03:34:50 [INFO]: Epoch 034 - training loss: 0.3182, validation loss: 0.4071
2024-06-02 03:34:51 [INFO]: Epoch 035 - training loss: 0.3188, validation loss: 0.4030
2024-06-02 03:34:52 [INFO]: Epoch 036 - training loss: 0.3183, validation loss: 0.4023
2024-06-02 03:34:53 [INFO]: Epoch 037 - training loss: 0.3190, validation loss: 0.4048
2024-06-02 03:34:54 [INFO]: Epoch 038 - training loss: 0.3138, validation loss: 0.4005
2024-06-02 03:34:54 [INFO]: Epoch 039 - training loss: 0.3126, validation loss: 0.4024
2024-06-02 03:34:55 [INFO]: Epoch 040 - training loss: 0.3070, validation loss: 0.4008
2024-06-02 03:34:55 [INFO]: Epoch 041 - training loss: 0.3085, validation loss: 0.4020
2024-06-02 03:34:56 [INFO]: Epoch 042 - training loss: 0.3121, validation loss: 0.4013
2024-06-02 03:34:57 [INFO]: Epoch 043 - training loss: 0.3162, validation loss: 0.3992
2024-06-02 03:34:57 [INFO]: Epoch 044 - training loss: 0.3083, validation loss: 0.4040
2024-06-02 03:34:58 [INFO]: Epoch 045 - training loss: 0.3053, validation loss: 0.3973
2024-06-02 03:34:59 [INFO]: Epoch 046 - training loss: 0.3014, validation loss: 0.4008
2024-06-02 03:34:59 [INFO]: Epoch 047 - training loss: 0.2980, validation loss: 0.3974
2024-06-02 03:35:00 [INFO]: Epoch 048 - training loss: 0.2976, validation loss: 0.3958
2024-06-02 03:35:00 [INFO]: Epoch 049 - training loss: 0.2958, validation loss: 0.3966
2024-06-02 03:35:01 [INFO]: Epoch 050 - training loss: 0.3001, validation loss: 0.3952
2024-06-02 03:35:02 [INFO]: Epoch 051 - training loss: 0.2985, validation loss: 0.4001
2024-06-02 03:35:02 [INFO]: Epoch 052 - training loss: 0.2960, validation loss: 0.3952
2024-06-02 03:35:03 [INFO]: Epoch 053 - training loss: 0.2912, validation loss: 0.3966
2024-06-02 03:35:03 [INFO]: Epoch 054 - training loss: 0.2915, validation loss: 0.3911
2024-06-02 03:35:04 [INFO]: Epoch 055 - training loss: 0.2921, validation loss: 0.3926
2024-06-02 03:35:05 [INFO]: Epoch 056 - training loss: 0.2881, validation loss: 0.3885
2024-06-02 03:35:05 [INFO]: Epoch 057 - training loss: 0.2900, validation loss: 0.3943
2024-06-02 03:35:06 [INFO]: Epoch 058 - training loss: 0.2896, validation loss: 0.3895
2024-06-02 03:35:07 [INFO]: Epoch 059 - training loss: 0.2879, validation loss: 0.3897
2024-06-02 03:35:07 [INFO]: Epoch 060 - training loss: 0.2845, validation loss: 0.3879
2024-06-02 03:35:08 [INFO]: Epoch 061 - training loss: 0.2870, validation loss: 0.3906
2024-06-02 03:35:08 [INFO]: Epoch 062 - training loss: 0.2847, validation loss: 0.3939
2024-06-02 03:35:09 [INFO]: Epoch 063 - training loss: 0.2852, validation loss: 0.3867
2024-06-02 03:35:10 [INFO]: Epoch 064 - training loss: 0.2869, validation loss: 0.3896
2024-06-02 03:35:10 [INFO]: Epoch 065 - training loss: 0.2867, validation loss: 0.3904
2024-06-02 03:35:11 [INFO]: Epoch 066 - training loss: 0.2827, validation loss: 0.3858
2024-06-02 03:35:12 [INFO]: Epoch 067 - training loss: 0.2826, validation loss: 0.3855
2024-06-02 03:35:12 [INFO]: Epoch 068 - training loss: 0.2802, validation loss: 0.3862
2024-06-02 03:35:13 [INFO]: Epoch 069 - training loss: 0.2788, validation loss: 0.3847
2024-06-02 03:35:13 [INFO]: Epoch 070 - training loss: 0.2771, validation loss: 0.3874
2024-06-02 03:35:14 [INFO]: Epoch 071 - training loss: 0.2765, validation loss: 0.3898
2024-06-02 03:35:15 [INFO]: Epoch 072 - training loss: 0.2807, validation loss: 0.3865
2024-06-02 03:35:15 [INFO]: Epoch 073 - training loss: 0.2807, validation loss: 0.3822
2024-06-02 03:35:16 [INFO]: Epoch 074 - training loss: 0.2759, validation loss: 0.3843
2024-06-02 03:35:17 [INFO]: Epoch 075 - training loss: 0.2746, validation loss: 0.3858
2024-06-02 03:35:17 [INFO]: Epoch 076 - training loss: 0.2726, validation loss: 0.3812
2024-06-02 03:35:18 [INFO]: Epoch 077 - training loss: 0.2700, validation loss: 0.3823
2024-06-02 03:35:18 [INFO]: Epoch 078 - training loss: 0.2708, validation loss: 0.3818
2024-06-02 03:35:19 [INFO]: Epoch 079 - training loss: 0.2714, validation loss: 0.3804
2024-06-02 03:35:20 [INFO]: Epoch 080 - training loss: 0.2708, validation loss: 0.3839
2024-06-02 03:35:21 [INFO]: Epoch 081 - training loss: 0.2659, validation loss: 0.3843
2024-06-02 03:35:22 [INFO]: Epoch 082 - training loss: 0.2690, validation loss: 0.3824
2024-06-02 03:35:23 [INFO]: Epoch 083 - training loss: 0.2711, validation loss: 0.3820
2024-06-02 03:35:24 [INFO]: Epoch 084 - training loss: 0.2681, validation loss: 0.3830
2024-06-02 03:35:25 [INFO]: Epoch 085 - training loss: 0.2663, validation loss: 0.3811
2024-06-02 03:35:26 [INFO]: Epoch 086 - training loss: 0.2656, validation loss: 0.3806
2024-06-02 03:35:28 [INFO]: Epoch 087 - training loss: 0.2661, validation loss: 0.3803
2024-06-02 03:35:29 [INFO]: Epoch 088 - training loss: 0.2671, validation loss: 0.3810
2024-06-02 03:35:30 [INFO]: Epoch 089 - training loss: 0.2636, validation loss: 0.3776
2024-06-02 03:35:31 [INFO]: Epoch 090 - training loss: 0.2648, validation loss: 0.3788
2024-06-02 03:35:32 [INFO]: Epoch 091 - training loss: 0.2653, validation loss: 0.3798
2024-06-02 03:35:33 [INFO]: Epoch 092 - training loss: 0.2641, validation loss: 0.3775
2024-06-02 03:35:35 [INFO]: Epoch 093 - training loss: 0.2692, validation loss: 0.3791
2024-06-02 03:35:36 [INFO]: Epoch 094 - training loss: 0.2664, validation loss: 0.3795
2024-06-02 03:35:37 [INFO]: Epoch 095 - training loss: 0.2653, validation loss: 0.3758
2024-06-02 03:35:38 [INFO]: Epoch 096 - training loss: 0.2614, validation loss: 0.3786
2024-06-02 03:35:39 [INFO]: Epoch 097 - training loss: 0.2648, validation loss: 0.3781
2024-06-02 03:35:41 [INFO]: Epoch 098 - training loss: 0.2614, validation loss: 0.3781
2024-06-02 03:35:42 [INFO]: Epoch 099 - training loss: 0.2599, validation loss: 0.3762
2024-06-02 03:35:43 [INFO]: Epoch 100 - training loss: 0.2571, validation loss: 0.3772
2024-06-02 03:35:43 [INFO]: Finished training. The best model is from epoch#95.
2024-06-02 03:35:43 [INFO]: Saved the model to results_point_rate01/PeMS/Pyraformer_PeMS/round_4/20240602_T033412/Pyraformer.pypots
2024-06-02 03:35:43 [INFO]: Successfully saved to results_point_rate01/PeMS/Pyraformer_PeMS/round_4/imputation.pkl
2024-06-02 03:35:43 [INFO]: Round4 - Pyraformer on PeMS: MAE=0.2890, MSE=0.5648, MRE=0.3583
2024-06-02 03:35:43 [INFO]: Done! Final results:
Averaged Pyraformer (n params: 4,048,606) on PeMS: MAE=0.2847 ± 0.002681328523723539, MSE=0.5640 ± 0.0027490586840712293, MRE=0.3529 ± 0.0033237738490845543, average inference time=0.10
