2024-06-02 01:50:20 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 01:50:20 [INFO]: Using the given device: cuda:0
2024-06-02 01:50:20 [INFO]: Model files will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_0/20240602_T015020
2024-06-02 01:50:20 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_0/20240602_T015020/tensorboard
2024-06-02 01:50:21 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 3,076,301
2024-06-02 01:50:57 [INFO]: Epoch 001 - training loss: 1.6549, validation loss: 0.9313
2024-06-02 01:51:31 [INFO]: Epoch 002 - training loss: 1.1837, validation loss: 0.8709
2024-06-02 01:52:04 [INFO]: Epoch 003 - training loss: 0.9803, validation loss: 0.8516
2024-06-02 01:52:37 [INFO]: Epoch 004 - training loss: 0.8704, validation loss: 0.8441
2024-06-02 01:53:10 [INFO]: Epoch 005 - training loss: 0.8351, validation loss: 0.8400
2024-06-02 01:53:43 [INFO]: Epoch 006 - training loss: 0.8202, validation loss: 0.8376
2024-06-02 01:54:17 [INFO]: Epoch 007 - training loss: 0.8171, validation loss: 0.8355
2024-06-02 01:54:47 [INFO]: Epoch 008 - training loss: 0.8104, validation loss: 0.8333
2024-06-02 01:55:09 [INFO]: Epoch 009 - training loss: 0.8046, validation loss: 0.8325
2024-06-02 01:55:32 [INFO]: Epoch 010 - training loss: 0.8104, validation loss: 0.8310
2024-06-02 01:55:56 [INFO]: Epoch 011 - training loss: 0.8046, validation loss: 0.8304
2024-06-02 01:56:18 [INFO]: Epoch 012 - training loss: 0.7982, validation loss: 0.8294
2024-06-02 01:56:40 [INFO]: Epoch 013 - training loss: 0.7933, validation loss: 0.8284
2024-06-02 01:57:04 [INFO]: Epoch 014 - training loss: 0.8098, validation loss: 0.8280
2024-06-02 01:57:26 [INFO]: Epoch 015 - training loss: 0.7947, validation loss: 0.8275
2024-06-02 01:57:48 [INFO]: Epoch 016 - training loss: 0.7874, validation loss: 0.8267
2024-06-02 01:58:11 [INFO]: Epoch 017 - training loss: 0.7880, validation loss: 0.8262
2024-06-02 01:58:34 [INFO]: Epoch 018 - training loss: 0.7861, validation loss: 0.8259
2024-06-02 01:58:56 [INFO]: Epoch 019 - training loss: 0.7838, validation loss: 0.8255
2024-06-02 01:59:19 [INFO]: Epoch 020 - training loss: 0.7800, validation loss: 0.8249
2024-06-02 01:59:41 [INFO]: Epoch 021 - training loss: 0.7768, validation loss: 0.8242
2024-06-02 02:00:02 [INFO]: Epoch 022 - training loss: 0.7795, validation loss: 0.8243
2024-06-02 02:00:25 [INFO]: Epoch 023 - training loss: 0.7909, validation loss: 0.8238
2024-06-02 02:00:48 [INFO]: Epoch 024 - training loss: 0.7804, validation loss: 0.8237
2024-06-02 02:01:10 [INFO]: Epoch 025 - training loss: 0.7880, validation loss: 0.8227
2024-06-02 02:01:33 [INFO]: Epoch 026 - training loss: 0.7778, validation loss: 0.8217
2024-06-02 02:01:55 [INFO]: Epoch 027 - training loss: 0.7745, validation loss: 0.8212
2024-06-02 02:02:17 [INFO]: Epoch 028 - training loss: 0.7732, validation loss: 0.8209
2024-06-02 02:02:40 [INFO]: Epoch 029 - training loss: 0.7691, validation loss: 0.8209
2024-06-02 02:03:02 [INFO]: Epoch 030 - training loss: 0.7688, validation loss: 0.8212
2024-06-02 02:03:24 [INFO]: Epoch 031 - training loss: 0.7717, validation loss: 0.8212
2024-06-02 02:03:47 [INFO]: Epoch 032 - training loss: 0.7799, validation loss: 0.8200
2024-06-02 02:04:10 [INFO]: Epoch 033 - training loss: 0.7782, validation loss: 0.8190
2024-06-02 02:04:33 [INFO]: Epoch 034 - training loss: 0.7754, validation loss: 0.8186
2024-06-02 02:04:55 [INFO]: Epoch 035 - training loss: 0.7696, validation loss: 0.8187
2024-06-02 02:05:17 [INFO]: Epoch 036 - training loss: 0.7651, validation loss: 0.8185
2024-06-02 02:05:40 [INFO]: Epoch 037 - training loss: 0.7628, validation loss: 0.8190
2024-06-02 02:06:02 [INFO]: Epoch 038 - training loss: 0.7593, validation loss: 0.8184
2024-06-02 02:06:25 [INFO]: Epoch 039 - training loss: 0.7651, validation loss: 0.8170
2024-06-02 02:06:47 [INFO]: Epoch 040 - training loss: 0.7643, validation loss: 0.8179
2024-06-02 02:07:10 [INFO]: Epoch 041 - training loss: 0.7626, validation loss: 0.8170
2024-06-02 02:07:34 [INFO]: Epoch 042 - training loss: 0.7716, validation loss: 0.8169
2024-06-02 02:07:56 [INFO]: Epoch 043 - training loss: 0.7667, validation loss: 0.8168
2024-06-02 02:08:19 [INFO]: Epoch 044 - training loss: 0.7578, validation loss: 0.8158
2024-06-02 02:08:41 [INFO]: Epoch 045 - training loss: 0.7675, validation loss: 0.8170
2024-06-02 02:09:04 [INFO]: Epoch 046 - training loss: 0.7639, validation loss: 0.8155
2024-06-02 02:09:26 [INFO]: Epoch 047 - training loss: 0.7639, validation loss: 0.8165
2024-06-02 02:09:49 [INFO]: Epoch 048 - training loss: 0.7602, validation loss: 0.8150
2024-06-02 02:10:13 [INFO]: Epoch 049 - training loss: 0.7585, validation loss: 0.8144
2024-06-02 02:10:35 [INFO]: Epoch 050 - training loss: 0.7564, validation loss: 0.8172
2024-06-02 02:10:57 [INFO]: Epoch 051 - training loss: 0.7594, validation loss: 0.8172
2024-06-02 02:11:11 [INFO]: Epoch 052 - training loss: 0.7648, validation loss: 0.8151
2024-06-02 02:11:26 [INFO]: Epoch 053 - training loss: 0.7622, validation loss: 0.8153
2024-06-02 02:11:41 [INFO]: Epoch 054 - training loss: 0.7642, validation loss: 0.8136
2024-06-02 02:11:56 [INFO]: Epoch 055 - training loss: 0.7672, validation loss: 0.8140
2024-06-02 02:12:11 [INFO]: Epoch 056 - training loss: 0.7559, validation loss: 0.8137
2024-06-02 02:12:25 [INFO]: Epoch 057 - training loss: 0.7532, validation loss: 0.8145
2024-06-02 02:12:40 [INFO]: Epoch 058 - training loss: 0.7488, validation loss: 0.8146
2024-06-02 02:12:55 [INFO]: Epoch 059 - training loss: 0.7590, validation loss: 0.8141
2024-06-02 02:13:10 [INFO]: Epoch 060 - training loss: 0.7515, validation loss: 0.8136
2024-06-02 02:13:25 [INFO]: Epoch 061 - training loss: 0.7531, validation loss: 0.8146
2024-06-02 02:13:37 [INFO]: Epoch 062 - training loss: 0.7528, validation loss: 0.8129
2024-06-02 02:13:48 [INFO]: Epoch 063 - training loss: 0.7521, validation loss: 0.8137
2024-06-02 02:13:58 [INFO]: Epoch 064 - training loss: 0.7554, validation loss: 0.8142
2024-06-02 02:14:09 [INFO]: Epoch 065 - training loss: 0.7535, validation loss: 0.8156
2024-06-02 02:14:20 [INFO]: Epoch 066 - training loss: 0.7515, validation loss: 0.8144
2024-06-02 02:14:30 [INFO]: Epoch 067 - training loss: 0.7546, validation loss: 0.8134
2024-06-02 02:14:41 [INFO]: Epoch 068 - training loss: 0.7557, validation loss: 0.8115
2024-06-02 02:14:52 [INFO]: Epoch 069 - training loss: 0.7530, validation loss: 0.8137
2024-06-02 02:15:02 [INFO]: Epoch 070 - training loss: 0.7519, validation loss: 0.8120
2024-06-02 02:15:13 [INFO]: Epoch 071 - training loss: 0.7490, validation loss: 0.8123
2024-06-02 02:15:23 [INFO]: Epoch 072 - training loss: 0.7515, validation loss: 0.8122
2024-06-02 02:15:34 [INFO]: Epoch 073 - training loss: 0.7551, validation loss: 0.8110
2024-06-02 02:15:45 [INFO]: Epoch 074 - training loss: 0.7506, validation loss: 0.8151
2024-06-02 02:15:55 [INFO]: Epoch 075 - training loss: 0.7495, validation loss: 0.8150
2024-06-02 02:16:06 [INFO]: Epoch 076 - training loss: 0.7544, validation loss: 0.8120
2024-06-02 02:16:17 [INFO]: Epoch 077 - training loss: 0.7515, validation loss: 0.8113
2024-06-02 02:16:27 [INFO]: Epoch 078 - training loss: 0.7520, validation loss: 0.8115
2024-06-02 02:16:38 [INFO]: Epoch 079 - training loss: 0.7516, validation loss: 0.8115
2024-06-02 02:16:48 [INFO]: Epoch 080 - training loss: 0.7498, validation loss: 0.8119
2024-06-02 02:16:59 [INFO]: Epoch 081 - training loss: 0.7483, validation loss: 0.8117
2024-06-02 02:17:10 [INFO]: Epoch 082 - training loss: 0.7507, validation loss: 0.8116
2024-06-02 02:17:20 [INFO]: Epoch 083 - training loss: 0.7514, validation loss: 0.8105
2024-06-02 02:17:31 [INFO]: Epoch 084 - training loss: 0.7522, validation loss: 0.8115
2024-06-02 02:17:42 [INFO]: Epoch 085 - training loss: 0.7462, validation loss: 0.8135
2024-06-02 02:17:52 [INFO]: Epoch 086 - training loss: 0.7453, validation loss: 0.8104
2024-06-02 02:18:03 [INFO]: Epoch 087 - training loss: 0.7451, validation loss: 0.8120
2024-06-02 02:18:13 [INFO]: Epoch 088 - training loss: 0.7427, validation loss: 0.8108
2024-06-02 02:18:24 [INFO]: Epoch 089 - training loss: 0.7469, validation loss: 0.8105
2024-06-02 02:18:35 [INFO]: Epoch 090 - training loss: 0.7493, validation loss: 0.8124
2024-06-02 02:18:45 [INFO]: Epoch 091 - training loss: 0.7482, validation loss: 0.8114
2024-06-02 02:18:56 [INFO]: Epoch 092 - training loss: 0.7449, validation loss: 0.8111
2024-06-02 02:19:07 [INFO]: Epoch 093 - training loss: 0.7427, validation loss: 0.8107
2024-06-02 02:19:17 [INFO]: Epoch 094 - training loss: 0.7432, validation loss: 0.8127
2024-06-02 02:19:28 [INFO]: Epoch 095 - training loss: 0.7419, validation loss: 0.8111
2024-06-02 02:19:38 [INFO]: Epoch 096 - training loss: 0.7476, validation loss: 0.8109
2024-06-02 02:19:38 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 02:19:38 [INFO]: Finished training. The best model is from epoch#86.
2024-06-02 02:19:38 [INFO]: Saved the model to results_point_rate01/PeMS/MRNN_PeMS/round_0/20240602_T015020/MRNN.pypots
2024-06-02 02:19:40 [INFO]: Successfully saved to results_point_rate01/PeMS/MRNN_PeMS/round_0/imputation.pkl
2024-06-02 02:19:40 [INFO]: Round0 - MRNN on PeMS: MAE=0.6236, MSE=1.0487, MRE=0.7730
2024-06-02 02:19:40 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 02:19:40 [INFO]: Using the given device: cuda:0
2024-06-02 02:19:40 [INFO]: Model files will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_1/20240602_T021940
2024-06-02 02:19:40 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_1/20240602_T021940/tensorboard
2024-06-02 02:19:40 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 3,076,301
2024-06-02 02:19:51 [INFO]: Epoch 001 - training loss: 1.6841, validation loss: 0.9331
2024-06-02 02:20:02 [INFO]: Epoch 002 - training loss: 1.2062, validation loss: 0.8723
2024-06-02 02:20:12 [INFO]: Epoch 003 - training loss: 0.9982, validation loss: 0.8523
2024-06-02 02:20:23 [INFO]: Epoch 004 - training loss: 0.8891, validation loss: 0.8446
2024-06-02 02:20:33 [INFO]: Epoch 005 - training loss: 0.8341, validation loss: 0.8401
2024-06-02 02:20:44 [INFO]: Epoch 006 - training loss: 0.8246, validation loss: 0.8378
2024-06-02 02:20:55 [INFO]: Epoch 007 - training loss: 0.8188, validation loss: 0.8353
2024-06-02 02:21:05 [INFO]: Epoch 008 - training loss: 0.8112, validation loss: 0.8339
2024-06-02 02:21:16 [INFO]: Epoch 009 - training loss: 0.8274, validation loss: 0.8325
2024-06-02 02:21:26 [INFO]: Epoch 010 - training loss: 0.8074, validation loss: 0.8313
2024-06-02 02:21:37 [INFO]: Epoch 011 - training loss: 0.8005, validation loss: 0.8303
2024-06-02 02:21:48 [INFO]: Epoch 012 - training loss: 0.7989, validation loss: 0.8291
2024-06-02 02:21:58 [INFO]: Epoch 013 - training loss: 0.7957, validation loss: 0.8287
2024-06-02 02:22:09 [INFO]: Epoch 014 - training loss: 0.7974, validation loss: 0.8283
2024-06-02 02:22:19 [INFO]: Epoch 015 - training loss: 0.7994, validation loss: 0.8274
2024-06-02 02:22:30 [INFO]: Epoch 016 - training loss: 0.7998, validation loss: 0.8268
2024-06-02 02:22:41 [INFO]: Epoch 017 - training loss: 0.7983, validation loss: 0.8269
2024-06-02 02:22:51 [INFO]: Epoch 018 - training loss: 0.7938, validation loss: 0.8266
2024-06-02 02:23:02 [INFO]: Epoch 019 - training loss: 0.7893, validation loss: 0.8257
2024-06-02 02:23:12 [INFO]: Epoch 020 - training loss: 0.7931, validation loss: 0.8257
2024-06-02 02:23:23 [INFO]: Epoch 021 - training loss: 0.7844, validation loss: 0.8247
2024-06-02 02:23:34 [INFO]: Epoch 022 - training loss: 0.7825, validation loss: 0.8247
2024-06-02 02:23:44 [INFO]: Epoch 023 - training loss: 0.7800, validation loss: 0.8239
2024-06-02 02:23:55 [INFO]: Epoch 024 - training loss: 0.7775, validation loss: 0.8233
2024-06-02 02:24:05 [INFO]: Epoch 025 - training loss: 0.7800, validation loss: 0.8229
2024-06-02 02:24:16 [INFO]: Epoch 026 - training loss: 0.7709, validation loss: 0.8224
2024-06-02 02:24:27 [INFO]: Epoch 027 - training loss: 0.7745, validation loss: 0.8217
2024-06-02 02:24:37 [INFO]: Epoch 028 - training loss: 0.7780, validation loss: 0.8235
2024-06-02 02:24:48 [INFO]: Epoch 029 - training loss: 0.7821, validation loss: 0.8212
2024-06-02 02:24:58 [INFO]: Epoch 030 - training loss: 0.7855, validation loss: 0.8211
2024-06-02 02:25:09 [INFO]: Epoch 031 - training loss: 0.7803, validation loss: 0.8211
2024-06-02 02:25:20 [INFO]: Epoch 032 - training loss: 0.7750, validation loss: 0.8207
2024-06-02 02:25:30 [INFO]: Epoch 033 - training loss: 0.7762, validation loss: 0.8209
2024-06-02 02:25:41 [INFO]: Epoch 034 - training loss: 0.7675, validation loss: 0.8210
2024-06-02 02:25:51 [INFO]: Epoch 035 - training loss: 0.7672, validation loss: 0.8191
2024-06-02 02:26:02 [INFO]: Epoch 036 - training loss: 0.7658, validation loss: 0.8192
2024-06-02 02:26:13 [INFO]: Epoch 037 - training loss: 0.7671, validation loss: 0.8199
2024-06-02 02:26:23 [INFO]: Epoch 038 - training loss: 0.7708, validation loss: 0.8186
2024-06-02 02:26:34 [INFO]: Epoch 039 - training loss: 0.7690, validation loss: 0.8182
2024-06-02 02:26:45 [INFO]: Epoch 040 - training loss: 0.7715, validation loss: 0.8188
2024-06-02 02:26:55 [INFO]: Epoch 041 - training loss: 0.7674, validation loss: 0.8173
2024-06-02 02:27:06 [INFO]: Epoch 042 - training loss: 0.7677, validation loss: 0.8180
2024-06-02 02:27:16 [INFO]: Epoch 043 - training loss: 0.7663, validation loss: 0.8170
2024-06-02 02:27:27 [INFO]: Epoch 044 - training loss: 0.7634, validation loss: 0.8170
2024-06-02 02:27:38 [INFO]: Epoch 045 - training loss: 0.7710, validation loss: 0.8159
2024-06-02 02:27:48 [INFO]: Epoch 046 - training loss: 0.7704, validation loss: 0.8170
2024-06-02 02:27:59 [INFO]: Epoch 047 - training loss: 0.7692, validation loss: 0.8175
2024-06-02 02:28:09 [INFO]: Epoch 048 - training loss: 0.7646, validation loss: 0.8183
2024-06-02 02:28:20 [INFO]: Epoch 049 - training loss: 0.7616, validation loss: 0.8147
2024-06-02 02:28:31 [INFO]: Epoch 050 - training loss: 0.7585, validation loss: 0.8157
2024-06-02 02:28:41 [INFO]: Epoch 051 - training loss: 0.7581, validation loss: 0.8151
2024-06-02 02:28:52 [INFO]: Epoch 052 - training loss: 0.7571, validation loss: 0.8158
2024-06-02 02:29:02 [INFO]: Epoch 053 - training loss: 0.7623, validation loss: 0.8143
2024-06-02 02:29:13 [INFO]: Epoch 054 - training loss: 0.7573, validation loss: 0.8162
2024-06-02 02:29:24 [INFO]: Epoch 055 - training loss: 0.7616, validation loss: 0.8164
2024-06-02 02:29:34 [INFO]: Epoch 056 - training loss: 0.7531, validation loss: 0.8134
2024-06-02 02:29:45 [INFO]: Epoch 057 - training loss: 0.7531, validation loss: 0.8129
2024-06-02 02:29:55 [INFO]: Epoch 058 - training loss: 0.7547, validation loss: 0.8144
2024-06-02 02:30:06 [INFO]: Epoch 059 - training loss: 0.7555, validation loss: 0.8166
2024-06-02 02:30:17 [INFO]: Epoch 060 - training loss: 0.7608, validation loss: 0.8145
2024-06-02 02:30:27 [INFO]: Epoch 061 - training loss: 0.7637, validation loss: 0.8141
2024-06-02 02:30:38 [INFO]: Epoch 062 - training loss: 0.7603, validation loss: 0.8137
2024-06-02 02:30:48 [INFO]: Epoch 063 - training loss: 0.7569, validation loss: 0.8140
2024-06-02 02:30:59 [INFO]: Epoch 064 - training loss: 0.7490, validation loss: 0.8123
2024-06-02 02:31:10 [INFO]: Epoch 065 - training loss: 0.7613, validation loss: 0.8137
2024-06-02 02:31:20 [INFO]: Epoch 066 - training loss: 0.7500, validation loss: 0.8122
2024-06-02 02:31:31 [INFO]: Epoch 067 - training loss: 0.7506, validation loss: 0.8139
2024-06-02 02:31:41 [INFO]: Epoch 068 - training loss: 0.7475, validation loss: 0.8133
2024-06-02 02:31:52 [INFO]: Epoch 069 - training loss: 0.7560, validation loss: 0.8137
2024-06-02 02:32:03 [INFO]: Epoch 070 - training loss: 0.7539, validation loss: 0.8111
2024-06-02 02:32:13 [INFO]: Epoch 071 - training loss: 0.7554, validation loss: 0.8132
2024-06-02 02:32:24 [INFO]: Epoch 072 - training loss: 0.7600, validation loss: 0.8118
2024-06-02 02:32:34 [INFO]: Epoch 073 - training loss: 0.7601, validation loss: 0.8150
2024-06-02 02:32:45 [INFO]: Epoch 074 - training loss: 0.7576, validation loss: 0.8120
2024-06-02 02:32:56 [INFO]: Epoch 075 - training loss: 0.7557, validation loss: 0.8142
2024-06-02 02:33:06 [INFO]: Epoch 076 - training loss: 0.7547, validation loss: 0.8127
2024-06-02 02:33:17 [INFO]: Epoch 077 - training loss: 0.7570, validation loss: 0.8117
2024-06-02 02:33:27 [INFO]: Epoch 078 - training loss: 0.7610, validation loss: 0.8124
2024-06-02 02:33:38 [INFO]: Epoch 079 - training loss: 0.7525, validation loss: 0.8119
2024-06-02 02:33:49 [INFO]: Epoch 080 - training loss: 0.7450, validation loss: 0.8115
2024-06-02 02:33:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 02:33:49 [INFO]: Finished training. The best model is from epoch#70.
2024-06-02 02:33:49 [INFO]: Saved the model to results_point_rate01/PeMS/MRNN_PeMS/round_1/20240602_T021940/MRNN.pypots
2024-06-02 02:33:50 [INFO]: Successfully saved to results_point_rate01/PeMS/MRNN_PeMS/round_1/imputation.pkl
2024-06-02 02:33:50 [INFO]: Round1 - MRNN on PeMS: MAE=0.6233, MSE=1.0489, MRE=0.7726
2024-06-02 02:33:50 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 02:33:50 [INFO]: Using the given device: cuda:0
2024-06-02 02:33:50 [INFO]: Model files will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_2/20240602_T023350
2024-06-02 02:33:50 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_2/20240602_T023350/tensorboard
2024-06-02 02:33:50 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 3,076,301
2024-06-02 02:34:01 [INFO]: Epoch 001 - training loss: 1.6982, validation loss: 0.9247
2024-06-02 02:34:12 [INFO]: Epoch 002 - training loss: 1.2115, validation loss: 0.8679
2024-06-02 02:34:22 [INFO]: Epoch 003 - training loss: 1.0042, validation loss: 0.8489
2024-06-02 02:34:33 [INFO]: Epoch 004 - training loss: 0.8937, validation loss: 0.8427
2024-06-02 02:34:44 [INFO]: Epoch 005 - training loss: 0.8421, validation loss: 0.8395
2024-06-02 02:34:54 [INFO]: Epoch 006 - training loss: 0.8381, validation loss: 0.8366
2024-06-02 02:35:05 [INFO]: Epoch 007 - training loss: 0.8319, validation loss: 0.8346
2024-06-02 02:35:15 [INFO]: Epoch 008 - training loss: 0.8172, validation loss: 0.8332
2024-06-02 02:35:26 [INFO]: Epoch 009 - training loss: 0.8156, validation loss: 0.8325
2024-06-02 02:35:37 [INFO]: Epoch 010 - training loss: 0.8076, validation loss: 0.8302
2024-06-02 02:35:47 [INFO]: Epoch 011 - training loss: 0.8031, validation loss: 0.8298
2024-06-02 02:35:58 [INFO]: Epoch 012 - training loss: 0.7981, validation loss: 0.8288
2024-06-02 02:36:08 [INFO]: Epoch 013 - training loss: 0.7991, validation loss: 0.8285
2024-06-02 02:36:19 [INFO]: Epoch 014 - training loss: 0.7952, validation loss: 0.8275
2024-06-02 02:36:30 [INFO]: Epoch 015 - training loss: 0.7917, validation loss: 0.8270
2024-06-02 02:36:40 [INFO]: Epoch 016 - training loss: 0.7884, validation loss: 0.8269
2024-06-02 02:36:51 [INFO]: Epoch 017 - training loss: 0.7938, validation loss: 0.8257
2024-06-02 02:37:02 [INFO]: Epoch 018 - training loss: 0.7944, validation loss: 0.8257
2024-06-02 02:37:12 [INFO]: Epoch 019 - training loss: 0.8004, validation loss: 0.8246
2024-06-02 02:37:23 [INFO]: Epoch 020 - training loss: 0.7924, validation loss: 0.8247
2024-06-02 02:37:33 [INFO]: Epoch 021 - training loss: 0.7830, validation loss: 0.8248
2024-06-02 02:37:44 [INFO]: Epoch 022 - training loss: 0.7827, validation loss: 0.8238
2024-06-02 02:37:55 [INFO]: Epoch 023 - training loss: 0.7799, validation loss: 0.8232
2024-06-02 02:38:05 [INFO]: Epoch 024 - training loss: 0.7781, validation loss: 0.8220
2024-06-02 02:38:16 [INFO]: Epoch 025 - training loss: 0.7807, validation loss: 0.8223
2024-06-02 02:38:26 [INFO]: Epoch 026 - training loss: 0.7768, validation loss: 0.8219
2024-06-02 02:38:37 [INFO]: Epoch 027 - training loss: 0.7772, validation loss: 0.8215
2024-06-02 02:38:48 [INFO]: Epoch 028 - training loss: 0.7829, validation loss: 0.8213
2024-06-02 02:38:58 [INFO]: Epoch 029 - training loss: 0.7839, validation loss: 0.8206
2024-06-02 02:39:09 [INFO]: Epoch 030 - training loss: 0.7788, validation loss: 0.8202
2024-06-02 02:39:20 [INFO]: Epoch 031 - training loss: 0.7681, validation loss: 0.8201
2024-06-02 02:39:30 [INFO]: Epoch 032 - training loss: 0.7769, validation loss: 0.8193
2024-06-02 02:39:41 [INFO]: Epoch 033 - training loss: 0.7704, validation loss: 0.8195
2024-06-02 02:39:51 [INFO]: Epoch 034 - training loss: 0.7682, validation loss: 0.8194
2024-06-02 02:40:02 [INFO]: Epoch 035 - training loss: 0.7669, validation loss: 0.8186
2024-06-02 02:40:13 [INFO]: Epoch 036 - training loss: 0.7693, validation loss: 0.8183
2024-06-02 02:40:23 [INFO]: Epoch 037 - training loss: 0.7689, validation loss: 0.8184
2024-06-02 02:40:34 [INFO]: Epoch 038 - training loss: 0.7685, validation loss: 0.8179
2024-06-02 02:40:44 [INFO]: Epoch 039 - training loss: 0.7640, validation loss: 0.8198
2024-06-02 02:40:55 [INFO]: Epoch 040 - training loss: 0.7701, validation loss: 0.8174
2024-06-02 02:41:06 [INFO]: Epoch 041 - training loss: 0.7686, validation loss: 0.8198
2024-06-02 02:41:16 [INFO]: Epoch 042 - training loss: 0.7645, validation loss: 0.8183
2024-06-02 02:41:27 [INFO]: Epoch 043 - training loss: 0.7634, validation loss: 0.8171
2024-06-02 02:41:37 [INFO]: Epoch 044 - training loss: 0.7625, validation loss: 0.8170
2024-06-02 02:41:48 [INFO]: Epoch 045 - training loss: 0.7595, validation loss: 0.8154
2024-06-02 02:41:59 [INFO]: Epoch 046 - training loss: 0.7594, validation loss: 0.8165
2024-06-02 02:42:09 [INFO]: Epoch 047 - training loss: 0.7581, validation loss: 0.8158
2024-06-02 02:42:20 [INFO]: Epoch 048 - training loss: 0.7562, validation loss: 0.8166
2024-06-02 02:42:31 [INFO]: Epoch 049 - training loss: 0.7553, validation loss: 0.8153
2024-06-02 02:42:41 [INFO]: Epoch 050 - training loss: 0.7591, validation loss: 0.8153
2024-06-02 02:42:52 [INFO]: Epoch 051 - training loss: 0.7586, validation loss: 0.8148
2024-06-02 02:43:02 [INFO]: Epoch 052 - training loss: 0.7603, validation loss: 0.8146
2024-06-02 02:43:13 [INFO]: Epoch 053 - training loss: 0.7595, validation loss: 0.8140
2024-06-02 02:43:24 [INFO]: Epoch 054 - training loss: 0.7544, validation loss: 0.8161
2024-06-02 02:43:34 [INFO]: Epoch 055 - training loss: 0.7552, validation loss: 0.8140
2024-06-02 02:43:45 [INFO]: Epoch 056 - training loss: 0.7573, validation loss: 0.8151
2024-06-02 02:43:55 [INFO]: Epoch 057 - training loss: 0.7612, validation loss: 0.8144
2024-06-02 02:44:06 [INFO]: Epoch 058 - training loss: 0.7586, validation loss: 0.8150
2024-06-02 02:44:17 [INFO]: Epoch 059 - training loss: 0.7519, validation loss: 0.8131
2024-06-02 02:44:27 [INFO]: Epoch 060 - training loss: 0.7565, validation loss: 0.8146
2024-06-02 02:44:38 [INFO]: Epoch 061 - training loss: 0.7533, validation loss: 0.8139
2024-06-02 02:44:49 [INFO]: Epoch 062 - training loss: 0.7531, validation loss: 0.8143
2024-06-02 02:44:59 [INFO]: Epoch 063 - training loss: 0.7519, validation loss: 0.8134
2024-06-02 02:45:10 [INFO]: Epoch 064 - training loss: 0.7503, validation loss: 0.8146
2024-06-02 02:45:20 [INFO]: Epoch 065 - training loss: 0.7549, validation loss: 0.8131
2024-06-02 02:45:31 [INFO]: Epoch 066 - training loss: 0.7503, validation loss: 0.8139
2024-06-02 02:45:42 [INFO]: Epoch 067 - training loss: 0.7461, validation loss: 0.8147
2024-06-02 02:45:52 [INFO]: Epoch 068 - training loss: 0.7498, validation loss: 0.8126
2024-06-02 02:46:03 [INFO]: Epoch 069 - training loss: 0.7491, validation loss: 0.8136
2024-06-02 02:46:13 [INFO]: Epoch 070 - training loss: 0.7463, validation loss: 0.8140
2024-06-02 02:46:24 [INFO]: Epoch 071 - training loss: 0.7514, validation loss: 0.8137
2024-06-02 02:46:35 [INFO]: Epoch 072 - training loss: 0.7525, validation loss: 0.8123
2024-06-02 02:46:45 [INFO]: Epoch 073 - training loss: 0.7483, validation loss: 0.8120
2024-06-02 02:46:56 [INFO]: Epoch 074 - training loss: 0.7489, validation loss: 0.8110
2024-06-02 02:47:06 [INFO]: Epoch 075 - training loss: 0.7466, validation loss: 0.8159
2024-06-02 02:47:17 [INFO]: Epoch 076 - training loss: 0.7524, validation loss: 0.8173
2024-06-02 02:47:28 [INFO]: Epoch 077 - training loss: 0.7528, validation loss: 0.8108
2024-06-02 02:47:38 [INFO]: Epoch 078 - training loss: 0.7466, validation loss: 0.8144
2024-06-02 02:47:49 [INFO]: Epoch 079 - training loss: 0.7502, validation loss: 0.8128
2024-06-02 02:48:00 [INFO]: Epoch 080 - training loss: 0.7504, validation loss: 0.8121
2024-06-02 02:48:10 [INFO]: Epoch 081 - training loss: 0.7489, validation loss: 0.8131
2024-06-02 02:48:21 [INFO]: Epoch 082 - training loss: 0.7502, validation loss: 0.8122
2024-06-02 02:48:31 [INFO]: Epoch 083 - training loss: 0.7516, validation loss: 0.8126
2024-06-02 02:48:42 [INFO]: Epoch 084 - training loss: 0.7491, validation loss: 0.8121
2024-06-02 02:48:53 [INFO]: Epoch 085 - training loss: 0.7505, validation loss: 0.8127
2024-06-02 02:49:03 [INFO]: Epoch 086 - training loss: 0.7466, validation loss: 0.8128
2024-06-02 02:49:14 [INFO]: Epoch 087 - training loss: 0.7485, validation loss: 0.8118
2024-06-02 02:49:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 02:49:14 [INFO]: Finished training. The best model is from epoch#77.
2024-06-02 02:49:14 [INFO]: Saved the model to results_point_rate01/PeMS/MRNN_PeMS/round_2/20240602_T023350/MRNN.pypots
2024-06-02 02:49:15 [INFO]: Successfully saved to results_point_rate01/PeMS/MRNN_PeMS/round_2/imputation.pkl
2024-06-02 02:49:15 [INFO]: Round2 - MRNN on PeMS: MAE=0.6236, MSE=1.0502, MRE=0.7730
2024-06-02 02:49:15 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 02:49:15 [INFO]: Using the given device: cuda:0
2024-06-02 02:49:15 [INFO]: Model files will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_3/20240602_T024915
2024-06-02 02:49:15 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_3/20240602_T024915/tensorboard
2024-06-02 02:49:15 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 3,076,301
2024-06-02 02:49:26 [INFO]: Epoch 001 - training loss: 1.7254, validation loss: 0.9308
2024-06-02 02:49:37 [INFO]: Epoch 002 - training loss: 1.2328, validation loss: 0.8687
2024-06-02 02:49:48 [INFO]: Epoch 003 - training loss: 1.0079, validation loss: 0.8494
2024-06-02 02:49:58 [INFO]: Epoch 004 - training loss: 0.8858, validation loss: 0.8430
2024-06-02 02:50:09 [INFO]: Epoch 005 - training loss: 0.8360, validation loss: 0.8392
2024-06-02 02:50:20 [INFO]: Epoch 006 - training loss: 0.8221, validation loss: 0.8367
2024-06-02 02:50:30 [INFO]: Epoch 007 - training loss: 0.8130, validation loss: 0.8354
2024-06-02 02:50:41 [INFO]: Epoch 008 - training loss: 0.8117, validation loss: 0.8331
2024-06-02 02:50:51 [INFO]: Epoch 009 - training loss: 0.8072, validation loss: 0.8324
2024-06-02 02:51:02 [INFO]: Epoch 010 - training loss: 0.8015, validation loss: 0.8306
2024-06-02 02:51:13 [INFO]: Epoch 011 - training loss: 0.8034, validation loss: 0.8293
2024-06-02 02:51:23 [INFO]: Epoch 012 - training loss: 0.7966, validation loss: 0.8289
2024-06-02 02:51:34 [INFO]: Epoch 013 - training loss: 0.7950, validation loss: 0.8284
2024-06-02 02:51:45 [INFO]: Epoch 014 - training loss: 0.7937, validation loss: 0.8275
2024-06-02 02:51:55 [INFO]: Epoch 015 - training loss: 0.7951, validation loss: 0.8273
2024-06-02 02:52:06 [INFO]: Epoch 016 - training loss: 0.7927, validation loss: 0.8259
2024-06-02 02:52:16 [INFO]: Epoch 017 - training loss: 0.7895, validation loss: 0.8262
2024-06-02 02:52:27 [INFO]: Epoch 018 - training loss: 0.7813, validation loss: 0.8256
2024-06-02 02:52:38 [INFO]: Epoch 019 - training loss: 0.7806, validation loss: 0.8251
2024-06-02 02:52:48 [INFO]: Epoch 020 - training loss: 0.7813, validation loss: 0.8247
2024-06-02 02:52:59 [INFO]: Epoch 021 - training loss: 0.7832, validation loss: 0.8241
2024-06-02 02:53:10 [INFO]: Epoch 022 - training loss: 0.7800, validation loss: 0.8236
2024-06-02 02:53:20 [INFO]: Epoch 023 - training loss: 0.7812, validation loss: 0.8228
2024-06-02 02:53:31 [INFO]: Epoch 024 - training loss: 0.7814, validation loss: 0.8229
2024-06-02 02:53:41 [INFO]: Epoch 025 - training loss: 0.7766, validation loss: 0.8225
2024-06-02 02:53:52 [INFO]: Epoch 026 - training loss: 0.7798, validation loss: 0.8216
2024-06-02 02:54:03 [INFO]: Epoch 027 - training loss: 0.7726, validation loss: 0.8220
2024-06-02 02:54:13 [INFO]: Epoch 028 - training loss: 0.7680, validation loss: 0.8202
2024-06-02 02:54:24 [INFO]: Epoch 029 - training loss: 0.7709, validation loss: 0.8209
2024-06-02 02:54:34 [INFO]: Epoch 030 - training loss: 0.7689, validation loss: 0.8197
2024-06-02 02:54:45 [INFO]: Epoch 031 - training loss: 0.7711, validation loss: 0.8208
2024-06-02 02:54:56 [INFO]: Epoch 032 - training loss: 0.7704, validation loss: 0.8196
2024-06-02 02:55:06 [INFO]: Epoch 033 - training loss: 0.7695, validation loss: 0.8190
2024-06-02 02:55:17 [INFO]: Epoch 034 - training loss: 0.7702, validation loss: 0.8189
2024-06-02 02:55:28 [INFO]: Epoch 035 - training loss: 0.7758, validation loss: 0.8183
2024-06-02 02:55:38 [INFO]: Epoch 036 - training loss: 0.7691, validation loss: 0.8192
2024-06-02 02:55:49 [INFO]: Epoch 037 - training loss: 0.7692, validation loss: 0.8177
2024-06-02 02:55:59 [INFO]: Epoch 038 - training loss: 0.7707, validation loss: 0.8175
2024-06-02 02:56:10 [INFO]: Epoch 039 - training loss: 0.7655, validation loss: 0.8173
2024-06-02 02:56:21 [INFO]: Epoch 040 - training loss: 0.7633, validation loss: 0.8171
2024-06-02 02:56:31 [INFO]: Epoch 041 - training loss: 0.7626, validation loss: 0.8167
2024-06-02 02:56:42 [INFO]: Epoch 042 - training loss: 0.7618, validation loss: 0.8159
2024-06-02 02:56:53 [INFO]: Epoch 043 - training loss: 0.7626, validation loss: 0.8155
2024-06-02 02:57:03 [INFO]: Epoch 044 - training loss: 0.7617, validation loss: 0.8172
2024-06-02 02:57:14 [INFO]: Epoch 045 - training loss: 0.7544, validation loss: 0.8151
2024-06-02 02:57:24 [INFO]: Epoch 046 - training loss: 0.7605, validation loss: 0.8162
2024-06-02 02:57:35 [INFO]: Epoch 047 - training loss: 0.7580, validation loss: 0.8169
2024-06-02 02:57:46 [INFO]: Epoch 048 - training loss: 0.7589, validation loss: 0.8152
2024-06-02 02:57:56 [INFO]: Epoch 049 - training loss: 0.7591, validation loss: 0.8153
2024-06-02 02:58:07 [INFO]: Epoch 050 - training loss: 0.7574, validation loss: 0.8153
2024-06-02 02:58:18 [INFO]: Epoch 051 - training loss: 0.7617, validation loss: 0.8150
2024-06-02 02:58:28 [INFO]: Epoch 052 - training loss: 0.7615, validation loss: 0.8145
2024-06-02 02:58:39 [INFO]: Epoch 053 - training loss: 0.7639, validation loss: 0.8141
2024-06-02 02:58:49 [INFO]: Epoch 054 - training loss: 0.7554, validation loss: 0.8141
2024-06-02 02:59:00 [INFO]: Epoch 055 - training loss: 0.7624, validation loss: 0.8146
2024-06-02 02:59:11 [INFO]: Epoch 056 - training loss: 0.7599, validation loss: 0.8148
2024-06-02 02:59:21 [INFO]: Epoch 057 - training loss: 0.7650, validation loss: 0.8144
2024-06-02 02:59:32 [INFO]: Epoch 058 - training loss: 0.7563, validation loss: 0.8155
2024-06-02 02:59:43 [INFO]: Epoch 059 - training loss: 0.7561, validation loss: 0.8140
2024-06-02 02:59:53 [INFO]: Epoch 060 - training loss: 0.7548, validation loss: 0.8151
2024-06-02 03:00:04 [INFO]: Epoch 061 - training loss: 0.7539, validation loss: 0.8141
2024-06-02 03:00:14 [INFO]: Epoch 062 - training loss: 0.7548, validation loss: 0.8147
2024-06-02 03:00:25 [INFO]: Epoch 063 - training loss: 0.7519, validation loss: 0.8133
2024-06-02 03:00:36 [INFO]: Epoch 064 - training loss: 0.7531, validation loss: 0.8139
2024-06-02 03:00:46 [INFO]: Epoch 065 - training loss: 0.7559, validation loss: 0.8127
2024-06-02 03:00:57 [INFO]: Epoch 066 - training loss: 0.7506, validation loss: 0.8127
2024-06-02 03:01:08 [INFO]: Epoch 067 - training loss: 0.7484, validation loss: 0.8131
2024-06-02 03:01:18 [INFO]: Epoch 068 - training loss: 0.7502, validation loss: 0.8127
2024-06-02 03:01:29 [INFO]: Epoch 069 - training loss: 0.7522, validation loss: 0.8141
2024-06-02 03:01:39 [INFO]: Epoch 070 - training loss: 0.7531, validation loss: 0.8125
2024-06-02 03:01:50 [INFO]: Epoch 071 - training loss: 0.7485, validation loss: 0.8146
2024-06-02 03:02:01 [INFO]: Epoch 072 - training loss: 0.7490, validation loss: 0.8125
2024-06-02 03:02:11 [INFO]: Epoch 073 - training loss: 0.7443, validation loss: 0.8141
2024-06-02 03:02:22 [INFO]: Epoch 074 - training loss: 0.7456, validation loss: 0.8126
2024-06-02 03:02:33 [INFO]: Epoch 075 - training loss: 0.7513, validation loss: 0.8126
2024-06-02 03:02:43 [INFO]: Epoch 076 - training loss: 0.7501, validation loss: 0.8129
2024-06-02 03:02:54 [INFO]: Epoch 077 - training loss: 0.7479, validation loss: 0.8128
2024-06-02 03:03:05 [INFO]: Epoch 078 - training loss: 0.7496, validation loss: 0.8126
2024-06-02 03:03:15 [INFO]: Epoch 079 - training loss: 0.7509, validation loss: 0.8113
2024-06-02 03:03:26 [INFO]: Epoch 080 - training loss: 0.7498, validation loss: 0.8135
2024-06-02 03:03:36 [INFO]: Epoch 081 - training loss: 0.7527, validation loss: 0.8106
2024-06-02 03:03:47 [INFO]: Epoch 082 - training loss: 0.7469, validation loss: 0.8137
2024-06-02 03:03:58 [INFO]: Epoch 083 - training loss: 0.7439, validation loss: 0.8130
2024-06-02 03:04:08 [INFO]: Epoch 084 - training loss: 0.7452, validation loss: 0.8114
2024-06-02 03:04:19 [INFO]: Epoch 085 - training loss: 0.7437, validation loss: 0.8126
2024-06-02 03:04:29 [INFO]: Epoch 086 - training loss: 0.7428, validation loss: 0.8139
2024-06-02 03:04:40 [INFO]: Epoch 087 - training loss: 0.7412, validation loss: 0.8122
2024-06-02 03:04:51 [INFO]: Epoch 088 - training loss: 0.7452, validation loss: 0.8127
2024-06-02 03:05:01 [INFO]: Epoch 089 - training loss: 0.7469, validation loss: 0.8116
2024-06-02 03:05:12 [INFO]: Epoch 090 - training loss: 0.7499, validation loss: 0.8121
2024-06-02 03:05:23 [INFO]: Epoch 091 - training loss: 0.7501, validation loss: 0.8119
2024-06-02 03:05:23 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 03:05:23 [INFO]: Finished training. The best model is from epoch#81.
2024-06-02 03:05:23 [INFO]: Saved the model to results_point_rate01/PeMS/MRNN_PeMS/round_3/20240602_T024915/MRNN.pypots
2024-06-02 03:05:24 [INFO]: Successfully saved to results_point_rate01/PeMS/MRNN_PeMS/round_3/imputation.pkl
2024-06-02 03:05:24 [INFO]: Round3 - MRNN on PeMS: MAE=0.6234, MSE=1.0495, MRE=0.7728
2024-06-02 03:05:24 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 03:05:24 [INFO]: Using the given device: cuda:0
2024-06-02 03:05:24 [INFO]: Model files will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_4/20240602_T030524
2024-06-02 03:05:24 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MRNN_PeMS/round_4/20240602_T030524/tensorboard
2024-06-02 03:05:24 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 3,076,301
2024-06-02 03:05:35 [INFO]: Epoch 001 - training loss: 1.6505, validation loss: 0.9279
2024-06-02 03:05:46 [INFO]: Epoch 002 - training loss: 1.1771, validation loss: 0.8697
2024-06-02 03:05:56 [INFO]: Epoch 003 - training loss: 0.9756, validation loss: 0.8507
2024-06-02 03:06:07 [INFO]: Epoch 004 - training loss: 0.8685, validation loss: 0.8442
2024-06-02 03:06:17 [INFO]: Epoch 005 - training loss: 0.8378, validation loss: 0.8393
2024-06-02 03:06:28 [INFO]: Epoch 006 - training loss: 0.8376, validation loss: 0.8377
2024-06-02 03:06:39 [INFO]: Epoch 007 - training loss: 0.8207, validation loss: 0.8349
2024-06-02 03:06:49 [INFO]: Epoch 008 - training loss: 0.8121, validation loss: 0.8340
2024-06-02 03:07:00 [INFO]: Epoch 009 - training loss: 0.8065, validation loss: 0.8319
2024-06-02 03:07:11 [INFO]: Epoch 010 - training loss: 0.8005, validation loss: 0.8302
2024-06-02 03:07:21 [INFO]: Epoch 011 - training loss: 0.8004, validation loss: 0.8306
2024-06-02 03:07:32 [INFO]: Epoch 012 - training loss: 0.7981, validation loss: 0.8287
2024-06-02 03:07:42 [INFO]: Epoch 013 - training loss: 0.7943, validation loss: 0.8285
2024-06-02 03:07:53 [INFO]: Epoch 014 - training loss: 0.7918, validation loss: 0.8281
2024-06-02 03:08:04 [INFO]: Epoch 015 - training loss: 0.7919, validation loss: 0.8268
2024-06-02 03:08:14 [INFO]: Epoch 016 - training loss: 0.7901, validation loss: 0.8268
2024-06-02 03:08:25 [INFO]: Epoch 017 - training loss: 0.7846, validation loss: 0.8262
2024-06-02 03:08:35 [INFO]: Epoch 018 - training loss: 0.7952, validation loss: 0.8255
2024-06-02 03:08:46 [INFO]: Epoch 019 - training loss: 0.7844, validation loss: 0.8254
2024-06-02 03:08:56 [INFO]: Epoch 020 - training loss: 0.7808, validation loss: 0.8245
2024-06-02 03:09:07 [INFO]: Epoch 021 - training loss: 0.7857, validation loss: 0.8245
2024-06-02 03:09:18 [INFO]: Epoch 022 - training loss: 0.7774, validation loss: 0.8237
2024-06-02 03:09:28 [INFO]: Epoch 023 - training loss: 0.7750, validation loss: 0.8240
2024-06-02 03:09:39 [INFO]: Epoch 024 - training loss: 0.7762, validation loss: 0.8229
2024-06-02 03:09:49 [INFO]: Epoch 025 - training loss: 0.7755, validation loss: 0.8222
2024-06-02 03:10:00 [INFO]: Epoch 026 - training loss: 0.7780, validation loss: 0.8223
2024-06-02 03:10:11 [INFO]: Epoch 027 - training loss: 0.7779, validation loss: 0.8212
2024-06-02 03:10:21 [INFO]: Epoch 028 - training loss: 0.7727, validation loss: 0.8221
2024-06-02 03:10:32 [INFO]: Epoch 029 - training loss: 0.7791, validation loss: 0.8206
2024-06-02 03:10:42 [INFO]: Epoch 030 - training loss: 0.7814, validation loss: 0.8203
2024-06-02 03:10:53 [INFO]: Epoch 031 - training loss: 0.7766, validation loss: 0.8213
2024-06-02 03:11:04 [INFO]: Epoch 032 - training loss: 0.7683, validation loss: 0.8203
2024-06-02 03:11:14 [INFO]: Epoch 033 - training loss: 0.7707, validation loss: 0.8193
2024-06-02 03:11:25 [INFO]: Epoch 034 - training loss: 0.7670, validation loss: 0.8195
2024-06-02 03:11:35 [INFO]: Epoch 035 - training loss: 0.7670, validation loss: 0.8184
2024-06-02 03:11:46 [INFO]: Epoch 036 - training loss: 0.7650, validation loss: 0.8178
2024-06-02 03:11:57 [INFO]: Epoch 037 - training loss: 0.7634, validation loss: 0.8182
2024-06-02 03:12:07 [INFO]: Epoch 038 - training loss: 0.7647, validation loss: 0.8196
2024-06-02 03:12:18 [INFO]: Epoch 039 - training loss: 0.7728, validation loss: 0.8179
2024-06-02 03:12:28 [INFO]: Epoch 040 - training loss: 0.7641, validation loss: 0.8190
2024-06-02 03:12:39 [INFO]: Epoch 041 - training loss: 0.7622, validation loss: 0.8191
2024-06-02 03:12:49 [INFO]: Epoch 042 - training loss: 0.7643, validation loss: 0.8170
2024-06-02 03:13:00 [INFO]: Epoch 043 - training loss: 0.7584, validation loss: 0.8165
2024-06-02 03:13:11 [INFO]: Epoch 044 - training loss: 0.7612, validation loss: 0.8164
2024-06-02 03:13:21 [INFO]: Epoch 045 - training loss: 0.7633, validation loss: 0.8164
2024-06-02 03:13:32 [INFO]: Epoch 046 - training loss: 0.7582, validation loss: 0.8172
2024-06-02 03:13:42 [INFO]: Epoch 047 - training loss: 0.7649, validation loss: 0.8160
2024-06-02 03:13:53 [INFO]: Epoch 048 - training loss: 0.7636, validation loss: 0.8157
2024-06-02 03:14:04 [INFO]: Epoch 049 - training loss: 0.7645, validation loss: 0.8154
2024-06-02 03:14:14 [INFO]: Epoch 050 - training loss: 0.7607, validation loss: 0.8154
2024-06-02 03:14:25 [INFO]: Epoch 051 - training loss: 0.7618, validation loss: 0.8150
2024-06-02 03:14:35 [INFO]: Epoch 052 - training loss: 0.7587, validation loss: 0.8142
2024-06-02 03:14:46 [INFO]: Epoch 053 - training loss: 0.7560, validation loss: 0.8152
2024-06-02 03:14:57 [INFO]: Epoch 054 - training loss: 0.7550, validation loss: 0.8147
2024-06-02 03:15:07 [INFO]: Epoch 055 - training loss: 0.7548, validation loss: 0.8160
2024-06-02 03:15:18 [INFO]: Epoch 056 - training loss: 0.7580, validation loss: 0.8150
2024-06-02 03:15:28 [INFO]: Epoch 057 - training loss: 0.7612, validation loss: 0.8153
2024-06-02 03:15:39 [INFO]: Epoch 058 - training loss: 0.7623, validation loss: 0.8143
2024-06-02 03:15:50 [INFO]: Epoch 059 - training loss: 0.7672, validation loss: 0.8167
2024-06-02 03:16:00 [INFO]: Epoch 060 - training loss: 0.7621, validation loss: 0.8125
2024-06-02 03:16:11 [INFO]: Epoch 061 - training loss: 0.7591, validation loss: 0.8129
2024-06-02 03:16:21 [INFO]: Epoch 062 - training loss: 0.7521, validation loss: 0.8139
2024-06-02 03:16:32 [INFO]: Epoch 063 - training loss: 0.7560, validation loss: 0.8143
2024-06-02 03:16:42 [INFO]: Epoch 064 - training loss: 0.7548, validation loss: 0.8135
2024-06-02 03:16:53 [INFO]: Epoch 065 - training loss: 0.7524, validation loss: 0.8132
2024-06-02 03:17:04 [INFO]: Epoch 066 - training loss: 0.7547, validation loss: 0.8126
2024-06-02 03:17:14 [INFO]: Epoch 067 - training loss: 0.7560, validation loss: 0.8118
2024-06-02 03:17:25 [INFO]: Epoch 068 - training loss: 0.7568, validation loss: 0.8123
2024-06-02 03:17:35 [INFO]: Epoch 069 - training loss: 0.7527, validation loss: 0.8135
2024-06-02 03:17:46 [INFO]: Epoch 070 - training loss: 0.7482, validation loss: 0.8134
2024-06-02 03:17:57 [INFO]: Epoch 071 - training loss: 0.7530, validation loss: 0.8132
2024-06-02 03:18:07 [INFO]: Epoch 072 - training loss: 0.7470, validation loss: 0.8129
2024-06-02 03:18:18 [INFO]: Epoch 073 - training loss: 0.7503, validation loss: 0.8132
2024-06-02 03:18:28 [INFO]: Epoch 074 - training loss: 0.7486, validation loss: 0.8115
2024-06-02 03:18:39 [INFO]: Epoch 075 - training loss: 0.7498, validation loss: 0.8135
2024-06-02 03:18:50 [INFO]: Epoch 076 - training loss: 0.7494, validation loss: 0.8124
2024-06-02 03:19:00 [INFO]: Epoch 077 - training loss: 0.7539, validation loss: 0.8118
2024-06-02 03:19:11 [INFO]: Epoch 078 - training loss: 0.7496, validation loss: 0.8132
2024-06-02 03:19:21 [INFO]: Epoch 079 - training loss: 0.7481, validation loss: 0.8123
2024-06-02 03:19:32 [INFO]: Epoch 080 - training loss: 0.7540, validation loss: 0.8131
2024-06-02 03:19:43 [INFO]: Epoch 081 - training loss: 0.7488, validation loss: 0.8113
2024-06-02 03:19:53 [INFO]: Epoch 082 - training loss: 0.7522, validation loss: 0.8115
2024-06-02 03:20:04 [INFO]: Epoch 083 - training loss: 0.7526, validation loss: 0.8134
2024-06-02 03:20:14 [INFO]: Epoch 084 - training loss: 0.7481, validation loss: 0.8109
2024-06-02 03:20:25 [INFO]: Epoch 085 - training loss: 0.7530, validation loss: 0.8142
2024-06-02 03:20:36 [INFO]: Epoch 086 - training loss: 0.7492, validation loss: 0.8113
2024-06-02 03:20:46 [INFO]: Epoch 087 - training loss: 0.7400, validation loss: 0.8108
2024-06-02 03:20:57 [INFO]: Epoch 088 - training loss: 0.7446, validation loss: 0.8118
2024-06-02 03:21:07 [INFO]: Epoch 089 - training loss: 0.7504, validation loss: 0.8113
2024-06-02 03:21:18 [INFO]: Epoch 090 - training loss: 0.7467, validation loss: 0.8131
2024-06-02 03:21:29 [INFO]: Epoch 091 - training loss: 0.7443, validation loss: 0.8115
2024-06-02 03:21:39 [INFO]: Epoch 092 - training loss: 0.7550, validation loss: 0.8114
2024-06-02 03:21:50 [INFO]: Epoch 093 - training loss: 0.7549, validation loss: 0.8123
2024-06-02 03:22:00 [INFO]: Epoch 094 - training loss: 0.7547, validation loss: 0.8114
2024-06-02 03:22:11 [INFO]: Epoch 095 - training loss: 0.7479, validation loss: 0.8123
2024-06-02 03:22:21 [INFO]: Epoch 096 - training loss: 0.7469, validation loss: 0.8116
2024-06-02 03:22:32 [INFO]: Epoch 097 - training loss: 0.7449, validation loss: 0.8120
2024-06-02 03:22:32 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 03:22:32 [INFO]: Finished training. The best model is from epoch#87.
2024-06-02 03:22:32 [INFO]: Saved the model to results_point_rate01/PeMS/MRNN_PeMS/round_4/20240602_T030524/MRNN.pypots
2024-06-02 03:22:34 [INFO]: Successfully saved to results_point_rate01/PeMS/MRNN_PeMS/round_4/imputation.pkl
2024-06-02 03:22:34 [INFO]: Round4 - MRNN on PeMS: MAE=0.6235, MSE=1.0487, MRE=0.7729
2024-06-02 03:22:34 [INFO]: Done! Final results:
Averaged MRNN (n params: 3,076,301) on PeMS: MAE=0.6235 ± 0.0001241670019334976, MSE=1.0492 ± 0.0005702426288041905, MRE=0.7729 ± 0.00015391736980172824, average inference time=1.37
