2024-06-02 03:50:59 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 03:50:59 [INFO]: Using the given device: cuda:0
2024-06-02 03:50:59 [INFO]: Model files will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_0/20240602_T035059
2024-06-02 03:50:59 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_0/20240602_T035059/tensorboard
2024-06-02 03:50:59 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=256
2024-06-02 03:50:59 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-02 03:50:59 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 23,135,326
2024-06-02 03:51:01 [INFO]: Epoch 001 - training loss: 0.9000, validation loss: 0.5698
2024-06-02 03:51:01 [INFO]: Epoch 002 - training loss: 0.5697, validation loss: 0.5126
2024-06-02 03:51:02 [INFO]: Epoch 003 - training loss: 0.5131, validation loss: 0.4901
2024-06-02 03:51:02 [INFO]: Epoch 004 - training loss: 0.4800, validation loss: 0.4856
2024-06-02 03:51:03 [INFO]: Epoch 005 - training loss: 0.4635, validation loss: 0.4769
2024-06-02 03:51:03 [INFO]: Epoch 006 - training loss: 0.4505, validation loss: 0.4690
2024-06-02 03:51:04 [INFO]: Epoch 007 - training loss: 0.4348, validation loss: 0.4679
2024-06-02 03:51:05 [INFO]: Epoch 008 - training loss: 0.4267, validation loss: 0.4605
2024-06-02 03:51:05 [INFO]: Epoch 009 - training loss: 0.4164, validation loss: 0.4593
2024-06-02 03:51:06 [INFO]: Epoch 010 - training loss: 0.4105, validation loss: 0.4547
2024-06-02 03:51:07 [INFO]: Epoch 011 - training loss: 0.4087, validation loss: 0.4493
2024-06-02 03:51:08 [INFO]: Epoch 012 - training loss: 0.4028, validation loss: 0.4511
2024-06-02 03:51:09 [INFO]: Epoch 013 - training loss: 0.3943, validation loss: 0.4477
2024-06-02 03:51:09 [INFO]: Epoch 014 - training loss: 0.3862, validation loss: 0.4415
2024-06-02 03:51:10 [INFO]: Epoch 015 - training loss: 0.3807, validation loss: 0.4385
2024-06-02 03:51:11 [INFO]: Epoch 016 - training loss: 0.3754, validation loss: 0.4391
2024-06-02 03:51:12 [INFO]: Epoch 017 - training loss: 0.3731, validation loss: 0.4357
2024-06-02 03:51:13 [INFO]: Epoch 018 - training loss: 0.3666, validation loss: 0.4311
2024-06-02 03:51:13 [INFO]: Epoch 019 - training loss: 0.3612, validation loss: 0.4322
2024-06-02 03:51:14 [INFO]: Epoch 020 - training loss: 0.3621, validation loss: 0.4296
2024-06-02 03:51:15 [INFO]: Epoch 021 - training loss: 0.3580, validation loss: 0.4249
2024-06-02 03:51:16 [INFO]: Epoch 022 - training loss: 0.3572, validation loss: 0.4230
2024-06-02 03:51:17 [INFO]: Epoch 023 - training loss: 0.3517, validation loss: 0.4245
2024-06-02 03:51:18 [INFO]: Epoch 024 - training loss: 0.3481, validation loss: 0.4227
2024-06-02 03:51:18 [INFO]: Epoch 025 - training loss: 0.3519, validation loss: 0.4208
2024-06-02 03:51:19 [INFO]: Epoch 026 - training loss: 0.3479, validation loss: 0.4165
2024-06-02 03:51:20 [INFO]: Epoch 027 - training loss: 0.3581, validation loss: 0.4188
2024-06-02 03:51:21 [INFO]: Epoch 028 - training loss: 0.3499, validation loss: 0.4153
2024-06-02 03:51:22 [INFO]: Epoch 029 - training loss: 0.3372, validation loss: 0.4131
2024-06-02 03:51:23 [INFO]: Epoch 030 - training loss: 0.3394, validation loss: 0.4109
2024-06-02 03:51:23 [INFO]: Epoch 031 - training loss: 0.3344, validation loss: 0.4110
2024-06-02 03:51:24 [INFO]: Epoch 032 - training loss: 0.3329, validation loss: 0.4100
2024-06-02 03:51:25 [INFO]: Epoch 033 - training loss: 0.3294, validation loss: 0.4070
2024-06-02 03:51:26 [INFO]: Epoch 034 - training loss: 0.3277, validation loss: 0.4073
2024-06-02 03:51:27 [INFO]: Epoch 035 - training loss: 0.3248, validation loss: 0.4058
2024-06-02 03:51:27 [INFO]: Epoch 036 - training loss: 0.3219, validation loss: 0.4068
2024-06-02 03:51:28 [INFO]: Epoch 037 - training loss: 0.3246, validation loss: 0.4058
2024-06-02 03:51:29 [INFO]: Epoch 038 - training loss: 0.3225, validation loss: 0.4045
2024-06-02 03:51:30 [INFO]: Epoch 039 - training loss: 0.3223, validation loss: 0.4031
2024-06-02 03:51:31 [INFO]: Epoch 040 - training loss: 0.3184, validation loss: 0.4013
2024-06-02 03:51:31 [INFO]: Epoch 041 - training loss: 0.3194, validation loss: 0.3998
2024-06-02 03:51:32 [INFO]: Epoch 042 - training loss: 0.3129, validation loss: 0.4007
2024-06-02 03:51:33 [INFO]: Epoch 043 - training loss: 0.3155, validation loss: 0.3991
2024-06-02 03:51:34 [INFO]: Epoch 044 - training loss: 0.3151, validation loss: 0.3965
2024-06-02 03:51:34 [INFO]: Epoch 045 - training loss: 0.3153, validation loss: 0.3964
2024-06-02 03:51:35 [INFO]: Epoch 046 - training loss: 0.3141, validation loss: 0.3966
2024-06-02 03:51:36 [INFO]: Epoch 047 - training loss: 0.3095, validation loss: 0.3942
2024-06-02 03:51:37 [INFO]: Epoch 048 - training loss: 0.3090, validation loss: 0.3950
2024-06-02 03:51:37 [INFO]: Epoch 049 - training loss: 0.3068, validation loss: 0.3948
2024-06-02 03:51:38 [INFO]: Epoch 050 - training loss: 0.3063, validation loss: 0.3916
2024-06-02 03:51:39 [INFO]: Epoch 051 - training loss: 0.3031, validation loss: 0.3912
2024-06-02 03:51:40 [INFO]: Epoch 052 - training loss: 0.3021, validation loss: 0.3906
2024-06-02 03:51:41 [INFO]: Epoch 053 - training loss: 0.3028, validation loss: 0.3890
2024-06-02 03:51:42 [INFO]: Epoch 054 - training loss: 0.2994, validation loss: 0.3910
2024-06-02 03:51:42 [INFO]: Epoch 055 - training loss: 0.3051, validation loss: 0.3897
2024-06-02 03:51:43 [INFO]: Epoch 056 - training loss: 0.3025, validation loss: 0.3909
2024-06-02 03:51:44 [INFO]: Epoch 057 - training loss: 0.2992, validation loss: 0.3871
2024-06-02 03:51:45 [INFO]: Epoch 058 - training loss: 0.2958, validation loss: 0.3846
2024-06-02 03:51:46 [INFO]: Epoch 059 - training loss: 0.2969, validation loss: 0.3859
2024-06-02 03:51:46 [INFO]: Epoch 060 - training loss: 0.2963, validation loss: 0.3853
2024-06-02 03:51:47 [INFO]: Epoch 061 - training loss: 0.2973, validation loss: 0.3863
2024-06-02 03:51:48 [INFO]: Epoch 062 - training loss: 0.2962, validation loss: 0.3849
2024-06-02 03:51:49 [INFO]: Epoch 063 - training loss: 0.2915, validation loss: 0.3840
2024-06-02 03:51:50 [INFO]: Epoch 064 - training loss: 0.2918, validation loss: 0.3834
2024-06-02 03:51:51 [INFO]: Epoch 065 - training loss: 0.2890, validation loss: 0.3862
2024-06-02 03:51:51 [INFO]: Epoch 066 - training loss: 0.2892, validation loss: 0.3835
2024-06-02 03:51:52 [INFO]: Epoch 067 - training loss: 0.2866, validation loss: 0.3806
2024-06-02 03:51:53 [INFO]: Epoch 068 - training loss: 0.2911, validation loss: 0.3827
2024-06-02 03:51:54 [INFO]: Epoch 069 - training loss: 0.2888, validation loss: 0.3807
2024-06-02 03:51:55 [INFO]: Epoch 070 - training loss: 0.2856, validation loss: 0.3799
2024-06-02 03:51:56 [INFO]: Epoch 071 - training loss: 0.2847, validation loss: 0.3794
2024-06-02 03:51:56 [INFO]: Epoch 072 - training loss: 0.2830, validation loss: 0.3795
2024-06-02 03:51:57 [INFO]: Epoch 073 - training loss: 0.2883, validation loss: 0.3787
2024-06-02 03:51:58 [INFO]: Epoch 074 - training loss: 0.2850, validation loss: 0.3781
2024-06-02 03:51:59 [INFO]: Epoch 075 - training loss: 0.2808, validation loss: 0.3766
2024-06-02 03:52:00 [INFO]: Epoch 076 - training loss: 0.2819, validation loss: 0.3775
2024-06-02 03:52:00 [INFO]: Epoch 077 - training loss: 0.2828, validation loss: 0.3787
2024-06-02 03:52:01 [INFO]: Epoch 078 - training loss: 0.2797, validation loss: 0.3772
2024-06-02 03:52:02 [INFO]: Epoch 079 - training loss: 0.2812, validation loss: 0.3792
2024-06-02 03:52:03 [INFO]: Epoch 080 - training loss: 0.2814, validation loss: 0.3793
2024-06-02 03:52:04 [INFO]: Epoch 081 - training loss: 0.2819, validation loss: 0.3770
2024-06-02 03:52:04 [INFO]: Epoch 082 - training loss: 0.2781, validation loss: 0.3766
2024-06-02 03:52:05 [INFO]: Epoch 083 - training loss: 0.2748, validation loss: 0.3748
2024-06-02 03:52:06 [INFO]: Epoch 084 - training loss: 0.2773, validation loss: 0.3758
2024-06-02 03:52:07 [INFO]: Epoch 085 - training loss: 0.2758, validation loss: 0.3778
2024-06-02 03:52:08 [INFO]: Epoch 086 - training loss: 0.2794, validation loss: 0.3731
2024-06-02 03:52:08 [INFO]: Epoch 087 - training loss: 0.2740, validation loss: 0.3771
2024-06-02 03:52:09 [INFO]: Epoch 088 - training loss: 0.2717, validation loss: 0.3755
2024-06-02 03:52:10 [INFO]: Epoch 089 - training loss: 0.2765, validation loss: 0.3735
2024-06-02 03:52:11 [INFO]: Epoch 090 - training loss: 0.2702, validation loss: 0.3720
2024-06-02 03:52:11 [INFO]: Epoch 091 - training loss: 0.2690, validation loss: 0.3733
2024-06-02 03:52:12 [INFO]: Epoch 092 - training loss: 0.2703, validation loss: 0.3709
2024-06-02 03:52:13 [INFO]: Epoch 093 - training loss: 0.2704, validation loss: 0.3702
2024-06-02 03:52:14 [INFO]: Epoch 094 - training loss: 0.2705, validation loss: 0.3705
2024-06-02 03:52:14 [INFO]: Epoch 095 - training loss: 0.2749, validation loss: 0.3742
2024-06-02 03:52:15 [INFO]: Epoch 096 - training loss: 0.2704, validation loss: 0.3713
2024-06-02 03:52:16 [INFO]: Epoch 097 - training loss: 0.2681, validation loss: 0.3705
2024-06-02 03:52:17 [INFO]: Epoch 098 - training loss: 0.2695, validation loss: 0.3702
2024-06-02 03:52:18 [INFO]: Epoch 099 - training loss: 0.2669, validation loss: 0.3713
2024-06-02 03:52:18 [INFO]: Epoch 100 - training loss: 0.2707, validation loss: 0.3699
2024-06-02 03:52:18 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 03:52:19 [INFO]: Saved the model to results_point_rate01/PeMS/Transformer_PeMS/round_0/20240602_T035059/Transformer.pypots
2024-06-02 03:52:19 [INFO]: Successfully saved to results_point_rate01/PeMS/Transformer_PeMS/round_0/imputation.pkl
2024-06-02 03:52:19 [INFO]: Round0 - Transformer on PeMS: MAE=0.2917, MSE=0.5540, MRE=0.3616
2024-06-02 03:52:19 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 03:52:19 [INFO]: Using the given device: cuda:0
2024-06-02 03:52:19 [INFO]: Model files will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_1/20240602_T035219
2024-06-02 03:52:19 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_1/20240602_T035219/tensorboard
2024-06-02 03:52:19 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=256
2024-06-02 03:52:19 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-02 03:52:19 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 23,135,326
2024-06-02 03:52:20 [INFO]: Epoch 001 - training loss: 0.8950, validation loss: 0.5689
2024-06-02 03:52:21 [INFO]: Epoch 002 - training loss: 0.5706, validation loss: 0.5098
2024-06-02 03:52:22 [INFO]: Epoch 003 - training loss: 0.5042, validation loss: 0.4882
2024-06-02 03:52:23 [INFO]: Epoch 004 - training loss: 0.4880, validation loss: 0.4801
2024-06-02 03:52:23 [INFO]: Epoch 005 - training loss: 0.4635, validation loss: 0.4836
2024-06-02 03:52:24 [INFO]: Epoch 006 - training loss: 0.4535, validation loss: 0.4757
2024-06-02 03:52:25 [INFO]: Epoch 007 - training loss: 0.4424, validation loss: 0.4674
2024-06-02 03:52:26 [INFO]: Epoch 008 - training loss: 0.4285, validation loss: 0.4613
2024-06-02 03:52:27 [INFO]: Epoch 009 - training loss: 0.4234, validation loss: 0.4610
2024-06-02 03:52:27 [INFO]: Epoch 010 - training loss: 0.4150, validation loss: 0.4560
2024-06-02 03:52:28 [INFO]: Epoch 011 - training loss: 0.4048, validation loss: 0.4557
2024-06-02 03:52:29 [INFO]: Epoch 012 - training loss: 0.4009, validation loss: 0.4489
2024-06-02 03:52:30 [INFO]: Epoch 013 - training loss: 0.3952, validation loss: 0.4503
2024-06-02 03:52:31 [INFO]: Epoch 014 - training loss: 0.3944, validation loss: 0.4464
2024-06-02 03:52:31 [INFO]: Epoch 015 - training loss: 0.3866, validation loss: 0.4428
2024-06-02 03:52:32 [INFO]: Epoch 016 - training loss: 0.3795, validation loss: 0.4453
2024-06-02 03:52:33 [INFO]: Epoch 017 - training loss: 0.3789, validation loss: 0.4374
2024-06-02 03:52:34 [INFO]: Epoch 018 - training loss: 0.3730, validation loss: 0.4344
2024-06-02 03:52:35 [INFO]: Epoch 019 - training loss: 0.3718, validation loss: 0.4341
2024-06-02 03:52:36 [INFO]: Epoch 020 - training loss: 0.3661, validation loss: 0.4343
2024-06-02 03:52:36 [INFO]: Epoch 021 - training loss: 0.3618, validation loss: 0.4283
2024-06-02 03:52:37 [INFO]: Epoch 022 - training loss: 0.3564, validation loss: 0.4273
2024-06-02 03:52:38 [INFO]: Epoch 023 - training loss: 0.3551, validation loss: 0.4265
2024-06-02 03:52:39 [INFO]: Epoch 024 - training loss: 0.3509, validation loss: 0.4263
2024-06-02 03:52:39 [INFO]: Epoch 025 - training loss: 0.3501, validation loss: 0.4233
2024-06-02 03:52:40 [INFO]: Epoch 026 - training loss: 0.3467, validation loss: 0.4168
2024-06-02 03:52:40 [INFO]: Epoch 027 - training loss: 0.3463, validation loss: 0.4193
2024-06-02 03:52:41 [INFO]: Epoch 028 - training loss: 0.3437, validation loss: 0.4158
2024-06-02 03:52:42 [INFO]: Epoch 029 - training loss: 0.3358, validation loss: 0.4177
2024-06-02 03:52:42 [INFO]: Epoch 030 - training loss: 0.3400, validation loss: 0.4153
2024-06-02 03:52:43 [INFO]: Epoch 031 - training loss: 0.3405, validation loss: 0.4145
2024-06-02 03:52:43 [INFO]: Epoch 032 - training loss: 0.3364, validation loss: 0.4148
2024-06-02 03:52:44 [INFO]: Epoch 033 - training loss: 0.3307, validation loss: 0.4136
2024-06-02 03:52:45 [INFO]: Epoch 034 - training loss: 0.3324, validation loss: 0.4092
2024-06-02 03:52:46 [INFO]: Epoch 035 - training loss: 0.3295, validation loss: 0.4066
2024-06-02 03:52:47 [INFO]: Epoch 036 - training loss: 0.3231, validation loss: 0.4062
2024-06-02 03:52:47 [INFO]: Epoch 037 - training loss: 0.3239, validation loss: 0.4051
2024-06-02 03:52:48 [INFO]: Epoch 038 - training loss: 0.3253, validation loss: 0.4079
2024-06-02 03:52:49 [INFO]: Epoch 039 - training loss: 0.3248, validation loss: 0.4074
2024-06-02 03:52:50 [INFO]: Epoch 040 - training loss: 0.3293, validation loss: 0.4044
2024-06-02 03:52:50 [INFO]: Epoch 041 - training loss: 0.3247, validation loss: 0.4008
2024-06-02 03:52:51 [INFO]: Epoch 042 - training loss: 0.3191, validation loss: 0.3986
2024-06-02 03:52:52 [INFO]: Epoch 043 - training loss: 0.3165, validation loss: 0.3994
2024-06-02 03:52:53 [INFO]: Epoch 044 - training loss: 0.3165, validation loss: 0.4009
2024-06-02 03:52:53 [INFO]: Epoch 045 - training loss: 0.3155, validation loss: 0.3965
2024-06-02 03:52:54 [INFO]: Epoch 046 - training loss: 0.3150, validation loss: 0.3981
2024-06-02 03:52:55 [INFO]: Epoch 047 - training loss: 0.3131, validation loss: 0.3951
2024-06-02 03:52:56 [INFO]: Epoch 048 - training loss: 0.3112, validation loss: 0.3943
2024-06-02 03:52:57 [INFO]: Epoch 049 - training loss: 0.3096, validation loss: 0.3950
2024-06-02 03:52:57 [INFO]: Epoch 050 - training loss: 0.3077, validation loss: 0.3962
2024-06-02 03:52:58 [INFO]: Epoch 051 - training loss: 0.3097, validation loss: 0.3926
2024-06-02 03:52:59 [INFO]: Epoch 052 - training loss: 0.3050, validation loss: 0.3914
2024-06-02 03:53:00 [INFO]: Epoch 053 - training loss: 0.3029, validation loss: 0.3905
2024-06-02 03:53:01 [INFO]: Epoch 054 - training loss: 0.3034, validation loss: 0.3899
2024-06-02 03:53:01 [INFO]: Epoch 055 - training loss: 0.2986, validation loss: 0.3905
2024-06-02 03:53:02 [INFO]: Epoch 056 - training loss: 0.3005, validation loss: 0.3900
2024-06-02 03:53:03 [INFO]: Epoch 057 - training loss: 0.3006, validation loss: 0.3886
2024-06-02 03:53:04 [INFO]: Epoch 058 - training loss: 0.2973, validation loss: 0.3881
2024-06-02 03:53:05 [INFO]: Epoch 059 - training loss: 0.2976, validation loss: 0.3875
2024-06-02 03:53:05 [INFO]: Epoch 060 - training loss: 0.3006, validation loss: 0.3878
2024-06-02 03:53:06 [INFO]: Epoch 061 - training loss: 0.2976, validation loss: 0.3858
2024-06-02 03:53:07 [INFO]: Epoch 062 - training loss: 0.2936, validation loss: 0.3869
2024-06-02 03:53:08 [INFO]: Epoch 063 - training loss: 0.3017, validation loss: 0.3857
2024-06-02 03:53:08 [INFO]: Epoch 064 - training loss: 0.2944, validation loss: 0.3862
2024-06-02 03:53:09 [INFO]: Epoch 065 - training loss: 0.2915, validation loss: 0.3838
2024-06-02 03:53:10 [INFO]: Epoch 066 - training loss: 0.2897, validation loss: 0.3825
2024-06-02 03:53:11 [INFO]: Epoch 067 - training loss: 0.2901, validation loss: 0.3829
2024-06-02 03:53:11 [INFO]: Epoch 068 - training loss: 0.2868, validation loss: 0.3822
2024-06-02 03:53:12 [INFO]: Epoch 069 - training loss: 0.2870, validation loss: 0.3803
2024-06-02 03:53:13 [INFO]: Epoch 070 - training loss: 0.2906, validation loss: 0.3829
2024-06-02 03:53:14 [INFO]: Epoch 071 - training loss: 0.2895, validation loss: 0.3843
2024-06-02 03:53:14 [INFO]: Epoch 072 - training loss: 0.2856, validation loss: 0.3813
2024-06-02 03:53:15 [INFO]: Epoch 073 - training loss: 0.2886, validation loss: 0.3800
2024-06-02 03:53:16 [INFO]: Epoch 074 - training loss: 0.2896, validation loss: 0.3818
2024-06-02 03:53:17 [INFO]: Epoch 075 - training loss: 0.2819, validation loss: 0.3782
2024-06-02 03:53:18 [INFO]: Epoch 076 - training loss: 0.2795, validation loss: 0.3801
2024-06-02 03:53:18 [INFO]: Epoch 077 - training loss: 0.2763, validation loss: 0.3794
2024-06-02 03:53:19 [INFO]: Epoch 078 - training loss: 0.2798, validation loss: 0.3769
2024-06-02 03:53:20 [INFO]: Epoch 079 - training loss: 0.2865, validation loss: 0.3790
2024-06-02 03:53:21 [INFO]: Epoch 080 - training loss: 0.2816, validation loss: 0.3763
2024-06-02 03:53:22 [INFO]: Epoch 081 - training loss: 0.2808, validation loss: 0.3795
2024-06-02 03:53:22 [INFO]: Epoch 082 - training loss: 0.2762, validation loss: 0.3760
2024-06-02 03:53:23 [INFO]: Epoch 083 - training loss: 0.2760, validation loss: 0.3772
2024-06-02 03:53:24 [INFO]: Epoch 084 - training loss: 0.2772, validation loss: 0.3758
2024-06-02 03:53:25 [INFO]: Epoch 085 - training loss: 0.2753, validation loss: 0.3758
2024-06-02 03:53:26 [INFO]: Epoch 086 - training loss: 0.2771, validation loss: 0.3742
2024-06-02 03:53:26 [INFO]: Epoch 087 - training loss: 0.2747, validation loss: 0.3754
2024-06-02 03:53:27 [INFO]: Epoch 088 - training loss: 0.2742, validation loss: 0.3748
2024-06-02 03:53:28 [INFO]: Epoch 089 - training loss: 0.2768, validation loss: 0.3763
2024-06-02 03:53:29 [INFO]: Epoch 090 - training loss: 0.2783, validation loss: 0.3745
2024-06-02 03:53:30 [INFO]: Epoch 091 - training loss: 0.2791, validation loss: 0.3746
2024-06-02 03:53:31 [INFO]: Epoch 092 - training loss: 0.2712, validation loss: 0.3744
2024-06-02 03:53:31 [INFO]: Epoch 093 - training loss: 0.2721, validation loss: 0.3724
2024-06-02 03:53:32 [INFO]: Epoch 094 - training loss: 0.2709, validation loss: 0.3737
2024-06-02 03:53:33 [INFO]: Epoch 095 - training loss: 0.2679, validation loss: 0.3738
2024-06-02 03:53:34 [INFO]: Epoch 096 - training loss: 0.2681, validation loss: 0.3719
2024-06-02 03:53:35 [INFO]: Epoch 097 - training loss: 0.2677, validation loss: 0.3717
2024-06-02 03:53:36 [INFO]: Epoch 098 - training loss: 0.2682, validation loss: 0.3694
2024-06-02 03:53:36 [INFO]: Epoch 099 - training loss: 0.2652, validation loss: 0.3711
2024-06-02 03:53:37 [INFO]: Epoch 100 - training loss: 0.2706, validation loss: 0.3714
2024-06-02 03:53:37 [INFO]: Finished training. The best model is from epoch#98.
2024-06-02 03:53:37 [INFO]: Saved the model to results_point_rate01/PeMS/Transformer_PeMS/round_1/20240602_T035219/Transformer.pypots
2024-06-02 03:53:37 [INFO]: Successfully saved to results_point_rate01/PeMS/Transformer_PeMS/round_1/imputation.pkl
2024-06-02 03:53:37 [INFO]: Round1 - Transformer on PeMS: MAE=0.2961, MSE=0.5570, MRE=0.3671
2024-06-02 03:53:37 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 03:53:37 [INFO]: Using the given device: cuda:0
2024-06-02 03:53:37 [INFO]: Model files will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_2/20240602_T035337
2024-06-02 03:53:37 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_2/20240602_T035337/tensorboard
2024-06-02 03:53:37 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=256
2024-06-02 03:53:37 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-02 03:53:38 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 23,135,326
2024-06-02 03:53:38 [INFO]: Epoch 001 - training loss: 0.8974, validation loss: 0.5716
2024-06-02 03:53:39 [INFO]: Epoch 002 - training loss: 0.5600, validation loss: 0.5058
2024-06-02 03:53:40 [INFO]: Epoch 003 - training loss: 0.5095, validation loss: 0.4859
2024-06-02 03:53:41 [INFO]: Epoch 004 - training loss: 0.4795, validation loss: 0.4901
2024-06-02 03:53:42 [INFO]: Epoch 005 - training loss: 0.4683, validation loss: 0.4776
2024-06-02 03:53:42 [INFO]: Epoch 006 - training loss: 0.4541, validation loss: 0.4696
2024-06-02 03:53:43 [INFO]: Epoch 007 - training loss: 0.4365, validation loss: 0.4684
2024-06-02 03:53:44 [INFO]: Epoch 008 - training loss: 0.4283, validation loss: 0.4633
2024-06-02 03:53:45 [INFO]: Epoch 009 - training loss: 0.4190, validation loss: 0.4556
2024-06-02 03:53:46 [INFO]: Epoch 010 - training loss: 0.4110, validation loss: 0.4553
2024-06-02 03:53:47 [INFO]: Epoch 011 - training loss: 0.4081, validation loss: 0.4523
2024-06-02 03:53:47 [INFO]: Epoch 012 - training loss: 0.4048, validation loss: 0.4511
2024-06-02 03:53:48 [INFO]: Epoch 013 - training loss: 0.3951, validation loss: 0.4495
2024-06-02 03:53:49 [INFO]: Epoch 014 - training loss: 0.3917, validation loss: 0.4481
2024-06-02 03:53:50 [INFO]: Epoch 015 - training loss: 0.3865, validation loss: 0.4396
2024-06-02 03:53:51 [INFO]: Epoch 016 - training loss: 0.3781, validation loss: 0.4409
2024-06-02 03:53:51 [INFO]: Epoch 017 - training loss: 0.3765, validation loss: 0.4381
2024-06-02 03:53:52 [INFO]: Epoch 018 - training loss: 0.3722, validation loss: 0.4344
2024-06-02 03:53:53 [INFO]: Epoch 019 - training loss: 0.3634, validation loss: 0.4344
2024-06-02 03:53:53 [INFO]: Epoch 020 - training loss: 0.3647, validation loss: 0.4366
2024-06-02 03:53:54 [INFO]: Epoch 021 - training loss: 0.3584, validation loss: 0.4276
2024-06-02 03:53:55 [INFO]: Epoch 022 - training loss: 0.3549, validation loss: 0.4284
2024-06-02 03:53:56 [INFO]: Epoch 023 - training loss: 0.3514, validation loss: 0.4258
2024-06-02 03:53:57 [INFO]: Epoch 024 - training loss: 0.3505, validation loss: 0.4217
2024-06-02 03:53:58 [INFO]: Epoch 025 - training loss: 0.3417, validation loss: 0.4219
2024-06-02 03:53:58 [INFO]: Epoch 026 - training loss: 0.3466, validation loss: 0.4182
2024-06-02 03:53:59 [INFO]: Epoch 027 - training loss: 0.3430, validation loss: 0.4168
2024-06-02 03:54:00 [INFO]: Epoch 028 - training loss: 0.3450, validation loss: 0.4168
2024-06-02 03:54:01 [INFO]: Epoch 029 - training loss: 0.3393, validation loss: 0.4161
2024-06-02 03:54:02 [INFO]: Epoch 030 - training loss: 0.3389, validation loss: 0.4149
2024-06-02 03:54:03 [INFO]: Epoch 031 - training loss: 0.3379, validation loss: 0.4139
2024-06-02 03:54:03 [INFO]: Epoch 032 - training loss: 0.3336, validation loss: 0.4125
2024-06-02 03:54:04 [INFO]: Epoch 033 - training loss: 0.3297, validation loss: 0.4088
2024-06-02 03:54:05 [INFO]: Epoch 034 - training loss: 0.3274, validation loss: 0.4095
2024-06-02 03:54:06 [INFO]: Epoch 035 - training loss: 0.3298, validation loss: 0.4109
2024-06-02 03:54:06 [INFO]: Epoch 036 - training loss: 0.3247, validation loss: 0.4089
2024-06-02 03:54:07 [INFO]: Epoch 037 - training loss: 0.3299, validation loss: 0.4066
2024-06-02 03:54:08 [INFO]: Epoch 038 - training loss: 0.3239, validation loss: 0.4041
2024-06-02 03:54:09 [INFO]: Epoch 039 - training loss: 0.3224, validation loss: 0.4064
2024-06-02 03:54:10 [INFO]: Epoch 040 - training loss: 0.3203, validation loss: 0.4049
2024-06-02 03:54:10 [INFO]: Epoch 041 - training loss: 0.3200, validation loss: 0.4017
2024-06-02 03:54:11 [INFO]: Epoch 042 - training loss: 0.3171, validation loss: 0.4027
2024-06-02 03:54:12 [INFO]: Epoch 043 - training loss: 0.3147, validation loss: 0.4030
2024-06-02 03:54:13 [INFO]: Epoch 044 - training loss: 0.3111, validation loss: 0.4020
2024-06-02 03:54:13 [INFO]: Epoch 045 - training loss: 0.3144, validation loss: 0.3986
2024-06-02 03:54:14 [INFO]: Epoch 046 - training loss: 0.3126, validation loss: 0.3997
2024-06-02 03:54:15 [INFO]: Epoch 047 - training loss: 0.3125, validation loss: 0.3964
2024-06-02 03:54:16 [INFO]: Epoch 048 - training loss: 0.3102, validation loss: 0.3946
2024-06-02 03:54:16 [INFO]: Epoch 049 - training loss: 0.3043, validation loss: 0.3938
2024-06-02 03:54:17 [INFO]: Epoch 050 - training loss: 0.3075, validation loss: 0.3938
2024-06-02 03:54:18 [INFO]: Epoch 051 - training loss: 0.3034, validation loss: 0.3966
2024-06-02 03:54:19 [INFO]: Epoch 052 - training loss: 0.3068, validation loss: 0.3909
2024-06-02 03:54:19 [INFO]: Epoch 053 - training loss: 0.3035, validation loss: 0.3894
2024-06-02 03:54:20 [INFO]: Epoch 054 - training loss: 0.3044, validation loss: 0.3896
2024-06-02 03:54:21 [INFO]: Epoch 055 - training loss: 0.3053, validation loss: 0.3936
2024-06-02 03:54:22 [INFO]: Epoch 056 - training loss: 0.3044, validation loss: 0.3892
2024-06-02 03:54:23 [INFO]: Epoch 057 - training loss: 0.2998, validation loss: 0.3886
2024-06-02 03:54:23 [INFO]: Epoch 058 - training loss: 0.3019, validation loss: 0.3890
2024-06-02 03:54:24 [INFO]: Epoch 059 - training loss: 0.2958, validation loss: 0.3879
2024-06-02 03:54:24 [INFO]: Epoch 060 - training loss: 0.2930, validation loss: 0.3862
2024-06-02 03:54:25 [INFO]: Epoch 061 - training loss: 0.2943, validation loss: 0.3855
2024-06-02 03:54:26 [INFO]: Epoch 062 - training loss: 0.2931, validation loss: 0.3838
2024-06-02 03:54:26 [INFO]: Epoch 063 - training loss: 0.2919, validation loss: 0.3845
2024-06-02 03:54:27 [INFO]: Epoch 064 - training loss: 0.2911, validation loss: 0.3858
2024-06-02 03:54:28 [INFO]: Epoch 065 - training loss: 0.2926, validation loss: 0.3817
2024-06-02 03:54:29 [INFO]: Epoch 066 - training loss: 0.2898, validation loss: 0.3816
2024-06-02 03:54:30 [INFO]: Epoch 067 - training loss: 0.2873, validation loss: 0.3817
2024-06-02 03:54:30 [INFO]: Epoch 068 - training loss: 0.2905, validation loss: 0.3848
2024-06-02 03:54:31 [INFO]: Epoch 069 - training loss: 0.2883, validation loss: 0.3797
2024-06-02 03:54:32 [INFO]: Epoch 070 - training loss: 0.2916, validation loss: 0.3812
2024-06-02 03:54:33 [INFO]: Epoch 071 - training loss: 0.2875, validation loss: 0.3790
2024-06-02 03:54:33 [INFO]: Epoch 072 - training loss: 0.2850, validation loss: 0.3803
2024-06-02 03:54:34 [INFO]: Epoch 073 - training loss: 0.2826, validation loss: 0.3792
2024-06-02 03:54:35 [INFO]: Epoch 074 - training loss: 0.2823, validation loss: 0.3771
2024-06-02 03:54:36 [INFO]: Epoch 075 - training loss: 0.2814, validation loss: 0.3781
2024-06-02 03:54:37 [INFO]: Epoch 076 - training loss: 0.2834, validation loss: 0.3789
2024-06-02 03:54:38 [INFO]: Epoch 077 - training loss: 0.2876, validation loss: 0.3781
2024-06-02 03:54:38 [INFO]: Epoch 078 - training loss: 0.2819, validation loss: 0.3770
2024-06-02 03:54:39 [INFO]: Epoch 079 - training loss: 0.2818, validation loss: 0.3803
2024-06-02 03:54:40 [INFO]: Epoch 080 - training loss: 0.2811, validation loss: 0.3773
2024-06-02 03:54:41 [INFO]: Epoch 081 - training loss: 0.2825, validation loss: 0.3774
2024-06-02 03:54:41 [INFO]: Epoch 082 - training loss: 0.2770, validation loss: 0.3763
2024-06-02 03:54:42 [INFO]: Epoch 083 - training loss: 0.2767, validation loss: 0.3733
2024-06-02 03:54:43 [INFO]: Epoch 084 - training loss: 0.2725, validation loss: 0.3760
2024-06-02 03:54:44 [INFO]: Epoch 085 - training loss: 0.2760, validation loss: 0.3756
2024-06-02 03:54:44 [INFO]: Epoch 086 - training loss: 0.2743, validation loss: 0.3752
2024-06-02 03:54:45 [INFO]: Epoch 087 - training loss: 0.2755, validation loss: 0.3773
2024-06-02 03:54:46 [INFO]: Epoch 088 - training loss: 0.2765, validation loss: 0.3730
2024-06-02 03:54:47 [INFO]: Epoch 089 - training loss: 0.2743, validation loss: 0.3735
2024-06-02 03:54:47 [INFO]: Epoch 090 - training loss: 0.2788, validation loss: 0.3771
2024-06-02 03:54:48 [INFO]: Epoch 091 - training loss: 0.2763, validation loss: 0.3740
2024-06-02 03:54:49 [INFO]: Epoch 092 - training loss: 0.2730, validation loss: 0.3716
2024-06-02 03:54:50 [INFO]: Epoch 093 - training loss: 0.2705, validation loss: 0.3699
2024-06-02 03:54:51 [INFO]: Epoch 094 - training loss: 0.2684, validation loss: 0.3695
2024-06-02 03:54:51 [INFO]: Epoch 095 - training loss: 0.2703, validation loss: 0.3705
2024-06-02 03:54:52 [INFO]: Epoch 096 - training loss: 0.2742, validation loss: 0.3695
2024-06-02 03:54:53 [INFO]: Epoch 097 - training loss: 0.2776, validation loss: 0.3715
2024-06-02 03:54:54 [INFO]: Epoch 098 - training loss: 0.2740, validation loss: 0.3731
2024-06-02 03:54:55 [INFO]: Epoch 099 - training loss: 0.2708, validation loss: 0.3705
2024-06-02 03:54:56 [INFO]: Epoch 100 - training loss: 0.2658, validation loss: 0.3718
2024-06-02 03:54:56 [INFO]: Finished training. The best model is from epoch#94.
2024-06-02 03:54:56 [INFO]: Saved the model to results_point_rate01/PeMS/Transformer_PeMS/round_2/20240602_T035337/Transformer.pypots
2024-06-02 03:54:56 [INFO]: Successfully saved to results_point_rate01/PeMS/Transformer_PeMS/round_2/imputation.pkl
2024-06-02 03:54:56 [INFO]: Round2 - Transformer on PeMS: MAE=0.2948, MSE=0.5561, MRE=0.3654
2024-06-02 03:54:56 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 03:54:56 [INFO]: Using the given device: cuda:0
2024-06-02 03:54:56 [INFO]: Model files will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_3/20240602_T035456
2024-06-02 03:54:56 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_3/20240602_T035456/tensorboard
2024-06-02 03:54:56 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=256
2024-06-02 03:54:56 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-02 03:54:56 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 23,135,326
2024-06-02 03:54:57 [INFO]: Epoch 001 - training loss: 0.8801, validation loss: 0.5613
2024-06-02 03:54:58 [INFO]: Epoch 002 - training loss: 0.5853, validation loss: 0.5065
2024-06-02 03:54:59 [INFO]: Epoch 003 - training loss: 0.5104, validation loss: 0.4913
2024-06-02 03:54:59 [INFO]: Epoch 004 - training loss: 0.4794, validation loss: 0.4840
2024-06-02 03:55:00 [INFO]: Epoch 005 - training loss: 0.4598, validation loss: 0.4762
2024-06-02 03:55:01 [INFO]: Epoch 006 - training loss: 0.4489, validation loss: 0.4689
2024-06-02 03:55:01 [INFO]: Epoch 007 - training loss: 0.4421, validation loss: 0.4622
2024-06-02 03:55:02 [INFO]: Epoch 008 - training loss: 0.4321, validation loss: 0.4627
2024-06-02 03:55:03 [INFO]: Epoch 009 - training loss: 0.4243, validation loss: 0.4572
2024-06-02 03:55:04 [INFO]: Epoch 010 - training loss: 0.4102, validation loss: 0.4528
2024-06-02 03:55:04 [INFO]: Epoch 011 - training loss: 0.4062, validation loss: 0.4511
2024-06-02 03:55:05 [INFO]: Epoch 012 - training loss: 0.4031, validation loss: 0.4451
2024-06-02 03:55:06 [INFO]: Epoch 013 - training loss: 0.3940, validation loss: 0.4452
2024-06-02 03:55:07 [INFO]: Epoch 014 - training loss: 0.3849, validation loss: 0.4423
2024-06-02 03:55:07 [INFO]: Epoch 015 - training loss: 0.3823, validation loss: 0.4410
2024-06-02 03:55:08 [INFO]: Epoch 016 - training loss: 0.3758, validation loss: 0.4374
2024-06-02 03:55:09 [INFO]: Epoch 017 - training loss: 0.3762, validation loss: 0.4321
2024-06-02 03:55:10 [INFO]: Epoch 018 - training loss: 0.3690, validation loss: 0.4332
2024-06-02 03:55:10 [INFO]: Epoch 019 - training loss: 0.3646, validation loss: 0.4344
2024-06-02 03:55:11 [INFO]: Epoch 020 - training loss: 0.3621, validation loss: 0.4282
2024-06-02 03:55:12 [INFO]: Epoch 021 - training loss: 0.3609, validation loss: 0.4299
2024-06-02 03:55:13 [INFO]: Epoch 022 - training loss: 0.3634, validation loss: 0.4300
2024-06-02 03:55:14 [INFO]: Epoch 023 - training loss: 0.3585, validation loss: 0.4273
2024-06-02 03:55:15 [INFO]: Epoch 024 - training loss: 0.3543, validation loss: 0.4204
2024-06-02 03:55:15 [INFO]: Epoch 025 - training loss: 0.3505, validation loss: 0.4201
2024-06-02 03:55:16 [INFO]: Epoch 026 - training loss: 0.3457, validation loss: 0.4198
2024-06-02 03:55:17 [INFO]: Epoch 027 - training loss: 0.3419, validation loss: 0.4170
2024-06-02 03:55:18 [INFO]: Epoch 028 - training loss: 0.3382, validation loss: 0.4178
2024-06-02 03:55:19 [INFO]: Epoch 029 - training loss: 0.3386, validation loss: 0.4153
2024-06-02 03:55:19 [INFO]: Epoch 030 - training loss: 0.3398, validation loss: 0.4149
2024-06-02 03:55:20 [INFO]: Epoch 031 - training loss: 0.3415, validation loss: 0.4106
2024-06-02 03:55:21 [INFO]: Epoch 032 - training loss: 0.3352, validation loss: 0.4118
2024-06-02 03:55:22 [INFO]: Epoch 033 - training loss: 0.3320, validation loss: 0.4080
2024-06-02 03:55:22 [INFO]: Epoch 034 - training loss: 0.3307, validation loss: 0.4048
2024-06-02 03:55:23 [INFO]: Epoch 035 - training loss: 0.3305, validation loss: 0.4041
2024-06-02 03:55:24 [INFO]: Epoch 036 - training loss: 0.3296, validation loss: 0.4041
2024-06-02 03:55:25 [INFO]: Epoch 037 - training loss: 0.3275, validation loss: 0.4046
2024-06-02 03:55:26 [INFO]: Epoch 038 - training loss: 0.3279, validation loss: 0.4037
2024-06-02 03:55:26 [INFO]: Epoch 039 - training loss: 0.3255, validation loss: 0.4001
2024-06-02 03:55:27 [INFO]: Epoch 040 - training loss: 0.3235, validation loss: 0.4005
2024-06-02 03:55:28 [INFO]: Epoch 041 - training loss: 0.3208, validation loss: 0.3989
2024-06-02 03:55:29 [INFO]: Epoch 042 - training loss: 0.3219, validation loss: 0.4001
2024-06-02 03:55:30 [INFO]: Epoch 043 - training loss: 0.3180, validation loss: 0.3976
2024-06-02 03:55:31 [INFO]: Epoch 044 - training loss: 0.3117, validation loss: 0.3960
2024-06-02 03:55:31 [INFO]: Epoch 045 - training loss: 0.3132, validation loss: 0.3951
2024-06-02 03:55:32 [INFO]: Epoch 046 - training loss: 0.3095, validation loss: 0.3984
2024-06-02 03:55:33 [INFO]: Epoch 047 - training loss: 0.3085, validation loss: 0.3938
2024-06-02 03:55:34 [INFO]: Epoch 048 - training loss: 0.3060, validation loss: 0.3943
2024-06-02 03:55:35 [INFO]: Epoch 049 - training loss: 0.3060, validation loss: 0.3923
2024-06-02 03:55:36 [INFO]: Epoch 050 - training loss: 0.3073, validation loss: 0.3947
2024-06-02 03:55:36 [INFO]: Epoch 051 - training loss: 0.3069, validation loss: 0.3925
2024-06-02 03:55:37 [INFO]: Epoch 052 - training loss: 0.3022, validation loss: 0.3921
2024-06-02 03:55:38 [INFO]: Epoch 053 - training loss: 0.3019, validation loss: 0.3918
2024-06-02 03:55:39 [INFO]: Epoch 054 - training loss: 0.3021, validation loss: 0.3890
2024-06-02 03:55:39 [INFO]: Epoch 055 - training loss: 0.2968, validation loss: 0.3891
2024-06-02 03:55:40 [INFO]: Epoch 056 - training loss: 0.3004, validation loss: 0.3877
2024-06-02 03:55:41 [INFO]: Epoch 057 - training loss: 0.3015, validation loss: 0.3874
2024-06-02 03:55:42 [INFO]: Epoch 058 - training loss: 0.2972, validation loss: 0.3899
2024-06-02 03:55:42 [INFO]: Epoch 059 - training loss: 0.2956, validation loss: 0.3869
2024-06-02 03:55:43 [INFO]: Epoch 060 - training loss: 0.2963, validation loss: 0.3885
2024-06-02 03:55:44 [INFO]: Epoch 061 - training loss: 0.2956, validation loss: 0.3867
2024-06-02 03:55:45 [INFO]: Epoch 062 - training loss: 0.2944, validation loss: 0.3846
2024-06-02 03:55:46 [INFO]: Epoch 063 - training loss: 0.2937, validation loss: 0.3836
2024-06-02 03:55:46 [INFO]: Epoch 064 - training loss: 0.2896, validation loss: 0.3830
2024-06-02 03:55:47 [INFO]: Epoch 065 - training loss: 0.2937, validation loss: 0.3809
2024-06-02 03:55:48 [INFO]: Epoch 066 - training loss: 0.2871, validation loss: 0.3840
2024-06-02 03:55:49 [INFO]: Epoch 067 - training loss: 0.2901, validation loss: 0.3795
2024-06-02 03:55:50 [INFO]: Epoch 068 - training loss: 0.2883, validation loss: 0.3837
2024-06-02 03:55:50 [INFO]: Epoch 069 - training loss: 0.2873, validation loss: 0.3816
2024-06-02 03:55:51 [INFO]: Epoch 070 - training loss: 0.2887, validation loss: 0.3798
2024-06-02 03:55:52 [INFO]: Epoch 071 - training loss: 0.2839, validation loss: 0.3792
2024-06-02 03:55:53 [INFO]: Epoch 072 - training loss: 0.2835, validation loss: 0.3807
2024-06-02 03:55:54 [INFO]: Epoch 073 - training loss: 0.2852, validation loss: 0.3785
2024-06-02 03:55:55 [INFO]: Epoch 074 - training loss: 0.2824, validation loss: 0.3786
2024-06-02 03:55:55 [INFO]: Epoch 075 - training loss: 0.2820, validation loss: 0.3791
2024-06-02 03:55:56 [INFO]: Epoch 076 - training loss: 0.2808, validation loss: 0.3811
2024-06-02 03:55:57 [INFO]: Epoch 077 - training loss: 0.2844, validation loss: 0.3796
2024-06-02 03:55:58 [INFO]: Epoch 078 - training loss: 0.2839, validation loss: 0.3766
2024-06-02 03:55:59 [INFO]: Epoch 079 - training loss: 0.2810, validation loss: 0.3780
2024-06-02 03:55:59 [INFO]: Epoch 080 - training loss: 0.2800, validation loss: 0.3759
2024-06-02 03:56:00 [INFO]: Epoch 081 - training loss: 0.2800, validation loss: 0.3777
2024-06-02 03:56:01 [INFO]: Epoch 082 - training loss: 0.2788, validation loss: 0.3749
2024-06-02 03:56:02 [INFO]: Epoch 083 - training loss: 0.2795, validation loss: 0.3767
2024-06-02 03:56:03 [INFO]: Epoch 084 - training loss: 0.2739, validation loss: 0.3773
2024-06-02 03:56:04 [INFO]: Epoch 085 - training loss: 0.2758, validation loss: 0.3740
2024-06-02 03:56:04 [INFO]: Epoch 086 - training loss: 0.2765, validation loss: 0.3745
2024-06-02 03:56:05 [INFO]: Epoch 087 - training loss: 0.2819, validation loss: 0.3742
2024-06-02 03:56:06 [INFO]: Epoch 088 - training loss: 0.2813, validation loss: 0.3726
2024-06-02 03:56:07 [INFO]: Epoch 089 - training loss: 0.2788, validation loss: 0.3748
2024-06-02 03:56:08 [INFO]: Epoch 090 - training loss: 0.2776, validation loss: 0.3728
2024-06-02 03:56:08 [INFO]: Epoch 091 - training loss: 0.2765, validation loss: 0.3711
2024-06-02 03:56:09 [INFO]: Epoch 092 - training loss: 0.2749, validation loss: 0.3698
2024-06-02 03:56:10 [INFO]: Epoch 093 - training loss: 0.2726, validation loss: 0.3702
2024-06-02 03:56:10 [INFO]: Epoch 094 - training loss: 0.2785, validation loss: 0.3697
2024-06-02 03:56:11 [INFO]: Epoch 095 - training loss: 0.2771, validation loss: 0.3717
2024-06-02 03:56:11 [INFO]: Epoch 096 - training loss: 0.2733, validation loss: 0.3699
2024-06-02 03:56:12 [INFO]: Epoch 097 - training loss: 0.2740, validation loss: 0.3730
2024-06-02 03:56:13 [INFO]: Epoch 098 - training loss: 0.2699, validation loss: 0.3695
2024-06-02 03:56:14 [INFO]: Epoch 099 - training loss: 0.2670, validation loss: 0.3683
2024-06-02 03:56:15 [INFO]: Epoch 100 - training loss: 0.2640, validation loss: 0.3694
2024-06-02 03:56:15 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 03:56:15 [INFO]: Saved the model to results_point_rate01/PeMS/Transformer_PeMS/round_3/20240602_T035456/Transformer.pypots
2024-06-02 03:56:15 [INFO]: Successfully saved to results_point_rate01/PeMS/Transformer_PeMS/round_3/imputation.pkl
2024-06-02 03:56:15 [INFO]: Round3 - Transformer on PeMS: MAE=0.2930, MSE=0.5543, MRE=0.3631
2024-06-02 03:56:15 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 03:56:15 [INFO]: Using the given device: cuda:0
2024-06-02 03:56:15 [INFO]: Model files will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_4/20240602_T035615
2024-06-02 03:56:15 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Transformer_PeMS/round_4/20240602_T035615/tensorboard
2024-06-02 03:56:15 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=256
2024-06-02 03:56:15 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-02 03:56:15 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 23,135,326
2024-06-02 03:56:16 [INFO]: Epoch 001 - training loss: 0.8954, validation loss: 0.5527
2024-06-02 03:56:17 [INFO]: Epoch 002 - training loss: 0.5673, validation loss: 0.5204
2024-06-02 03:56:18 [INFO]: Epoch 003 - training loss: 0.5130, validation loss: 0.4902
2024-06-02 03:56:18 [INFO]: Epoch 004 - training loss: 0.4812, validation loss: 0.4861
2024-06-02 03:56:19 [INFO]: Epoch 005 - training loss: 0.4699, validation loss: 0.4748
2024-06-02 03:56:20 [INFO]: Epoch 006 - training loss: 0.4559, validation loss: 0.4770
2024-06-02 03:56:21 [INFO]: Epoch 007 - training loss: 0.4355, validation loss: 0.4785
2024-06-02 03:56:21 [INFO]: Epoch 008 - training loss: 0.4276, validation loss: 0.4623
2024-06-02 03:56:22 [INFO]: Epoch 009 - training loss: 0.4182, validation loss: 0.4581
2024-06-02 03:56:23 [INFO]: Epoch 010 - training loss: 0.4128, validation loss: 0.4576
2024-06-02 03:56:24 [INFO]: Epoch 011 - training loss: 0.4066, validation loss: 0.4553
2024-06-02 03:56:25 [INFO]: Epoch 012 - training loss: 0.4019, validation loss: 0.4478
2024-06-02 03:56:25 [INFO]: Epoch 013 - training loss: 0.3920, validation loss: 0.4512
2024-06-02 03:56:26 [INFO]: Epoch 014 - training loss: 0.3929, validation loss: 0.4468
2024-06-02 03:56:27 [INFO]: Epoch 015 - training loss: 0.3861, validation loss: 0.4452
2024-06-02 03:56:28 [INFO]: Epoch 016 - training loss: 0.3817, validation loss: 0.4385
2024-06-02 03:56:29 [INFO]: Epoch 017 - training loss: 0.3758, validation loss: 0.4379
2024-06-02 03:56:29 [INFO]: Epoch 018 - training loss: 0.3709, validation loss: 0.4319
2024-06-02 03:56:30 [INFO]: Epoch 019 - training loss: 0.3662, validation loss: 0.4322
2024-06-02 03:56:31 [INFO]: Epoch 020 - training loss: 0.3631, validation loss: 0.4283
2024-06-02 03:56:32 [INFO]: Epoch 021 - training loss: 0.3615, validation loss: 0.4280
2024-06-02 03:56:32 [INFO]: Epoch 022 - training loss: 0.3604, validation loss: 0.4260
2024-06-02 03:56:33 [INFO]: Epoch 023 - training loss: 0.3535, validation loss: 0.4261
2024-06-02 03:56:34 [INFO]: Epoch 024 - training loss: 0.3532, validation loss: 0.4264
2024-06-02 03:56:35 [INFO]: Epoch 025 - training loss: 0.3532, validation loss: 0.4221
2024-06-02 03:56:35 [INFO]: Epoch 026 - training loss: 0.3469, validation loss: 0.4208
2024-06-02 03:56:36 [INFO]: Epoch 027 - training loss: 0.3452, validation loss: 0.4167
2024-06-02 03:56:37 [INFO]: Epoch 028 - training loss: 0.3371, validation loss: 0.4177
2024-06-02 03:56:38 [INFO]: Epoch 029 - training loss: 0.3375, validation loss: 0.4149
2024-06-02 03:56:39 [INFO]: Epoch 030 - training loss: 0.3422, validation loss: 0.4123
2024-06-02 03:56:40 [INFO]: Epoch 031 - training loss: 0.3357, validation loss: 0.4150
2024-06-02 03:56:40 [INFO]: Epoch 032 - training loss: 0.3324, validation loss: 0.4123
2024-06-02 03:56:41 [INFO]: Epoch 033 - training loss: 0.3319, validation loss: 0.4108
2024-06-02 03:56:42 [INFO]: Epoch 034 - training loss: 0.3293, validation loss: 0.4103
2024-06-02 03:56:43 [INFO]: Epoch 035 - training loss: 0.3280, validation loss: 0.4070
2024-06-02 03:56:43 [INFO]: Epoch 036 - training loss: 0.3267, validation loss: 0.4098
2024-06-02 03:56:44 [INFO]: Epoch 037 - training loss: 0.3242, validation loss: 0.4054
2024-06-02 03:56:45 [INFO]: Epoch 038 - training loss: 0.3199, validation loss: 0.4039
2024-06-02 03:56:46 [INFO]: Epoch 039 - training loss: 0.3188, validation loss: 0.4045
2024-06-02 03:56:46 [INFO]: Epoch 040 - training loss: 0.3204, validation loss: 0.4013
2024-06-02 03:56:47 [INFO]: Epoch 041 - training loss: 0.3173, validation loss: 0.4024
2024-06-02 03:56:48 [INFO]: Epoch 042 - training loss: 0.3189, validation loss: 0.4007
2024-06-02 03:56:49 [INFO]: Epoch 043 - training loss: 0.3221, validation loss: 0.4024
2024-06-02 03:56:49 [INFO]: Epoch 044 - training loss: 0.3200, validation loss: 0.4019
2024-06-02 03:56:50 [INFO]: Epoch 045 - training loss: 0.3155, validation loss: 0.3955
2024-06-02 03:56:51 [INFO]: Epoch 046 - training loss: 0.3120, validation loss: 0.3960
2024-06-02 03:56:52 [INFO]: Epoch 047 - training loss: 0.3124, validation loss: 0.3975
2024-06-02 03:56:53 [INFO]: Epoch 048 - training loss: 0.3096, validation loss: 0.3963
2024-06-02 03:56:54 [INFO]: Epoch 049 - training loss: 0.3095, validation loss: 0.3923
2024-06-02 03:56:54 [INFO]: Epoch 050 - training loss: 0.3054, validation loss: 0.3954
2024-06-02 03:56:55 [INFO]: Epoch 051 - training loss: 0.3033, validation loss: 0.3920
2024-06-02 03:56:56 [INFO]: Epoch 052 - training loss: 0.3030, validation loss: 0.3928
2024-06-02 03:56:57 [INFO]: Epoch 053 - training loss: 0.3023, validation loss: 0.3900
2024-06-02 03:56:58 [INFO]: Epoch 054 - training loss: 0.3009, validation loss: 0.3926
2024-06-02 03:56:59 [INFO]: Epoch 055 - training loss: 0.3018, validation loss: 0.3907
2024-06-02 03:56:59 [INFO]: Epoch 056 - training loss: 0.3010, validation loss: 0.3904
2024-06-02 03:57:00 [INFO]: Epoch 057 - training loss: 0.2973, validation loss: 0.3895
2024-06-02 03:57:01 [INFO]: Epoch 058 - training loss: 0.3005, validation loss: 0.3875
2024-06-02 03:57:02 [INFO]: Epoch 059 - training loss: 0.2974, validation loss: 0.3882
2024-06-02 03:57:03 [INFO]: Epoch 060 - training loss: 0.2964, validation loss: 0.3880
2024-06-02 03:57:03 [INFO]: Epoch 061 - training loss: 0.2933, validation loss: 0.3865
2024-06-02 03:57:04 [INFO]: Epoch 062 - training loss: 0.2938, validation loss: 0.3855
2024-06-02 03:57:05 [INFO]: Epoch 063 - training loss: 0.2945, validation loss: 0.3846
2024-06-02 03:57:06 [INFO]: Epoch 064 - training loss: 0.2980, validation loss: 0.3835
2024-06-02 03:57:06 [INFO]: Epoch 065 - training loss: 0.2945, validation loss: 0.3842
2024-06-02 03:57:07 [INFO]: Epoch 066 - training loss: 0.2913, validation loss: 0.3833
2024-06-02 03:57:08 [INFO]: Epoch 067 - training loss: 0.2908, validation loss: 0.3819
2024-06-02 03:57:09 [INFO]: Epoch 068 - training loss: 0.2896, validation loss: 0.3838
2024-06-02 03:57:10 [INFO]: Epoch 069 - training loss: 0.2881, validation loss: 0.3841
2024-06-02 03:57:10 [INFO]: Epoch 070 - training loss: 0.2855, validation loss: 0.3811
2024-06-02 03:57:11 [INFO]: Epoch 071 - training loss: 0.2845, validation loss: 0.3831
2024-06-02 03:57:12 [INFO]: Epoch 072 - training loss: 0.2877, validation loss: 0.3849
2024-06-02 03:57:13 [INFO]: Epoch 073 - training loss: 0.2838, validation loss: 0.3804
2024-06-02 03:57:14 [INFO]: Epoch 074 - training loss: 0.2827, validation loss: 0.3804
2024-06-02 03:57:15 [INFO]: Epoch 075 - training loss: 0.2807, validation loss: 0.3790
2024-06-02 03:57:15 [INFO]: Epoch 076 - training loss: 0.2827, validation loss: 0.3793
2024-06-02 03:57:16 [INFO]: Epoch 077 - training loss: 0.2823, validation loss: 0.3763
2024-06-02 03:57:17 [INFO]: Epoch 078 - training loss: 0.2801, validation loss: 0.3767
2024-06-02 03:57:17 [INFO]: Epoch 079 - training loss: 0.2779, validation loss: 0.3789
2024-06-02 03:57:18 [INFO]: Epoch 080 - training loss: 0.2790, validation loss: 0.3762
2024-06-02 03:57:19 [INFO]: Epoch 081 - training loss: 0.2803, validation loss: 0.3793
2024-06-02 03:57:20 [INFO]: Epoch 082 - training loss: 0.2768, validation loss: 0.3767
2024-06-02 03:57:21 [INFO]: Epoch 083 - training loss: 0.2775, validation loss: 0.3779
2024-06-02 03:57:21 [INFO]: Epoch 084 - training loss: 0.2771, validation loss: 0.3777
2024-06-02 03:57:22 [INFO]: Epoch 085 - training loss: 0.2750, validation loss: 0.3789
2024-06-02 03:57:23 [INFO]: Epoch 086 - training loss: 0.2855, validation loss: 0.3741
2024-06-02 03:57:24 [INFO]: Epoch 087 - training loss: 0.2804, validation loss: 0.3764
2024-06-02 03:57:24 [INFO]: Epoch 088 - training loss: 0.2753, validation loss: 0.3754
2024-06-02 03:57:25 [INFO]: Epoch 089 - training loss: 0.2756, validation loss: 0.3726
2024-06-02 03:57:26 [INFO]: Epoch 090 - training loss: 0.2723, validation loss: 0.3765
2024-06-02 03:57:27 [INFO]: Epoch 091 - training loss: 0.2740, validation loss: 0.3742
2024-06-02 03:57:27 [INFO]: Epoch 092 - training loss: 0.2804, validation loss: 0.3743
2024-06-02 03:57:28 [INFO]: Epoch 093 - training loss: 0.2751, validation loss: 0.3742
2024-06-02 03:57:29 [INFO]: Epoch 094 - training loss: 0.2725, validation loss: 0.3731
2024-06-02 03:57:30 [INFO]: Epoch 095 - training loss: 0.2693, validation loss: 0.3701
2024-06-02 03:57:31 [INFO]: Epoch 096 - training loss: 0.2729, validation loss: 0.3737
2024-06-02 03:57:32 [INFO]: Epoch 097 - training loss: 0.2742, validation loss: 0.3717
2024-06-02 03:57:32 [INFO]: Epoch 098 - training loss: 0.2738, validation loss: 0.3711
2024-06-02 03:57:33 [INFO]: Epoch 099 - training loss: 0.2699, validation loss: 0.3730
2024-06-02 03:57:34 [INFO]: Epoch 100 - training loss: 0.2672, validation loss: 0.3708
2024-06-02 03:57:34 [INFO]: Finished training. The best model is from epoch#95.
2024-06-02 03:57:34 [INFO]: Saved the model to results_point_rate01/PeMS/Transformer_PeMS/round_4/20240602_T035615/Transformer.pypots
2024-06-02 03:57:34 [INFO]: Successfully saved to results_point_rate01/PeMS/Transformer_PeMS/round_4/imputation.pkl
2024-06-02 03:57:34 [INFO]: Round4 - Transformer on PeMS: MAE=0.2935, MSE=0.5563, MRE=0.3639
2024-06-02 03:57:34 [INFO]: Done! Final results:
Averaged Transformer (n params: 23,135,326) on PeMS: MAE=0.2938 ± 0.0015244403069948828, MSE=0.5555 ± 0.001181854464691362, MRE=0.3642 ± 0.0018896956423093195, average inference time=0.05
