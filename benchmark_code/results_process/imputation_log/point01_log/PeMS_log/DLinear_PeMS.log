2024-06-02 01:21:15 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 01:21:15 [INFO]: Using the given device: cuda:0
2024-06-02 01:21:16 [INFO]: Model files will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_0/20240602_T012116
2024-06-02 01:21:16 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_0/20240602_T012116/tensorboard
2024-06-02 01:21:16 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-02 01:21:25 [INFO]: Epoch 001 - training loss: 3.8003, validation loss: 1.2370
2024-06-02 01:21:26 [INFO]: Epoch 002 - training loss: 1.0816, validation loss: 1.0360
2024-06-02 01:21:26 [INFO]: Epoch 003 - training loss: 0.7366, validation loss: 0.6844
2024-06-02 01:21:27 [INFO]: Epoch 004 - training loss: 0.5759, validation loss: 0.6390
2024-06-02 01:21:28 [INFO]: Epoch 005 - training loss: 0.5255, validation loss: 0.5891
2024-06-02 01:21:28 [INFO]: Epoch 006 - training loss: 0.4955, validation loss: 0.5639
2024-06-02 01:21:29 [INFO]: Epoch 007 - training loss: 0.4728, validation loss: 0.5430
2024-06-02 01:21:30 [INFO]: Epoch 008 - training loss: 0.4590, validation loss: 0.5396
2024-06-02 01:21:31 [INFO]: Epoch 009 - training loss: 0.4433, validation loss: 0.5169
2024-06-02 01:21:31 [INFO]: Epoch 010 - training loss: 0.4345, validation loss: 0.5064
2024-06-02 01:21:32 [INFO]: Epoch 011 - training loss: 0.4208, validation loss: 0.5052
2024-06-02 01:21:33 [INFO]: Epoch 012 - training loss: 0.4060, validation loss: 0.4904
2024-06-02 01:21:33 [INFO]: Epoch 013 - training loss: 0.3991, validation loss: 0.4638
2024-06-02 01:21:34 [INFO]: Epoch 014 - training loss: 0.3955, validation loss: 0.4721
2024-06-02 01:21:35 [INFO]: Epoch 015 - training loss: 0.3862, validation loss: 0.4736
2024-06-02 01:21:36 [INFO]: Epoch 016 - training loss: 0.3729, validation loss: 0.4547
2024-06-02 01:21:36 [INFO]: Epoch 017 - training loss: 0.3715, validation loss: 0.4402
2024-06-02 01:21:37 [INFO]: Epoch 018 - training loss: 0.3658, validation loss: 0.4564
2024-06-02 01:21:38 [INFO]: Epoch 019 - training loss: 0.3691, validation loss: 0.4295
2024-06-02 01:21:39 [INFO]: Epoch 020 - training loss: 0.3672, validation loss: 0.4341
2024-06-02 01:21:40 [INFO]: Epoch 021 - training loss: 0.3548, validation loss: 0.4370
2024-06-02 01:21:40 [INFO]: Epoch 022 - training loss: 0.3446, validation loss: 0.4234
2024-06-02 01:21:41 [INFO]: Epoch 023 - training loss: 0.3435, validation loss: 0.4142
2024-06-02 01:21:42 [INFO]: Epoch 024 - training loss: 0.3498, validation loss: 0.4174
2024-06-02 01:21:43 [INFO]: Epoch 025 - training loss: 0.3394, validation loss: 0.4200
2024-06-02 01:21:44 [INFO]: Epoch 026 - training loss: 0.3344, validation loss: 0.4223
2024-06-02 01:21:45 [INFO]: Epoch 027 - training loss: 0.3349, validation loss: 0.4038
2024-06-02 01:21:45 [INFO]: Epoch 028 - training loss: 0.3426, validation loss: 0.4040
2024-06-02 01:21:46 [INFO]: Epoch 029 - training loss: 0.3342, validation loss: 0.4060
2024-06-02 01:21:47 [INFO]: Epoch 030 - training loss: 0.3379, validation loss: 0.4116
2024-06-02 01:21:48 [INFO]: Epoch 031 - training loss: 0.3284, validation loss: 0.4162
2024-06-02 01:21:48 [INFO]: Epoch 032 - training loss: 0.3267, validation loss: 0.4062
2024-06-02 01:21:49 [INFO]: Epoch 033 - training loss: 0.3297, validation loss: 0.4083
2024-06-02 01:21:50 [INFO]: Epoch 034 - training loss: 0.3261, validation loss: 0.4012
2024-06-02 01:21:51 [INFO]: Epoch 035 - training loss: 0.3163, validation loss: 0.3934
2024-06-02 01:21:51 [INFO]: Epoch 036 - training loss: 0.3147, validation loss: 0.3892
2024-06-02 01:21:52 [INFO]: Epoch 037 - training loss: 0.3149, validation loss: 0.3909
2024-06-02 01:21:53 [INFO]: Epoch 038 - training loss: 0.3152, validation loss: 0.3841
2024-06-02 01:21:54 [INFO]: Epoch 039 - training loss: 0.3135, validation loss: 0.3893
2024-06-02 01:21:54 [INFO]: Epoch 040 - training loss: 0.3099, validation loss: 0.3898
2024-06-02 01:21:55 [INFO]: Epoch 041 - training loss: 0.3152, validation loss: 0.3783
2024-06-02 01:21:56 [INFO]: Epoch 042 - training loss: 0.3106, validation loss: 0.3816
2024-06-02 01:21:56 [INFO]: Epoch 043 - training loss: 0.3090, validation loss: 0.3719
2024-06-02 01:21:57 [INFO]: Epoch 044 - training loss: 0.3095, validation loss: 0.3757
2024-06-02 01:21:58 [INFO]: Epoch 045 - training loss: 0.3099, validation loss: 0.3708
2024-06-02 01:21:58 [INFO]: Epoch 046 - training loss: 0.3088, validation loss: 0.3779
2024-06-02 01:21:59 [INFO]: Epoch 047 - training loss: 0.3136, validation loss: 0.3820
2024-06-02 01:22:00 [INFO]: Epoch 048 - training loss: 0.3103, validation loss: 0.3764
2024-06-02 01:22:01 [INFO]: Epoch 049 - training loss: 0.3071, validation loss: 0.3733
2024-06-02 01:22:01 [INFO]: Epoch 050 - training loss: 0.3080, validation loss: 0.3792
2024-06-02 01:22:02 [INFO]: Epoch 051 - training loss: 0.3042, validation loss: 0.3732
2024-06-02 01:22:03 [INFO]: Epoch 052 - training loss: 0.3001, validation loss: 0.3671
2024-06-02 01:22:03 [INFO]: Epoch 053 - training loss: 0.2994, validation loss: 0.3713
2024-06-02 01:22:04 [INFO]: Epoch 054 - training loss: 0.2996, validation loss: 0.3830
2024-06-02 01:22:05 [INFO]: Epoch 055 - training loss: 0.3033, validation loss: 0.3763
2024-06-02 01:22:06 [INFO]: Epoch 056 - training loss: 0.3042, validation loss: 0.3651
2024-06-02 01:22:07 [INFO]: Epoch 057 - training loss: 0.3010, validation loss: 0.3604
2024-06-02 01:22:07 [INFO]: Epoch 058 - training loss: 0.2970, validation loss: 0.3635
2024-06-02 01:22:08 [INFO]: Epoch 059 - training loss: 0.2979, validation loss: 0.3562
2024-06-02 01:22:09 [INFO]: Epoch 060 - training loss: 0.3027, validation loss: 0.3889
2024-06-02 01:22:10 [INFO]: Epoch 061 - training loss: 0.3103, validation loss: 0.3617
2024-06-02 01:22:11 [INFO]: Epoch 062 - training loss: 0.3012, validation loss: 0.3637
2024-06-02 01:22:11 [INFO]: Epoch 063 - training loss: 0.3015, validation loss: 0.3627
2024-06-02 01:22:12 [INFO]: Epoch 064 - training loss: 0.2949, validation loss: 0.3639
2024-06-02 01:22:13 [INFO]: Epoch 065 - training loss: 0.2959, validation loss: 0.3624
2024-06-02 01:22:14 [INFO]: Epoch 066 - training loss: 0.2951, validation loss: 0.3574
2024-06-02 01:22:15 [INFO]: Epoch 067 - training loss: 0.2955, validation loss: 0.3604
2024-06-02 01:22:15 [INFO]: Epoch 068 - training loss: 0.2925, validation loss: 0.3590
2024-06-02 01:22:16 [INFO]: Epoch 069 - training loss: 0.2925, validation loss: 0.3582
2024-06-02 01:22:16 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:22:16 [INFO]: Finished training. The best model is from epoch#59.
2024-06-02 01:22:16 [INFO]: Saved the model to results_point_rate01/PeMS/DLinear_PeMS/round_0/20240602_T012116/DLinear.pypots
2024-06-02 01:22:16 [INFO]: Successfully saved to results_point_rate01/PeMS/DLinear_PeMS/round_0/imputation.pkl
2024-06-02 01:22:16 [INFO]: Round0 - DLinear on PeMS: MAE=0.3559, MSE=0.5346, MRE=0.4412
2024-06-02 01:22:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 01:22:16 [INFO]: Using the given device: cuda:0
2024-06-02 01:22:16 [INFO]: Model files will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_1/20240602_T012216
2024-06-02 01:22:16 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_1/20240602_T012216/tensorboard
2024-06-02 01:22:17 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-02 01:22:17 [INFO]: Epoch 001 - training loss: 3.6866, validation loss: 1.3333
2024-06-02 01:22:18 [INFO]: Epoch 002 - training loss: 1.0913, validation loss: 1.0000
2024-06-02 01:22:19 [INFO]: Epoch 003 - training loss: 0.7096, validation loss: 0.6488
2024-06-02 01:22:20 [INFO]: Epoch 004 - training loss: 0.5721, validation loss: 0.6253
2024-06-02 01:22:21 [INFO]: Epoch 005 - training loss: 0.5235, validation loss: 0.5676
2024-06-02 01:22:21 [INFO]: Epoch 006 - training loss: 0.4898, validation loss: 0.5489
2024-06-02 01:22:22 [INFO]: Epoch 007 - training loss: 0.4713, validation loss: 0.5367
2024-06-02 01:22:23 [INFO]: Epoch 008 - training loss: 0.4527, validation loss: 0.5102
2024-06-02 01:22:23 [INFO]: Epoch 009 - training loss: 0.4373, validation loss: 0.5100
2024-06-02 01:22:24 [INFO]: Epoch 010 - training loss: 0.4213, validation loss: 0.4789
2024-06-02 01:22:25 [INFO]: Epoch 011 - training loss: 0.4119, validation loss: 0.4725
2024-06-02 01:22:26 [INFO]: Epoch 012 - training loss: 0.4014, validation loss: 0.4671
2024-06-02 01:22:26 [INFO]: Epoch 013 - training loss: 0.3908, validation loss: 0.4714
2024-06-02 01:22:27 [INFO]: Epoch 014 - training loss: 0.3790, validation loss: 0.4652
2024-06-02 01:22:28 [INFO]: Epoch 015 - training loss: 0.3752, validation loss: 0.4483
2024-06-02 01:22:28 [INFO]: Epoch 016 - training loss: 0.3727, validation loss: 0.4673
2024-06-02 01:22:29 [INFO]: Epoch 017 - training loss: 0.3707, validation loss: 0.4444
2024-06-02 01:22:30 [INFO]: Epoch 018 - training loss: 0.3609, validation loss: 0.4456
2024-06-02 01:22:30 [INFO]: Epoch 019 - training loss: 0.3559, validation loss: 0.4364
2024-06-02 01:22:31 [INFO]: Epoch 020 - training loss: 0.3512, validation loss: 0.4362
2024-06-02 01:22:32 [INFO]: Epoch 021 - training loss: 0.3499, validation loss: 0.4274
2024-06-02 01:22:33 [INFO]: Epoch 022 - training loss: 0.3439, validation loss: 0.4176
2024-06-02 01:22:33 [INFO]: Epoch 023 - training loss: 0.3405, validation loss: 0.4242
2024-06-02 01:22:34 [INFO]: Epoch 024 - training loss: 0.3338, validation loss: 0.4228
2024-06-02 01:22:35 [INFO]: Epoch 025 - training loss: 0.3344, validation loss: 0.4236
2024-06-02 01:22:36 [INFO]: Epoch 026 - training loss: 0.3338, validation loss: 0.4215
2024-06-02 01:22:36 [INFO]: Epoch 027 - training loss: 0.3310, validation loss: 0.4060
2024-06-02 01:22:37 [INFO]: Epoch 028 - training loss: 0.3302, validation loss: 0.4241
2024-06-02 01:22:38 [INFO]: Epoch 029 - training loss: 0.3409, validation loss: 0.4321
2024-06-02 01:22:38 [INFO]: Epoch 030 - training loss: 0.3361, validation loss: 0.4053
2024-06-02 01:22:39 [INFO]: Epoch 031 - training loss: 0.3227, validation loss: 0.4046
2024-06-02 01:22:40 [INFO]: Epoch 032 - training loss: 0.3194, validation loss: 0.3966
2024-06-02 01:22:41 [INFO]: Epoch 033 - training loss: 0.3129, validation loss: 0.3885
2024-06-02 01:22:41 [INFO]: Epoch 034 - training loss: 0.3183, validation loss: 0.3823
2024-06-02 01:22:42 [INFO]: Epoch 035 - training loss: 0.3177, validation loss: 0.3844
2024-06-02 01:22:43 [INFO]: Epoch 036 - training loss: 0.3157, validation loss: 0.3917
2024-06-02 01:22:44 [INFO]: Epoch 037 - training loss: 0.3211, validation loss: 0.4104
2024-06-02 01:22:44 [INFO]: Epoch 038 - training loss: 0.3190, validation loss: 0.3818
2024-06-02 01:22:45 [INFO]: Epoch 039 - training loss: 0.3099, validation loss: 0.3810
2024-06-02 01:22:46 [INFO]: Epoch 040 - training loss: 0.3091, validation loss: 0.3802
2024-06-02 01:22:46 [INFO]: Epoch 041 - training loss: 0.3078, validation loss: 0.3778
2024-06-02 01:22:47 [INFO]: Epoch 042 - training loss: 0.3062, validation loss: 0.3816
2024-06-02 01:22:47 [INFO]: Epoch 043 - training loss: 0.3098, validation loss: 0.3765
2024-06-02 01:22:48 [INFO]: Epoch 044 - training loss: 0.3049, validation loss: 0.3755
2024-06-02 01:22:48 [INFO]: Epoch 045 - training loss: 0.3027, validation loss: 0.3827
2024-06-02 01:22:49 [INFO]: Epoch 046 - training loss: 0.3032, validation loss: 0.3812
2024-06-02 01:22:49 [INFO]: Epoch 047 - training loss: 0.3056, validation loss: 0.3681
2024-06-02 01:22:50 [INFO]: Epoch 048 - training loss: 0.3061, validation loss: 0.3684
2024-06-02 01:22:51 [INFO]: Epoch 049 - training loss: 0.3020, validation loss: 0.3718
2024-06-02 01:22:51 [INFO]: Epoch 050 - training loss: 0.3012, validation loss: 0.3659
2024-06-02 01:22:52 [INFO]: Epoch 051 - training loss: 0.3027, validation loss: 0.3652
2024-06-02 01:22:52 [INFO]: Epoch 052 - training loss: 0.3039, validation loss: 0.3600
2024-06-02 01:22:53 [INFO]: Epoch 053 - training loss: 0.3020, validation loss: 0.3620
2024-06-02 01:22:54 [INFO]: Epoch 054 - training loss: 0.3028, validation loss: 0.3609
2024-06-02 01:22:54 [INFO]: Epoch 055 - training loss: 0.2980, validation loss: 0.3624
2024-06-02 01:22:55 [INFO]: Epoch 056 - training loss: 0.2959, validation loss: 0.3668
2024-06-02 01:22:55 [INFO]: Epoch 057 - training loss: 0.2958, validation loss: 0.3670
2024-06-02 01:22:56 [INFO]: Epoch 058 - training loss: 0.2967, validation loss: 0.3592
2024-06-02 01:22:57 [INFO]: Epoch 059 - training loss: 0.2988, validation loss: 0.3632
2024-06-02 01:22:57 [INFO]: Epoch 060 - training loss: 0.3003, validation loss: 0.3585
2024-06-02 01:22:58 [INFO]: Epoch 061 - training loss: 0.2974, validation loss: 0.3615
2024-06-02 01:22:58 [INFO]: Epoch 062 - training loss: 0.2943, validation loss: 0.3653
2024-06-02 01:22:59 [INFO]: Epoch 063 - training loss: 0.2951, validation loss: 0.3565
2024-06-02 01:22:59 [INFO]: Epoch 064 - training loss: 0.2936, validation loss: 0.3580
2024-06-02 01:23:00 [INFO]: Epoch 065 - training loss: 0.2936, validation loss: 0.3700
2024-06-02 01:23:00 [INFO]: Epoch 066 - training loss: 0.2953, validation loss: 0.3678
2024-06-02 01:23:01 [INFO]: Epoch 067 - training loss: 0.2943, validation loss: 0.3608
2024-06-02 01:23:01 [INFO]: Epoch 068 - training loss: 0.2960, validation loss: 0.3616
2024-06-02 01:23:02 [INFO]: Epoch 069 - training loss: 0.2945, validation loss: 0.3615
2024-06-02 01:23:02 [INFO]: Epoch 070 - training loss: 0.2924, validation loss: 0.3590
2024-06-02 01:23:03 [INFO]: Epoch 071 - training loss: 0.2938, validation loss: 0.3626
2024-06-02 01:23:03 [INFO]: Epoch 072 - training loss: 0.2945, validation loss: 0.3622
2024-06-02 01:23:04 [INFO]: Epoch 073 - training loss: 0.2926, validation loss: 0.3588
2024-06-02 01:23:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:23:04 [INFO]: Finished training. The best model is from epoch#63.
2024-06-02 01:23:04 [INFO]: Saved the model to results_point_rate01/PeMS/DLinear_PeMS/round_1/20240602_T012216/DLinear.pypots
2024-06-02 01:23:04 [INFO]: Successfully saved to results_point_rate01/PeMS/DLinear_PeMS/round_1/imputation.pkl
2024-06-02 01:23:04 [INFO]: Round1 - DLinear on PeMS: MAE=0.3591, MSE=0.5317, MRE=0.4452
2024-06-02 01:23:04 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 01:23:04 [INFO]: Using the given device: cuda:0
2024-06-02 01:23:04 [INFO]: Model files will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_2/20240602_T012304
2024-06-02 01:23:04 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_2/20240602_T012304/tensorboard
2024-06-02 01:23:04 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-02 01:23:05 [INFO]: Epoch 001 - training loss: 3.6995, validation loss: 1.0926
2024-06-02 01:23:05 [INFO]: Epoch 002 - training loss: 1.0340, validation loss: 0.9612
2024-06-02 01:23:06 [INFO]: Epoch 003 - training loss: 0.7320, validation loss: 0.7217
2024-06-02 01:23:06 [INFO]: Epoch 004 - training loss: 0.5873, validation loss: 0.6236
2024-06-02 01:23:07 [INFO]: Epoch 005 - training loss: 0.5253, validation loss: 0.6006
2024-06-02 01:23:07 [INFO]: Epoch 006 - training loss: 0.4847, validation loss: 0.5428
2024-06-02 01:23:08 [INFO]: Epoch 007 - training loss: 0.4532, validation loss: 0.5167
2024-06-02 01:23:08 [INFO]: Epoch 008 - training loss: 0.4333, validation loss: 0.4947
2024-06-02 01:23:09 [INFO]: Epoch 009 - training loss: 0.4199, validation loss: 0.4831
2024-06-02 01:23:10 [INFO]: Epoch 010 - training loss: 0.4059, validation loss: 0.4754
2024-06-02 01:23:10 [INFO]: Epoch 011 - training loss: 0.3963, validation loss: 0.4719
2024-06-02 01:23:11 [INFO]: Epoch 012 - training loss: 0.3887, validation loss: 0.4700
2024-06-02 01:23:12 [INFO]: Epoch 013 - training loss: 0.3788, validation loss: 0.4565
2024-06-02 01:23:13 [INFO]: Epoch 014 - training loss: 0.3700, validation loss: 0.4451
2024-06-02 01:23:13 [INFO]: Epoch 015 - training loss: 0.3740, validation loss: 0.4414
2024-06-02 01:23:14 [INFO]: Epoch 016 - training loss: 0.3670, validation loss: 0.4501
2024-06-02 01:23:14 [INFO]: Epoch 017 - training loss: 0.3563, validation loss: 0.4351
2024-06-02 01:23:15 [INFO]: Epoch 018 - training loss: 0.3517, validation loss: 0.4399
2024-06-02 01:23:15 [INFO]: Epoch 019 - training loss: 0.3486, validation loss: 0.4243
2024-06-02 01:23:16 [INFO]: Epoch 020 - training loss: 0.3455, validation loss: 0.4321
2024-06-02 01:23:17 [INFO]: Epoch 021 - training loss: 0.3415, validation loss: 0.4184
2024-06-02 01:23:17 [INFO]: Epoch 022 - training loss: 0.3393, validation loss: 0.4180
2024-06-02 01:23:18 [INFO]: Epoch 023 - training loss: 0.3352, validation loss: 0.4152
2024-06-02 01:23:18 [INFO]: Epoch 024 - training loss: 0.3360, validation loss: 0.4087
2024-06-02 01:23:19 [INFO]: Epoch 025 - training loss: 0.3361, validation loss: 0.4056
2024-06-02 01:23:20 [INFO]: Epoch 026 - training loss: 0.3357, validation loss: 0.4034
2024-06-02 01:23:20 [INFO]: Epoch 027 - training loss: 0.3307, validation loss: 0.4079
2024-06-02 01:23:21 [INFO]: Epoch 028 - training loss: 0.3269, validation loss: 0.4133
2024-06-02 01:23:21 [INFO]: Epoch 029 - training loss: 0.3254, validation loss: 0.4023
2024-06-02 01:23:22 [INFO]: Epoch 030 - training loss: 0.3212, validation loss: 0.3961
2024-06-02 01:23:23 [INFO]: Epoch 031 - training loss: 0.3169, validation loss: 0.3990
2024-06-02 01:23:23 [INFO]: Epoch 032 - training loss: 0.3170, validation loss: 0.4000
2024-06-02 01:23:24 [INFO]: Epoch 033 - training loss: 0.3153, validation loss: 0.3919
2024-06-02 01:23:25 [INFO]: Epoch 034 - training loss: 0.3154, validation loss: 0.3935
2024-06-02 01:23:25 [INFO]: Epoch 035 - training loss: 0.3219, validation loss: 0.3939
2024-06-02 01:23:26 [INFO]: Epoch 036 - training loss: 0.3153, validation loss: 0.3943
2024-06-02 01:23:26 [INFO]: Epoch 037 - training loss: 0.3119, validation loss: 0.3855
2024-06-02 01:23:27 [INFO]: Epoch 038 - training loss: 0.3115, validation loss: 0.3882
2024-06-02 01:23:28 [INFO]: Epoch 039 - training loss: 0.3119, validation loss: 0.3847
2024-06-02 01:23:28 [INFO]: Epoch 040 - training loss: 0.3126, validation loss: 0.3789
2024-06-02 01:23:29 [INFO]: Epoch 041 - training loss: 0.3089, validation loss: 0.3926
2024-06-02 01:23:29 [INFO]: Epoch 042 - training loss: 0.3122, validation loss: 0.3799
2024-06-02 01:23:30 [INFO]: Epoch 043 - training loss: 0.3084, validation loss: 0.3797
2024-06-02 01:23:30 [INFO]: Epoch 044 - training loss: 0.3085, validation loss: 0.3736
2024-06-02 01:23:31 [INFO]: Epoch 045 - training loss: 0.3039, validation loss: 0.3715
2024-06-02 01:23:32 [INFO]: Epoch 046 - training loss: 0.3030, validation loss: 0.3767
2024-06-02 01:23:32 [INFO]: Epoch 047 - training loss: 0.3044, validation loss: 0.3711
2024-06-02 01:23:33 [INFO]: Epoch 048 - training loss: 0.3013, validation loss: 0.3774
2024-06-02 01:23:33 [INFO]: Epoch 049 - training loss: 0.3044, validation loss: 0.3642
2024-06-02 01:23:34 [INFO]: Epoch 050 - training loss: 0.3003, validation loss: 0.3761
2024-06-02 01:23:34 [INFO]: Epoch 051 - training loss: 0.3022, validation loss: 0.3593
2024-06-02 01:23:35 [INFO]: Epoch 052 - training loss: 0.3064, validation loss: 0.3621
2024-06-02 01:23:35 [INFO]: Epoch 053 - training loss: 0.3113, validation loss: 0.3547
2024-06-02 01:23:36 [INFO]: Epoch 054 - training loss: 0.3057, validation loss: 0.3710
2024-06-02 01:23:37 [INFO]: Epoch 055 - training loss: 0.3026, validation loss: 0.3627
2024-06-02 01:23:37 [INFO]: Epoch 056 - training loss: 0.2975, validation loss: 0.3619
2024-06-02 01:23:38 [INFO]: Epoch 057 - training loss: 0.2978, validation loss: 0.3628
2024-06-02 01:23:39 [INFO]: Epoch 058 - training loss: 0.2949, validation loss: 0.3568
2024-06-02 01:23:39 [INFO]: Epoch 059 - training loss: 0.2946, validation loss: 0.3586
2024-06-02 01:23:39 [INFO]: Epoch 060 - training loss: 0.2959, validation loss: 0.3608
2024-06-02 01:23:40 [INFO]: Epoch 061 - training loss: 0.2946, validation loss: 0.3526
2024-06-02 01:23:40 [INFO]: Epoch 062 - training loss: 0.2973, validation loss: 0.3550
2024-06-02 01:23:41 [INFO]: Epoch 063 - training loss: 0.2956, validation loss: 0.3546
2024-06-02 01:23:41 [INFO]: Epoch 064 - training loss: 0.2928, validation loss: 0.3611
2024-06-02 01:23:42 [INFO]: Epoch 065 - training loss: 0.2970, validation loss: 0.3673
2024-06-02 01:23:43 [INFO]: Epoch 066 - training loss: 0.2932, validation loss: 0.3586
2024-06-02 01:23:43 [INFO]: Epoch 067 - training loss: 0.2964, validation loss: 0.3592
2024-06-02 01:23:44 [INFO]: Epoch 068 - training loss: 0.2954, validation loss: 0.3554
2024-06-02 01:23:44 [INFO]: Epoch 069 - training loss: 0.2932, validation loss: 0.3673
2024-06-02 01:23:45 [INFO]: Epoch 070 - training loss: 0.2952, validation loss: 0.3626
2024-06-02 01:23:46 [INFO]: Epoch 071 - training loss: 0.2933, validation loss: 0.3441
2024-06-02 01:23:46 [INFO]: Epoch 072 - training loss: 0.2941, validation loss: 0.3542
2024-06-02 01:23:47 [INFO]: Epoch 073 - training loss: 0.2951, validation loss: 0.3600
2024-06-02 01:23:48 [INFO]: Epoch 074 - training loss: 0.2919, validation loss: 0.3558
2024-06-02 01:23:48 [INFO]: Epoch 075 - training loss: 0.2914, validation loss: 0.3471
2024-06-02 01:23:49 [INFO]: Epoch 076 - training loss: 0.2912, validation loss: 0.3541
2024-06-02 01:23:49 [INFO]: Epoch 077 - training loss: 0.2907, validation loss: 0.3474
2024-06-02 01:23:50 [INFO]: Epoch 078 - training loss: 0.2887, validation loss: 0.3472
2024-06-02 01:23:51 [INFO]: Epoch 079 - training loss: 0.2867, validation loss: 0.3502
2024-06-02 01:23:51 [INFO]: Epoch 080 - training loss: 0.2896, validation loss: 0.3449
2024-06-02 01:23:52 [INFO]: Epoch 081 - training loss: 0.2887, validation loss: 0.3497
2024-06-02 01:23:52 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:23:52 [INFO]: Finished training. The best model is from epoch#71.
2024-06-02 01:23:52 [INFO]: Saved the model to results_point_rate01/PeMS/DLinear_PeMS/round_2/20240602_T012304/DLinear.pypots
2024-06-02 01:23:52 [INFO]: Successfully saved to results_point_rate01/PeMS/DLinear_PeMS/round_2/imputation.pkl
2024-06-02 01:23:52 [INFO]: Round2 - DLinear on PeMS: MAE=0.3573, MSE=0.5232, MRE=0.4429
2024-06-02 01:23:52 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 01:23:52 [INFO]: Using the given device: cuda:0
2024-06-02 01:23:52 [INFO]: Model files will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_3/20240602_T012352
2024-06-02 01:23:52 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_3/20240602_T012352/tensorboard
2024-06-02 01:23:52 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-02 01:23:53 [INFO]: Epoch 001 - training loss: 3.4294, validation loss: 1.3458
2024-06-02 01:23:53 [INFO]: Epoch 002 - training loss: 1.0311, validation loss: 0.8827
2024-06-02 01:23:54 [INFO]: Epoch 003 - training loss: 0.7254, validation loss: 0.6672
2024-06-02 01:23:54 [INFO]: Epoch 004 - training loss: 0.5749, validation loss: 0.5945
2024-06-02 01:23:55 [INFO]: Epoch 005 - training loss: 0.5110, validation loss: 0.5789
2024-06-02 01:23:56 [INFO]: Epoch 006 - training loss: 0.4858, validation loss: 0.5513
2024-06-02 01:23:56 [INFO]: Epoch 007 - training loss: 0.4642, validation loss: 0.5317
2024-06-02 01:23:57 [INFO]: Epoch 008 - training loss: 0.4421, validation loss: 0.5166
2024-06-02 01:23:57 [INFO]: Epoch 009 - training loss: 0.4228, validation loss: 0.4989
2024-06-02 01:23:58 [INFO]: Epoch 010 - training loss: 0.4142, validation loss: 0.4974
2024-06-02 01:23:58 [INFO]: Epoch 011 - training loss: 0.4013, validation loss: 0.4815
2024-06-02 01:23:59 [INFO]: Epoch 012 - training loss: 0.3989, validation loss: 0.4748
2024-06-02 01:23:59 [INFO]: Epoch 013 - training loss: 0.3867, validation loss: 0.4698
2024-06-02 01:24:00 [INFO]: Epoch 014 - training loss: 0.3830, validation loss: 0.4659
2024-06-02 01:24:00 [INFO]: Epoch 015 - training loss: 0.3974, validation loss: 0.4885
2024-06-02 01:24:01 [INFO]: Epoch 016 - training loss: 0.3886, validation loss: 0.4598
2024-06-02 01:24:02 [INFO]: Epoch 017 - training loss: 0.3705, validation loss: 0.4503
2024-06-02 01:24:02 [INFO]: Epoch 018 - training loss: 0.3657, validation loss: 0.4462
2024-06-02 01:24:03 [INFO]: Epoch 019 - training loss: 0.3616, validation loss: 0.4481
2024-06-02 01:24:04 [INFO]: Epoch 020 - training loss: 0.3545, validation loss: 0.4401
2024-06-02 01:24:04 [INFO]: Epoch 021 - training loss: 0.3525, validation loss: 0.4565
2024-06-02 01:24:05 [INFO]: Epoch 022 - training loss: 0.3493, validation loss: 0.4331
2024-06-02 01:24:05 [INFO]: Epoch 023 - training loss: 0.3471, validation loss: 0.4335
2024-06-02 01:24:06 [INFO]: Epoch 024 - training loss: 0.3462, validation loss: 0.4494
2024-06-02 01:24:06 [INFO]: Epoch 025 - training loss: 0.3492, validation loss: 0.4325
2024-06-02 01:24:07 [INFO]: Epoch 026 - training loss: 0.3441, validation loss: 0.4322
2024-06-02 01:24:07 [INFO]: Epoch 027 - training loss: 0.3385, validation loss: 0.4266
2024-06-02 01:24:08 [INFO]: Epoch 028 - training loss: 0.3394, validation loss: 0.4214
2024-06-02 01:24:09 [INFO]: Epoch 029 - training loss: 0.3344, validation loss: 0.4197
2024-06-02 01:24:09 [INFO]: Epoch 030 - training loss: 0.3332, validation loss: 0.4190
2024-06-02 01:24:10 [INFO]: Epoch 031 - training loss: 0.3310, validation loss: 0.4328
2024-06-02 01:24:10 [INFO]: Epoch 032 - training loss: 0.3356, validation loss: 0.4226
2024-06-02 01:24:11 [INFO]: Epoch 033 - training loss: 0.3347, validation loss: 0.4151
2024-06-02 01:24:12 [INFO]: Epoch 034 - training loss: 0.3333, validation loss: 0.4036
2024-06-02 01:24:12 [INFO]: Epoch 035 - training loss: 0.3307, validation loss: 0.4146
2024-06-02 01:24:13 [INFO]: Epoch 036 - training loss: 0.3241, validation loss: 0.4065
2024-06-02 01:24:13 [INFO]: Epoch 037 - training loss: 0.3270, validation loss: 0.4096
2024-06-02 01:24:14 [INFO]: Epoch 038 - training loss: 0.3300, validation loss: 0.4025
2024-06-02 01:24:14 [INFO]: Epoch 039 - training loss: 0.3222, validation loss: 0.4132
2024-06-02 01:24:15 [INFO]: Epoch 040 - training loss: 0.3233, validation loss: 0.4070
2024-06-02 01:24:15 [INFO]: Epoch 041 - training loss: 0.3255, validation loss: 0.4230
2024-06-02 01:24:16 [INFO]: Epoch 042 - training loss: 0.3225, validation loss: 0.4095
2024-06-02 01:24:16 [INFO]: Epoch 043 - training loss: 0.3212, validation loss: 0.4235
2024-06-02 01:24:16 [INFO]: Epoch 044 - training loss: 0.3228, validation loss: 0.4030
2024-06-02 01:24:17 [INFO]: Epoch 045 - training loss: 0.3144, validation loss: 0.4024
2024-06-02 01:24:18 [INFO]: Epoch 046 - training loss: 0.3119, validation loss: 0.4029
2024-06-02 01:24:18 [INFO]: Epoch 047 - training loss: 0.3133, validation loss: 0.3939
2024-06-02 01:24:19 [INFO]: Epoch 048 - training loss: 0.3137, validation loss: 0.3980
2024-06-02 01:24:19 [INFO]: Epoch 049 - training loss: 0.3108, validation loss: 0.3989
2024-06-02 01:24:20 [INFO]: Epoch 050 - training loss: 0.3108, validation loss: 0.4005
2024-06-02 01:24:20 [INFO]: Epoch 051 - training loss: 0.3118, validation loss: 0.3979
2024-06-02 01:24:21 [INFO]: Epoch 052 - training loss: 0.3104, validation loss: 0.3960
2024-06-02 01:24:21 [INFO]: Epoch 053 - training loss: 0.3118, validation loss: 0.4017
2024-06-02 01:24:22 [INFO]: Epoch 054 - training loss: 0.3096, validation loss: 0.3948
2024-06-02 01:24:23 [INFO]: Epoch 055 - training loss: 0.3095, validation loss: 0.4011
2024-06-02 01:24:23 [INFO]: Epoch 056 - training loss: 0.3130, validation loss: 0.4113
2024-06-02 01:24:24 [INFO]: Epoch 057 - training loss: 0.3097, validation loss: 0.3931
2024-06-02 01:24:25 [INFO]: Epoch 058 - training loss: 0.3085, validation loss: 0.3855
2024-06-02 01:24:25 [INFO]: Epoch 059 - training loss: 0.3073, validation loss: 0.3906
2024-06-02 01:24:25 [INFO]: Epoch 060 - training loss: 0.3089, validation loss: 0.3835
2024-06-02 01:24:26 [INFO]: Epoch 061 - training loss: 0.3077, validation loss: 0.3858
2024-06-02 01:24:27 [INFO]: Epoch 062 - training loss: 0.3038, validation loss: 0.3905
2024-06-02 01:24:27 [INFO]: Epoch 063 - training loss: 0.3058, validation loss: 0.4019
2024-06-02 01:24:28 [INFO]: Epoch 064 - training loss: 0.3029, validation loss: 0.3921
2024-06-02 01:24:28 [INFO]: Epoch 065 - training loss: 0.3028, validation loss: 0.3841
2024-06-02 01:24:29 [INFO]: Epoch 066 - training loss: 0.3035, validation loss: 0.3863
2024-06-02 01:24:30 [INFO]: Epoch 067 - training loss: 0.3084, validation loss: 0.3752
2024-06-02 01:24:31 [INFO]: Epoch 068 - training loss: 0.3024, validation loss: 0.3849
2024-06-02 01:24:31 [INFO]: Epoch 069 - training loss: 0.3031, validation loss: 0.3845
2024-06-02 01:24:32 [INFO]: Epoch 070 - training loss: 0.3030, validation loss: 0.3876
2024-06-02 01:24:32 [INFO]: Epoch 071 - training loss: 0.2997, validation loss: 0.3782
2024-06-02 01:24:33 [INFO]: Epoch 072 - training loss: 0.2989, validation loss: 0.3787
2024-06-02 01:24:34 [INFO]: Epoch 073 - training loss: 0.2987, validation loss: 0.3816
2024-06-02 01:24:34 [INFO]: Epoch 074 - training loss: 0.3015, validation loss: 0.3811
2024-06-02 01:24:35 [INFO]: Epoch 075 - training loss: 0.2994, validation loss: 0.3862
2024-06-02 01:24:35 [INFO]: Epoch 076 - training loss: 0.3006, validation loss: 0.3848
2024-06-02 01:24:36 [INFO]: Epoch 077 - training loss: 0.3012, validation loss: 0.3816
2024-06-02 01:24:36 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 01:24:36 [INFO]: Finished training. The best model is from epoch#67.
2024-06-02 01:24:36 [INFO]: Saved the model to results_point_rate01/PeMS/DLinear_PeMS/round_3/20240602_T012352/DLinear.pypots
2024-06-02 01:24:36 [INFO]: Successfully saved to results_point_rate01/PeMS/DLinear_PeMS/round_3/imputation.pkl
2024-06-02 01:24:36 [INFO]: Round3 - DLinear on PeMS: MAE=0.3794, MSE=0.5744, MRE=0.4703
2024-06-02 01:24:36 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 01:24:36 [INFO]: Using the given device: cuda:0
2024-06-02 01:24:36 [INFO]: Model files will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_4/20240602_T012436
2024-06-02 01:24:36 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/DLinear_PeMS/round_4/20240602_T012436/tensorboard
2024-06-02 01:24:36 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-02 01:24:37 [INFO]: Epoch 001 - training loss: 3.7859, validation loss: 1.4739
2024-06-02 01:24:38 [INFO]: Epoch 002 - training loss: 1.0969, validation loss: 1.0032
2024-06-02 01:24:38 [INFO]: Epoch 003 - training loss: 0.7027, validation loss: 0.7510
2024-06-02 01:24:39 [INFO]: Epoch 004 - training loss: 0.5785, validation loss: 0.6963
2024-06-02 01:24:39 [INFO]: Epoch 005 - training loss: 0.5260, validation loss: 0.6370
2024-06-02 01:24:40 [INFO]: Epoch 006 - training loss: 0.4972, validation loss: 0.6053
2024-06-02 01:24:40 [INFO]: Epoch 007 - training loss: 0.4734, validation loss: 0.5668
2024-06-02 01:24:40 [INFO]: Epoch 008 - training loss: 0.4506, validation loss: 0.5337
2024-06-02 01:24:41 [INFO]: Epoch 009 - training loss: 0.4326, validation loss: 0.5029
2024-06-02 01:24:41 [INFO]: Epoch 010 - training loss: 0.4208, validation loss: 0.5076
2024-06-02 01:24:42 [INFO]: Epoch 011 - training loss: 0.4172, validation loss: 0.4859
2024-06-02 01:24:42 [INFO]: Epoch 012 - training loss: 0.4024, validation loss: 0.4739
2024-06-02 01:24:43 [INFO]: Epoch 013 - training loss: 0.3949, validation loss: 0.4757
2024-06-02 01:24:44 [INFO]: Epoch 014 - training loss: 0.3838, validation loss: 0.4686
2024-06-02 01:24:44 [INFO]: Epoch 015 - training loss: 0.3796, validation loss: 0.4522
2024-06-02 01:24:44 [INFO]: Epoch 016 - training loss: 0.3725, validation loss: 0.4671
2024-06-02 01:24:45 [INFO]: Epoch 017 - training loss: 0.3832, validation loss: 0.4430
2024-06-02 01:24:46 [INFO]: Epoch 018 - training loss: 0.3651, validation loss: 0.4373
2024-06-02 01:24:46 [INFO]: Epoch 019 - training loss: 0.3558, validation loss: 0.4341
2024-06-02 01:24:46 [INFO]: Epoch 020 - training loss: 0.3517, validation loss: 0.4254
2024-06-02 01:24:47 [INFO]: Epoch 021 - training loss: 0.3500, validation loss: 0.4264
2024-06-02 01:24:47 [INFO]: Epoch 022 - training loss: 0.3475, validation loss: 0.4321
2024-06-02 01:24:48 [INFO]: Epoch 023 - training loss: 0.3502, validation loss: 0.4194
2024-06-02 01:24:49 [INFO]: Epoch 024 - training loss: 0.3413, validation loss: 0.4163
2024-06-02 01:24:49 [INFO]: Epoch 025 - training loss: 0.3395, validation loss: 0.4234
2024-06-02 01:24:50 [INFO]: Epoch 026 - training loss: 0.3344, validation loss: 0.4030
2024-06-02 01:24:50 [INFO]: Epoch 027 - training loss: 0.3317, validation loss: 0.4209
2024-06-02 01:24:51 [INFO]: Epoch 028 - training loss: 0.3389, validation loss: 0.4289
2024-06-02 01:24:52 [INFO]: Epoch 029 - training loss: 0.3319, validation loss: 0.4022
2024-06-02 01:24:52 [INFO]: Epoch 030 - training loss: 0.3277, validation loss: 0.4018
2024-06-02 01:24:53 [INFO]: Epoch 031 - training loss: 0.3208, validation loss: 0.4041
2024-06-02 01:24:53 [INFO]: Epoch 032 - training loss: 0.3183, validation loss: 0.3998
2024-06-02 01:24:54 [INFO]: Epoch 033 - training loss: 0.3220, validation loss: 0.3973
2024-06-02 01:24:54 [INFO]: Epoch 034 - training loss: 0.3181, validation loss: 0.3983
2024-06-02 01:24:55 [INFO]: Epoch 035 - training loss: 0.3178, validation loss: 0.3897
2024-06-02 01:24:56 [INFO]: Epoch 036 - training loss: 0.3178, validation loss: 0.3909
2024-06-02 01:24:57 [INFO]: Epoch 037 - training loss: 0.3150, validation loss: 0.3849
2024-06-02 01:24:57 [INFO]: Epoch 038 - training loss: 0.3139, validation loss: 0.3836
2024-06-02 01:24:58 [INFO]: Epoch 039 - training loss: 0.3165, validation loss: 0.3878
2024-06-02 01:24:58 [INFO]: Epoch 040 - training loss: 0.3122, validation loss: 0.3816
2024-06-02 01:24:59 [INFO]: Epoch 041 - training loss: 0.3105, validation loss: 0.3835
2024-06-02 01:24:59 [INFO]: Epoch 042 - training loss: 0.3079, validation loss: 0.3816
2024-06-02 01:25:00 [INFO]: Epoch 043 - training loss: 0.3067, validation loss: 0.3782
2024-06-02 01:25:00 [INFO]: Epoch 044 - training loss: 0.3076, validation loss: 0.3816
2024-06-02 01:25:01 [INFO]: Epoch 045 - training loss: 0.3048, validation loss: 0.3745
2024-06-02 01:25:01 [INFO]: Epoch 046 - training loss: 0.3047, validation loss: 0.3685
2024-06-02 01:25:02 [INFO]: Epoch 047 - training loss: 0.3033, validation loss: 0.3717
2024-06-02 01:25:02 [INFO]: Epoch 048 - training loss: 0.3096, validation loss: 0.3875
2024-06-02 01:25:03 [INFO]: Epoch 049 - training loss: 0.3101, validation loss: 0.3739
2024-06-02 01:25:03 [INFO]: Epoch 050 - training loss: 0.3036, validation loss: 0.3714
2024-06-02 01:25:04 [INFO]: Epoch 051 - training loss: 0.3007, validation loss: 0.3697
2024-06-02 01:25:05 [INFO]: Epoch 052 - training loss: 0.3012, validation loss: 0.3682
2024-06-02 01:25:05 [INFO]: Epoch 053 - training loss: 0.3006, validation loss: 0.3703
2024-06-02 01:25:06 [INFO]: Epoch 054 - training loss: 0.3072, validation loss: 0.3640
2024-06-02 01:25:07 [INFO]: Epoch 055 - training loss: 0.3200, validation loss: 0.3682
2024-06-02 01:25:07 [INFO]: Epoch 056 - training loss: 0.3099, validation loss: 0.3730
2024-06-02 01:25:08 [INFO]: Epoch 057 - training loss: 0.3036, validation loss: 0.3627
2024-06-02 01:25:09 [INFO]: Epoch 058 - training loss: 0.3009, validation loss: 0.3618
2024-06-02 01:25:09 [INFO]: Epoch 059 - training loss: 0.2934, validation loss: 0.3637
2024-06-02 01:25:10 [INFO]: Epoch 060 - training loss: 0.2969, validation loss: 0.3640
2024-06-02 01:25:11 [INFO]: Epoch 061 - training loss: 0.2978, validation loss: 0.3613
2024-06-02 01:25:11 [INFO]: Epoch 062 - training loss: 0.2980, validation loss: 0.3585
2024-06-02 01:25:12 [INFO]: Epoch 063 - training loss: 0.2944, validation loss: 0.3643
2024-06-02 01:25:13 [INFO]: Epoch 064 - training loss: 0.2906, validation loss: 0.3601
2024-06-02 01:25:13 [INFO]: Epoch 065 - training loss: 0.2935, validation loss: 0.3550
2024-06-02 01:25:14 [INFO]: Epoch 066 - training loss: 0.2910, validation loss: 0.3532
2024-06-02 01:25:14 [INFO]: Epoch 067 - training loss: 0.2935, validation loss: 0.3556
2024-06-02 01:25:15 [INFO]: Epoch 068 - training loss: 0.2926, validation loss: 0.3566
2024-06-02 01:25:15 [INFO]: Epoch 069 - training loss: 0.2948, validation loss: 0.3507
2024-06-02 01:25:16 [INFO]: Epoch 070 - training loss: 0.2975, validation loss: 0.3510
2024-06-02 01:25:17 [INFO]: Epoch 071 - training loss: 0.2940, validation loss: 0.3496
2024-06-02 01:25:17 [INFO]: Epoch 072 - training loss: 0.2936, validation loss: 0.3677
2024-06-02 01:25:18 [INFO]: Epoch 073 - training loss: 0.2918, validation loss: 0.3607
2024-06-02 01:25:18 [INFO]: Epoch 074 - training loss: 0.2889, validation loss: 0.3565
2024-06-02 01:25:19 [INFO]: Epoch 075 - training loss: 0.2884, validation loss: 0.3508
2024-06-02 01:25:19 [INFO]: Epoch 076 - training loss: 0.2895, validation loss: 0.3513
2024-06-02 01:25:20 [INFO]: Epoch 077 - training loss: 0.2906, validation loss: 0.3523
2024-06-02 01:25:20 [INFO]: Epoch 078 - training loss: 0.2900, validation loss: 0.3519
2024-06-02 01:25:20 [INFO]: Epoch 079 - training loss: 0.2896, validation loss: 0.3556
2024-06-02 01:25:21 [INFO]: Epoch 080 - training loss: 0.2884, validation loss: 0.3525
2024-06-02 01:25:21 [INFO]: Epoch 081 - training loss: 0.2873, validation loss: 0.3493
2024-06-02 01:25:22 [INFO]: Epoch 082 - training loss: 0.2870, validation loss: 0.3487
2024-06-02 01:25:23 [INFO]: Epoch 083 - training loss: 0.2910, validation loss: 0.3468
2024-06-02 01:25:23 [INFO]: Epoch 084 - training loss: 0.2955, validation loss: 0.3495
2024-06-02 01:25:24 [INFO]: Epoch 085 - training loss: 0.2887, validation loss: 0.3474
2024-06-02 01:25:25 [INFO]: Epoch 086 - training loss: 0.2885, validation loss: 0.3451
2024-06-02 01:25:25 [INFO]: Epoch 087 - training loss: 0.2885, validation loss: 0.3507
2024-06-02 01:25:26 [INFO]: Epoch 088 - training loss: 0.2893, validation loss: 0.3473
2024-06-02 01:25:26 [INFO]: Epoch 089 - training loss: 0.2862, validation loss: 0.3519
2024-06-02 01:25:26 [INFO]: Epoch 090 - training loss: 0.2867, validation loss: 0.3572
2024-06-02 01:25:27 [INFO]: Epoch 091 - training loss: 0.2856, validation loss: 0.3492
2024-06-02 01:25:27 [INFO]: Epoch 092 - training loss: 0.2864, validation loss: 0.3434
2024-06-02 01:25:28 [INFO]: Epoch 093 - training loss: 0.2863, validation loss: 0.3509
2024-06-02 01:25:28 [INFO]: Epoch 094 - training loss: 0.2873, validation loss: 0.3414
2024-06-02 01:25:29 [INFO]: Epoch 095 - training loss: 0.2866, validation loss: 0.3394
2024-06-02 01:25:30 [INFO]: Epoch 096 - training loss: 0.2851, validation loss: 0.3411
2024-06-02 01:25:30 [INFO]: Epoch 097 - training loss: 0.2828, validation loss: 0.3431
2024-06-02 01:25:31 [INFO]: Epoch 098 - training loss: 0.2885, validation loss: 0.3436
2024-06-02 01:25:31 [INFO]: Epoch 099 - training loss: 0.2892, validation loss: 0.3476
2024-06-02 01:25:32 [INFO]: Epoch 100 - training loss: 0.2884, validation loss: 0.3479
2024-06-02 01:25:32 [INFO]: Finished training. The best model is from epoch#95.
2024-06-02 01:25:32 [INFO]: Saved the model to results_point_rate01/PeMS/DLinear_PeMS/round_4/20240602_T012436/DLinear.pypots
2024-06-02 01:25:32 [INFO]: Successfully saved to results_point_rate01/PeMS/DLinear_PeMS/round_4/imputation.pkl
2024-06-02 01:25:32 [INFO]: Round4 - DLinear on PeMS: MAE=0.3579, MSE=0.5221, MRE=0.4437
2024-06-02 01:25:32 [INFO]: Done! Final results:
Averaged DLinear (n params: 5,301,100) on PeMS: MAE=0.3620 ± 0.008800557656552544, MSE=0.5372 ± 0.019233693281901, MRE=0.4487 ± 0.01090916802525547, average inference time=0.05
