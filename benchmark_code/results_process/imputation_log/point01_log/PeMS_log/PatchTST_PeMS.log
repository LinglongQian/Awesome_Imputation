2024-06-02 01:50:20 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 01:50:20 [INFO]: Using the given device: cuda:0
2024-06-02 01:50:20 [INFO]: Model files will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_0/20240602_T015020
2024-06-02 01:50:20 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_0/20240602_T015020/tensorboard
2024-06-02 01:50:20 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-02 01:50:20 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-02 01:50:20 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-02 01:50:23 [INFO]: Epoch 001 - training loss: 1.0607, validation loss: 0.8021
2024-06-02 01:50:26 [INFO]: Epoch 002 - training loss: 0.6752, validation loss: 0.6602
2024-06-02 01:50:29 [INFO]: Epoch 003 - training loss: 0.5927, validation loss: 0.6059
2024-06-02 01:50:31 [INFO]: Epoch 004 - training loss: 0.5515, validation loss: 0.5698
2024-06-02 01:50:34 [INFO]: Epoch 005 - training loss: 0.5226, validation loss: 0.5425
2024-06-02 01:50:37 [INFO]: Epoch 006 - training loss: 0.5013, validation loss: 0.5316
2024-06-02 01:50:40 [INFO]: Epoch 007 - training loss: 0.4880, validation loss: 0.5176
2024-06-02 01:50:42 [INFO]: Epoch 008 - training loss: 0.4732, validation loss: 0.5137
2024-06-02 01:50:45 [INFO]: Epoch 009 - training loss: 0.4664, validation loss: 0.5064
2024-06-02 01:50:48 [INFO]: Epoch 010 - training loss: 0.4716, validation loss: 0.4943
2024-06-02 01:50:51 [INFO]: Epoch 011 - training loss: 0.4584, validation loss: 0.5048
2024-06-02 01:50:54 [INFO]: Epoch 012 - training loss: 0.4423, validation loss: 0.4902
2024-06-02 01:50:56 [INFO]: Epoch 013 - training loss: 0.4305, validation loss: 0.4883
2024-06-02 01:50:59 [INFO]: Epoch 014 - training loss: 0.4177, validation loss: 0.4783
2024-06-02 01:51:02 [INFO]: Epoch 015 - training loss: 0.4188, validation loss: 0.4727
2024-06-02 01:51:05 [INFO]: Epoch 016 - training loss: 0.4118, validation loss: 0.4719
2024-06-02 01:51:08 [INFO]: Epoch 017 - training loss: 0.4039, validation loss: 0.4742
2024-06-02 01:51:11 [INFO]: Epoch 018 - training loss: 0.4012, validation loss: 0.4673
2024-06-02 01:51:14 [INFO]: Epoch 019 - training loss: 0.3973, validation loss: 0.4594
2024-06-02 01:51:16 [INFO]: Epoch 020 - training loss: 0.3952, validation loss: 0.4590
2024-06-02 01:51:19 [INFO]: Epoch 021 - training loss: 0.3902, validation loss: 0.4589
2024-06-02 01:51:22 [INFO]: Epoch 022 - training loss: 0.3836, validation loss: 0.4515
2024-06-02 01:51:25 [INFO]: Epoch 023 - training loss: 0.3777, validation loss: 0.4475
2024-06-02 01:51:27 [INFO]: Epoch 024 - training loss: 0.3719, validation loss: 0.4450
2024-06-02 01:51:30 [INFO]: Epoch 025 - training loss: 0.3698, validation loss: 0.4407
2024-06-02 01:51:33 [INFO]: Epoch 026 - training loss: 0.3719, validation loss: 0.4371
2024-06-02 01:51:36 [INFO]: Epoch 027 - training loss: 0.3670, validation loss: 0.4373
2024-06-02 01:51:39 [INFO]: Epoch 028 - training loss: 0.3644, validation loss: 0.4361
2024-06-02 01:51:41 [INFO]: Epoch 029 - training loss: 0.3621, validation loss: 0.4337
2024-06-02 01:51:44 [INFO]: Epoch 030 - training loss: 0.3597, validation loss: 0.4308
2024-06-02 01:51:47 [INFO]: Epoch 031 - training loss: 0.3564, validation loss: 0.4296
2024-06-02 01:51:49 [INFO]: Epoch 032 - training loss: 0.3544, validation loss: 0.4274
2024-06-02 01:51:52 [INFO]: Epoch 033 - training loss: 0.3550, validation loss: 0.4243
2024-06-02 01:51:55 [INFO]: Epoch 034 - training loss: 0.3534, validation loss: 0.4265
2024-06-02 01:51:58 [INFO]: Epoch 035 - training loss: 0.3524, validation loss: 0.4293
2024-06-02 01:52:01 [INFO]: Epoch 036 - training loss: 0.3505, validation loss: 0.4233
2024-06-02 01:52:04 [INFO]: Epoch 037 - training loss: 0.3465, validation loss: 0.4213
2024-06-02 01:52:06 [INFO]: Epoch 038 - training loss: 0.3488, validation loss: 0.4185
2024-06-02 01:52:09 [INFO]: Epoch 039 - training loss: 0.3452, validation loss: 0.4191
2024-06-02 01:52:12 [INFO]: Epoch 040 - training loss: 0.3416, validation loss: 0.4176
2024-06-02 01:52:15 [INFO]: Epoch 041 - training loss: 0.3393, validation loss: 0.4137
2024-06-02 01:52:17 [INFO]: Epoch 042 - training loss: 0.3374, validation loss: 0.4150
2024-06-02 01:52:20 [INFO]: Epoch 043 - training loss: 0.3358, validation loss: 0.4132
2024-06-02 01:52:23 [INFO]: Epoch 044 - training loss: 0.3357, validation loss: 0.4105
2024-06-02 01:52:26 [INFO]: Epoch 045 - training loss: 0.3384, validation loss: 0.4129
2024-06-02 01:52:28 [INFO]: Epoch 046 - training loss: 0.3344, validation loss: 0.4111
2024-06-02 01:52:31 [INFO]: Epoch 047 - training loss: 0.3346, validation loss: 0.4097
2024-06-02 01:52:34 [INFO]: Epoch 048 - training loss: 0.3360, validation loss: 0.4091
2024-06-02 01:52:37 [INFO]: Epoch 049 - training loss: 0.3308, validation loss: 0.4092
2024-06-02 01:52:39 [INFO]: Epoch 050 - training loss: 0.3332, validation loss: 0.4095
2024-06-02 01:52:42 [INFO]: Epoch 051 - training loss: 0.3295, validation loss: 0.4055
2024-06-02 01:52:45 [INFO]: Epoch 052 - training loss: 0.3266, validation loss: 0.4046
2024-06-02 01:52:48 [INFO]: Epoch 053 - training loss: 0.3292, validation loss: 0.4066
2024-06-02 01:52:51 [INFO]: Epoch 054 - training loss: 0.3275, validation loss: 0.4083
2024-06-02 01:52:53 [INFO]: Epoch 055 - training loss: 0.3274, validation loss: 0.4017
2024-06-02 01:52:56 [INFO]: Epoch 056 - training loss: 0.3300, validation loss: 0.4013
2024-06-02 01:52:59 [INFO]: Epoch 057 - training loss: 0.3242, validation loss: 0.4017
2024-06-02 01:53:02 [INFO]: Epoch 058 - training loss: 0.3250, validation loss: 0.4022
2024-06-02 01:53:05 [INFO]: Epoch 059 - training loss: 0.3203, validation loss: 0.4016
2024-06-02 01:53:07 [INFO]: Epoch 060 - training loss: 0.3203, validation loss: 0.4018
2024-06-02 01:53:10 [INFO]: Epoch 061 - training loss: 0.3180, validation loss: 0.3984
2024-06-02 01:53:13 [INFO]: Epoch 062 - training loss: 0.3176, validation loss: 0.3977
2024-06-02 01:53:16 [INFO]: Epoch 063 - training loss: 0.3159, validation loss: 0.3993
2024-06-02 01:53:19 [INFO]: Epoch 064 - training loss: 0.3162, validation loss: 0.3974
2024-06-02 01:53:22 [INFO]: Epoch 065 - training loss: 0.3137, validation loss: 0.3968
2024-06-02 01:53:24 [INFO]: Epoch 066 - training loss: 0.3169, validation loss: 0.4008
2024-06-02 01:53:27 [INFO]: Epoch 067 - training loss: 0.3177, validation loss: 0.3989
2024-06-02 01:53:30 [INFO]: Epoch 068 - training loss: 0.3151, validation loss: 0.4003
2024-06-02 01:53:33 [INFO]: Epoch 069 - training loss: 0.3157, validation loss: 0.3949
2024-06-02 01:53:36 [INFO]: Epoch 070 - training loss: 0.3091, validation loss: 0.3961
2024-06-02 01:53:38 [INFO]: Epoch 071 - training loss: 0.3123, validation loss: 0.3948
2024-06-02 01:53:41 [INFO]: Epoch 072 - training loss: 0.3093, validation loss: 0.3917
2024-06-02 01:53:44 [INFO]: Epoch 073 - training loss: 0.3149, validation loss: 0.3937
2024-06-02 01:53:46 [INFO]: Epoch 074 - training loss: 0.3088, validation loss: 0.3942
2024-06-02 01:53:49 [INFO]: Epoch 075 - training loss: 0.3112, validation loss: 0.3902
2024-06-02 01:53:52 [INFO]: Epoch 076 - training loss: 0.3169, validation loss: 0.3909
2024-06-02 01:53:55 [INFO]: Epoch 077 - training loss: 0.3169, validation loss: 0.3926
2024-06-02 01:53:57 [INFO]: Epoch 078 - training loss: 0.3153, validation loss: 0.3896
2024-06-02 01:54:00 [INFO]: Epoch 079 - training loss: 0.3107, validation loss: 0.3933
2024-06-02 01:54:03 [INFO]: Epoch 080 - training loss: 0.3094, validation loss: 0.3880
2024-06-02 01:54:06 [INFO]: Epoch 081 - training loss: 0.3056, validation loss: 0.3884
2024-06-02 01:54:09 [INFO]: Epoch 082 - training loss: 0.3049, validation loss: 0.3930
2024-06-02 01:54:11 [INFO]: Epoch 083 - training loss: 0.3043, validation loss: 0.3895
2024-06-02 01:54:14 [INFO]: Epoch 084 - training loss: 0.3002, validation loss: 0.3900
2024-06-02 01:54:17 [INFO]: Epoch 085 - training loss: 0.3011, validation loss: 0.3883
2024-06-02 01:54:20 [INFO]: Epoch 086 - training loss: 0.3048, validation loss: 0.3858
2024-06-02 01:54:23 [INFO]: Epoch 087 - training loss: 0.3114, validation loss: 0.3908
2024-06-02 01:54:26 [INFO]: Epoch 088 - training loss: 0.3073, validation loss: 0.3886
2024-06-02 01:54:29 [INFO]: Epoch 089 - training loss: 0.3036, validation loss: 0.3882
2024-06-02 01:54:32 [INFO]: Epoch 090 - training loss: 0.3020, validation loss: 0.3858
2024-06-02 01:54:34 [INFO]: Epoch 091 - training loss: 0.3022, validation loss: 0.3854
2024-06-02 01:54:37 [INFO]: Epoch 092 - training loss: 0.3020, validation loss: 0.3885
2024-06-02 01:54:40 [INFO]: Epoch 093 - training loss: 0.3018, validation loss: 0.3849
2024-06-02 01:54:43 [INFO]: Epoch 094 - training loss: 0.3036, validation loss: 0.3887
2024-06-02 01:54:46 [INFO]: Epoch 095 - training loss: 0.3041, validation loss: 0.3858
2024-06-02 01:54:48 [INFO]: Epoch 096 - training loss: 0.2994, validation loss: 0.3867
2024-06-02 01:54:51 [INFO]: Epoch 097 - training loss: 0.2973, validation loss: 0.3887
2024-06-02 01:54:54 [INFO]: Epoch 098 - training loss: 0.3005, validation loss: 0.3836
2024-06-02 01:54:56 [INFO]: Epoch 099 - training loss: 0.2971, validation loss: 0.3839
2024-06-02 01:54:59 [INFO]: Epoch 100 - training loss: 0.2970, validation loss: 0.3822
2024-06-02 01:54:59 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 01:54:59 [INFO]: Saved the model to results_point_rate01/PeMS/PatchTST_PeMS/round_0/20240602_T015020/PatchTST.pypots
2024-06-02 01:54:59 [INFO]: Successfully saved to results_point_rate01/PeMS/PatchTST_PeMS/round_0/imputation.pkl
2024-06-02 01:54:59 [INFO]: Round0 - PatchTST on PeMS: MAE=0.3317, MSE=0.5813, MRE=0.4111
2024-06-02 01:54:59 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 01:54:59 [INFO]: Using the given device: cuda:0
2024-06-02 01:54:59 [INFO]: Model files will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_1/20240602_T015459
2024-06-02 01:54:59 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_1/20240602_T015459/tensorboard
2024-06-02 01:54:59 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-02 01:54:59 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-02 01:54:59 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-02 01:55:02 [INFO]: Epoch 001 - training loss: 1.0897, validation loss: 0.8197
2024-06-02 01:55:05 [INFO]: Epoch 002 - training loss: 0.7215, validation loss: 0.7052
2024-06-02 01:55:08 [INFO]: Epoch 003 - training loss: 0.6195, validation loss: 0.6643
2024-06-02 01:55:11 [INFO]: Epoch 004 - training loss: 0.5643, validation loss: 0.6181
2024-06-02 01:55:14 [INFO]: Epoch 005 - training loss: 0.5342, validation loss: 0.5839
2024-06-02 01:55:17 [INFO]: Epoch 006 - training loss: 0.5125, validation loss: 0.5530
2024-06-02 01:55:20 [INFO]: Epoch 007 - training loss: 0.5028, validation loss: 0.5493
2024-06-02 01:55:22 [INFO]: Epoch 008 - training loss: 0.4863, validation loss: 0.5374
2024-06-02 01:55:25 [INFO]: Epoch 009 - training loss: 0.4744, validation loss: 0.5334
2024-06-02 01:55:28 [INFO]: Epoch 010 - training loss: 0.4635, validation loss: 0.5366
2024-06-02 01:55:30 [INFO]: Epoch 011 - training loss: 0.4507, validation loss: 0.5221
2024-06-02 01:55:33 [INFO]: Epoch 012 - training loss: 0.4489, validation loss: 0.5284
2024-06-02 01:55:36 [INFO]: Epoch 013 - training loss: 0.4430, validation loss: 0.5223
2024-06-02 01:55:38 [INFO]: Epoch 014 - training loss: 0.4371, validation loss: 0.5116
2024-06-02 01:55:41 [INFO]: Epoch 015 - training loss: 0.4330, validation loss: 0.5140
2024-06-02 01:55:44 [INFO]: Epoch 016 - training loss: 0.4254, validation loss: 0.5164
2024-06-02 01:55:47 [INFO]: Epoch 017 - training loss: 0.4177, validation loss: 0.5119
2024-06-02 01:55:49 [INFO]: Epoch 018 - training loss: 0.4161, validation loss: 0.5125
2024-06-02 01:55:52 [INFO]: Epoch 019 - training loss: 0.4189, validation loss: 0.5096
2024-06-02 01:55:55 [INFO]: Epoch 020 - training loss: 0.4136, validation loss: 0.5083
2024-06-02 01:55:58 [INFO]: Epoch 021 - training loss: 0.3999, validation loss: 0.5028
2024-06-02 01:56:00 [INFO]: Epoch 022 - training loss: 0.4005, validation loss: 0.5094
2024-06-02 01:56:03 [INFO]: Epoch 023 - training loss: 0.4061, validation loss: 0.5170
2024-06-02 01:56:06 [INFO]: Epoch 024 - training loss: 0.4055, validation loss: 0.5010
2024-06-02 01:56:09 [INFO]: Epoch 025 - training loss: 0.3930, validation loss: 0.5000
2024-06-02 01:56:12 [INFO]: Epoch 026 - training loss: 0.3906, validation loss: 0.5018
2024-06-02 01:56:14 [INFO]: Epoch 027 - training loss: 0.3812, validation loss: 0.4922
2024-06-02 01:56:17 [INFO]: Epoch 028 - training loss: 0.3789, validation loss: 0.4913
2024-06-02 01:56:20 [INFO]: Epoch 029 - training loss: 0.3761, validation loss: 0.4881
2024-06-02 01:56:23 [INFO]: Epoch 030 - training loss: 0.3775, validation loss: 0.4838
2024-06-02 01:56:26 [INFO]: Epoch 031 - training loss: 0.3782, validation loss: 0.4881
2024-06-02 01:56:28 [INFO]: Epoch 032 - training loss: 0.3694, validation loss: 0.4831
2024-06-02 01:56:31 [INFO]: Epoch 033 - training loss: 0.3663, validation loss: 0.4848
2024-06-02 01:56:34 [INFO]: Epoch 034 - training loss: 0.3722, validation loss: 0.4792
2024-06-02 01:56:37 [INFO]: Epoch 035 - training loss: 0.3661, validation loss: 0.4775
2024-06-02 01:56:40 [INFO]: Epoch 036 - training loss: 0.3678, validation loss: 0.4727
2024-06-02 01:56:42 [INFO]: Epoch 037 - training loss: 0.3617, validation loss: 0.4764
2024-06-02 01:56:45 [INFO]: Epoch 038 - training loss: 0.3614, validation loss: 0.4760
2024-06-02 01:56:48 [INFO]: Epoch 039 - training loss: 0.3598, validation loss: 0.4702
2024-06-02 01:56:51 [INFO]: Epoch 040 - training loss: 0.3525, validation loss: 0.4650
2024-06-02 01:56:54 [INFO]: Epoch 041 - training loss: 0.3563, validation loss: 0.4706
2024-06-02 01:56:56 [INFO]: Epoch 042 - training loss: 0.3536, validation loss: 0.4709
2024-06-02 01:56:59 [INFO]: Epoch 043 - training loss: 0.3536, validation loss: 0.4688
2024-06-02 01:57:02 [INFO]: Epoch 044 - training loss: 0.3560, validation loss: 0.4681
2024-06-02 01:57:04 [INFO]: Epoch 045 - training loss: 0.3524, validation loss: 0.4694
2024-06-02 01:57:07 [INFO]: Epoch 046 - training loss: 0.3562, validation loss: 0.4640
2024-06-02 01:57:10 [INFO]: Epoch 047 - training loss: 0.3513, validation loss: 0.4621
2024-06-02 01:57:13 [INFO]: Epoch 048 - training loss: 0.3455, validation loss: 0.4604
2024-06-02 01:57:15 [INFO]: Epoch 049 - training loss: 0.3466, validation loss: 0.4638
2024-06-02 01:57:18 [INFO]: Epoch 050 - training loss: 0.3446, validation loss: 0.4611
2024-06-02 01:57:21 [INFO]: Epoch 051 - training loss: 0.3387, validation loss: 0.4592
2024-06-02 01:57:23 [INFO]: Epoch 052 - training loss: 0.3386, validation loss: 0.4653
2024-06-02 01:57:26 [INFO]: Epoch 053 - training loss: 0.3391, validation loss: 0.4552
2024-06-02 01:57:29 [INFO]: Epoch 054 - training loss: 0.3366, validation loss: 0.4580
2024-06-02 01:57:32 [INFO]: Epoch 055 - training loss: 0.3355, validation loss: 0.4574
2024-06-02 01:57:34 [INFO]: Epoch 056 - training loss: 0.3344, validation loss: 0.4543
2024-06-02 01:57:37 [INFO]: Epoch 057 - training loss: 0.3356, validation loss: 0.4480
2024-06-02 01:57:40 [INFO]: Epoch 058 - training loss: 0.3317, validation loss: 0.4537
2024-06-02 01:57:43 [INFO]: Epoch 059 - training loss: 0.3335, validation loss: 0.4585
2024-06-02 01:57:46 [INFO]: Epoch 060 - training loss: 0.3320, validation loss: 0.4520
2024-06-02 01:57:48 [INFO]: Epoch 061 - training loss: 0.3289, validation loss: 0.4529
2024-06-02 01:57:51 [INFO]: Epoch 062 - training loss: 0.3295, validation loss: 0.4502
2024-06-02 01:57:54 [INFO]: Epoch 063 - training loss: 0.3315, validation loss: 0.4493
2024-06-02 01:57:57 [INFO]: Epoch 064 - training loss: 0.3278, validation loss: 0.4508
2024-06-02 01:57:59 [INFO]: Epoch 065 - training loss: 0.3284, validation loss: 0.4487
2024-06-02 01:58:02 [INFO]: Epoch 066 - training loss: 0.3253, validation loss: 0.4458
2024-06-02 01:58:05 [INFO]: Epoch 067 - training loss: 0.3220, validation loss: 0.4448
2024-06-02 01:58:07 [INFO]: Epoch 068 - training loss: 0.3262, validation loss: 0.4522
2024-06-02 01:58:10 [INFO]: Epoch 069 - training loss: 0.3286, validation loss: 0.4472
2024-06-02 01:58:13 [INFO]: Epoch 070 - training loss: 0.3192, validation loss: 0.4408
2024-06-02 01:58:16 [INFO]: Epoch 071 - training loss: 0.3223, validation loss: 0.4441
2024-06-02 01:58:18 [INFO]: Epoch 072 - training loss: 0.3205, validation loss: 0.4451
2024-06-02 01:58:21 [INFO]: Epoch 073 - training loss: 0.3186, validation loss: 0.4439
2024-06-02 01:58:24 [INFO]: Epoch 074 - training loss: 0.3163, validation loss: 0.4403
2024-06-02 01:58:27 [INFO]: Epoch 075 - training loss: 0.3146, validation loss: 0.4402
2024-06-02 01:58:29 [INFO]: Epoch 076 - training loss: 0.3201, validation loss: 0.4406
2024-06-02 01:58:32 [INFO]: Epoch 077 - training loss: 0.3126, validation loss: 0.4347
2024-06-02 01:58:35 [INFO]: Epoch 078 - training loss: 0.3212, validation loss: 0.4397
2024-06-02 01:58:38 [INFO]: Epoch 079 - training loss: 0.3126, validation loss: 0.4344
2024-06-02 01:58:41 [INFO]: Epoch 080 - training loss: 0.3132, validation loss: 0.4334
2024-06-02 01:58:44 [INFO]: Epoch 081 - training loss: 0.3143, validation loss: 0.4348
2024-06-02 01:58:46 [INFO]: Epoch 082 - training loss: 0.3121, validation loss: 0.4357
2024-06-02 01:58:49 [INFO]: Epoch 083 - training loss: 0.3122, validation loss: 0.4317
2024-06-02 01:58:52 [INFO]: Epoch 084 - training loss: 0.3133, validation loss: 0.4357
2024-06-02 01:58:55 [INFO]: Epoch 085 - training loss: 0.3138, validation loss: 0.4333
2024-06-02 01:58:58 [INFO]: Epoch 086 - training loss: 0.3085, validation loss: 0.4312
2024-06-02 01:59:00 [INFO]: Epoch 087 - training loss: 0.3094, validation loss: 0.4303
2024-06-02 01:59:03 [INFO]: Epoch 088 - training loss: 0.3076, validation loss: 0.4340
2024-06-02 01:59:06 [INFO]: Epoch 089 - training loss: 0.3123, validation loss: 0.4285
2024-06-02 01:59:09 [INFO]: Epoch 090 - training loss: 0.3090, validation loss: 0.4285
2024-06-02 01:59:11 [INFO]: Epoch 091 - training loss: 0.3066, validation loss: 0.4276
2024-06-02 01:59:14 [INFO]: Epoch 092 - training loss: 0.3075, validation loss: 0.4297
2024-06-02 01:59:17 [INFO]: Epoch 093 - training loss: 0.3100, validation loss: 0.4290
2024-06-02 01:59:19 [INFO]: Epoch 094 - training loss: 0.3107, validation loss: 0.4281
2024-06-02 01:59:22 [INFO]: Epoch 095 - training loss: 0.3104, validation loss: 0.4264
2024-06-02 01:59:25 [INFO]: Epoch 096 - training loss: 0.3084, validation loss: 0.4267
2024-06-02 01:59:28 [INFO]: Epoch 097 - training loss: 0.3047, validation loss: 0.4249
2024-06-02 01:59:31 [INFO]: Epoch 098 - training loss: 0.3058, validation loss: 0.4286
2024-06-02 01:59:34 [INFO]: Epoch 099 - training loss: 0.3034, validation loss: 0.4230
2024-06-02 01:59:36 [INFO]: Epoch 100 - training loss: 0.3015, validation loss: 0.4223
2024-06-02 01:59:36 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 01:59:36 [INFO]: Saved the model to results_point_rate01/PeMS/PatchTST_PeMS/round_1/20240602_T015459/PatchTST.pypots
2024-06-02 01:59:37 [INFO]: Successfully saved to results_point_rate01/PeMS/PatchTST_PeMS/round_1/imputation.pkl
2024-06-02 01:59:37 [INFO]: Round1 - PatchTST on PeMS: MAE=0.3536, MSE=0.6693, MRE=0.4383
2024-06-02 01:59:37 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 01:59:37 [INFO]: Using the given device: cuda:0
2024-06-02 01:59:37 [INFO]: Model files will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_2/20240602_T015937
2024-06-02 01:59:37 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_2/20240602_T015937/tensorboard
2024-06-02 01:59:37 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-02 01:59:37 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-02 01:59:37 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-02 01:59:40 [INFO]: Epoch 001 - training loss: 1.1852, validation loss: 0.8553
2024-06-02 01:59:43 [INFO]: Epoch 002 - training loss: 0.7480, validation loss: 0.7203
2024-06-02 01:59:45 [INFO]: Epoch 003 - training loss: 0.6180, validation loss: 0.6138
2024-06-02 01:59:48 [INFO]: Epoch 004 - training loss: 0.5640, validation loss: 0.5867
2024-06-02 01:59:51 [INFO]: Epoch 005 - training loss: 0.5223, validation loss: 0.5504
2024-06-02 01:59:54 [INFO]: Epoch 006 - training loss: 0.5045, validation loss: 0.5356
2024-06-02 01:59:57 [INFO]: Epoch 007 - training loss: 0.4863, validation loss: 0.5129
2024-06-02 02:00:00 [INFO]: Epoch 008 - training loss: 0.4689, validation loss: 0.5129
2024-06-02 02:00:03 [INFO]: Epoch 009 - training loss: 0.4519, validation loss: 0.5047
2024-06-02 02:00:05 [INFO]: Epoch 010 - training loss: 0.4415, validation loss: 0.5034
2024-06-02 02:00:08 [INFO]: Epoch 011 - training loss: 0.4351, validation loss: 0.4841
2024-06-02 02:00:11 [INFO]: Epoch 012 - training loss: 0.4379, validation loss: 0.4742
2024-06-02 02:00:14 [INFO]: Epoch 013 - training loss: 0.4268, validation loss: 0.4744
2024-06-02 02:00:16 [INFO]: Epoch 014 - training loss: 0.4126, validation loss: 0.4658
2024-06-02 02:00:19 [INFO]: Epoch 015 - training loss: 0.4019, validation loss: 0.4607
2024-06-02 02:00:22 [INFO]: Epoch 016 - training loss: 0.3986, validation loss: 0.4536
2024-06-02 02:00:25 [INFO]: Epoch 017 - training loss: 0.3923, validation loss: 0.4569
2024-06-02 02:00:27 [INFO]: Epoch 018 - training loss: 0.3907, validation loss: 0.4521
2024-06-02 02:00:30 [INFO]: Epoch 019 - training loss: 0.3849, validation loss: 0.4441
2024-06-02 02:00:33 [INFO]: Epoch 020 - training loss: 0.3846, validation loss: 0.4422
2024-06-02 02:00:35 [INFO]: Epoch 021 - training loss: 0.3824, validation loss: 0.4412
2024-06-02 02:00:38 [INFO]: Epoch 022 - training loss: 0.3754, validation loss: 0.4358
2024-06-02 02:00:41 [INFO]: Epoch 023 - training loss: 0.3694, validation loss: 0.4334
2024-06-02 02:00:44 [INFO]: Epoch 024 - training loss: 0.3708, validation loss: 0.4318
2024-06-02 02:00:47 [INFO]: Epoch 025 - training loss: 0.3618, validation loss: 0.4304
2024-06-02 02:00:49 [INFO]: Epoch 026 - training loss: 0.3664, validation loss: 0.4278
2024-06-02 02:00:52 [INFO]: Epoch 027 - training loss: 0.3668, validation loss: 0.4243
2024-06-02 02:00:55 [INFO]: Epoch 028 - training loss: 0.3680, validation loss: 0.4293
2024-06-02 02:00:57 [INFO]: Epoch 029 - training loss: 0.3638, validation loss: 0.4228
2024-06-02 02:01:00 [INFO]: Epoch 030 - training loss: 0.3571, validation loss: 0.4242
2024-06-02 02:01:03 [INFO]: Epoch 031 - training loss: 0.3542, validation loss: 0.4224
2024-06-02 02:01:06 [INFO]: Epoch 032 - training loss: 0.3507, validation loss: 0.4228
2024-06-02 02:01:09 [INFO]: Epoch 033 - training loss: 0.3537, validation loss: 0.4239
2024-06-02 02:01:11 [INFO]: Epoch 034 - training loss: 0.3520, validation loss: 0.4166
2024-06-02 02:01:14 [INFO]: Epoch 035 - training loss: 0.3445, validation loss: 0.4171
2024-06-02 02:01:17 [INFO]: Epoch 036 - training loss: 0.3459, validation loss: 0.4129
2024-06-02 02:01:20 [INFO]: Epoch 037 - training loss: 0.3448, validation loss: 0.4132
2024-06-02 02:01:22 [INFO]: Epoch 038 - training loss: 0.3400, validation loss: 0.4130
2024-06-02 02:01:25 [INFO]: Epoch 039 - training loss: 0.3402, validation loss: 0.4108
2024-06-02 02:01:28 [INFO]: Epoch 040 - training loss: 0.3400, validation loss: 0.4139
2024-06-02 02:01:30 [INFO]: Epoch 041 - training loss: 0.3364, validation loss: 0.4109
2024-06-02 02:01:33 [INFO]: Epoch 042 - training loss: 0.3339, validation loss: 0.4109
2024-06-02 02:01:36 [INFO]: Epoch 043 - training loss: 0.3306, validation loss: 0.4070
2024-06-02 02:01:39 [INFO]: Epoch 044 - training loss: 0.3353, validation loss: 0.4068
2024-06-02 02:01:41 [INFO]: Epoch 045 - training loss: 0.3379, validation loss: 0.4097
2024-06-02 02:01:44 [INFO]: Epoch 046 - training loss: 0.3319, validation loss: 0.4042
2024-06-02 02:01:47 [INFO]: Epoch 047 - training loss: 0.3279, validation loss: 0.4054
2024-06-02 02:01:50 [INFO]: Epoch 048 - training loss: 0.3296, validation loss: 0.4055
2024-06-02 02:01:53 [INFO]: Epoch 049 - training loss: 0.3253, validation loss: 0.4053
2024-06-02 02:01:56 [INFO]: Epoch 050 - training loss: 0.3259, validation loss: 0.4018
2024-06-02 02:01:58 [INFO]: Epoch 051 - training loss: 0.3255, validation loss: 0.4028
2024-06-02 02:02:01 [INFO]: Epoch 052 - training loss: 0.3220, validation loss: 0.4021
2024-06-02 02:02:04 [INFO]: Epoch 053 - training loss: 0.3239, validation loss: 0.3997
2024-06-02 02:02:06 [INFO]: Epoch 054 - training loss: 0.3231, validation loss: 0.3988
2024-06-02 02:02:09 [INFO]: Epoch 055 - training loss: 0.3201, validation loss: 0.4015
2024-06-02 02:02:12 [INFO]: Epoch 056 - training loss: 0.3182, validation loss: 0.4010
2024-06-02 02:02:15 [INFO]: Epoch 057 - training loss: 0.3203, validation loss: 0.3988
2024-06-02 02:02:17 [INFO]: Epoch 058 - training loss: 0.3176, validation loss: 0.3941
2024-06-02 02:02:20 [INFO]: Epoch 059 - training loss: 0.3180, validation loss: 0.3967
2024-06-02 02:02:23 [INFO]: Epoch 060 - training loss: 0.3179, validation loss: 0.4003
2024-06-02 02:02:26 [INFO]: Epoch 061 - training loss: 0.3165, validation loss: 0.3955
2024-06-02 02:02:28 [INFO]: Epoch 062 - training loss: 0.3131, validation loss: 0.3959
2024-06-02 02:02:31 [INFO]: Epoch 063 - training loss: 0.3110, validation loss: 0.3961
2024-06-02 02:02:34 [INFO]: Epoch 064 - training loss: 0.3175, validation loss: 0.3961
2024-06-02 02:02:37 [INFO]: Epoch 065 - training loss: 0.3185, validation loss: 0.3936
2024-06-02 02:02:39 [INFO]: Epoch 066 - training loss: 0.3188, validation loss: 0.3939
2024-06-02 02:02:42 [INFO]: Epoch 067 - training loss: 0.3133, validation loss: 0.3980
2024-06-02 02:02:45 [INFO]: Epoch 068 - training loss: 0.3108, validation loss: 0.3925
2024-06-02 02:02:47 [INFO]: Epoch 069 - training loss: 0.3080, validation loss: 0.3933
2024-06-02 02:02:50 [INFO]: Epoch 070 - training loss: 0.3057, validation loss: 0.3896
2024-06-02 02:02:53 [INFO]: Epoch 071 - training loss: 0.3085, validation loss: 0.3910
2024-06-02 02:02:56 [INFO]: Epoch 072 - training loss: 0.3057, validation loss: 0.3905
2024-06-02 02:02:59 [INFO]: Epoch 073 - training loss: 0.3078, validation loss: 0.3889
2024-06-02 02:03:01 [INFO]: Epoch 074 - training loss: 0.3044, validation loss: 0.3888
2024-06-02 02:03:04 [INFO]: Epoch 075 - training loss: 0.3041, validation loss: 0.3877
2024-06-02 02:03:07 [INFO]: Epoch 076 - training loss: 0.3021, validation loss: 0.3890
2024-06-02 02:03:10 [INFO]: Epoch 077 - training loss: 0.3051, validation loss: 0.3900
2024-06-02 02:03:13 [INFO]: Epoch 078 - training loss: 0.3083, validation loss: 0.3883
2024-06-02 02:03:16 [INFO]: Epoch 079 - training loss: 0.3010, validation loss: 0.3887
2024-06-02 02:03:19 [INFO]: Epoch 080 - training loss: 0.2986, validation loss: 0.3873
2024-06-02 02:03:22 [INFO]: Epoch 081 - training loss: 0.2954, validation loss: 0.3856
2024-06-02 02:03:24 [INFO]: Epoch 082 - training loss: 0.3002, validation loss: 0.3877
2024-06-02 02:03:27 [INFO]: Epoch 083 - training loss: 0.3002, validation loss: 0.3845
2024-06-02 02:03:30 [INFO]: Epoch 084 - training loss: 0.2972, validation loss: 0.3854
2024-06-02 02:03:33 [INFO]: Epoch 085 - training loss: 0.2952, validation loss: 0.3839
2024-06-02 02:03:36 [INFO]: Epoch 086 - training loss: 0.2958, validation loss: 0.3861
2024-06-02 02:03:38 [INFO]: Epoch 087 - training loss: 0.2991, validation loss: 0.3851
2024-06-02 02:03:41 [INFO]: Epoch 088 - training loss: 0.2991, validation loss: 0.3837
2024-06-02 02:03:44 [INFO]: Epoch 089 - training loss: 0.2960, validation loss: 0.3867
2024-06-02 02:03:46 [INFO]: Epoch 090 - training loss: 0.2969, validation loss: 0.3847
2024-06-02 02:03:49 [INFO]: Epoch 091 - training loss: 0.2952, validation loss: 0.3844
2024-06-02 02:03:52 [INFO]: Epoch 092 - training loss: 0.2933, validation loss: 0.3820
2024-06-02 02:03:55 [INFO]: Epoch 093 - training loss: 0.2932, validation loss: 0.3843
2024-06-02 02:03:57 [INFO]: Epoch 094 - training loss: 0.2940, validation loss: 0.3841
2024-06-02 02:04:00 [INFO]: Epoch 095 - training loss: 0.2949, validation loss: 0.3850
2024-06-02 02:04:03 [INFO]: Epoch 096 - training loss: 0.2928, validation loss: 0.3820
2024-06-02 02:04:06 [INFO]: Epoch 097 - training loss: 0.2919, validation loss: 0.3810
2024-06-02 02:04:08 [INFO]: Epoch 098 - training loss: 0.2905, validation loss: 0.3839
2024-06-02 02:04:11 [INFO]: Epoch 099 - training loss: 0.2898, validation loss: 0.3844
2024-06-02 02:04:14 [INFO]: Epoch 100 - training loss: 0.2929, validation loss: 0.3832
2024-06-02 02:04:14 [INFO]: Finished training. The best model is from epoch#97.
2024-06-02 02:04:14 [INFO]: Saved the model to results_point_rate01/PeMS/PatchTST_PeMS/round_2/20240602_T015937/PatchTST.pypots
2024-06-02 02:04:14 [INFO]: Successfully saved to results_point_rate01/PeMS/PatchTST_PeMS/round_2/imputation.pkl
2024-06-02 02:04:14 [INFO]: Round2 - PatchTST on PeMS: MAE=0.3169, MSE=0.5838, MRE=0.3928
2024-06-02 02:04:14 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 02:04:14 [INFO]: Using the given device: cuda:0
2024-06-02 02:04:14 [INFO]: Model files will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_3/20240602_T020414
2024-06-02 02:04:14 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_3/20240602_T020414/tensorboard
2024-06-02 02:04:14 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-02 02:04:14 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-02 02:04:14 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-02 02:04:17 [INFO]: Epoch 001 - training loss: 1.0832, validation loss: 0.7999
2024-06-02 02:04:20 [INFO]: Epoch 002 - training loss: 0.7122, validation loss: 0.7296
2024-06-02 02:04:23 [INFO]: Epoch 003 - training loss: 0.6027, validation loss: 0.6429
2024-06-02 02:04:25 [INFO]: Epoch 004 - training loss: 0.5585, validation loss: 0.6080
2024-06-02 02:04:28 [INFO]: Epoch 005 - training loss: 0.5314, validation loss: 0.5649
2024-06-02 02:04:31 [INFO]: Epoch 006 - training loss: 0.5061, validation loss: 0.5457
2024-06-02 02:04:34 [INFO]: Epoch 007 - training loss: 0.4884, validation loss: 0.5473
2024-06-02 02:04:37 [INFO]: Epoch 008 - training loss: 0.4805, validation loss: 0.5372
2024-06-02 02:04:39 [INFO]: Epoch 009 - training loss: 0.4710, validation loss: 0.5076
2024-06-02 02:04:42 [INFO]: Epoch 010 - training loss: 0.4620, validation loss: 0.5060
2024-06-02 02:04:45 [INFO]: Epoch 011 - training loss: 0.4443, validation loss: 0.4972
2024-06-02 02:04:47 [INFO]: Epoch 012 - training loss: 0.4369, validation loss: 0.4933
2024-06-02 02:04:50 [INFO]: Epoch 013 - training loss: 0.4262, validation loss: 0.4927
2024-06-02 02:04:53 [INFO]: Epoch 014 - training loss: 0.4202, validation loss: 0.4818
2024-06-02 02:04:56 [INFO]: Epoch 015 - training loss: 0.4141, validation loss: 0.4785
2024-06-02 02:04:59 [INFO]: Epoch 016 - training loss: 0.4073, validation loss: 0.4718
2024-06-02 02:05:02 [INFO]: Epoch 017 - training loss: 0.4012, validation loss: 0.4717
2024-06-02 02:05:04 [INFO]: Epoch 018 - training loss: 0.3946, validation loss: 0.4655
2024-06-02 02:05:07 [INFO]: Epoch 019 - training loss: 0.3891, validation loss: 0.4630
2024-06-02 02:05:10 [INFO]: Epoch 020 - training loss: 0.3885, validation loss: 0.4556
2024-06-02 02:05:13 [INFO]: Epoch 021 - training loss: 0.3843, validation loss: 0.4507
2024-06-02 02:05:15 [INFO]: Epoch 022 - training loss: 0.3833, validation loss: 0.4495
2024-06-02 02:05:18 [INFO]: Epoch 023 - training loss: 0.3800, validation loss: 0.4513
2024-06-02 02:05:21 [INFO]: Epoch 024 - training loss: 0.3677, validation loss: 0.4441
2024-06-02 02:05:24 [INFO]: Epoch 025 - training loss: 0.3655, validation loss: 0.4424
2024-06-02 02:05:26 [INFO]: Epoch 026 - training loss: 0.3648, validation loss: 0.4376
2024-06-02 02:05:29 [INFO]: Epoch 027 - training loss: 0.3635, validation loss: 0.4319
2024-06-02 02:05:32 [INFO]: Epoch 028 - training loss: 0.3573, validation loss: 0.4318
2024-06-02 02:05:35 [INFO]: Epoch 029 - training loss: 0.3539, validation loss: 0.4283
2024-06-02 02:05:38 [INFO]: Epoch 030 - training loss: 0.3538, validation loss: 0.4336
2024-06-02 02:05:41 [INFO]: Epoch 031 - training loss: 0.3524, validation loss: 0.4264
2024-06-02 02:05:43 [INFO]: Epoch 032 - training loss: 0.3494, validation loss: 0.4266
2024-06-02 02:05:46 [INFO]: Epoch 033 - training loss: 0.3514, validation loss: 0.4208
2024-06-02 02:05:49 [INFO]: Epoch 034 - training loss: 0.3484, validation loss: 0.4223
2024-06-02 02:05:51 [INFO]: Epoch 035 - training loss: 0.3416, validation loss: 0.4254
2024-06-02 02:05:54 [INFO]: Epoch 036 - training loss: 0.3430, validation loss: 0.4184
2024-06-02 02:05:57 [INFO]: Epoch 037 - training loss: 0.3426, validation loss: 0.4158
2024-06-02 02:06:00 [INFO]: Epoch 038 - training loss: 0.3399, validation loss: 0.4152
2024-06-02 02:06:03 [INFO]: Epoch 039 - training loss: 0.3453, validation loss: 0.4222
2024-06-02 02:06:05 [INFO]: Epoch 040 - training loss: 0.3413, validation loss: 0.4154
2024-06-02 02:06:08 [INFO]: Epoch 041 - training loss: 0.3341, validation loss: 0.4109
2024-06-02 02:06:11 [INFO]: Epoch 042 - training loss: 0.3313, validation loss: 0.4114
2024-06-02 02:06:14 [INFO]: Epoch 043 - training loss: 0.3304, validation loss: 0.4093
2024-06-02 02:06:17 [INFO]: Epoch 044 - training loss: 0.3313, validation loss: 0.4101
2024-06-02 02:06:19 [INFO]: Epoch 045 - training loss: 0.3269, validation loss: 0.4094
2024-06-02 02:06:22 [INFO]: Epoch 046 - training loss: 0.3267, validation loss: 0.4093
2024-06-02 02:06:25 [INFO]: Epoch 047 - training loss: 0.3274, validation loss: 0.4074
2024-06-02 02:06:28 [INFO]: Epoch 048 - training loss: 0.3271, validation loss: 0.4071
2024-06-02 02:06:30 [INFO]: Epoch 049 - training loss: 0.3361, validation loss: 0.4013
2024-06-02 02:06:33 [INFO]: Epoch 050 - training loss: 0.3253, validation loss: 0.4029
2024-06-02 02:06:35 [INFO]: Epoch 051 - training loss: 0.3222, validation loss: 0.3997
2024-06-02 02:06:38 [INFO]: Epoch 052 - training loss: 0.3189, validation loss: 0.4015
2024-06-02 02:06:41 [INFO]: Epoch 053 - training loss: 0.3175, validation loss: 0.4027
2024-06-02 02:06:44 [INFO]: Epoch 054 - training loss: 0.3155, validation loss: 0.3994
2024-06-02 02:06:46 [INFO]: Epoch 055 - training loss: 0.3181, validation loss: 0.3973
2024-06-02 02:06:49 [INFO]: Epoch 056 - training loss: 0.3195, validation loss: 0.4004
2024-06-02 02:06:52 [INFO]: Epoch 057 - training loss: 0.3178, validation loss: 0.3994
2024-06-02 02:06:55 [INFO]: Epoch 058 - training loss: 0.3185, validation loss: 0.3984
2024-06-02 02:06:58 [INFO]: Epoch 059 - training loss: 0.3154, validation loss: 0.3965
2024-06-02 02:07:00 [INFO]: Epoch 060 - training loss: 0.3141, validation loss: 0.3970
2024-06-02 02:07:03 [INFO]: Epoch 061 - training loss: 0.3123, validation loss: 0.3953
2024-06-02 02:07:06 [INFO]: Epoch 062 - training loss: 0.3125, validation loss: 0.3960
2024-06-02 02:07:08 [INFO]: Epoch 063 - training loss: 0.3076, validation loss: 0.3945
2024-06-02 02:07:11 [INFO]: Epoch 064 - training loss: 0.3086, validation loss: 0.3923
2024-06-02 02:07:14 [INFO]: Epoch 065 - training loss: 0.3044, validation loss: 0.3940
2024-06-02 02:07:17 [INFO]: Epoch 066 - training loss: 0.3035, validation loss: 0.3952
2024-06-02 02:07:19 [INFO]: Epoch 067 - training loss: 0.3094, validation loss: 0.3921
2024-06-02 02:07:22 [INFO]: Epoch 068 - training loss: 0.3059, validation loss: 0.3909
2024-06-02 02:07:25 [INFO]: Epoch 069 - training loss: 0.3040, validation loss: 0.3912
2024-06-02 02:07:27 [INFO]: Epoch 070 - training loss: 0.3042, validation loss: 0.3907
2024-06-02 02:07:30 [INFO]: Epoch 071 - training loss: 0.3018, validation loss: 0.3905
2024-06-02 02:07:33 [INFO]: Epoch 072 - training loss: 0.3027, validation loss: 0.3908
2024-06-02 02:07:35 [INFO]: Epoch 073 - training loss: 0.3009, validation loss: 0.3901
2024-06-02 02:07:38 [INFO]: Epoch 074 - training loss: 0.3014, validation loss: 0.3880
2024-06-02 02:07:40 [INFO]: Epoch 075 - training loss: 0.3020, validation loss: 0.3886
2024-06-02 02:07:43 [INFO]: Epoch 076 - training loss: 0.3016, validation loss: 0.3886
2024-06-02 02:07:46 [INFO]: Epoch 077 - training loss: 0.2985, validation loss: 0.3871
2024-06-02 02:07:49 [INFO]: Epoch 078 - training loss: 0.2964, validation loss: 0.3844
2024-06-02 02:07:52 [INFO]: Epoch 079 - training loss: 0.2990, validation loss: 0.3894
2024-06-02 02:07:54 [INFO]: Epoch 080 - training loss: 0.2989, validation loss: 0.3888
2024-06-02 02:07:57 [INFO]: Epoch 081 - training loss: 0.2974, validation loss: 0.3836
2024-06-02 02:08:00 [INFO]: Epoch 082 - training loss: 0.2940, validation loss: 0.3865
2024-06-02 02:08:03 [INFO]: Epoch 083 - training loss: 0.2944, validation loss: 0.3855
2024-06-02 02:08:06 [INFO]: Epoch 084 - training loss: 0.2933, validation loss: 0.3833
2024-06-02 02:08:08 [INFO]: Epoch 085 - training loss: 0.2934, validation loss: 0.3851
2024-06-02 02:08:11 [INFO]: Epoch 086 - training loss: 0.2912, validation loss: 0.3817
2024-06-02 02:08:14 [INFO]: Epoch 087 - training loss: 0.2927, validation loss: 0.3821
2024-06-02 02:08:16 [INFO]: Epoch 088 - training loss: 0.2981, validation loss: 0.3871
2024-06-02 02:08:19 [INFO]: Epoch 089 - training loss: 0.2981, validation loss: 0.3840
2024-06-02 02:08:22 [INFO]: Epoch 090 - training loss: 0.2925, validation loss: 0.3816
2024-06-02 02:08:25 [INFO]: Epoch 091 - training loss: 0.2949, validation loss: 0.3823
2024-06-02 02:08:28 [INFO]: Epoch 092 - training loss: 0.2901, validation loss: 0.3816
2024-06-02 02:08:30 [INFO]: Epoch 093 - training loss: 0.2903, validation loss: 0.3822
2024-06-02 02:08:33 [INFO]: Epoch 094 - training loss: 0.2888, validation loss: 0.3823
2024-06-02 02:08:36 [INFO]: Epoch 095 - training loss: 0.2907, validation loss: 0.3793
2024-06-02 02:08:39 [INFO]: Epoch 096 - training loss: 0.2851, validation loss: 0.3819
2024-06-02 02:08:41 [INFO]: Epoch 097 - training loss: 0.2910, validation loss: 0.3793
2024-06-02 02:08:44 [INFO]: Epoch 098 - training loss: 0.2872, validation loss: 0.3807
2024-06-02 02:08:47 [INFO]: Epoch 099 - training loss: 0.2873, validation loss: 0.3811
2024-06-02 02:08:50 [INFO]: Epoch 100 - training loss: 0.2843, validation loss: 0.3809
2024-06-02 02:08:50 [INFO]: Finished training. The best model is from epoch#95.
2024-06-02 02:08:50 [INFO]: Saved the model to results_point_rate01/PeMS/PatchTST_PeMS/round_3/20240602_T020414/PatchTST.pypots
2024-06-02 02:08:50 [INFO]: Successfully saved to results_point_rate01/PeMS/PatchTST_PeMS/round_3/imputation.pkl
2024-06-02 02:08:50 [INFO]: Round3 - PatchTST on PeMS: MAE=0.3242, MSE=0.5890, MRE=0.4019
2024-06-02 02:08:50 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 02:08:50 [INFO]: Using the given device: cuda:0
2024-06-02 02:08:50 [INFO]: Model files will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_4/20240602_T020850
2024-06-02 02:08:50 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/PatchTST_PeMS/round_4/20240602_T020850/tensorboard
2024-06-02 02:08:50 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-02 02:08:50 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-02 02:08:50 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-02 02:08:53 [INFO]: Epoch 001 - training loss: 1.1275, validation loss: 0.8022
2024-06-02 02:08:55 [INFO]: Epoch 002 - training loss: 0.6881, validation loss: 0.6646
2024-06-02 02:08:58 [INFO]: Epoch 003 - training loss: 0.5873, validation loss: 0.5834
2024-06-02 02:09:01 [INFO]: Epoch 004 - training loss: 0.5540, validation loss: 0.5576
2024-06-02 02:09:04 [INFO]: Epoch 005 - training loss: 0.5276, validation loss: 0.5258
2024-06-02 02:09:06 [INFO]: Epoch 006 - training loss: 0.5116, validation loss: 0.5246
2024-06-02 02:09:09 [INFO]: Epoch 007 - training loss: 0.4968, validation loss: 0.5127
2024-06-02 02:09:12 [INFO]: Epoch 008 - training loss: 0.4751, validation loss: 0.5056
2024-06-02 02:09:15 [INFO]: Epoch 009 - training loss: 0.4599, validation loss: 0.4983
2024-06-02 02:09:17 [INFO]: Epoch 010 - training loss: 0.4567, validation loss: 0.4908
2024-06-02 02:09:20 [INFO]: Epoch 011 - training loss: 0.4417, validation loss: 0.4830
2024-06-02 02:09:23 [INFO]: Epoch 012 - training loss: 0.4312, validation loss: 0.4777
2024-06-02 02:09:26 [INFO]: Epoch 013 - training loss: 0.4344, validation loss: 0.4788
2024-06-02 02:09:29 [INFO]: Epoch 014 - training loss: 0.4267, validation loss: 0.4741
2024-06-02 02:09:31 [INFO]: Epoch 015 - training loss: 0.4096, validation loss: 0.4669
2024-06-02 02:09:34 [INFO]: Epoch 016 - training loss: 0.4084, validation loss: 0.4643
2024-06-02 02:09:37 [INFO]: Epoch 017 - training loss: 0.4001, validation loss: 0.4576
2024-06-02 02:09:40 [INFO]: Epoch 018 - training loss: 0.3973, validation loss: 0.4600
2024-06-02 02:09:42 [INFO]: Epoch 019 - training loss: 0.3972, validation loss: 0.4553
2024-06-02 02:09:45 [INFO]: Epoch 020 - training loss: 0.3914, validation loss: 0.4524
2024-06-02 02:09:48 [INFO]: Epoch 021 - training loss: 0.3877, validation loss: 0.4461
2024-06-02 02:09:51 [INFO]: Epoch 022 - training loss: 0.3813, validation loss: 0.4494
2024-06-02 02:09:53 [INFO]: Epoch 023 - training loss: 0.3829, validation loss: 0.4432
2024-06-02 02:09:56 [INFO]: Epoch 024 - training loss: 0.3770, validation loss: 0.4419
2024-06-02 02:09:59 [INFO]: Epoch 025 - training loss: 0.3787, validation loss: 0.4419
2024-06-02 02:10:02 [INFO]: Epoch 026 - training loss: 0.3731, validation loss: 0.4382
2024-06-02 02:10:04 [INFO]: Epoch 027 - training loss: 0.3749, validation loss: 0.4393
2024-06-02 02:10:07 [INFO]: Epoch 028 - training loss: 0.3642, validation loss: 0.4343
2024-06-02 02:10:09 [INFO]: Epoch 029 - training loss: 0.3615, validation loss: 0.4334
2024-06-02 02:10:12 [INFO]: Epoch 030 - training loss: 0.3551, validation loss: 0.4313
2024-06-02 02:10:15 [INFO]: Epoch 031 - training loss: 0.3592, validation loss: 0.4275
2024-06-02 02:10:18 [INFO]: Epoch 032 - training loss: 0.3550, validation loss: 0.4255
2024-06-02 02:10:21 [INFO]: Epoch 033 - training loss: 0.3485, validation loss: 0.4240
2024-06-02 02:10:23 [INFO]: Epoch 034 - training loss: 0.3510, validation loss: 0.4242
2024-06-02 02:10:26 [INFO]: Epoch 035 - training loss: 0.3508, validation loss: 0.4250
2024-06-02 02:10:29 [INFO]: Epoch 036 - training loss: 0.3501, validation loss: 0.4198
2024-06-02 02:10:32 [INFO]: Epoch 037 - training loss: 0.3462, validation loss: 0.4209
2024-06-02 02:10:34 [INFO]: Epoch 038 - training loss: 0.3453, validation loss: 0.4176
2024-06-02 02:10:37 [INFO]: Epoch 039 - training loss: 0.3394, validation loss: 0.4220
2024-06-02 02:10:40 [INFO]: Epoch 040 - training loss: 0.3465, validation loss: 0.4185
2024-06-02 02:10:43 [INFO]: Epoch 041 - training loss: 0.3460, validation loss: 0.4152
2024-06-02 02:10:45 [INFO]: Epoch 042 - training loss: 0.3403, validation loss: 0.4166
2024-06-02 02:10:48 [INFO]: Epoch 043 - training loss: 0.3419, validation loss: 0.4125
2024-06-02 02:10:51 [INFO]: Epoch 044 - training loss: 0.3371, validation loss: 0.4116
2024-06-02 02:10:54 [INFO]: Epoch 045 - training loss: 0.3360, validation loss: 0.4153
2024-06-02 02:10:57 [INFO]: Epoch 046 - training loss: 0.3336, validation loss: 0.4124
2024-06-02 02:10:59 [INFO]: Epoch 047 - training loss: 0.3303, validation loss: 0.4118
2024-06-02 02:11:02 [INFO]: Epoch 048 - training loss: 0.3293, validation loss: 0.4102
2024-06-02 02:11:05 [INFO]: Epoch 049 - training loss: 0.3277, validation loss: 0.4118
2024-06-02 02:11:08 [INFO]: Epoch 050 - training loss: 0.3300, validation loss: 0.4102
2024-06-02 02:11:11 [INFO]: Epoch 051 - training loss: 0.3326, validation loss: 0.4119
2024-06-02 02:11:14 [INFO]: Epoch 052 - training loss: 0.3288, validation loss: 0.4121
2024-06-02 02:11:16 [INFO]: Epoch 053 - training loss: 0.3288, validation loss: 0.4078
2024-06-02 02:11:19 [INFO]: Epoch 054 - training loss: 0.3256, validation loss: 0.4105
2024-06-02 02:11:22 [INFO]: Epoch 055 - training loss: 0.3251, validation loss: 0.4053
2024-06-02 02:11:25 [INFO]: Epoch 056 - training loss: 0.3225, validation loss: 0.4080
2024-06-02 02:11:28 [INFO]: Epoch 057 - training loss: 0.3197, validation loss: 0.4029
2024-06-02 02:11:31 [INFO]: Epoch 058 - training loss: 0.3200, validation loss: 0.4050
2024-06-02 02:11:34 [INFO]: Epoch 059 - training loss: 0.3193, validation loss: 0.4023
2024-06-02 02:11:36 [INFO]: Epoch 060 - training loss: 0.3175, validation loss: 0.4014
2024-06-02 02:11:39 [INFO]: Epoch 061 - training loss: 0.3194, validation loss: 0.3997
2024-06-02 02:11:42 [INFO]: Epoch 062 - training loss: 0.3185, validation loss: 0.4023
2024-06-02 02:11:45 [INFO]: Epoch 063 - training loss: 0.3156, validation loss: 0.4020
2024-06-02 02:11:48 [INFO]: Epoch 064 - training loss: 0.3184, validation loss: 0.4004
2024-06-02 02:11:51 [INFO]: Epoch 065 - training loss: 0.3174, validation loss: 0.4009
2024-06-02 02:11:53 [INFO]: Epoch 066 - training loss: 0.3148, validation loss: 0.3977
2024-06-02 02:11:56 [INFO]: Epoch 067 - training loss: 0.3116, validation loss: 0.3967
2024-06-02 02:11:59 [INFO]: Epoch 068 - training loss: 0.3109, validation loss: 0.4009
2024-06-02 02:12:02 [INFO]: Epoch 069 - training loss: 0.3156, validation loss: 0.3984
2024-06-02 02:12:05 [INFO]: Epoch 070 - training loss: 0.3138, validation loss: 0.3935
2024-06-02 02:12:08 [INFO]: Epoch 071 - training loss: 0.3095, validation loss: 0.3956
2024-06-02 02:12:10 [INFO]: Epoch 072 - training loss: 0.3112, validation loss: 0.3935
2024-06-02 02:12:13 [INFO]: Epoch 073 - training loss: 0.3118, validation loss: 0.3966
2024-06-02 02:12:16 [INFO]: Epoch 074 - training loss: 0.3196, validation loss: 0.3935
2024-06-02 02:12:19 [INFO]: Epoch 075 - training loss: 0.3180, validation loss: 0.3950
2024-06-02 02:12:22 [INFO]: Epoch 076 - training loss: 0.3112, validation loss: 0.3952
2024-06-02 02:12:25 [INFO]: Epoch 077 - training loss: 0.3097, validation loss: 0.3922
2024-06-02 02:12:27 [INFO]: Epoch 078 - training loss: 0.3056, validation loss: 0.3919
2024-06-02 02:12:30 [INFO]: Epoch 079 - training loss: 0.3062, validation loss: 0.3925
2024-06-02 02:12:33 [INFO]: Epoch 080 - training loss: 0.3046, validation loss: 0.3907
2024-06-02 02:12:36 [INFO]: Epoch 081 - training loss: 0.3043, validation loss: 0.3897
2024-06-02 02:12:39 [INFO]: Epoch 082 - training loss: 0.3056, validation loss: 0.3893
2024-06-02 02:12:42 [INFO]: Epoch 083 - training loss: 0.3039, validation loss: 0.3899
2024-06-02 02:12:44 [INFO]: Epoch 084 - training loss: 0.3015, validation loss: 0.3899
2024-06-02 02:12:47 [INFO]: Epoch 085 - training loss: 0.3022, validation loss: 0.3870
2024-06-02 02:12:50 [INFO]: Epoch 086 - training loss: 0.3016, validation loss: 0.3890
2024-06-02 02:12:53 [INFO]: Epoch 087 - training loss: 0.3042, validation loss: 0.3889
2024-06-02 02:12:56 [INFO]: Epoch 088 - training loss: 0.3031, validation loss: 0.3890
2024-06-02 02:12:59 [INFO]: Epoch 089 - training loss: 0.2986, validation loss: 0.3872
2024-06-02 02:13:02 [INFO]: Epoch 090 - training loss: 0.2992, validation loss: 0.3865
2024-06-02 02:13:04 [INFO]: Epoch 091 - training loss: 0.2978, validation loss: 0.3863
2024-06-02 02:13:07 [INFO]: Epoch 092 - training loss: 0.2978, validation loss: 0.3873
2024-06-02 02:13:10 [INFO]: Epoch 093 - training loss: 0.2961, validation loss: 0.3860
2024-06-02 02:13:13 [INFO]: Epoch 094 - training loss: 0.2955, validation loss: 0.3849
2024-06-02 02:13:16 [INFO]: Epoch 095 - training loss: 0.3000, validation loss: 0.3842
2024-06-02 02:13:19 [INFO]: Epoch 096 - training loss: 0.2986, validation loss: 0.3863
2024-06-02 02:13:21 [INFO]: Epoch 097 - training loss: 0.2943, validation loss: 0.3835
2024-06-02 02:13:24 [INFO]: Epoch 098 - training loss: 0.2930, validation loss: 0.3829
2024-06-02 02:13:27 [INFO]: Epoch 099 - training loss: 0.3069, validation loss: 0.3869
2024-06-02 02:13:30 [INFO]: Epoch 100 - training loss: 0.2978, validation loss: 0.3814
2024-06-02 02:13:30 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 02:13:30 [INFO]: Saved the model to results_point_rate01/PeMS/PatchTST_PeMS/round_4/20240602_T020850/PatchTST.pypots
2024-06-02 02:13:30 [INFO]: Successfully saved to results_point_rate01/PeMS/PatchTST_PeMS/round_4/imputation.pkl
2024-06-02 02:13:30 [INFO]: Round4 - PatchTST on PeMS: MAE=0.3216, MSE=0.5767, MRE=0.3987
2024-06-02 02:13:30 [INFO]: Done! Final results:
Averaged PatchTST (n params: 3,045,238) on PeMS: MAE=0.3296 ± 0.0129118114177764, MSE=0.6000 ± 0.034864921463731026, MRE=0.4086 ± 0.01600547667136281, average inference time=0.21
