2024-06-01 21:52:20 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-01 21:52:20 [INFO]: Using the given device: cuda:0
2024-06-01 21:52:20 [INFO]: Model files will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_0/20240601_T215220
2024-06-01 21:52:20 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_0/20240601_T215220/tensorboard
2024-06-01 21:52:21 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-01 21:52:24 [INFO]: Epoch 001 - training loss: 1.1192, validation loss: 0.7692
2024-06-01 21:52:27 [INFO]: Epoch 002 - training loss: 0.6886, validation loss: 0.6378
2024-06-01 21:52:31 [INFO]: Epoch 003 - training loss: 0.5872, validation loss: 0.6171
2024-06-01 21:52:34 [INFO]: Epoch 004 - training loss: 0.5470, validation loss: 0.5757
2024-06-01 21:52:38 [INFO]: Epoch 005 - training loss: 0.5142, validation loss: 0.5429
2024-06-01 21:52:41 [INFO]: Epoch 006 - training loss: 0.4971, validation loss: 0.5242
2024-06-01 21:52:45 [INFO]: Epoch 007 - training loss: 0.4881, validation loss: 0.5216
2024-06-01 21:52:49 [INFO]: Epoch 008 - training loss: 0.4810, validation loss: 0.5238
2024-06-01 21:52:52 [INFO]: Epoch 009 - training loss: 0.4792, validation loss: 0.5154
2024-06-01 21:52:56 [INFO]: Epoch 010 - training loss: 0.4538, validation loss: 0.4954
2024-06-01 21:53:00 [INFO]: Epoch 011 - training loss: 0.4358, validation loss: 0.4828
2024-06-01 21:53:03 [INFO]: Epoch 012 - training loss: 0.4262, validation loss: 0.4824
2024-06-01 21:53:07 [INFO]: Epoch 013 - training loss: 0.4231, validation loss: 0.4780
2024-06-01 21:53:10 [INFO]: Epoch 014 - training loss: 0.4113, validation loss: 0.4712
2024-06-01 21:53:14 [INFO]: Epoch 015 - training loss: 0.4072, validation loss: 0.4668
2024-06-01 21:53:17 [INFO]: Epoch 016 - training loss: 0.4075, validation loss: 0.4590
2024-06-01 21:53:21 [INFO]: Epoch 017 - training loss: 0.4026, validation loss: 0.4549
2024-06-01 21:53:24 [INFO]: Epoch 018 - training loss: 0.3867, validation loss: 0.4491
2024-06-01 21:53:28 [INFO]: Epoch 019 - training loss: 0.3873, validation loss: 0.4461
2024-06-01 21:53:32 [INFO]: Epoch 020 - training loss: 0.3752, validation loss: 0.4432
2024-06-01 21:53:35 [INFO]: Epoch 021 - training loss: 0.3756, validation loss: 0.4420
2024-06-01 21:53:39 [INFO]: Epoch 022 - training loss: 0.3716, validation loss: 0.4312
2024-06-01 21:53:42 [INFO]: Epoch 023 - training loss: 0.3728, validation loss: 0.4287
2024-06-01 21:53:46 [INFO]: Epoch 024 - training loss: 0.3673, validation loss: 0.4334
2024-06-01 21:53:49 [INFO]: Epoch 025 - training loss: 0.3598, validation loss: 0.4260
2024-06-01 21:53:53 [INFO]: Epoch 026 - training loss: 0.3567, validation loss: 0.4232
2024-06-01 21:53:56 [INFO]: Epoch 027 - training loss: 0.3539, validation loss: 0.4242
2024-06-01 21:54:00 [INFO]: Epoch 028 - training loss: 0.3495, validation loss: 0.4210
2024-06-01 21:54:03 [INFO]: Epoch 029 - training loss: 0.3472, validation loss: 0.4180
2024-06-01 21:54:06 [INFO]: Epoch 030 - training loss: 0.3484, validation loss: 0.4175
2024-06-01 21:54:10 [INFO]: Epoch 031 - training loss: 0.3482, validation loss: 0.4212
2024-06-01 21:54:14 [INFO]: Epoch 032 - training loss: 0.3513, validation loss: 0.4172
2024-06-01 21:54:17 [INFO]: Epoch 033 - training loss: 0.3435, validation loss: 0.4145
2024-06-01 21:54:21 [INFO]: Epoch 034 - training loss: 0.3485, validation loss: 0.4108
2024-06-01 21:54:24 [INFO]: Epoch 035 - training loss: 0.3410, validation loss: 0.4086
2024-06-01 21:54:28 [INFO]: Epoch 036 - training loss: 0.3369, validation loss: 0.4070
2024-06-01 21:54:32 [INFO]: Epoch 037 - training loss: 0.3345, validation loss: 0.4049
2024-06-01 21:54:35 [INFO]: Epoch 038 - training loss: 0.3306, validation loss: 0.4070
2024-06-01 21:54:39 [INFO]: Epoch 039 - training loss: 0.3356, validation loss: 0.4106
2024-06-01 21:54:43 [INFO]: Epoch 040 - training loss: 0.3337, validation loss: 0.4093
2024-06-01 21:54:46 [INFO]: Epoch 041 - training loss: 0.3297, validation loss: 0.4051
2024-06-01 21:54:49 [INFO]: Epoch 042 - training loss: 0.3268, validation loss: 0.4005
2024-06-01 21:54:53 [INFO]: Epoch 043 - training loss: 0.3241, validation loss: 0.4018
2024-06-01 21:54:56 [INFO]: Epoch 044 - training loss: 0.3215, validation loss: 0.3985
2024-06-01 21:55:00 [INFO]: Epoch 045 - training loss: 0.3198, validation loss: 0.3995
2024-06-01 21:55:03 [INFO]: Epoch 046 - training loss: 0.3203, validation loss: 0.3981
2024-06-01 21:55:07 [INFO]: Epoch 047 - training loss: 0.3228, validation loss: 0.3977
2024-06-01 21:55:11 [INFO]: Epoch 048 - training loss: 0.3215, validation loss: 0.3998
2024-06-01 21:55:14 [INFO]: Epoch 049 - training loss: 0.3206, validation loss: 0.3996
2024-06-01 21:55:18 [INFO]: Epoch 050 - training loss: 0.3241, validation loss: 0.4034
2024-06-01 21:55:22 [INFO]: Epoch 051 - training loss: 0.3224, validation loss: 0.3993
2024-06-01 21:55:25 [INFO]: Epoch 052 - training loss: 0.3147, validation loss: 0.3991
2024-06-01 21:55:29 [INFO]: Epoch 053 - training loss: 0.3150, validation loss: 0.3941
2024-06-01 21:55:32 [INFO]: Epoch 054 - training loss: 0.3101, validation loss: 0.3923
2024-06-01 21:55:35 [INFO]: Epoch 055 - training loss: 0.3085, validation loss: 0.3892
2024-06-01 21:55:39 [INFO]: Epoch 056 - training loss: 0.3054, validation loss: 0.3914
2024-06-01 21:55:42 [INFO]: Epoch 057 - training loss: 0.3071, validation loss: 0.3923
2024-06-01 21:55:46 [INFO]: Epoch 058 - training loss: 0.3098, validation loss: 0.3913
2024-06-01 21:55:50 [INFO]: Epoch 059 - training loss: 0.3091, validation loss: 0.3882
2024-06-01 21:55:54 [INFO]: Epoch 060 - training loss: 0.3119, validation loss: 0.3883
2024-06-01 21:55:57 [INFO]: Epoch 061 - training loss: 0.3087, validation loss: 0.3928
2024-06-01 21:56:01 [INFO]: Epoch 062 - training loss: 0.3036, validation loss: 0.3877
2024-06-01 21:56:04 [INFO]: Epoch 063 - training loss: 0.3002, validation loss: 0.3866
2024-06-01 21:56:08 [INFO]: Epoch 064 - training loss: 0.3064, validation loss: 0.3913
2024-06-01 21:56:11 [INFO]: Epoch 065 - training loss: 0.3055, validation loss: 0.3841
2024-06-01 21:56:15 [INFO]: Epoch 066 - training loss: 0.2987, validation loss: 0.3854
2024-06-01 21:56:18 [INFO]: Epoch 067 - training loss: 0.2997, validation loss: 0.3873
2024-06-01 21:56:22 [INFO]: Epoch 068 - training loss: 0.2975, validation loss: 0.3831
2024-06-01 21:56:25 [INFO]: Epoch 069 - training loss: 0.3011, validation loss: 0.3890
2024-06-01 21:56:29 [INFO]: Epoch 070 - training loss: 0.2993, validation loss: 0.3843
2024-06-01 21:56:32 [INFO]: Epoch 071 - training loss: 0.2998, validation loss: 0.3819
2024-06-01 21:56:36 [INFO]: Epoch 072 - training loss: 0.2997, validation loss: 0.3806
2024-06-01 21:56:40 [INFO]: Epoch 073 - training loss: 0.2970, validation loss: 0.3831
2024-06-01 21:56:43 [INFO]: Epoch 074 - training loss: 0.2975, validation loss: 0.3865
2024-06-01 21:56:47 [INFO]: Epoch 075 - training loss: 0.2927, validation loss: 0.3786
2024-06-01 21:56:50 [INFO]: Epoch 076 - training loss: 0.2928, validation loss: 0.3791
2024-06-01 21:56:54 [INFO]: Epoch 077 - training loss: 0.2904, validation loss: 0.3769
2024-06-01 21:56:58 [INFO]: Epoch 078 - training loss: 0.2892, validation loss: 0.3772
2024-06-01 21:57:01 [INFO]: Epoch 079 - training loss: 0.2895, validation loss: 0.3788
2024-06-01 21:57:04 [INFO]: Epoch 080 - training loss: 0.3042, validation loss: 0.3803
2024-06-01 21:57:08 [INFO]: Epoch 081 - training loss: 0.2962, validation loss: 0.3777
2024-06-01 21:57:11 [INFO]: Epoch 082 - training loss: 0.2911, validation loss: 0.3729
2024-06-01 21:57:15 [INFO]: Epoch 083 - training loss: 0.2903, validation loss: 0.3758
2024-06-01 21:57:18 [INFO]: Epoch 084 - training loss: 0.2930, validation loss: 0.3784
2024-06-01 21:57:22 [INFO]: Epoch 085 - training loss: 0.2857, validation loss: 0.3780
2024-06-01 21:57:26 [INFO]: Epoch 086 - training loss: 0.2882, validation loss: 0.3761
2024-06-01 21:57:29 [INFO]: Epoch 087 - training loss: 0.2847, validation loss: 0.3770
2024-06-01 21:57:33 [INFO]: Epoch 088 - training loss: 0.2843, validation loss: 0.3744
2024-06-01 21:57:36 [INFO]: Epoch 089 - training loss: 0.2867, validation loss: 0.3721
2024-06-01 21:57:40 [INFO]: Epoch 090 - training loss: 0.2839, validation loss: 0.3726
2024-06-01 21:57:44 [INFO]: Epoch 091 - training loss: 0.2846, validation loss: 0.3728
2024-06-01 21:57:47 [INFO]: Epoch 092 - training loss: 0.2901, validation loss: 0.3826
2024-06-01 21:57:51 [INFO]: Epoch 093 - training loss: 0.2862, validation loss: 0.3742
2024-06-01 21:57:54 [INFO]: Epoch 094 - training loss: 0.2832, validation loss: 0.3724
2024-06-01 21:57:57 [INFO]: Epoch 095 - training loss: 0.2821, validation loss: 0.3721
2024-06-01 21:58:01 [INFO]: Epoch 096 - training loss: 0.2798, validation loss: 0.3726
2024-06-01 21:58:05 [INFO]: Epoch 097 - training loss: 0.2790, validation loss: 0.3726
2024-06-01 21:58:08 [INFO]: Epoch 098 - training loss: 0.2799, validation loss: 0.3722
2024-06-01 21:58:12 [INFO]: Epoch 099 - training loss: 0.2836, validation loss: 0.3734
2024-06-01 21:58:12 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-01 21:58:12 [INFO]: Finished training. The best model is from epoch#89.
2024-06-01 21:58:12 [INFO]: Saved the model to results_point_rate01/PeMS/Crossformer_PeMS/round_0/20240601_T215220/Crossformer.pypots
2024-06-01 21:58:12 [INFO]: Successfully saved to results_point_rate01/PeMS/Crossformer_PeMS/round_0/imputation.pkl
2024-06-01 21:58:12 [INFO]: Round0 - Crossformer on PeMS: MAE=0.3420, MSE=0.5630, MRE=0.4239
2024-06-01 21:58:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-01 21:58:12 [INFO]: Using the given device: cuda:0
2024-06-01 21:58:12 [INFO]: Model files will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_1/20240601_T215812
2024-06-01 21:58:12 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_1/20240601_T215812/tensorboard
2024-06-01 21:58:13 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-01 21:58:16 [INFO]: Epoch 001 - training loss: 1.1373, validation loss: 0.8647
2024-06-01 21:58:20 [INFO]: Epoch 002 - training loss: 0.7027, validation loss: 0.6570
2024-06-01 21:58:23 [INFO]: Epoch 003 - training loss: 0.6063, validation loss: 0.6469
2024-06-01 21:58:27 [INFO]: Epoch 004 - training loss: 0.5656, validation loss: 0.6192
2024-06-01 21:58:31 [INFO]: Epoch 005 - training loss: 0.5491, validation loss: 0.6001
2024-06-01 21:58:34 [INFO]: Epoch 006 - training loss: 0.5191, validation loss: 0.5675
2024-06-01 21:58:37 [INFO]: Epoch 007 - training loss: 0.5026, validation loss: 0.5528
2024-06-01 21:58:40 [INFO]: Epoch 008 - training loss: 0.4880, validation loss: 0.5506
2024-06-01 21:58:44 [INFO]: Epoch 009 - training loss: 0.4811, validation loss: 0.5295
2024-06-01 21:58:47 [INFO]: Epoch 010 - training loss: 0.4653, validation loss: 0.5123
2024-06-01 21:58:51 [INFO]: Epoch 011 - training loss: 0.4563, validation loss: 0.5055
2024-06-01 21:58:55 [INFO]: Epoch 012 - training loss: 0.4464, validation loss: 0.4990
2024-06-01 21:58:58 [INFO]: Epoch 013 - training loss: 0.4327, validation loss: 0.4978
2024-06-01 21:59:02 [INFO]: Epoch 014 - training loss: 0.4334, validation loss: 0.4818
2024-06-01 21:59:05 [INFO]: Epoch 015 - training loss: 0.4285, validation loss: 0.4841
2024-06-01 21:59:09 [INFO]: Epoch 016 - training loss: 0.4166, validation loss: 0.4774
2024-06-01 21:59:13 [INFO]: Epoch 017 - training loss: 0.4022, validation loss: 0.4704
2024-06-01 21:59:16 [INFO]: Epoch 018 - training loss: 0.3990, validation loss: 0.4559
2024-06-01 21:59:20 [INFO]: Epoch 019 - training loss: 0.3895, validation loss: 0.4606
2024-06-01 21:59:23 [INFO]: Epoch 020 - training loss: 0.3867, validation loss: 0.4502
2024-06-01 21:59:26 [INFO]: Epoch 021 - training loss: 0.3811, validation loss: 0.4499
2024-06-01 21:59:30 [INFO]: Epoch 022 - training loss: 0.3767, validation loss: 0.4429
2024-06-01 21:59:33 [INFO]: Epoch 023 - training loss: 0.3817, validation loss: 0.4388
2024-06-01 21:59:37 [INFO]: Epoch 024 - training loss: 0.3711, validation loss: 0.4383
2024-06-01 21:59:40 [INFO]: Epoch 025 - training loss: 0.3663, validation loss: 0.4423
2024-06-01 21:59:44 [INFO]: Epoch 026 - training loss: 0.3706, validation loss: 0.4343
2024-06-01 21:59:48 [INFO]: Epoch 027 - training loss: 0.3699, validation loss: 0.4351
2024-06-01 21:59:51 [INFO]: Epoch 028 - training loss: 0.3582, validation loss: 0.4291
2024-06-01 21:59:55 [INFO]: Epoch 029 - training loss: 0.3582, validation loss: 0.4290
2024-06-01 21:59:59 [INFO]: Epoch 030 - training loss: 0.3527, validation loss: 0.4273
2024-06-01 22:00:02 [INFO]: Epoch 031 - training loss: 0.3457, validation loss: 0.4245
2024-06-01 22:00:06 [INFO]: Epoch 032 - training loss: 0.3515, validation loss: 0.4215
2024-06-01 22:00:09 [INFO]: Epoch 033 - training loss: 0.3490, validation loss: 0.4172
2024-06-01 22:00:12 [INFO]: Epoch 034 - training loss: 0.3498, validation loss: 0.4252
2024-06-01 22:00:16 [INFO]: Epoch 035 - training loss: 0.3439, validation loss: 0.4189
2024-06-01 22:00:19 [INFO]: Epoch 036 - training loss: 0.3401, validation loss: 0.4206
2024-06-01 22:00:23 [INFO]: Epoch 037 - training loss: 0.3397, validation loss: 0.4163
2024-06-01 22:00:27 [INFO]: Epoch 038 - training loss: 0.3376, validation loss: 0.4145
2024-06-01 22:00:30 [INFO]: Epoch 039 - training loss: 0.3406, validation loss: 0.4135
2024-06-01 22:00:34 [INFO]: Epoch 040 - training loss: 0.3324, validation loss: 0.4131
2024-06-01 22:00:37 [INFO]: Epoch 041 - training loss: 0.3388, validation loss: 0.4104
2024-06-01 22:00:41 [INFO]: Epoch 042 - training loss: 0.3381, validation loss: 0.4131
2024-06-01 22:00:45 [INFO]: Epoch 043 - training loss: 0.3319, validation loss: 0.4054
2024-06-01 22:00:48 [INFO]: Epoch 044 - training loss: 0.3291, validation loss: 0.4081
2024-06-01 22:00:52 [INFO]: Epoch 045 - training loss: 0.3243, validation loss: 0.4060
2024-06-01 22:00:55 [INFO]: Epoch 046 - training loss: 0.3255, validation loss: 0.4030
2024-06-01 22:00:58 [INFO]: Epoch 047 - training loss: 0.3265, validation loss: 0.4086
2024-06-01 22:01:02 [INFO]: Epoch 048 - training loss: 0.3274, validation loss: 0.4022
2024-06-01 22:01:05 [INFO]: Epoch 049 - training loss: 0.3209, validation loss: 0.4081
2024-06-01 22:01:09 [INFO]: Epoch 050 - training loss: 0.3226, validation loss: 0.3971
2024-06-01 22:01:12 [INFO]: Epoch 051 - training loss: 0.3141, validation loss: 0.3976
2024-06-01 22:01:16 [INFO]: Epoch 052 - training loss: 0.3230, validation loss: 0.4011
2024-06-01 22:01:19 [INFO]: Epoch 053 - training loss: 0.3239, validation loss: 0.4001
2024-06-01 22:01:23 [INFO]: Epoch 054 - training loss: 0.3155, validation loss: 0.3981
2024-06-01 22:01:27 [INFO]: Epoch 055 - training loss: 0.3159, validation loss: 0.3973
2024-06-01 22:01:30 [INFO]: Epoch 056 - training loss: 0.3172, validation loss: 0.3964
2024-06-01 22:01:34 [INFO]: Epoch 057 - training loss: 0.3109, validation loss: 0.3978
2024-06-01 22:01:37 [INFO]: Epoch 058 - training loss: 0.3131, validation loss: 0.3940
2024-06-01 22:01:41 [INFO]: Epoch 059 - training loss: 0.3134, validation loss: 0.3926
2024-06-01 22:01:44 [INFO]: Epoch 060 - training loss: 0.3094, validation loss: 0.3910
2024-06-01 22:01:47 [INFO]: Epoch 061 - training loss: 0.3126, validation loss: 0.3896
2024-06-01 22:01:51 [INFO]: Epoch 062 - training loss: 0.3074, validation loss: 0.3903
2024-06-01 22:01:55 [INFO]: Epoch 063 - training loss: 0.3096, validation loss: 0.3905
2024-06-01 22:01:58 [INFO]: Epoch 064 - training loss: 0.3088, validation loss: 0.3897
2024-06-01 22:02:02 [INFO]: Epoch 065 - training loss: 0.3041, validation loss: 0.3903
2024-06-01 22:02:06 [INFO]: Epoch 066 - training loss: 0.3019, validation loss: 0.3919
2024-06-01 22:02:09 [INFO]: Epoch 067 - training loss: 0.3066, validation loss: 0.3908
2024-06-01 22:02:13 [INFO]: Epoch 068 - training loss: 0.3014, validation loss: 0.3945
2024-06-01 22:02:16 [INFO]: Epoch 069 - training loss: 0.3040, validation loss: 0.3902
2024-06-01 22:02:20 [INFO]: Epoch 070 - training loss: 0.3027, validation loss: 0.3892
2024-06-01 22:02:23 [INFO]: Epoch 071 - training loss: 0.3010, validation loss: 0.3856
2024-06-01 22:02:27 [INFO]: Epoch 072 - training loss: 0.3039, validation loss: 0.3930
2024-06-01 22:02:30 [INFO]: Epoch 073 - training loss: 0.3004, validation loss: 0.3868
2024-06-01 22:02:33 [INFO]: Epoch 074 - training loss: 0.2958, validation loss: 0.3882
2024-06-01 22:02:37 [INFO]: Epoch 075 - training loss: 0.2956, validation loss: 0.3846
2024-06-01 22:02:40 [INFO]: Epoch 076 - training loss: 0.2979, validation loss: 0.3858
2024-06-01 22:02:44 [INFO]: Epoch 077 - training loss: 0.3008, validation loss: 0.3822
2024-06-01 22:02:47 [INFO]: Epoch 078 - training loss: 0.2961, validation loss: 0.3831
2024-06-01 22:02:51 [INFO]: Epoch 079 - training loss: 0.2931, validation loss: 0.3875
2024-06-01 22:02:54 [INFO]: Epoch 080 - training loss: 0.2969, validation loss: 0.3808
2024-06-01 22:02:58 [INFO]: Epoch 081 - training loss: 0.2950, validation loss: 0.3817
2024-06-01 22:03:02 [INFO]: Epoch 082 - training loss: 0.2914, validation loss: 0.3811
2024-06-01 22:03:05 [INFO]: Epoch 083 - training loss: 0.2937, validation loss: 0.3853
2024-06-01 22:03:09 [INFO]: Epoch 084 - training loss: 0.2970, validation loss: 0.3804
2024-06-01 22:03:13 [INFO]: Epoch 085 - training loss: 0.3036, validation loss: 0.3802
2024-06-01 22:03:16 [INFO]: Epoch 086 - training loss: 0.2985, validation loss: 0.3811
2024-06-01 22:03:19 [INFO]: Epoch 087 - training loss: 0.3024, validation loss: 0.3859
2024-06-01 22:03:23 [INFO]: Epoch 088 - training loss: 0.2976, validation loss: 0.3781
2024-06-01 22:03:26 [INFO]: Epoch 089 - training loss: 0.2903, validation loss: 0.3789
2024-06-01 22:03:30 [INFO]: Epoch 090 - training loss: 0.2851, validation loss: 0.3770
2024-06-01 22:03:33 [INFO]: Epoch 091 - training loss: 0.2914, validation loss: 0.3801
2024-06-01 22:03:37 [INFO]: Epoch 092 - training loss: 0.2872, validation loss: 0.3792
2024-06-01 22:03:40 [INFO]: Epoch 093 - training loss: 0.2873, validation loss: 0.3785
2024-06-01 22:03:44 [INFO]: Epoch 094 - training loss: 0.2846, validation loss: 0.3773
2024-06-01 22:03:48 [INFO]: Epoch 095 - training loss: 0.2854, validation loss: 0.3782
2024-06-01 22:03:51 [INFO]: Epoch 096 - training loss: 0.2905, validation loss: 0.3799
2024-06-01 22:03:55 [INFO]: Epoch 097 - training loss: 0.2850, validation loss: 0.3771
2024-06-01 22:03:59 [INFO]: Epoch 098 - training loss: 0.2823, validation loss: 0.3785
2024-06-01 22:04:02 [INFO]: Epoch 099 - training loss: 0.2843, validation loss: 0.3760
2024-06-01 22:04:05 [INFO]: Epoch 100 - training loss: 0.2834, validation loss: 0.3767
2024-06-01 22:04:05 [INFO]: Finished training. The best model is from epoch#99.
2024-06-01 22:04:05 [INFO]: Saved the model to results_point_rate01/PeMS/Crossformer_PeMS/round_1/20240601_T215812/Crossformer.pypots
2024-06-01 22:04:06 [INFO]: Successfully saved to results_point_rate01/PeMS/Crossformer_PeMS/round_1/imputation.pkl
2024-06-01 22:04:06 [INFO]: Round1 - Crossformer on PeMS: MAE=0.3369, MSE=0.5654, MRE=0.4177
2024-06-01 22:04:06 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-01 22:04:06 [INFO]: Using the given device: cuda:0
2024-06-01 22:04:06 [INFO]: Model files will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_2/20240601_T220406
2024-06-01 22:04:06 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_2/20240601_T220406/tensorboard
2024-06-01 22:04:06 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-01 22:04:09 [INFO]: Epoch 001 - training loss: 1.1300, validation loss: 0.8073
2024-06-01 22:04:13 [INFO]: Epoch 002 - training loss: 0.7411, validation loss: 0.7810
2024-06-01 22:04:17 [INFO]: Epoch 003 - training loss: 0.6281, validation loss: 0.6617
2024-06-01 22:04:20 [INFO]: Epoch 004 - training loss: 0.5778, validation loss: 0.6562
2024-06-01 22:04:24 [INFO]: Epoch 005 - training loss: 0.5555, validation loss: 0.6128
2024-06-01 22:04:27 [INFO]: Epoch 006 - training loss: 0.5215, validation loss: 0.5892
2024-06-01 22:04:31 [INFO]: Epoch 007 - training loss: 0.5015, validation loss: 0.5565
2024-06-01 22:04:35 [INFO]: Epoch 008 - training loss: 0.4857, validation loss: 0.5309
2024-06-01 22:04:38 [INFO]: Epoch 009 - training loss: 0.4761, validation loss: 0.5193
2024-06-01 22:04:42 [INFO]: Epoch 010 - training loss: 0.4610, validation loss: 0.5054
2024-06-01 22:04:45 [INFO]: Epoch 011 - training loss: 0.4676, validation loss: 0.5050
2024-06-01 22:04:49 [INFO]: Epoch 012 - training loss: 0.4477, validation loss: 0.4942
2024-06-01 22:04:52 [INFO]: Epoch 013 - training loss: 0.4416, validation loss: 0.4839
2024-06-01 22:04:55 [INFO]: Epoch 014 - training loss: 0.4338, validation loss: 0.4905
2024-06-01 22:04:59 [INFO]: Epoch 015 - training loss: 0.4296, validation loss: 0.4720
2024-06-01 22:05:03 [INFO]: Epoch 016 - training loss: 0.4222, validation loss: 0.4709
2024-06-01 22:05:06 [INFO]: Epoch 017 - training loss: 0.4117, validation loss: 0.4690
2024-06-01 22:05:10 [INFO]: Epoch 018 - training loss: 0.4113, validation loss: 0.4668
2024-06-01 22:05:13 [INFO]: Epoch 019 - training loss: 0.4029, validation loss: 0.4607
2024-06-01 22:05:17 [INFO]: Epoch 020 - training loss: 0.4026, validation loss: 0.4687
2024-06-01 22:05:21 [INFO]: Epoch 021 - training loss: 0.4003, validation loss: 0.4528
2024-06-01 22:05:24 [INFO]: Epoch 022 - training loss: 0.3931, validation loss: 0.4534
2024-06-01 22:05:28 [INFO]: Epoch 023 - training loss: 0.3846, validation loss: 0.4476
2024-06-01 22:05:32 [INFO]: Epoch 024 - training loss: 0.3770, validation loss: 0.4420
2024-06-01 22:05:35 [INFO]: Epoch 025 - training loss: 0.3748, validation loss: 0.4432
2024-06-01 22:05:38 [INFO]: Epoch 026 - training loss: 0.3739, validation loss: 0.4413
2024-06-01 22:05:42 [INFO]: Epoch 027 - training loss: 0.3755, validation loss: 0.4404
2024-06-01 22:05:45 [INFO]: Epoch 028 - training loss: 0.3772, validation loss: 0.4426
2024-06-01 22:05:49 [INFO]: Epoch 029 - training loss: 0.3723, validation loss: 0.4363
2024-06-01 22:05:53 [INFO]: Epoch 030 - training loss: 0.3656, validation loss: 0.4369
2024-06-01 22:05:56 [INFO]: Epoch 031 - training loss: 0.3659, validation loss: 0.4334
2024-06-01 22:06:00 [INFO]: Epoch 032 - training loss: 0.3704, validation loss: 0.4333
2024-06-01 22:06:04 [INFO]: Epoch 033 - training loss: 0.3620, validation loss: 0.4260
2024-06-01 22:06:07 [INFO]: Epoch 034 - training loss: 0.3568, validation loss: 0.4219
2024-06-01 22:06:11 [INFO]: Epoch 035 - training loss: 0.3488, validation loss: 0.4199
2024-06-01 22:06:14 [INFO]: Epoch 036 - training loss: 0.3456, validation loss: 0.4195
2024-06-01 22:06:18 [INFO]: Epoch 037 - training loss: 0.3508, validation loss: 0.4270
2024-06-01 22:06:21 [INFO]: Epoch 038 - training loss: 0.3593, validation loss: 0.4233
2024-06-01 22:06:25 [INFO]: Epoch 039 - training loss: 0.3536, validation loss: 0.4217
2024-06-01 22:06:28 [INFO]: Epoch 040 - training loss: 0.3477, validation loss: 0.4208
2024-06-01 22:06:32 [INFO]: Epoch 041 - training loss: 0.3395, validation loss: 0.4141
2024-06-01 22:06:35 [INFO]: Epoch 042 - training loss: 0.3376, validation loss: 0.4160
2024-06-01 22:06:39 [INFO]: Epoch 043 - training loss: 0.3366, validation loss: 0.4124
2024-06-01 22:06:43 [INFO]: Epoch 044 - training loss: 0.3395, validation loss: 0.4102
2024-06-01 22:06:46 [INFO]: Epoch 045 - training loss: 0.3319, validation loss: 0.4131
2024-06-01 22:06:50 [INFO]: Epoch 046 - training loss: 0.3305, validation loss: 0.4167
2024-06-01 22:06:53 [INFO]: Epoch 047 - training loss: 0.3295, validation loss: 0.4115
2024-06-01 22:06:57 [INFO]: Epoch 048 - training loss: 0.3280, validation loss: 0.4052
2024-06-01 22:07:01 [INFO]: Epoch 049 - training loss: 0.3248, validation loss: 0.4078
2024-06-01 22:07:04 [INFO]: Epoch 050 - training loss: 0.3250, validation loss: 0.4100
2024-06-01 22:07:08 [INFO]: Epoch 051 - training loss: 0.3285, validation loss: 0.4058
2024-06-01 22:07:11 [INFO]: Epoch 052 - training loss: 0.3223, validation loss: 0.4047
2024-06-01 22:07:14 [INFO]: Epoch 053 - training loss: 0.3213, validation loss: 0.4029
2024-06-01 22:07:18 [INFO]: Epoch 054 - training loss: 0.3229, validation loss: 0.4029
2024-06-01 22:07:21 [INFO]: Epoch 055 - training loss: 0.3190, validation loss: 0.4013
2024-06-01 22:07:25 [INFO]: Epoch 056 - training loss: 0.3149, validation loss: 0.4016
2024-06-01 22:07:29 [INFO]: Epoch 057 - training loss: 0.3158, validation loss: 0.3990
2024-06-01 22:07:32 [INFO]: Epoch 058 - training loss: 0.3189, validation loss: 0.4012
2024-06-01 22:07:36 [INFO]: Epoch 059 - training loss: 0.3181, validation loss: 0.3959
2024-06-01 22:07:40 [INFO]: Epoch 060 - training loss: 0.3103, validation loss: 0.3985
2024-06-01 22:07:43 [INFO]: Epoch 061 - training loss: 0.3139, validation loss: 0.3973
2024-06-01 22:07:46 [INFO]: Epoch 062 - training loss: 0.3128, validation loss: 0.3975
2024-06-01 22:07:50 [INFO]: Epoch 063 - training loss: 0.3129, validation loss: 0.3932
2024-06-01 22:07:53 [INFO]: Epoch 064 - training loss: 0.3062, validation loss: 0.3958
2024-06-01 22:07:56 [INFO]: Epoch 065 - training loss: 0.3135, validation loss: 0.3931
2024-06-01 22:07:59 [INFO]: Epoch 066 - training loss: 0.3068, validation loss: 0.3899
2024-06-01 22:08:02 [INFO]: Epoch 067 - training loss: 0.3035, validation loss: 0.3894
2024-06-01 22:08:05 [INFO]: Epoch 068 - training loss: 0.3003, validation loss: 0.3941
2024-06-01 22:08:09 [INFO]: Epoch 069 - training loss: 0.3037, validation loss: 0.3916
2024-06-01 22:08:12 [INFO]: Epoch 070 - training loss: 0.3029, validation loss: 0.3880
2024-06-01 22:08:15 [INFO]: Epoch 071 - training loss: 0.3053, validation loss: 0.3888
2024-06-01 22:08:19 [INFO]: Epoch 072 - training loss: 0.3017, validation loss: 0.3904
2024-06-01 22:08:22 [INFO]: Epoch 073 - training loss: 0.3014, validation loss: 0.3868
2024-06-01 22:08:25 [INFO]: Epoch 074 - training loss: 0.3084, validation loss: 0.3879
2024-06-01 22:08:29 [INFO]: Epoch 075 - training loss: 0.3094, validation loss: 0.3837
2024-06-01 22:08:32 [INFO]: Epoch 076 - training loss: 0.2988, validation loss: 0.3851
2024-06-01 22:08:35 [INFO]: Epoch 077 - training loss: 0.2988, validation loss: 0.3839
2024-06-01 22:08:38 [INFO]: Epoch 078 - training loss: 0.2942, validation loss: 0.3865
2024-06-01 22:08:41 [INFO]: Epoch 079 - training loss: 0.2977, validation loss: 0.3860
2024-06-01 22:08:45 [INFO]: Epoch 080 - training loss: 0.3051, validation loss: 0.3878
2024-06-01 22:08:48 [INFO]: Epoch 081 - training loss: 0.3055, validation loss: 0.3853
2024-06-01 22:08:51 [INFO]: Epoch 082 - training loss: 0.2979, validation loss: 0.3859
2024-06-01 22:08:54 [INFO]: Epoch 083 - training loss: 0.3003, validation loss: 0.3874
2024-06-01 22:08:58 [INFO]: Epoch 084 - training loss: 0.2954, validation loss: 0.3856
2024-06-01 22:09:01 [INFO]: Epoch 085 - training loss: 0.2976, validation loss: 0.3880
2024-06-01 22:09:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-01 22:09:01 [INFO]: Finished training. The best model is from epoch#75.
2024-06-01 22:09:01 [INFO]: Saved the model to results_point_rate01/PeMS/Crossformer_PeMS/round_2/20240601_T220406/Crossformer.pypots
2024-06-01 22:09:01 [INFO]: Successfully saved to results_point_rate01/PeMS/Crossformer_PeMS/round_2/imputation.pkl
2024-06-01 22:09:01 [INFO]: Round2 - Crossformer on PeMS: MAE=0.3478, MSE=0.5785, MRE=0.4311
2024-06-01 22:09:01 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-01 22:09:01 [INFO]: Using the given device: cuda:0
2024-06-01 22:09:01 [INFO]: Model files will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_3/20240601_T220901
2024-06-01 22:09:01 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_3/20240601_T220901/tensorboard
2024-06-01 22:09:02 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-01 22:09:05 [INFO]: Epoch 001 - training loss: 1.1001, validation loss: 0.8289
2024-06-01 22:09:08 [INFO]: Epoch 002 - training loss: 0.7036, validation loss: 0.6433
2024-06-01 22:09:11 [INFO]: Epoch 003 - training loss: 0.5982, validation loss: 0.6239
2024-06-01 22:09:15 [INFO]: Epoch 004 - training loss: 0.5475, validation loss: 0.5731
2024-06-01 22:09:18 [INFO]: Epoch 005 - training loss: 0.5220, validation loss: 0.5434
2024-06-01 22:09:21 [INFO]: Epoch 006 - training loss: 0.5049, validation loss: 0.5321
2024-06-01 22:09:24 [INFO]: Epoch 007 - training loss: 0.4936, validation loss: 0.5339
2024-06-01 22:09:27 [INFO]: Epoch 008 - training loss: 0.4840, validation loss: 0.5161
2024-06-01 22:09:31 [INFO]: Epoch 009 - training loss: 0.4826, validation loss: 0.5041
2024-06-01 22:09:34 [INFO]: Epoch 010 - training loss: 0.4602, validation loss: 0.4927
2024-06-01 22:09:37 [INFO]: Epoch 011 - training loss: 0.4341, validation loss: 0.4872
2024-06-01 22:09:41 [INFO]: Epoch 012 - training loss: 0.4306, validation loss: 0.4819
2024-06-01 22:09:44 [INFO]: Epoch 013 - training loss: 0.4245, validation loss: 0.4717
2024-06-01 22:09:47 [INFO]: Epoch 014 - training loss: 0.4106, validation loss: 0.4668
2024-06-01 22:09:51 [INFO]: Epoch 015 - training loss: 0.4048, validation loss: 0.4673
2024-06-01 22:09:54 [INFO]: Epoch 016 - training loss: 0.3983, validation loss: 0.4618
2024-06-01 22:09:57 [INFO]: Epoch 017 - training loss: 0.3943, validation loss: 0.4518
2024-06-01 22:10:01 [INFO]: Epoch 018 - training loss: 0.3855, validation loss: 0.4496
2024-06-01 22:10:04 [INFO]: Epoch 019 - training loss: 0.3834, validation loss: 0.4435
2024-06-01 22:10:07 [INFO]: Epoch 020 - training loss: 0.3793, validation loss: 0.4388
2024-06-01 22:10:10 [INFO]: Epoch 021 - training loss: 0.3795, validation loss: 0.4398
2024-06-01 22:10:13 [INFO]: Epoch 022 - training loss: 0.3766, validation loss: 0.4376
2024-06-01 22:10:16 [INFO]: Epoch 023 - training loss: 0.3720, validation loss: 0.4334
2024-06-01 22:10:19 [INFO]: Epoch 024 - training loss: 0.3645, validation loss: 0.4263
2024-06-01 22:10:22 [INFO]: Epoch 025 - training loss: 0.3660, validation loss: 0.4263
2024-06-01 22:10:26 [INFO]: Epoch 026 - training loss: 0.3585, validation loss: 0.4240
2024-06-01 22:10:29 [INFO]: Epoch 027 - training loss: 0.3569, validation loss: 0.4242
2024-06-01 22:10:33 [INFO]: Epoch 028 - training loss: 0.3595, validation loss: 0.4195
2024-06-01 22:10:36 [INFO]: Epoch 029 - training loss: 0.3510, validation loss: 0.4198
2024-06-01 22:10:39 [INFO]: Epoch 030 - training loss: 0.3545, validation loss: 0.4231
2024-06-01 22:10:42 [INFO]: Epoch 031 - training loss: 0.3443, validation loss: 0.4180
2024-06-01 22:10:46 [INFO]: Epoch 032 - training loss: 0.3462, validation loss: 0.4189
2024-06-01 22:10:49 [INFO]: Epoch 033 - training loss: 0.3450, validation loss: 0.4163
2024-06-01 22:10:52 [INFO]: Epoch 034 - training loss: 0.3441, validation loss: 0.4126
2024-06-01 22:10:55 [INFO]: Epoch 035 - training loss: 0.3402, validation loss: 0.4136
2024-06-01 22:10:58 [INFO]: Epoch 036 - training loss: 0.3386, validation loss: 0.4118
2024-06-01 22:11:01 [INFO]: Epoch 037 - training loss: 0.3395, validation loss: 0.4077
2024-06-01 22:11:04 [INFO]: Epoch 038 - training loss: 0.3423, validation loss: 0.4131
2024-06-01 22:11:08 [INFO]: Epoch 039 - training loss: 0.3368, validation loss: 0.4124
2024-06-01 22:11:11 [INFO]: Epoch 040 - training loss: 0.3363, validation loss: 0.4064
2024-06-01 22:11:14 [INFO]: Epoch 041 - training loss: 0.3339, validation loss: 0.4068
2024-06-01 22:11:17 [INFO]: Epoch 042 - training loss: 0.3260, validation loss: 0.4039
2024-06-01 22:11:21 [INFO]: Epoch 043 - training loss: 0.3282, validation loss: 0.4013
2024-06-01 22:11:24 [INFO]: Epoch 044 - training loss: 0.3284, validation loss: 0.4069
2024-06-01 22:11:27 [INFO]: Epoch 045 - training loss: 0.3309, validation loss: 0.3992
2024-06-01 22:11:31 [INFO]: Epoch 046 - training loss: 0.3236, validation loss: 0.4009
2024-06-01 22:11:34 [INFO]: Epoch 047 - training loss: 0.3249, validation loss: 0.4025
2024-06-01 22:11:37 [INFO]: Epoch 048 - training loss: 0.3187, validation loss: 0.3997
2024-06-01 22:11:40 [INFO]: Epoch 049 - training loss: 0.3225, validation loss: 0.3982
2024-06-01 22:11:44 [INFO]: Epoch 050 - training loss: 0.3171, validation loss: 0.3951
2024-06-01 22:11:47 [INFO]: Epoch 051 - training loss: 0.3183, validation loss: 0.3970
2024-06-01 22:11:50 [INFO]: Epoch 052 - training loss: 0.3151, validation loss: 0.3987
2024-06-01 22:11:53 [INFO]: Epoch 053 - training loss: 0.3199, validation loss: 0.3933
2024-06-01 22:11:56 [INFO]: Epoch 054 - training loss: 0.3139, validation loss: 0.3958
2024-06-01 22:12:00 [INFO]: Epoch 055 - training loss: 0.3153, validation loss: 0.3913
2024-06-01 22:12:03 [INFO]: Epoch 056 - training loss: 0.3178, validation loss: 0.3923
2024-06-01 22:12:06 [INFO]: Epoch 057 - training loss: 0.3151, validation loss: 0.3964
2024-06-01 22:12:10 [INFO]: Epoch 058 - training loss: 0.3096, validation loss: 0.3973
2024-06-01 22:12:13 [INFO]: Epoch 059 - training loss: 0.3112, validation loss: 0.3925
2024-06-01 22:12:16 [INFO]: Epoch 060 - training loss: 0.3066, validation loss: 0.3908
2024-06-01 22:12:20 [INFO]: Epoch 061 - training loss: 0.3072, validation loss: 0.3873
2024-06-01 22:12:23 [INFO]: Epoch 062 - training loss: 0.3079, validation loss: 0.3981
2024-06-01 22:12:26 [INFO]: Epoch 063 - training loss: 0.3043, validation loss: 0.3868
2024-06-01 22:12:29 [INFO]: Epoch 064 - training loss: 0.3012, validation loss: 0.3859
2024-06-01 22:12:32 [INFO]: Epoch 065 - training loss: 0.3019, validation loss: 0.3883
2024-06-01 22:12:35 [INFO]: Epoch 066 - training loss: 0.3049, validation loss: 0.3850
2024-06-01 22:12:38 [INFO]: Epoch 067 - training loss: 0.2975, validation loss: 0.3869
2024-06-01 22:12:41 [INFO]: Epoch 068 - training loss: 0.2970, validation loss: 0.3889
2024-06-01 22:12:44 [INFO]: Epoch 069 - training loss: 0.3028, validation loss: 0.3851
2024-06-01 22:12:48 [INFO]: Epoch 070 - training loss: 0.2977, validation loss: 0.3862
2024-06-01 22:12:51 [INFO]: Epoch 071 - training loss: 0.2987, validation loss: 0.3860
2024-06-01 22:12:54 [INFO]: Epoch 072 - training loss: 0.2964, validation loss: 0.3854
2024-06-01 22:12:58 [INFO]: Epoch 073 - training loss: 0.2990, validation loss: 0.3823
2024-06-01 22:13:01 [INFO]: Epoch 074 - training loss: 0.2934, validation loss: 0.3802
2024-06-01 22:13:04 [INFO]: Epoch 075 - training loss: 0.2921, validation loss: 0.3821
2024-06-01 22:13:07 [INFO]: Epoch 076 - training loss: 0.2928, validation loss: 0.3820
2024-06-01 22:13:11 [INFO]: Epoch 077 - training loss: 0.2955, validation loss: 0.3798
2024-06-01 22:13:14 [INFO]: Epoch 078 - training loss: 0.2906, validation loss: 0.3817
2024-06-01 22:13:17 [INFO]: Epoch 079 - training loss: 0.2884, validation loss: 0.3793
2024-06-01 22:13:20 [INFO]: Epoch 080 - training loss: 0.2933, validation loss: 0.3809
2024-06-01 22:13:23 [INFO]: Epoch 081 - training loss: 0.2915, validation loss: 0.3831
2024-06-01 22:13:26 [INFO]: Epoch 082 - training loss: 0.2977, validation loss: 0.3799
2024-06-01 22:13:29 [INFO]: Epoch 083 - training loss: 0.2953, validation loss: 0.3829
2024-06-01 22:13:33 [INFO]: Epoch 084 - training loss: 0.2933, validation loss: 0.3805
2024-06-01 22:13:36 [INFO]: Epoch 085 - training loss: 0.2905, validation loss: 0.3777
2024-06-01 22:13:39 [INFO]: Epoch 086 - training loss: 0.2870, validation loss: 0.3793
2024-06-01 22:13:43 [INFO]: Epoch 087 - training loss: 0.2829, validation loss: 0.3769
2024-06-01 22:13:46 [INFO]: Epoch 088 - training loss: 0.2910, validation loss: 0.3782
2024-06-01 22:13:49 [INFO]: Epoch 089 - training loss: 0.2980, validation loss: 0.3781
2024-06-01 22:13:52 [INFO]: Epoch 090 - training loss: 0.2866, validation loss: 0.3783
2024-06-01 22:13:56 [INFO]: Epoch 091 - training loss: 0.2848, validation loss: 0.3795
2024-06-01 22:13:59 [INFO]: Epoch 092 - training loss: 0.2821, validation loss: 0.3771
2024-06-01 22:14:02 [INFO]: Epoch 093 - training loss: 0.2842, validation loss: 0.3755
2024-06-01 22:14:05 [INFO]: Epoch 094 - training loss: 0.2821, validation loss: 0.3771
2024-06-01 22:14:08 [INFO]: Epoch 095 - training loss: 0.2818, validation loss: 0.3807
2024-06-01 22:14:11 [INFO]: Epoch 096 - training loss: 0.2827, validation loss: 0.3720
2024-06-01 22:14:14 [INFO]: Epoch 097 - training loss: 0.2806, validation loss: 0.3765
2024-06-01 22:14:18 [INFO]: Epoch 098 - training loss: 0.2803, validation loss: 0.3731
2024-06-01 22:14:21 [INFO]: Epoch 099 - training loss: 0.2800, validation loss: 0.3752
2024-06-01 22:14:24 [INFO]: Epoch 100 - training loss: 0.2790, validation loss: 0.3751
2024-06-01 22:14:24 [INFO]: Finished training. The best model is from epoch#96.
2024-06-01 22:14:24 [INFO]: Saved the model to results_point_rate01/PeMS/Crossformer_PeMS/round_3/20240601_T220901/Crossformer.pypots
2024-06-01 22:14:25 [INFO]: Successfully saved to results_point_rate01/PeMS/Crossformer_PeMS/round_3/imputation.pkl
2024-06-01 22:14:25 [INFO]: Round3 - Crossformer on PeMS: MAE=0.3267, MSE=0.5607, MRE=0.4050
2024-06-01 22:14:25 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-01 22:14:25 [INFO]: Using the given device: cuda:0
2024-06-01 22:14:25 [INFO]: Model files will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_4/20240601_T221425
2024-06-01 22:14:25 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/Crossformer_PeMS/round_4/20240601_T221425/tensorboard
2024-06-01 22:14:25 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-01 22:14:28 [INFO]: Epoch 001 - training loss: 1.1654, validation loss: 0.9037
2024-06-01 22:14:32 [INFO]: Epoch 002 - training loss: 0.6961, validation loss: 0.6655
2024-06-01 22:14:35 [INFO]: Epoch 003 - training loss: 0.5928, validation loss: 0.5996
2024-06-01 22:14:38 [INFO]: Epoch 004 - training loss: 0.5428, validation loss: 0.5725
2024-06-01 22:14:42 [INFO]: Epoch 005 - training loss: 0.5203, validation loss: 0.5380
2024-06-01 22:14:45 [INFO]: Epoch 006 - training loss: 0.5101, validation loss: 0.5346
2024-06-01 22:14:48 [INFO]: Epoch 007 - training loss: 0.4901, validation loss: 0.5299
2024-06-01 22:14:51 [INFO]: Epoch 008 - training loss: 0.4710, validation loss: 0.5069
2024-06-01 22:14:54 [INFO]: Epoch 009 - training loss: 0.4651, validation loss: 0.4993
2024-06-01 22:14:57 [INFO]: Epoch 010 - training loss: 0.4684, validation loss: 0.4932
2024-06-01 22:15:01 [INFO]: Epoch 011 - training loss: 0.4444, validation loss: 0.4809
2024-06-01 22:15:04 [INFO]: Epoch 012 - training loss: 0.4240, validation loss: 0.4859
2024-06-01 22:15:07 [INFO]: Epoch 013 - training loss: 0.4244, validation loss: 0.4743
2024-06-01 22:15:11 [INFO]: Epoch 014 - training loss: 0.4116, validation loss: 0.4674
2024-06-01 22:15:14 [INFO]: Epoch 015 - training loss: 0.4019, validation loss: 0.4604
2024-06-01 22:15:17 [INFO]: Epoch 016 - training loss: 0.3952, validation loss: 0.4556
2024-06-01 22:15:21 [INFO]: Epoch 017 - training loss: 0.3906, validation loss: 0.4502
2024-06-01 22:15:24 [INFO]: Epoch 018 - training loss: 0.3874, validation loss: 0.4430
2024-06-01 22:15:27 [INFO]: Epoch 019 - training loss: 0.3789, validation loss: 0.4414
2024-06-01 22:15:30 [INFO]: Epoch 020 - training loss: 0.3771, validation loss: 0.4393
2024-06-01 22:15:33 [INFO]: Epoch 021 - training loss: 0.3761, validation loss: 0.4457
2024-06-01 22:15:36 [INFO]: Epoch 022 - training loss: 0.3757, validation loss: 0.4340
2024-06-01 22:15:39 [INFO]: Epoch 023 - training loss: 0.3667, validation loss: 0.4275
2024-06-01 22:15:43 [INFO]: Epoch 024 - training loss: 0.3678, validation loss: 0.4305
2024-06-01 22:15:46 [INFO]: Epoch 025 - training loss: 0.3598, validation loss: 0.4226
2024-06-01 22:15:49 [INFO]: Epoch 026 - training loss: 0.3492, validation loss: 0.4247
2024-06-01 22:15:52 [INFO]: Epoch 027 - training loss: 0.3527, validation loss: 0.4189
2024-06-01 22:15:55 [INFO]: Epoch 028 - training loss: 0.3506, validation loss: 0.4166
2024-06-01 22:15:59 [INFO]: Epoch 029 - training loss: 0.3484, validation loss: 0.4190
2024-06-01 22:16:02 [INFO]: Epoch 030 - training loss: 0.3487, validation loss: 0.4148
2024-06-01 22:16:05 [INFO]: Epoch 031 - training loss: 0.3441, validation loss: 0.4110
2024-06-01 22:16:09 [INFO]: Epoch 032 - training loss: 0.3426, validation loss: 0.4165
2024-06-01 22:16:12 [INFO]: Epoch 033 - training loss: 0.3522, validation loss: 0.4163
2024-06-01 22:16:15 [INFO]: Epoch 034 - training loss: 0.3488, validation loss: 0.4090
2024-06-01 22:16:18 [INFO]: Epoch 035 - training loss: 0.3385, validation loss: 0.4068
2024-06-01 22:16:21 [INFO]: Epoch 036 - training loss: 0.3385, validation loss: 0.4087
2024-06-01 22:16:24 [INFO]: Epoch 037 - training loss: 0.3340, validation loss: 0.4054
2024-06-01 22:16:27 [INFO]: Epoch 038 - training loss: 0.3282, validation loss: 0.4032
2024-06-01 22:16:30 [INFO]: Epoch 039 - training loss: 0.3286, validation loss: 0.4057
2024-06-01 22:16:34 [INFO]: Epoch 040 - training loss: 0.3316, validation loss: 0.4053
2024-06-01 22:16:37 [INFO]: Epoch 041 - training loss: 0.3289, validation loss: 0.4012
2024-06-01 22:16:40 [INFO]: Epoch 042 - training loss: 0.3245, validation loss: 0.4008
2024-06-01 22:16:43 [INFO]: Epoch 043 - training loss: 0.3228, validation loss: 0.4011
2024-06-01 22:16:47 [INFO]: Epoch 044 - training loss: 0.3237, validation loss: 0.3952
2024-06-01 22:16:50 [INFO]: Epoch 045 - training loss: 0.3201, validation loss: 0.3980
2024-06-01 22:16:53 [INFO]: Epoch 046 - training loss: 0.3175, validation loss: 0.3943
2024-06-01 22:16:57 [INFO]: Epoch 047 - training loss: 0.3202, validation loss: 0.3938
2024-06-01 22:17:00 [INFO]: Epoch 048 - training loss: 0.3155, validation loss: 0.3951
2024-06-01 22:17:03 [INFO]: Epoch 049 - training loss: 0.3140, validation loss: 0.3911
2024-06-01 22:17:06 [INFO]: Epoch 050 - training loss: 0.3177, validation loss: 0.3905
2024-06-01 22:17:09 [INFO]: Epoch 051 - training loss: 0.3160, validation loss: 0.3886
2024-06-01 22:17:12 [INFO]: Epoch 052 - training loss: 0.3161, validation loss: 0.3902
2024-06-01 22:17:15 [INFO]: Epoch 053 - training loss: 0.3124, validation loss: 0.3895
2024-06-01 22:17:19 [INFO]: Epoch 054 - training loss: 0.3094, validation loss: 0.3874
2024-06-01 22:17:22 [INFO]: Epoch 055 - training loss: 0.3076, validation loss: 0.3864
2024-06-01 22:17:25 [INFO]: Epoch 056 - training loss: 0.3084, validation loss: 0.3877
2024-06-01 22:17:29 [INFO]: Epoch 057 - training loss: 0.3058, validation loss: 0.3875
2024-06-01 22:17:32 [INFO]: Epoch 058 - training loss: 0.3050, validation loss: 0.3855
2024-06-01 22:17:35 [INFO]: Epoch 059 - training loss: 0.3059, validation loss: 0.3844
2024-06-01 22:17:39 [INFO]: Epoch 060 - training loss: 0.3073, validation loss: 0.3842
2024-06-01 22:17:42 [INFO]: Epoch 061 - training loss: 0.3091, validation loss: 0.3846
2024-06-01 22:17:45 [INFO]: Epoch 062 - training loss: 0.3066, validation loss: 0.3863
2024-06-01 22:17:48 [INFO]: Epoch 063 - training loss: 0.3112, validation loss: 0.3825
2024-06-01 22:17:51 [INFO]: Epoch 064 - training loss: 0.3051, validation loss: 0.3834
2024-06-01 22:17:54 [INFO]: Epoch 065 - training loss: 0.3005, validation loss: 0.3856
2024-06-01 22:17:57 [INFO]: Epoch 066 - training loss: 0.3001, validation loss: 0.3809
2024-06-01 22:18:00 [INFO]: Epoch 067 - training loss: 0.2949, validation loss: 0.3800
2024-06-01 22:18:04 [INFO]: Epoch 068 - training loss: 0.2918, validation loss: 0.3793
2024-06-01 22:18:07 [INFO]: Epoch 069 - training loss: 0.2911, validation loss: 0.3779
2024-06-01 22:18:10 [INFO]: Epoch 070 - training loss: 0.2970, validation loss: 0.3827
2024-06-01 22:18:14 [INFO]: Epoch 071 - training loss: 0.2967, validation loss: 0.3804
2024-06-01 22:18:17 [INFO]: Epoch 072 - training loss: 0.2982, validation loss: 0.3790
2024-06-01 22:18:20 [INFO]: Epoch 073 - training loss: 0.3004, validation loss: 0.3822
2024-06-01 22:18:24 [INFO]: Epoch 074 - training loss: 0.3030, validation loss: 0.3824
2024-06-01 22:18:27 [INFO]: Epoch 075 - training loss: 0.2946, validation loss: 0.3803
2024-06-01 22:18:30 [INFO]: Epoch 076 - training loss: 0.2895, validation loss: 0.3763
2024-06-01 22:18:33 [INFO]: Epoch 077 - training loss: 0.2884, validation loss: 0.3764
2024-06-01 22:18:36 [INFO]: Epoch 078 - training loss: 0.2879, validation loss: 0.3762
2024-06-01 22:18:39 [INFO]: Epoch 079 - training loss: 0.2901, validation loss: 0.3759
2024-06-01 22:18:42 [INFO]: Epoch 080 - training loss: 0.2941, validation loss: 0.3813
2024-06-01 22:18:46 [INFO]: Epoch 081 - training loss: 0.2894, validation loss: 0.3771
2024-06-01 22:18:49 [INFO]: Epoch 082 - training loss: 0.2934, validation loss: 0.3795
2024-06-01 22:18:52 [INFO]: Epoch 083 - training loss: 0.2995, validation loss: 0.3779
2024-06-01 22:18:55 [INFO]: Epoch 084 - training loss: 0.2924, validation loss: 0.3770
2024-06-01 22:18:59 [INFO]: Epoch 085 - training loss: 0.2866, validation loss: 0.3744
2024-06-01 22:19:02 [INFO]: Epoch 086 - training loss: 0.2842, validation loss: 0.3754
2024-06-01 22:19:05 [INFO]: Epoch 087 - training loss: 0.2840, validation loss: 0.3731
2024-06-01 22:19:08 [INFO]: Epoch 088 - training loss: 0.2810, validation loss: 0.3721
2024-06-01 22:19:12 [INFO]: Epoch 089 - training loss: 0.2852, validation loss: 0.3704
2024-06-01 22:19:15 [INFO]: Epoch 090 - training loss: 0.2834, validation loss: 0.3715
2024-06-01 22:19:18 [INFO]: Epoch 091 - training loss: 0.2823, validation loss: 0.3695
2024-06-01 22:19:22 [INFO]: Epoch 092 - training loss: 0.2805, validation loss: 0.3681
2024-06-01 22:19:25 [INFO]: Epoch 093 - training loss: 0.2796, validation loss: 0.3723
2024-06-01 22:19:28 [INFO]: Epoch 094 - training loss: 0.2830, validation loss: 0.3697
2024-06-01 22:19:31 [INFO]: Epoch 095 - training loss: 0.2838, validation loss: 0.3739
2024-06-01 22:19:34 [INFO]: Epoch 096 - training loss: 0.2889, validation loss: 0.3759
2024-06-01 22:19:37 [INFO]: Epoch 097 - training loss: 0.2805, validation loss: 0.3719
2024-06-01 22:19:41 [INFO]: Epoch 098 - training loss: 0.2763, validation loss: 0.3700
2024-06-01 22:19:44 [INFO]: Epoch 099 - training loss: 0.2767, validation loss: 0.3698
2024-06-01 22:19:47 [INFO]: Epoch 100 - training loss: 0.2771, validation loss: 0.3726
2024-06-01 22:19:47 [INFO]: Finished training. The best model is from epoch#92.
2024-06-01 22:19:47 [INFO]: Saved the model to results_point_rate01/PeMS/Crossformer_PeMS/round_4/20240601_T221425/Crossformer.pypots
2024-06-01 22:19:48 [INFO]: Successfully saved to results_point_rate01/PeMS/Crossformer_PeMS/round_4/imputation.pkl
2024-06-01 22:19:48 [INFO]: Round4 - Crossformer on PeMS: MAE=0.3336, MSE=0.5572, MRE=0.4135
2024-06-01 22:19:48 [INFO]: Done! Final results:
Averaged Crossformer (n params: 12,645,238) on PeMS: MAE=0.3374 ± 0.007162093918548651, MSE=0.5650 ± 0.007301573659919532, MRE=0.4182 ± 0.008878128979921336, average inference time=0.26
