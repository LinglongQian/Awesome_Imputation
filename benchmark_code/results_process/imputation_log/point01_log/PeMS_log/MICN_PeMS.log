2024-06-02 11:04:31 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 11:04:31 [INFO]: Using the given device: cuda:0
2024-06-02 11:04:32 [INFO]: Model files will be saved to results_point_rate01/PeMS/MICN_PeMS/round_0/20240602_T110432
2024-06-02 11:04:32 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MICN_PeMS/round_0/20240602_T110432/tensorboard
2024-06-02 11:04:33 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 15,490,402
2024-06-02 11:04:38 [INFO]: Epoch 001 - training loss: 0.7636, validation loss: 0.3236
2024-06-02 11:04:41 [INFO]: Epoch 002 - training loss: 0.5262, validation loss: 0.3034
2024-06-02 11:04:43 [INFO]: Epoch 003 - training loss: 0.4952, validation loss: 0.2872
2024-06-02 11:04:46 [INFO]: Epoch 004 - training loss: 0.4718, validation loss: 0.2743
2024-06-02 11:04:49 [INFO]: Epoch 005 - training loss: 0.4599, validation loss: 0.2709
2024-06-02 11:04:52 [INFO]: Epoch 006 - training loss: 0.4508, validation loss: 0.2695
2024-06-02 11:04:55 [INFO]: Epoch 007 - training loss: 0.4448, validation loss: 0.2658
2024-06-02 11:04:58 [INFO]: Epoch 008 - training loss: 0.4429, validation loss: 0.2624
2024-06-02 11:05:00 [INFO]: Epoch 009 - training loss: 0.4424, validation loss: 0.2569
2024-06-02 11:05:03 [INFO]: Epoch 010 - training loss: 0.4396, validation loss: 0.2614
2024-06-02 11:05:06 [INFO]: Epoch 011 - training loss: 0.4365, validation loss: 0.2548
2024-06-02 11:05:09 [INFO]: Epoch 012 - training loss: 0.4357, validation loss: 0.2542
2024-06-02 11:05:12 [INFO]: Epoch 013 - training loss: 0.4370, validation loss: 0.2510
2024-06-02 11:05:14 [INFO]: Epoch 014 - training loss: 0.4361, validation loss: 0.2504
2024-06-02 11:05:17 [INFO]: Epoch 015 - training loss: 0.4290, validation loss: 0.2522
2024-06-02 11:05:20 [INFO]: Epoch 016 - training loss: 0.4293, validation loss: 0.2542
2024-06-02 11:05:23 [INFO]: Epoch 017 - training loss: 0.4323, validation loss: 0.2482
2024-06-02 11:05:26 [INFO]: Epoch 018 - training loss: 0.4338, validation loss: 0.2491
2024-06-02 11:05:28 [INFO]: Epoch 019 - training loss: 0.4307, validation loss: 0.2477
2024-06-02 11:05:31 [INFO]: Epoch 020 - training loss: 0.4276, validation loss: 0.2476
2024-06-02 11:05:34 [INFO]: Epoch 021 - training loss: 0.4298, validation loss: 0.2463
2024-06-02 11:05:37 [INFO]: Epoch 022 - training loss: 0.4271, validation loss: 0.2464
2024-06-02 11:05:39 [INFO]: Epoch 023 - training loss: 0.4254, validation loss: 0.2526
2024-06-02 11:05:42 [INFO]: Epoch 024 - training loss: 0.4276, validation loss: 0.2445
2024-06-02 11:05:45 [INFO]: Epoch 025 - training loss: 0.4270, validation loss: 0.2461
2024-06-02 11:05:48 [INFO]: Epoch 026 - training loss: 0.4243, validation loss: 0.2461
2024-06-02 11:05:50 [INFO]: Epoch 027 - training loss: 0.4257, validation loss: 0.2453
2024-06-02 11:05:53 [INFO]: Epoch 028 - training loss: 0.4244, validation loss: 0.2448
2024-06-02 11:05:56 [INFO]: Epoch 029 - training loss: 0.4213, validation loss: 0.2463
2024-06-02 11:05:59 [INFO]: Epoch 030 - training loss: 0.4220, validation loss: 0.2459
2024-06-02 11:06:02 [INFO]: Epoch 031 - training loss: 0.4175, validation loss: 0.2466
2024-06-02 11:06:04 [INFO]: Epoch 032 - training loss: 0.4199, validation loss: 0.2424
2024-06-02 11:06:07 [INFO]: Epoch 033 - training loss: 0.4183, validation loss: 0.2426
2024-06-02 11:06:10 [INFO]: Epoch 034 - training loss: 0.4199, validation loss: 0.2423
2024-06-02 11:06:13 [INFO]: Epoch 035 - training loss: 0.4146, validation loss: 0.2418
2024-06-02 11:06:15 [INFO]: Epoch 036 - training loss: 0.4180, validation loss: 0.2439
2024-06-02 11:06:18 [INFO]: Epoch 037 - training loss: 0.4141, validation loss: 0.2439
2024-06-02 11:06:21 [INFO]: Epoch 038 - training loss: 0.4135, validation loss: 0.2423
2024-06-02 11:06:24 [INFO]: Epoch 039 - training loss: 0.4143, validation loss: 0.2428
2024-06-02 11:06:27 [INFO]: Epoch 040 - training loss: 0.4137, validation loss: 0.2437
2024-06-02 11:06:29 [INFO]: Epoch 041 - training loss: 0.4140, validation loss: 0.2425
2024-06-02 11:06:32 [INFO]: Epoch 042 - training loss: 0.4126, validation loss: 0.2452
2024-06-02 11:06:35 [INFO]: Epoch 043 - training loss: 0.4107, validation loss: 0.2453
2024-06-02 11:06:38 [INFO]: Epoch 044 - training loss: 0.4104, validation loss: 0.2423
2024-06-02 11:06:41 [INFO]: Epoch 045 - training loss: 0.4128, validation loss: 0.2467
2024-06-02 11:06:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:06:41 [INFO]: Finished training. The best model is from epoch#35.
2024-06-02 11:06:41 [INFO]: Saved the model to results_point_rate01/PeMS/MICN_PeMS/round_0/20240602_T110432/MICN.pypots
2024-06-02 11:06:41 [INFO]: Successfully saved to results_point_rate01/PeMS/MICN_PeMS/round_0/imputation.pkl
2024-06-02 11:06:41 [INFO]: Round0 - MICN on PeMS: MAE=0.2808, MSE=0.3590, MRE=0.3481
2024-06-02 11:06:41 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 11:06:41 [INFO]: Using the given device: cuda:0
2024-06-02 11:06:41 [INFO]: Model files will be saved to results_point_rate01/PeMS/MICN_PeMS/round_1/20240602_T110641
2024-06-02 11:06:41 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MICN_PeMS/round_1/20240602_T110641/tensorboard
2024-06-02 11:06:42 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 15,490,402
2024-06-02 11:06:44 [INFO]: Epoch 001 - training loss: 0.7630, validation loss: 0.3577
2024-06-02 11:06:47 [INFO]: Epoch 002 - training loss: 0.5288, validation loss: 0.3373
2024-06-02 11:06:50 [INFO]: Epoch 003 - training loss: 0.4916, validation loss: 0.3215
2024-06-02 11:06:53 [INFO]: Epoch 004 - training loss: 0.4634, validation loss: 0.3080
2024-06-02 11:06:56 [INFO]: Epoch 005 - training loss: 0.4585, validation loss: 0.2898
2024-06-02 11:06:59 [INFO]: Epoch 006 - training loss: 0.4502, validation loss: 0.2914
2024-06-02 11:07:01 [INFO]: Epoch 007 - training loss: 0.4456, validation loss: 0.2977
2024-06-02 11:07:04 [INFO]: Epoch 008 - training loss: 0.4454, validation loss: 0.2888
2024-06-02 11:07:07 [INFO]: Epoch 009 - training loss: 0.4443, validation loss: 0.2898
2024-06-02 11:07:10 [INFO]: Epoch 010 - training loss: 0.4397, validation loss: 0.2813
2024-06-02 11:07:12 [INFO]: Epoch 011 - training loss: 0.4389, validation loss: 0.2829
2024-06-02 11:07:15 [INFO]: Epoch 012 - training loss: 0.4337, validation loss: 0.2757
2024-06-02 11:07:18 [INFO]: Epoch 013 - training loss: 0.4356, validation loss: 0.2754
2024-06-02 11:07:21 [INFO]: Epoch 014 - training loss: 0.4359, validation loss: 0.2747
2024-06-02 11:07:24 [INFO]: Epoch 015 - training loss: 0.4342, validation loss: 0.2732
2024-06-02 11:07:27 [INFO]: Epoch 016 - training loss: 0.4324, validation loss: 0.2757
2024-06-02 11:07:30 [INFO]: Epoch 017 - training loss: 0.4302, validation loss: 0.2709
2024-06-02 11:07:33 [INFO]: Epoch 018 - training loss: 0.4280, validation loss: 0.2739
2024-06-02 11:07:35 [INFO]: Epoch 019 - training loss: 0.4308, validation loss: 0.2688
2024-06-02 11:07:38 [INFO]: Epoch 020 - training loss: 0.4287, validation loss: 0.2698
2024-06-02 11:07:41 [INFO]: Epoch 021 - training loss: 0.4241, validation loss: 0.2731
2024-06-02 11:07:44 [INFO]: Epoch 022 - training loss: 0.4308, validation loss: 0.2676
2024-06-02 11:07:47 [INFO]: Epoch 023 - training loss: 0.4249, validation loss: 0.2723
2024-06-02 11:07:49 [INFO]: Epoch 024 - training loss: 0.4256, validation loss: 0.2663
2024-06-02 11:07:52 [INFO]: Epoch 025 - training loss: 0.4257, validation loss: 0.2682
2024-06-02 11:07:55 [INFO]: Epoch 026 - training loss: 0.4275, validation loss: 0.2683
2024-06-02 11:07:57 [INFO]: Epoch 027 - training loss: 0.4236, validation loss: 0.2677
2024-06-02 11:08:00 [INFO]: Epoch 028 - training loss: 0.4236, validation loss: 0.2657
2024-06-02 11:08:03 [INFO]: Epoch 029 - training loss: 0.4212, validation loss: 0.2675
2024-06-02 11:08:06 [INFO]: Epoch 030 - training loss: 0.4207, validation loss: 0.2709
2024-06-02 11:08:08 [INFO]: Epoch 031 - training loss: 0.4209, validation loss: 0.2664
2024-06-02 11:08:11 [INFO]: Epoch 032 - training loss: 0.4173, validation loss: 0.2653
2024-06-02 11:08:14 [INFO]: Epoch 033 - training loss: 0.4180, validation loss: 0.2668
2024-06-02 11:08:17 [INFO]: Epoch 034 - training loss: 0.4165, validation loss: 0.2670
2024-06-02 11:08:20 [INFO]: Epoch 035 - training loss: 0.4160, validation loss: 0.2626
2024-06-02 11:08:22 [INFO]: Epoch 036 - training loss: 0.4167, validation loss: 0.2646
2024-06-02 11:08:26 [INFO]: Epoch 037 - training loss: 0.4138, validation loss: 0.2642
2024-06-02 11:08:28 [INFO]: Epoch 038 - training loss: 0.4153, validation loss: 0.2622
2024-06-02 11:08:31 [INFO]: Epoch 039 - training loss: 0.4145, validation loss: 0.2624
2024-06-02 11:08:34 [INFO]: Epoch 040 - training loss: 0.4158, validation loss: 0.2645
2024-06-02 11:08:37 [INFO]: Epoch 041 - training loss: 0.4153, validation loss: 0.2632
2024-06-02 11:08:39 [INFO]: Epoch 042 - training loss: 0.4081, validation loss: 0.2612
2024-06-02 11:08:42 [INFO]: Epoch 043 - training loss: 0.4112, validation loss: 0.2595
2024-06-02 11:08:45 [INFO]: Epoch 044 - training loss: 0.4135, validation loss: 0.2636
2024-06-02 11:08:48 [INFO]: Epoch 045 - training loss: 0.4102, validation loss: 0.2584
2024-06-02 11:08:50 [INFO]: Epoch 046 - training loss: 0.4077, validation loss: 0.2619
2024-06-02 11:08:53 [INFO]: Epoch 047 - training loss: 0.4088, validation loss: 0.2584
2024-06-02 11:08:56 [INFO]: Epoch 048 - training loss: 0.4091, validation loss: 0.2577
2024-06-02 11:08:58 [INFO]: Epoch 049 - training loss: 0.4095, validation loss: 0.2576
2024-06-02 11:09:01 [INFO]: Epoch 050 - training loss: 0.4050, validation loss: 0.2572
2024-06-02 11:09:04 [INFO]: Epoch 051 - training loss: 0.4099, validation loss: 0.2600
2024-06-02 11:09:06 [INFO]: Epoch 052 - training loss: 0.4074, validation loss: 0.2564
2024-06-02 11:09:09 [INFO]: Epoch 053 - training loss: 0.4084, validation loss: 0.2558
2024-06-02 11:09:12 [INFO]: Epoch 054 - training loss: 0.4067, validation loss: 0.2597
2024-06-02 11:09:15 [INFO]: Epoch 055 - training loss: 0.4083, validation loss: 0.2577
2024-06-02 11:09:18 [INFO]: Epoch 056 - training loss: 0.4054, validation loss: 0.2554
2024-06-02 11:09:20 [INFO]: Epoch 057 - training loss: 0.4067, validation loss: 0.2576
2024-06-02 11:09:23 [INFO]: Epoch 058 - training loss: 0.4045, validation loss: 0.2585
2024-06-02 11:09:26 [INFO]: Epoch 059 - training loss: 0.4040, validation loss: 0.2562
2024-06-02 11:09:29 [INFO]: Epoch 060 - training loss: 0.4020, validation loss: 0.2561
2024-06-02 11:09:31 [INFO]: Epoch 061 - training loss: 0.4046, validation loss: 0.2589
2024-06-02 11:09:34 [INFO]: Epoch 062 - training loss: 0.3994, validation loss: 0.2539
2024-06-02 11:09:37 [INFO]: Epoch 063 - training loss: 0.4026, validation loss: 0.2570
2024-06-02 11:09:40 [INFO]: Epoch 064 - training loss: 0.4003, validation loss: 0.2582
2024-06-02 11:09:43 [INFO]: Epoch 065 - training loss: 0.4020, validation loss: 0.2568
2024-06-02 11:09:45 [INFO]: Epoch 066 - training loss: 0.3975, validation loss: 0.2598
2024-06-02 11:09:48 [INFO]: Epoch 067 - training loss: 0.3984, validation loss: 0.2555
2024-06-02 11:09:51 [INFO]: Epoch 068 - training loss: 0.3997, validation loss: 0.2560
2024-06-02 11:09:54 [INFO]: Epoch 069 - training loss: 0.3983, validation loss: 0.2543
2024-06-02 11:09:56 [INFO]: Epoch 070 - training loss: 0.3992, validation loss: 0.2529
2024-06-02 11:09:59 [INFO]: Epoch 071 - training loss: 0.3978, validation loss: 0.2563
2024-06-02 11:10:02 [INFO]: Epoch 072 - training loss: 0.3987, validation loss: 0.2555
2024-06-02 11:10:05 [INFO]: Epoch 073 - training loss: 0.3991, validation loss: 0.2552
2024-06-02 11:10:07 [INFO]: Epoch 074 - training loss: 0.3997, validation loss: 0.2528
2024-06-02 11:10:10 [INFO]: Epoch 075 - training loss: 0.3973, validation loss: 0.2546
2024-06-02 11:10:13 [INFO]: Epoch 076 - training loss: 0.3974, validation loss: 0.2523
2024-06-02 11:10:16 [INFO]: Epoch 077 - training loss: 0.3979, validation loss: 0.2505
2024-06-02 11:10:19 [INFO]: Epoch 078 - training loss: 0.3953, validation loss: 0.2503
2024-06-02 11:10:21 [INFO]: Epoch 079 - training loss: 0.3962, validation loss: 0.2532
2024-06-02 11:10:24 [INFO]: Epoch 080 - training loss: 0.3967, validation loss: 0.2519
2024-06-02 11:10:27 [INFO]: Epoch 081 - training loss: 0.3941, validation loss: 0.2495
2024-06-02 11:10:30 [INFO]: Epoch 082 - training loss: 0.3931, validation loss: 0.2507
2024-06-02 11:10:33 [INFO]: Epoch 083 - training loss: 0.3943, validation loss: 0.2519
2024-06-02 11:10:35 [INFO]: Epoch 084 - training loss: 0.3942, validation loss: 0.2530
2024-06-02 11:10:38 [INFO]: Epoch 085 - training loss: 0.3940, validation loss: 0.2547
2024-06-02 11:10:41 [INFO]: Epoch 086 - training loss: 0.3968, validation loss: 0.2530
2024-06-02 11:10:44 [INFO]: Epoch 087 - training loss: 0.3924, validation loss: 0.2507
2024-06-02 11:10:47 [INFO]: Epoch 088 - training loss: 0.3926, validation loss: 0.2506
2024-06-02 11:10:50 [INFO]: Epoch 089 - training loss: 0.3903, validation loss: 0.2506
2024-06-02 11:10:52 [INFO]: Epoch 090 - training loss: 0.3918, validation loss: 0.2509
2024-06-02 11:10:55 [INFO]: Epoch 091 - training loss: 0.3934, validation loss: 0.2523
2024-06-02 11:10:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:10:55 [INFO]: Finished training. The best model is from epoch#81.
2024-06-02 11:10:55 [INFO]: Saved the model to results_point_rate01/PeMS/MICN_PeMS/round_1/20240602_T110641/MICN.pypots
2024-06-02 11:10:56 [INFO]: Successfully saved to results_point_rate01/PeMS/MICN_PeMS/round_1/imputation.pkl
2024-06-02 11:10:56 [INFO]: Round1 - MICN on PeMS: MAE=0.2825, MSE=0.3748, MRE=0.3502
2024-06-02 11:10:56 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 11:10:56 [INFO]: Using the given device: cuda:0
2024-06-02 11:10:56 [INFO]: Model files will be saved to results_point_rate01/PeMS/MICN_PeMS/round_2/20240602_T111056
2024-06-02 11:10:56 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MICN_PeMS/round_2/20240602_T111056/tensorboard
2024-06-02 11:10:56 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 15,490,402
2024-06-02 11:10:59 [INFO]: Epoch 001 - training loss: 0.7796, validation loss: 0.3445
2024-06-02 11:11:02 [INFO]: Epoch 002 - training loss: 0.5360, validation loss: 0.3163
2024-06-02 11:11:05 [INFO]: Epoch 003 - training loss: 0.4991, validation loss: 0.2970
2024-06-02 11:11:07 [INFO]: Epoch 004 - training loss: 0.4706, validation loss: 0.3053
2024-06-02 11:11:10 [INFO]: Epoch 005 - training loss: 0.4587, validation loss: 0.2890
2024-06-02 11:11:13 [INFO]: Epoch 006 - training loss: 0.4569, validation loss: 0.2853
2024-06-02 11:11:16 [INFO]: Epoch 007 - training loss: 0.4486, validation loss: 0.2760
2024-06-02 11:11:18 [INFO]: Epoch 008 - training loss: 0.4467, validation loss: 0.2746
2024-06-02 11:11:21 [INFO]: Epoch 009 - training loss: 0.4458, validation loss: 0.2682
2024-06-02 11:11:24 [INFO]: Epoch 010 - training loss: 0.4412, validation loss: 0.2690
2024-06-02 11:11:27 [INFO]: Epoch 011 - training loss: 0.4380, validation loss: 0.2654
2024-06-02 11:11:30 [INFO]: Epoch 012 - training loss: 0.4376, validation loss: 0.2652
2024-06-02 11:11:33 [INFO]: Epoch 013 - training loss: 0.4358, validation loss: 0.2623
2024-06-02 11:11:36 [INFO]: Epoch 014 - training loss: 0.4319, validation loss: 0.2663
2024-06-02 11:11:39 [INFO]: Epoch 015 - training loss: 0.4346, validation loss: 0.2633
2024-06-02 11:11:42 [INFO]: Epoch 016 - training loss: 0.4305, validation loss: 0.2593
2024-06-02 11:11:44 [INFO]: Epoch 017 - training loss: 0.4330, validation loss: 0.2571
2024-06-02 11:11:47 [INFO]: Epoch 018 - training loss: 0.4273, validation loss: 0.2577
2024-06-02 11:11:50 [INFO]: Epoch 019 - training loss: 0.4275, validation loss: 0.2569
2024-06-02 11:11:53 [INFO]: Epoch 020 - training loss: 0.4299, validation loss: 0.2560
2024-06-02 11:11:56 [INFO]: Epoch 021 - training loss: 0.4293, validation loss: 0.2559
2024-06-02 11:11:59 [INFO]: Epoch 022 - training loss: 0.4257, validation loss: 0.2516
2024-06-02 11:12:02 [INFO]: Epoch 023 - training loss: 0.4251, validation loss: 0.2543
2024-06-02 11:12:05 [INFO]: Epoch 024 - training loss: 0.4244, validation loss: 0.2555
2024-06-02 11:12:07 [INFO]: Epoch 025 - training loss: 0.4280, validation loss: 0.2524
2024-06-02 11:12:10 [INFO]: Epoch 026 - training loss: 0.4230, validation loss: 0.2520
2024-06-02 11:12:13 [INFO]: Epoch 027 - training loss: 0.4220, validation loss: 0.2544
2024-06-02 11:12:16 [INFO]: Epoch 028 - training loss: 0.4205, validation loss: 0.2549
2024-06-02 11:12:18 [INFO]: Epoch 029 - training loss: 0.4200, validation loss: 0.2522
2024-06-02 11:12:21 [INFO]: Epoch 030 - training loss: 0.4202, validation loss: 0.2529
2024-06-02 11:12:24 [INFO]: Epoch 031 - training loss: 0.4177, validation loss: 0.2557
2024-06-02 11:12:27 [INFO]: Epoch 032 - training loss: 0.4184, validation loss: 0.2544
2024-06-02 11:12:27 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:12:27 [INFO]: Finished training. The best model is from epoch#22.
2024-06-02 11:12:27 [INFO]: Saved the model to results_point_rate01/PeMS/MICN_PeMS/round_2/20240602_T111056/MICN.pypots
2024-06-02 11:12:27 [INFO]: Successfully saved to results_point_rate01/PeMS/MICN_PeMS/round_2/imputation.pkl
2024-06-02 11:12:27 [INFO]: Round2 - MICN on PeMS: MAE=0.2851, MSE=0.3753, MRE=0.3534
2024-06-02 11:12:27 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 11:12:27 [INFO]: Using the given device: cuda:0
2024-06-02 11:12:27 [INFO]: Model files will be saved to results_point_rate01/PeMS/MICN_PeMS/round_3/20240602_T111227
2024-06-02 11:12:27 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MICN_PeMS/round_3/20240602_T111227/tensorboard
2024-06-02 11:12:28 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 15,490,402
2024-06-02 11:12:31 [INFO]: Epoch 001 - training loss: 0.7503, validation loss: 0.3208
2024-06-02 11:12:34 [INFO]: Epoch 002 - training loss: 0.5160, validation loss: 0.3061
2024-06-02 11:12:37 [INFO]: Epoch 003 - training loss: 0.4873, validation loss: 0.2937
2024-06-02 11:12:40 [INFO]: Epoch 004 - training loss: 0.4700, validation loss: 0.2826
2024-06-02 11:12:43 [INFO]: Epoch 005 - training loss: 0.4569, validation loss: 0.2827
2024-06-02 11:12:45 [INFO]: Epoch 006 - training loss: 0.4524, validation loss: 0.2707
2024-06-02 11:12:48 [INFO]: Epoch 007 - training loss: 0.4457, validation loss: 0.2690
2024-06-02 11:12:51 [INFO]: Epoch 008 - training loss: 0.4443, validation loss: 0.2662
2024-06-02 11:12:54 [INFO]: Epoch 009 - training loss: 0.4409, validation loss: 0.2578
2024-06-02 11:12:57 [INFO]: Epoch 010 - training loss: 0.4407, validation loss: 0.2586
2024-06-02 11:13:00 [INFO]: Epoch 011 - training loss: 0.4391, validation loss: 0.2567
2024-06-02 11:13:03 [INFO]: Epoch 012 - training loss: 0.4362, validation loss: 0.2534
2024-06-02 11:13:05 [INFO]: Epoch 013 - training loss: 0.4356, validation loss: 0.2529
2024-06-02 11:13:08 [INFO]: Epoch 014 - training loss: 0.4338, validation loss: 0.2568
2024-06-02 11:13:11 [INFO]: Epoch 015 - training loss: 0.4301, validation loss: 0.2559
2024-06-02 11:13:13 [INFO]: Epoch 016 - training loss: 0.4310, validation loss: 0.2538
2024-06-02 11:13:16 [INFO]: Epoch 017 - training loss: 0.4317, validation loss: 0.2552
2024-06-02 11:13:19 [INFO]: Epoch 018 - training loss: 0.4313, validation loss: 0.2542
2024-06-02 11:13:22 [INFO]: Epoch 019 - training loss: 0.4263, validation loss: 0.2545
2024-06-02 11:13:25 [INFO]: Epoch 020 - training loss: 0.4286, validation loss: 0.2537
2024-06-02 11:13:28 [INFO]: Epoch 021 - training loss: 0.4284, validation loss: 0.2545
2024-06-02 11:13:30 [INFO]: Epoch 022 - training loss: 0.4256, validation loss: 0.2519
2024-06-02 11:13:33 [INFO]: Epoch 023 - training loss: 0.4283, validation loss: 0.2528
2024-06-02 11:13:36 [INFO]: Epoch 024 - training loss: 0.4258, validation loss: 0.2475
2024-06-02 11:13:39 [INFO]: Epoch 025 - training loss: 0.4223, validation loss: 0.2527
2024-06-02 11:13:42 [INFO]: Epoch 026 - training loss: 0.4232, validation loss: 0.2527
2024-06-02 11:13:45 [INFO]: Epoch 027 - training loss: 0.4246, validation loss: 0.2478
2024-06-02 11:13:48 [INFO]: Epoch 028 - training loss: 0.4216, validation loss: 0.2477
2024-06-02 11:13:50 [INFO]: Epoch 029 - training loss: 0.4234, validation loss: 0.2472
2024-06-02 11:13:53 [INFO]: Epoch 030 - training loss: 0.4211, validation loss: 0.2530
2024-06-02 11:13:55 [INFO]: Epoch 031 - training loss: 0.4215, validation loss: 0.2469
2024-06-02 11:13:57 [INFO]: Epoch 032 - training loss: 0.4205, validation loss: 0.2452
2024-06-02 11:14:00 [INFO]: Epoch 033 - training loss: 0.4166, validation loss: 0.2493
2024-06-02 11:14:03 [INFO]: Epoch 034 - training loss: 0.4162, validation loss: 0.2451
2024-06-02 11:14:05 [INFO]: Epoch 035 - training loss: 0.4194, validation loss: 0.2484
2024-06-02 11:14:08 [INFO]: Epoch 036 - training loss: 0.4153, validation loss: 0.2456
2024-06-02 11:14:10 [INFO]: Epoch 037 - training loss: 0.4192, validation loss: 0.2430
2024-06-02 11:14:12 [INFO]: Epoch 038 - training loss: 0.4178, validation loss: 0.2474
2024-06-02 11:14:15 [INFO]: Epoch 039 - training loss: 0.4183, validation loss: 0.2473
2024-06-02 11:14:17 [INFO]: Epoch 040 - training loss: 0.4163, validation loss: 0.2480
2024-06-02 11:14:20 [INFO]: Epoch 041 - training loss: 0.4132, validation loss: 0.2471
2024-06-02 11:14:22 [INFO]: Epoch 042 - training loss: 0.4122, validation loss: 0.2468
2024-06-02 11:14:25 [INFO]: Epoch 043 - training loss: 0.4118, validation loss: 0.2437
2024-06-02 11:14:27 [INFO]: Epoch 044 - training loss: 0.4115, validation loss: 0.2459
2024-06-02 11:14:30 [INFO]: Epoch 045 - training loss: 0.4117, validation loss: 0.2459
2024-06-02 11:14:32 [INFO]: Epoch 046 - training loss: 0.4128, validation loss: 0.2414
2024-06-02 11:14:35 [INFO]: Epoch 047 - training loss: 0.4110, validation loss: 0.2426
2024-06-02 11:14:37 [INFO]: Epoch 048 - training loss: 0.4116, validation loss: 0.2434
2024-06-02 11:14:40 [INFO]: Epoch 049 - training loss: 0.4114, validation loss: 0.2438
2024-06-02 11:14:42 [INFO]: Epoch 050 - training loss: 0.4101, validation loss: 0.2430
2024-06-02 11:14:45 [INFO]: Epoch 051 - training loss: 0.4087, validation loss: 0.2412
2024-06-02 11:14:47 [INFO]: Epoch 052 - training loss: 0.4082, validation loss: 0.2448
2024-06-02 11:14:49 [INFO]: Epoch 053 - training loss: 0.4095, validation loss: 0.2420
2024-06-02 11:14:52 [INFO]: Epoch 054 - training loss: 0.4070, validation loss: 0.2435
2024-06-02 11:14:55 [INFO]: Epoch 055 - training loss: 0.4083, validation loss: 0.2433
2024-06-02 11:14:57 [INFO]: Epoch 056 - training loss: 0.4069, validation loss: 0.2437
2024-06-02 11:15:00 [INFO]: Epoch 057 - training loss: 0.4057, validation loss: 0.2413
2024-06-02 11:15:02 [INFO]: Epoch 058 - training loss: 0.4076, validation loss: 0.2415
2024-06-02 11:15:05 [INFO]: Epoch 059 - training loss: 0.4068, validation loss: 0.2414
2024-06-02 11:15:07 [INFO]: Epoch 060 - training loss: 0.4044, validation loss: 0.2418
2024-06-02 11:15:10 [INFO]: Epoch 061 - training loss: 0.4043, validation loss: 0.2473
2024-06-02 11:15:10 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:15:10 [INFO]: Finished training. The best model is from epoch#51.
2024-06-02 11:15:10 [INFO]: Saved the model to results_point_rate01/PeMS/MICN_PeMS/round_3/20240602_T111227/MICN.pypots
2024-06-02 11:15:10 [INFO]: Successfully saved to results_point_rate01/PeMS/MICN_PeMS/round_3/imputation.pkl
2024-06-02 11:15:10 [INFO]: Round3 - MICN on PeMS: MAE=0.2778, MSE=0.3599, MRE=0.3443
2024-06-02 11:15:10 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 11:15:10 [INFO]: Using the given device: cuda:0
2024-06-02 11:15:10 [INFO]: Model files will be saved to results_point_rate01/PeMS/MICN_PeMS/round_4/20240602_T111510
2024-06-02 11:15:10 [INFO]: Tensorboard file will be saved to results_point_rate01/PeMS/MICN_PeMS/round_4/20240602_T111510/tensorboard
2024-06-02 11:15:11 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 15,490,402
2024-06-02 11:15:13 [INFO]: Epoch 001 - training loss: 0.7574, validation loss: 0.3340
2024-06-02 11:15:16 [INFO]: Epoch 002 - training loss: 0.5214, validation loss: 0.3031
2024-06-02 11:15:18 [INFO]: Epoch 003 - training loss: 0.4804, validation loss: 0.2893
2024-06-02 11:15:20 [INFO]: Epoch 004 - training loss: 0.4648, validation loss: 0.2795
2024-06-02 11:15:23 [INFO]: Epoch 005 - training loss: 0.4557, validation loss: 0.2699
2024-06-02 11:15:25 [INFO]: Epoch 006 - training loss: 0.4478, validation loss: 0.2658
2024-06-02 11:15:28 [INFO]: Epoch 007 - training loss: 0.4453, validation loss: 0.2652
2024-06-02 11:15:31 [INFO]: Epoch 008 - training loss: 0.4413, validation loss: 0.2626
2024-06-02 11:15:33 [INFO]: Epoch 009 - training loss: 0.4385, validation loss: 0.2640
2024-06-02 11:15:36 [INFO]: Epoch 010 - training loss: 0.4397, validation loss: 0.2645
2024-06-02 11:15:38 [INFO]: Epoch 011 - training loss: 0.4381, validation loss: 0.2596
2024-06-02 11:15:41 [INFO]: Epoch 012 - training loss: 0.4316, validation loss: 0.2596
2024-06-02 11:15:44 [INFO]: Epoch 013 - training loss: 0.4333, validation loss: 0.2554
2024-06-02 11:15:46 [INFO]: Epoch 014 - training loss: 0.4318, validation loss: 0.2564
2024-06-02 11:15:49 [INFO]: Epoch 015 - training loss: 0.4283, validation loss: 0.2573
2024-06-02 11:15:51 [INFO]: Epoch 016 - training loss: 0.4267, validation loss: 0.2606
2024-06-02 11:15:53 [INFO]: Epoch 017 - training loss: 0.4240, validation loss: 0.2582
2024-06-02 11:15:56 [INFO]: Epoch 018 - training loss: 0.4270, validation loss: 0.2528
2024-06-02 11:15:58 [INFO]: Epoch 019 - training loss: 0.4279, validation loss: 0.2555
2024-06-02 11:16:01 [INFO]: Epoch 020 - training loss: 0.4239, validation loss: 0.2512
2024-06-02 11:16:03 [INFO]: Epoch 021 - training loss: 0.4266, validation loss: 0.2503
2024-06-02 11:16:06 [INFO]: Epoch 022 - training loss: 0.4250, validation loss: 0.2503
2024-06-02 11:16:08 [INFO]: Epoch 023 - training loss: 0.4223, validation loss: 0.2537
2024-06-02 11:16:11 [INFO]: Epoch 024 - training loss: 0.4233, validation loss: 0.2552
2024-06-02 11:16:13 [INFO]: Epoch 025 - training loss: 0.4213, validation loss: 0.2482
2024-06-02 11:16:16 [INFO]: Epoch 026 - training loss: 0.4194, validation loss: 0.2516
2024-06-02 11:16:18 [INFO]: Epoch 027 - training loss: 0.4196, validation loss: 0.2525
2024-06-02 11:16:20 [INFO]: Epoch 028 - training loss: 0.4210, validation loss: 0.2489
2024-06-02 11:16:23 [INFO]: Epoch 029 - training loss: 0.4211, validation loss: 0.2493
2024-06-02 11:16:25 [INFO]: Epoch 030 - training loss: 0.4193, validation loss: 0.2479
2024-06-02 11:16:28 [INFO]: Epoch 031 - training loss: 0.4203, validation loss: 0.2488
2024-06-02 11:16:30 [INFO]: Epoch 032 - training loss: 0.4135, validation loss: 0.2507
2024-06-02 11:16:33 [INFO]: Epoch 033 - training loss: 0.4138, validation loss: 0.2519
2024-06-02 11:16:35 [INFO]: Epoch 034 - training loss: 0.4157, validation loss: 0.2484
2024-06-02 11:16:38 [INFO]: Epoch 035 - training loss: 0.4144, validation loss: 0.2500
2024-06-02 11:16:40 [INFO]: Epoch 036 - training loss: 0.4109, validation loss: 0.2468
2024-06-02 11:16:43 [INFO]: Epoch 037 - training loss: 0.4141, validation loss: 0.2477
2024-06-02 11:16:45 [INFO]: Epoch 038 - training loss: 0.4144, validation loss: 0.2490
2024-06-02 11:16:47 [INFO]: Epoch 039 - training loss: 0.4142, validation loss: 0.2471
2024-06-02 11:16:50 [INFO]: Epoch 040 - training loss: 0.4108, validation loss: 0.2477
2024-06-02 11:16:52 [INFO]: Epoch 041 - training loss: 0.4086, validation loss: 0.2466
2024-06-02 11:16:55 [INFO]: Epoch 042 - training loss: 0.4109, validation loss: 0.2506
2024-06-02 11:16:57 [INFO]: Epoch 043 - training loss: 0.4111, validation loss: 0.2466
2024-06-02 11:17:00 [INFO]: Epoch 044 - training loss: 0.4109, validation loss: 0.2474
2024-06-02 11:17:02 [INFO]: Epoch 045 - training loss: 0.4130, validation loss: 0.2465
2024-06-02 11:17:05 [INFO]: Epoch 046 - training loss: 0.4113, validation loss: 0.2470
2024-06-02 11:17:07 [INFO]: Epoch 047 - training loss: 0.4064, validation loss: 0.2463
2024-06-02 11:17:09 [INFO]: Epoch 048 - training loss: 0.4064, validation loss: 0.2468
2024-06-02 11:17:12 [INFO]: Epoch 049 - training loss: 0.4080, validation loss: 0.2496
2024-06-02 11:17:15 [INFO]: Epoch 050 - training loss: 0.4044, validation loss: 0.2479
2024-06-02 11:17:17 [INFO]: Epoch 051 - training loss: 0.4061, validation loss: 0.2460
2024-06-02 11:17:20 [INFO]: Epoch 052 - training loss: 0.4061, validation loss: 0.2479
2024-06-02 11:17:22 [INFO]: Epoch 053 - training loss: 0.4065, validation loss: 0.2446
2024-06-02 11:17:25 [INFO]: Epoch 054 - training loss: 0.4002, validation loss: 0.2453
2024-06-02 11:17:27 [INFO]: Epoch 055 - training loss: 0.4059, validation loss: 0.2457
2024-06-02 11:17:30 [INFO]: Epoch 056 - training loss: 0.4017, validation loss: 0.2454
2024-06-02 11:17:32 [INFO]: Epoch 057 - training loss: 0.4029, validation loss: 0.2475
2024-06-02 11:17:35 [INFO]: Epoch 058 - training loss: 0.4020, validation loss: 0.2477
2024-06-02 11:17:37 [INFO]: Epoch 059 - training loss: 0.3999, validation loss: 0.2458
2024-06-02 11:17:40 [INFO]: Epoch 060 - training loss: 0.4036, validation loss: 0.2460
2024-06-02 11:17:42 [INFO]: Epoch 061 - training loss: 0.4010, validation loss: 0.2451
2024-06-02 11:17:45 [INFO]: Epoch 062 - training loss: 0.4003, validation loss: 0.2441
2024-06-02 11:17:48 [INFO]: Epoch 063 - training loss: 0.4016, validation loss: 0.2439
2024-06-02 11:17:50 [INFO]: Epoch 064 - training loss: 0.4012, validation loss: 0.2434
2024-06-02 11:17:53 [INFO]: Epoch 065 - training loss: 0.3992, validation loss: 0.2453
2024-06-02 11:17:55 [INFO]: Epoch 066 - training loss: 0.4007, validation loss: 0.2469
2024-06-02 11:17:58 [INFO]: Epoch 067 - training loss: 0.3991, validation loss: 0.2437
2024-06-02 11:18:00 [INFO]: Epoch 068 - training loss: 0.3960, validation loss: 0.2439
2024-06-02 11:18:03 [INFO]: Epoch 069 - training loss: 0.3966, validation loss: 0.2442
2024-06-02 11:18:05 [INFO]: Epoch 070 - training loss: 0.3951, validation loss: 0.2452
2024-06-02 11:18:08 [INFO]: Epoch 071 - training loss: 0.3980, validation loss: 0.2442
2024-06-02 11:18:10 [INFO]: Epoch 072 - training loss: 0.3988, validation loss: 0.2435
2024-06-02 11:18:13 [INFO]: Epoch 073 - training loss: 0.3968, validation loss: 0.2462
2024-06-02 11:18:15 [INFO]: Epoch 074 - training loss: 0.3938, validation loss: 0.2437
2024-06-02 11:18:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 11:18:15 [INFO]: Finished training. The best model is from epoch#64.
2024-06-02 11:18:16 [INFO]: Saved the model to results_point_rate01/PeMS/MICN_PeMS/round_4/20240602_T111510/MICN.pypots
2024-06-02 11:18:16 [INFO]: Successfully saved to results_point_rate01/PeMS/MICN_PeMS/round_4/imputation.pkl
2024-06-02 11:18:16 [INFO]: Round4 - MICN on PeMS: MAE=0.2785, MSE=0.3623, MRE=0.3453
2024-06-02 11:18:16 [INFO]: Done! Final results:
Averaged MICN (15,490,402 params) on PeMS: MAE=0.2809 ± 0.0026566308991237547, MSE=0.3662 ± 0.007264418045502194, MRE=0.3482 ± 0.003293158682739586, average inference time=0.15
