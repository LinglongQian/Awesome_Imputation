2024-06-03 10:17:24 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:17:24 [INFO]: Using the given device: cuda:0
2024-06-03 10:17:25 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_0/20240603_T101725
2024-06-03 10:17:25 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_0/20240603_T101725/tensorboard
2024-06-03 10:17:32 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 15,262,215
2024-06-03 10:17:52 [INFO]: Epoch 001 - training loss: 2.0797, validation loss: 0.7566
2024-06-03 10:17:57 [INFO]: Epoch 002 - training loss: 1.0550, validation loss: 0.6340
2024-06-03 10:18:03 [INFO]: Epoch 003 - training loss: 0.7895, validation loss: 0.6070
2024-06-03 10:18:09 [INFO]: Epoch 004 - training loss: 0.6693, validation loss: 0.5530
2024-06-03 10:18:14 [INFO]: Epoch 005 - training loss: 0.6542, validation loss: 0.5468
2024-06-03 10:18:19 [INFO]: Epoch 006 - training loss: 0.6018, validation loss: 0.4923
2024-06-03 10:18:25 [INFO]: Epoch 007 - training loss: 0.5827, validation loss: 0.5191
2024-06-03 10:18:30 [INFO]: Epoch 008 - training loss: 0.5806, validation loss: 0.4951
2024-06-03 10:18:36 [INFO]: Epoch 009 - training loss: 0.5716, validation loss: 0.5190
2024-06-03 10:18:41 [INFO]: Epoch 010 - training loss: 0.5421, validation loss: 0.5105
2024-06-03 10:18:46 [INFO]: Epoch 011 - training loss: 0.5035, validation loss: 0.4901
2024-06-03 10:18:52 [INFO]: Epoch 012 - training loss: 0.5131, validation loss: 0.4801
2024-06-03 10:18:57 [INFO]: Epoch 013 - training loss: 0.5212, validation loss: 0.4724
2024-06-03 10:19:02 [INFO]: Epoch 014 - training loss: 0.4953, validation loss: 0.4737
2024-06-03 10:19:07 [INFO]: Epoch 015 - training loss: 0.4635, validation loss: 0.4711
2024-06-03 10:19:12 [INFO]: Epoch 016 - training loss: 0.5187, validation loss: 0.4771
2024-06-03 10:19:17 [INFO]: Epoch 017 - training loss: 0.4957, validation loss: 0.4919
2024-06-03 10:19:22 [INFO]: Epoch 018 - training loss: 0.4528, validation loss: 0.4449
2024-06-03 10:19:28 [INFO]: Epoch 019 - training loss: 0.4409, validation loss: 0.3988
2024-06-03 10:19:33 [INFO]: Epoch 020 - training loss: 0.4253, validation loss: 0.4143
2024-06-03 10:19:38 [INFO]: Epoch 021 - training loss: 0.4227, validation loss: 0.4118
2024-06-03 10:19:44 [INFO]: Epoch 022 - training loss: 0.4325, validation loss: 0.3859
2024-06-03 10:19:49 [INFO]: Epoch 023 - training loss: 0.4064, validation loss: 0.3766
2024-06-03 10:19:55 [INFO]: Epoch 024 - training loss: 0.3933, validation loss: 0.3625
2024-06-03 10:20:00 [INFO]: Epoch 025 - training loss: 0.4050, validation loss: 0.3427
2024-06-03 10:20:06 [INFO]: Epoch 026 - training loss: 0.4080, validation loss: 0.3523
2024-06-03 10:20:11 [INFO]: Epoch 027 - training loss: 0.4298, validation loss: 0.3377
2024-06-03 10:20:17 [INFO]: Epoch 028 - training loss: 0.4072, validation loss: 0.3559
2024-06-03 10:20:22 [INFO]: Epoch 029 - training loss: 0.3910, validation loss: 0.3473
2024-06-03 10:20:27 [INFO]: Epoch 030 - training loss: 0.3862, validation loss: 0.3533
2024-06-03 10:20:32 [INFO]: Epoch 031 - training loss: 0.3726, validation loss: 0.3397
2024-06-03 10:20:38 [INFO]: Epoch 032 - training loss: 0.3678, validation loss: 0.3116
2024-06-03 10:20:43 [INFO]: Epoch 033 - training loss: 0.3481, validation loss: 0.3232
2024-06-03 10:20:48 [INFO]: Epoch 034 - training loss: 0.3511, validation loss: 0.2983
2024-06-03 10:20:53 [INFO]: Epoch 035 - training loss: 0.3695, validation loss: 0.2874
2024-06-03 10:20:58 [INFO]: Epoch 036 - training loss: 0.3489, validation loss: 0.2818
2024-06-03 10:21:03 [INFO]: Epoch 037 - training loss: 0.3451, validation loss: 0.3298
2024-06-03 10:21:09 [INFO]: Epoch 038 - training loss: 0.3406, validation loss: 0.3047
2024-06-03 10:21:14 [INFO]: Epoch 039 - training loss: 0.3349, validation loss: 0.2914
2024-06-03 10:21:19 [INFO]: Epoch 040 - training loss: 0.3238, validation loss: 0.3001
2024-06-03 10:21:23 [INFO]: Epoch 041 - training loss: 0.3304, validation loss: 0.2977
2024-06-03 10:21:28 [INFO]: Epoch 042 - training loss: 0.3409, validation loss: 0.3016
2024-06-03 10:21:33 [INFO]: Epoch 043 - training loss: 0.3464, validation loss: 0.3097
2024-06-03 10:21:37 [INFO]: Epoch 044 - training loss: 0.3285, validation loss: 0.2875
2024-06-03 10:21:41 [INFO]: Epoch 045 - training loss: 0.3430, validation loss: 0.3063
2024-06-03 10:21:46 [INFO]: Epoch 046 - training loss: 0.3388, validation loss: 0.2914
2024-06-03 10:21:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:21:46 [INFO]: Finished training. The best model is from epoch#36.
2024-06-03 10:21:47 [INFO]: Saved the model to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_0/20240603_T101725/Pyraformer.pypots
2024-06-03 10:21:50 [INFO]: Successfully saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_0/imputation.pkl
2024-06-03 10:21:50 [INFO]: Round0 - Pyraformer on ETT_h1: MAE=0.4447, MSE=0.4255, MRE=0.5522
2024-06-03 10:21:50 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:21:50 [INFO]: Using the given device: cuda:0
2024-06-03 10:21:50 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_1/20240603_T102150
2024-06-03 10:21:50 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_1/20240603_T102150/tensorboard
2024-06-03 10:21:52 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 15,262,215
2024-06-03 10:21:56 [INFO]: Epoch 001 - training loss: 2.0844, validation loss: 0.7903
2024-06-03 10:22:01 [INFO]: Epoch 002 - training loss: 0.9617, validation loss: 0.7061
2024-06-03 10:22:06 [INFO]: Epoch 003 - training loss: 0.8131, validation loss: 0.7607
2024-06-03 10:22:10 [INFO]: Epoch 004 - training loss: 0.7368, validation loss: 0.5976
2024-06-03 10:22:15 [INFO]: Epoch 005 - training loss: 0.6258, validation loss: 0.6107
2024-06-03 10:22:19 [INFO]: Epoch 006 - training loss: 0.6188, validation loss: 0.5448
2024-06-03 10:22:24 [INFO]: Epoch 007 - training loss: 0.5968, validation loss: 0.5379
2024-06-03 10:22:28 [INFO]: Epoch 008 - training loss: 0.5671, validation loss: 0.5602
2024-06-03 10:22:32 [INFO]: Epoch 009 - training loss: 0.5506, validation loss: 0.5187
2024-06-03 10:22:36 [INFO]: Epoch 010 - training loss: 0.5242, validation loss: 0.4719
2024-06-03 10:22:40 [INFO]: Epoch 011 - training loss: 0.5270, validation loss: 0.4733
2024-06-03 10:22:44 [INFO]: Epoch 012 - training loss: 0.5245, validation loss: 0.4569
2024-06-03 10:22:48 [INFO]: Epoch 013 - training loss: 0.5064, validation loss: 0.4691
2024-06-03 10:22:52 [INFO]: Epoch 014 - training loss: 0.5342, validation loss: 0.4323
2024-06-03 10:22:56 [INFO]: Epoch 015 - training loss: 0.5056, validation loss: 0.4241
2024-06-03 10:23:01 [INFO]: Epoch 016 - training loss: 0.5013, validation loss: 0.4844
2024-06-03 10:23:05 [INFO]: Epoch 017 - training loss: 0.4781, validation loss: 0.4204
2024-06-03 10:23:09 [INFO]: Epoch 018 - training loss: 0.4718, validation loss: 0.4167
2024-06-03 10:23:13 [INFO]: Epoch 019 - training loss: 0.4535, validation loss: 0.4441
2024-06-03 10:23:18 [INFO]: Epoch 020 - training loss: 0.4685, validation loss: 0.4373
2024-06-03 10:23:22 [INFO]: Epoch 021 - training loss: 0.4581, validation loss: 0.4270
2024-06-03 10:23:26 [INFO]: Epoch 022 - training loss: 0.4396, validation loss: 0.3998
2024-06-03 10:23:30 [INFO]: Epoch 023 - training loss: 0.4554, validation loss: 0.3906
2024-06-03 10:23:34 [INFO]: Epoch 024 - training loss: 0.4156, validation loss: 0.3647
2024-06-03 10:23:38 [INFO]: Epoch 025 - training loss: 0.4189, validation loss: 0.3717
2024-06-03 10:23:41 [INFO]: Epoch 026 - training loss: 0.4068, validation loss: 0.3500
2024-06-03 10:23:45 [INFO]: Epoch 027 - training loss: 0.4093, validation loss: 0.3782
2024-06-03 10:23:49 [INFO]: Epoch 028 - training loss: 0.3999, validation loss: 0.3664
2024-06-03 10:23:53 [INFO]: Epoch 029 - training loss: 0.3934, validation loss: 0.3526
2024-06-03 10:23:57 [INFO]: Epoch 030 - training loss: 0.3880, validation loss: 0.3755
2024-06-03 10:24:02 [INFO]: Epoch 031 - training loss: 0.3763, validation loss: 0.3550
2024-06-03 10:24:06 [INFO]: Epoch 032 - training loss: 0.3891, validation loss: 0.3591
2024-06-03 10:24:10 [INFO]: Epoch 033 - training loss: 0.3905, validation loss: 0.3602
2024-06-03 10:24:14 [INFO]: Epoch 034 - training loss: 0.3616, validation loss: 0.3348
2024-06-03 10:24:17 [INFO]: Epoch 035 - training loss: 0.3608, validation loss: 0.3581
2024-06-03 10:24:20 [INFO]: Epoch 036 - training loss: 0.3821, validation loss: 0.3633
2024-06-03 10:24:24 [INFO]: Epoch 037 - training loss: 0.3611, validation loss: 0.3549
2024-06-03 10:24:28 [INFO]: Epoch 038 - training loss: 0.3643, validation loss: 0.3067
2024-06-03 10:24:31 [INFO]: Epoch 039 - training loss: 0.3362, validation loss: 0.3155
2024-06-03 10:24:35 [INFO]: Epoch 040 - training loss: 0.3405, validation loss: 0.3334
2024-06-03 10:24:39 [INFO]: Epoch 041 - training loss: 0.3257, validation loss: 0.2922
2024-06-03 10:24:42 [INFO]: Epoch 042 - training loss: 0.3251, validation loss: 0.3040
2024-06-03 10:24:46 [INFO]: Epoch 043 - training loss: 0.3366, validation loss: 0.3067
2024-06-03 10:24:49 [INFO]: Epoch 044 - training loss: 0.3366, validation loss: 0.3143
2024-06-03 10:24:53 [INFO]: Epoch 045 - training loss: 0.3274, validation loss: 0.2924
2024-06-03 10:24:56 [INFO]: Epoch 046 - training loss: 0.3229, validation loss: 0.2971
2024-06-03 10:25:00 [INFO]: Epoch 047 - training loss: 0.3250, validation loss: 0.3182
2024-06-03 10:25:03 [INFO]: Epoch 048 - training loss: 0.3181, validation loss: 0.3028
2024-06-03 10:25:07 [INFO]: Epoch 049 - training loss: 0.3199, validation loss: 0.2848
2024-06-03 10:25:10 [INFO]: Epoch 050 - training loss: 0.3093, validation loss: 0.2836
2024-06-03 10:25:14 [INFO]: Epoch 051 - training loss: 0.2935, validation loss: 0.2939
2024-06-03 10:25:17 [INFO]: Epoch 052 - training loss: 0.2970, validation loss: 0.2805
2024-06-03 10:25:21 [INFO]: Epoch 053 - training loss: 0.2949, validation loss: 0.2787
2024-06-03 10:25:24 [INFO]: Epoch 054 - training loss: 0.2927, validation loss: 0.2829
2024-06-03 10:25:28 [INFO]: Epoch 055 - training loss: 0.2993, validation loss: 0.3001
2024-06-03 10:25:31 [INFO]: Epoch 056 - training loss: 0.3076, validation loss: 0.2961
2024-06-03 10:25:35 [INFO]: Epoch 057 - training loss: 0.3119, validation loss: 0.3091
2024-06-03 10:25:39 [INFO]: Epoch 058 - training loss: 0.2999, validation loss: 0.2918
2024-06-03 10:25:43 [INFO]: Epoch 059 - training loss: 0.3086, validation loss: 0.3074
2024-06-03 10:25:46 [INFO]: Epoch 060 - training loss: 0.2953, validation loss: 0.2686
2024-06-03 10:25:50 [INFO]: Epoch 061 - training loss: 0.3058, validation loss: 0.2944
2024-06-03 10:25:53 [INFO]: Epoch 062 - training loss: 0.2984, validation loss: 0.2856
2024-06-03 10:25:57 [INFO]: Epoch 063 - training loss: 0.2943, validation loss: 0.2969
2024-06-03 10:26:00 [INFO]: Epoch 064 - training loss: 0.2864, validation loss: 0.2893
2024-06-03 10:26:03 [INFO]: Epoch 065 - training loss: 0.2852, validation loss: 0.2788
2024-06-03 10:26:07 [INFO]: Epoch 066 - training loss: 0.2777, validation loss: 0.2775
2024-06-03 10:26:10 [INFO]: Epoch 067 - training loss: 0.2858, validation loss: 0.2689
2024-06-03 10:26:14 [INFO]: Epoch 068 - training loss: 0.2791, validation loss: 0.2681
2024-06-03 10:26:17 [INFO]: Epoch 069 - training loss: 0.2890, validation loss: 0.2553
2024-06-03 10:26:20 [INFO]: Epoch 070 - training loss: 0.3117, validation loss: 0.2770
2024-06-03 10:26:24 [INFO]: Epoch 071 - training loss: 0.3019, validation loss: 0.2691
2024-06-03 10:26:27 [INFO]: Epoch 072 - training loss: 0.3082, validation loss: 0.3011
2024-06-03 10:26:30 [INFO]: Epoch 073 - training loss: 0.3092, validation loss: 0.2852
2024-06-03 10:26:33 [INFO]: Epoch 074 - training loss: 0.3014, validation loss: 0.2620
2024-06-03 10:26:36 [INFO]: Epoch 075 - training loss: 0.2857, validation loss: 0.2770
2024-06-03 10:26:40 [INFO]: Epoch 076 - training loss: 0.2925, validation loss: 0.2839
2024-06-03 10:26:43 [INFO]: Epoch 077 - training loss: 0.2976, validation loss: 0.2482
2024-06-03 10:26:47 [INFO]: Epoch 078 - training loss: 0.2947, validation loss: 0.2730
2024-06-03 10:26:50 [INFO]: Epoch 079 - training loss: 0.2893, validation loss: 0.2436
2024-06-03 10:26:53 [INFO]: Epoch 080 - training loss: 0.2675, validation loss: 0.2622
2024-06-03 10:26:57 [INFO]: Epoch 081 - training loss: 0.2510, validation loss: 0.2645
2024-06-03 10:27:00 [INFO]: Epoch 082 - training loss: 0.2535, validation loss: 0.2669
2024-06-03 10:27:03 [INFO]: Epoch 083 - training loss: 0.2553, validation loss: 0.2474
2024-06-03 10:27:07 [INFO]: Epoch 084 - training loss: 0.2623, validation loss: 0.2988
2024-06-03 10:27:10 [INFO]: Epoch 085 - training loss: 0.2542, validation loss: 0.2573
2024-06-03 10:27:14 [INFO]: Epoch 086 - training loss: 0.2650, validation loss: 0.2489
2024-06-03 10:27:17 [INFO]: Epoch 087 - training loss: 0.2610, validation loss: 0.2590
2024-06-03 10:27:20 [INFO]: Epoch 088 - training loss: 0.2694, validation loss: 0.2639
2024-06-03 10:27:23 [INFO]: Epoch 089 - training loss: 0.2710, validation loss: 0.3094
2024-06-03 10:27:23 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:27:23 [INFO]: Finished training. The best model is from epoch#79.
2024-06-03 10:27:24 [INFO]: Saved the model to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_1/20240603_T102150/Pyraformer.pypots
2024-06-03 10:27:26 [INFO]: Successfully saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_1/imputation.pkl
2024-06-03 10:27:26 [INFO]: Round1 - Pyraformer on ETT_h1: MAE=0.4489, MSE=0.4489, MRE=0.5574
2024-06-03 10:27:26 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:27:26 [INFO]: Using the given device: cuda:0
2024-06-03 10:27:26 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_2/20240603_T102726
2024-06-03 10:27:26 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_2/20240603_T102726/tensorboard
2024-06-03 10:27:27 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 15,262,215
2024-06-03 10:27:30 [INFO]: Epoch 001 - training loss: 2.2855, validation loss: 0.7874
2024-06-03 10:27:32 [INFO]: Epoch 002 - training loss: 1.1047, validation loss: 0.5279
2024-06-03 10:27:35 [INFO]: Epoch 003 - training loss: 0.8135, validation loss: 0.6076
2024-06-03 10:27:37 [INFO]: Epoch 004 - training loss: 0.7746, validation loss: 0.5510
2024-06-03 10:27:40 [INFO]: Epoch 005 - training loss: 0.6631, validation loss: 0.5586
2024-06-03 10:27:42 [INFO]: Epoch 006 - training loss: 0.5996, validation loss: 0.5563
2024-06-03 10:27:45 [INFO]: Epoch 007 - training loss: 0.5908, validation loss: 0.5455
2024-06-03 10:27:47 [INFO]: Epoch 008 - training loss: 0.5298, validation loss: 0.5074
2024-06-03 10:27:49 [INFO]: Epoch 009 - training loss: 0.5115, validation loss: 0.5244
2024-06-03 10:27:52 [INFO]: Epoch 010 - training loss: 0.4895, validation loss: 0.4779
2024-06-03 10:27:54 [INFO]: Epoch 011 - training loss: 0.4922, validation loss: 0.5092
2024-06-03 10:27:56 [INFO]: Epoch 012 - training loss: 0.4735, validation loss: 0.5062
2024-06-03 10:27:58 [INFO]: Epoch 013 - training loss: 0.4796, validation loss: 0.4774
2024-06-03 10:28:00 [INFO]: Epoch 014 - training loss: 0.4720, validation loss: 0.4770
2024-06-03 10:28:02 [INFO]: Epoch 015 - training loss: 0.4616, validation loss: 0.4566
2024-06-03 10:28:05 [INFO]: Epoch 016 - training loss: 0.4651, validation loss: 0.4238
2024-06-03 10:28:07 [INFO]: Epoch 017 - training loss: 0.4387, validation loss: 0.4295
2024-06-03 10:28:10 [INFO]: Epoch 018 - training loss: 0.4513, validation loss: 0.4451
2024-06-03 10:28:12 [INFO]: Epoch 019 - training loss: 0.4457, validation loss: 0.4239
2024-06-03 10:28:15 [INFO]: Epoch 020 - training loss: 0.4407, validation loss: 0.4427
2024-06-03 10:28:16 [INFO]: Epoch 021 - training loss: 0.4560, validation loss: 0.4168
2024-06-03 10:28:19 [INFO]: Epoch 022 - training loss: 0.4429, validation loss: 0.4318
2024-06-03 10:28:21 [INFO]: Epoch 023 - training loss: 0.4316, validation loss: 0.4286
2024-06-03 10:28:23 [INFO]: Epoch 024 - training loss: 0.4375, validation loss: 0.4112
2024-06-03 10:28:26 [INFO]: Epoch 025 - training loss: 0.4129, validation loss: 0.4196
2024-06-03 10:28:28 [INFO]: Epoch 026 - training loss: 0.4374, validation loss: 0.3873
2024-06-03 10:28:30 [INFO]: Epoch 027 - training loss: 0.4231, validation loss: 0.3789
2024-06-03 10:28:32 [INFO]: Epoch 028 - training loss: 0.3956, validation loss: 0.3518
2024-06-03 10:28:34 [INFO]: Epoch 029 - training loss: 0.3671, validation loss: 0.3663
2024-06-03 10:28:35 [INFO]: Epoch 030 - training loss: 0.3705, validation loss: 0.3490
2024-06-03 10:28:38 [INFO]: Epoch 031 - training loss: 0.3808, validation loss: 0.3363
2024-06-03 10:28:40 [INFO]: Epoch 032 - training loss: 0.3742, validation loss: 0.3548
2024-06-03 10:28:42 [INFO]: Epoch 033 - training loss: 0.3757, validation loss: 0.3085
2024-06-03 10:28:44 [INFO]: Epoch 034 - training loss: 0.3765, validation loss: 0.3558
2024-06-03 10:28:46 [INFO]: Epoch 035 - training loss: 0.3660, validation loss: 0.3342
2024-06-03 10:28:48 [INFO]: Epoch 036 - training loss: 0.3637, validation loss: 0.3648
2024-06-03 10:28:50 [INFO]: Epoch 037 - training loss: 0.3431, validation loss: 0.3206
2024-06-03 10:28:52 [INFO]: Epoch 038 - training loss: 0.3576, validation loss: 0.3204
2024-06-03 10:28:53 [INFO]: Epoch 039 - training loss: 0.3435, validation loss: 0.3145
2024-06-03 10:28:55 [INFO]: Epoch 040 - training loss: 0.3220, validation loss: 0.3131
2024-06-03 10:28:57 [INFO]: Epoch 041 - training loss: 0.3511, validation loss: 0.3126
2024-06-03 10:28:58 [INFO]: Epoch 042 - training loss: 0.3600, validation loss: 0.3042
2024-06-03 10:29:00 [INFO]: Epoch 043 - training loss: 0.3576, validation loss: 0.3160
2024-06-03 10:29:02 [INFO]: Epoch 044 - training loss: 0.3380, validation loss: 0.2948
2024-06-03 10:29:03 [INFO]: Epoch 045 - training loss: 0.3437, validation loss: 0.3044
2024-06-03 10:29:05 [INFO]: Epoch 046 - training loss: 0.3368, validation loss: 0.2887
2024-06-03 10:29:06 [INFO]: Epoch 047 - training loss: 0.3230, validation loss: 0.3038
2024-06-03 10:29:08 [INFO]: Epoch 048 - training loss: 0.3258, validation loss: 0.2784
2024-06-03 10:29:10 [INFO]: Epoch 049 - training loss: 0.3252, validation loss: 0.2915
2024-06-03 10:29:11 [INFO]: Epoch 050 - training loss: 0.3013, validation loss: 0.2636
2024-06-03 10:29:13 [INFO]: Epoch 051 - training loss: 0.3043, validation loss: 0.2965
2024-06-03 10:29:15 [INFO]: Epoch 052 - training loss: 0.3229, validation loss: 0.2855
2024-06-03 10:29:16 [INFO]: Epoch 053 - training loss: 0.3045, validation loss: 0.2800
2024-06-03 10:29:18 [INFO]: Epoch 054 - training loss: 0.3030, validation loss: 0.2698
2024-06-03 10:29:19 [INFO]: Epoch 055 - training loss: 0.2983, validation loss: 0.2837
2024-06-03 10:29:21 [INFO]: Epoch 056 - training loss: 0.3100, validation loss: 0.3036
2024-06-03 10:29:23 [INFO]: Epoch 057 - training loss: 0.3168, validation loss: 0.2959
2024-06-03 10:29:24 [INFO]: Epoch 058 - training loss: 0.3115, validation loss: 0.2885
2024-06-03 10:29:26 [INFO]: Epoch 059 - training loss: 0.3011, validation loss: 0.2763
2024-06-03 10:29:27 [INFO]: Epoch 060 - training loss: 0.3059, validation loss: 0.3084
2024-06-03 10:29:27 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:29:27 [INFO]: Finished training. The best model is from epoch#50.
2024-06-03 10:29:28 [INFO]: Saved the model to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_2/20240603_T102726/Pyraformer.pypots
2024-06-03 10:29:29 [INFO]: Successfully saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_2/imputation.pkl
2024-06-03 10:29:29 [INFO]: Round2 - Pyraformer on ETT_h1: MAE=0.4710, MSE=0.4697, MRE=0.5849
2024-06-03 10:29:29 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:29:29 [INFO]: Using the given device: cuda:0
2024-06-03 10:29:29 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_3/20240603_T102929
2024-06-03 10:29:29 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_3/20240603_T102929/tensorboard
2024-06-03 10:29:30 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 15,262,215
2024-06-03 10:29:32 [INFO]: Epoch 001 - training loss: 2.1529, validation loss: 0.9255
2024-06-03 10:29:34 [INFO]: Epoch 002 - training loss: 0.9876, validation loss: 0.6474
2024-06-03 10:29:36 [INFO]: Epoch 003 - training loss: 0.8179, validation loss: 0.5894
2024-06-03 10:29:38 [INFO]: Epoch 004 - training loss: 0.7136, validation loss: 0.6141
2024-06-03 10:29:40 [INFO]: Epoch 005 - training loss: 0.6676, validation loss: 0.5634
2024-06-03 10:29:42 [INFO]: Epoch 006 - training loss: 0.6498, validation loss: 0.4819
2024-06-03 10:29:44 [INFO]: Epoch 007 - training loss: 0.5787, validation loss: 0.4896
2024-06-03 10:29:46 [INFO]: Epoch 008 - training loss: 0.5709, validation loss: 0.5204
2024-06-03 10:29:48 [INFO]: Epoch 009 - training loss: 0.6206, validation loss: 0.5124
2024-06-03 10:29:49 [INFO]: Epoch 010 - training loss: 0.5581, validation loss: 0.5115
2024-06-03 10:29:51 [INFO]: Epoch 011 - training loss: 0.5292, validation loss: 0.4694
2024-06-03 10:29:52 [INFO]: Epoch 012 - training loss: 0.4925, validation loss: 0.4773
2024-06-03 10:29:54 [INFO]: Epoch 013 - training loss: 0.4733, validation loss: 0.4892
2024-06-03 10:29:56 [INFO]: Epoch 014 - training loss: 0.4713, validation loss: 0.4323
2024-06-03 10:29:58 [INFO]: Epoch 015 - training loss: 0.4825, validation loss: 0.4438
2024-06-03 10:29:59 [INFO]: Epoch 016 - training loss: 0.4767, validation loss: 0.4462
2024-06-03 10:30:01 [INFO]: Epoch 017 - training loss: 0.4576, validation loss: 0.4311
2024-06-03 10:30:03 [INFO]: Epoch 018 - training loss: 0.4555, validation loss: 0.4198
2024-06-03 10:30:05 [INFO]: Epoch 019 - training loss: 0.4635, validation loss: 0.3944
2024-06-03 10:30:06 [INFO]: Epoch 020 - training loss: 0.4596, validation loss: 0.3737
2024-06-03 10:30:08 [INFO]: Epoch 021 - training loss: 0.4440, validation loss: 0.3776
2024-06-03 10:30:10 [INFO]: Epoch 022 - training loss: 0.4369, validation loss: 0.4215
2024-06-03 10:30:12 [INFO]: Epoch 023 - training loss: 0.4199, validation loss: 0.3839
2024-06-03 10:30:14 [INFO]: Epoch 024 - training loss: 0.3882, validation loss: 0.3971
2024-06-03 10:30:15 [INFO]: Epoch 025 - training loss: 0.3832, validation loss: 0.3407
2024-06-03 10:30:17 [INFO]: Epoch 026 - training loss: 0.3838, validation loss: 0.3292
2024-06-03 10:30:19 [INFO]: Epoch 027 - training loss: 0.3855, validation loss: 0.3576
2024-06-03 10:30:21 [INFO]: Epoch 028 - training loss: 0.3916, validation loss: 0.3269
2024-06-03 10:30:23 [INFO]: Epoch 029 - training loss: 0.3786, validation loss: 0.3798
2024-06-03 10:30:24 [INFO]: Epoch 030 - training loss: 0.3869, validation loss: 0.3160
2024-06-03 10:30:26 [INFO]: Epoch 031 - training loss: 0.4053, validation loss: 0.3238
2024-06-03 10:30:28 [INFO]: Epoch 032 - training loss: 0.4037, validation loss: 0.3239
2024-06-03 10:30:29 [INFO]: Epoch 033 - training loss: 0.4088, validation loss: 0.3530
2024-06-03 10:30:31 [INFO]: Epoch 034 - training loss: 0.4127, validation loss: 0.3232
2024-06-03 10:30:33 [INFO]: Epoch 035 - training loss: 0.4128, validation loss: 0.3204
2024-06-03 10:30:35 [INFO]: Epoch 036 - training loss: 0.3871, validation loss: 0.3008
2024-06-03 10:30:36 [INFO]: Epoch 037 - training loss: 0.3654, validation loss: 0.3221
2024-06-03 10:30:38 [INFO]: Epoch 038 - training loss: 0.3666, validation loss: 0.2881
2024-06-03 10:30:40 [INFO]: Epoch 039 - training loss: 0.3413, validation loss: 0.2754
2024-06-03 10:30:41 [INFO]: Epoch 040 - training loss: 0.3574, validation loss: 0.2896
2024-06-03 10:30:43 [INFO]: Epoch 041 - training loss: 0.3410, validation loss: 0.3041
2024-06-03 10:30:45 [INFO]: Epoch 042 - training loss: 0.3357, validation loss: 0.2770
2024-06-03 10:30:47 [INFO]: Epoch 043 - training loss: 0.3391, validation loss: 0.2802
2024-06-03 10:30:48 [INFO]: Epoch 044 - training loss: 0.3400, validation loss: 0.2900
2024-06-03 10:30:50 [INFO]: Epoch 045 - training loss: 0.3419, validation loss: 0.2801
2024-06-03 10:30:52 [INFO]: Epoch 046 - training loss: 0.3277, validation loss: 0.2901
2024-06-03 10:30:53 [INFO]: Epoch 047 - training loss: 0.3296, validation loss: 0.2765
2024-06-03 10:30:55 [INFO]: Epoch 048 - training loss: 0.3311, validation loss: 0.2740
2024-06-03 10:30:57 [INFO]: Epoch 049 - training loss: 0.3201, validation loss: 0.2823
2024-06-03 10:30:58 [INFO]: Epoch 050 - training loss: 0.3196, validation loss: 0.2893
2024-06-03 10:31:00 [INFO]: Epoch 051 - training loss: 0.3340, validation loss: 0.2827
2024-06-03 10:31:01 [INFO]: Epoch 052 - training loss: 0.3464, validation loss: 0.2741
2024-06-03 10:31:03 [INFO]: Epoch 053 - training loss: 0.3442, validation loss: 0.3253
2024-06-03 10:31:05 [INFO]: Epoch 054 - training loss: 0.3324, validation loss: 0.2791
2024-06-03 10:31:06 [INFO]: Epoch 055 - training loss: 0.3182, validation loss: 0.2788
2024-06-03 10:31:08 [INFO]: Epoch 056 - training loss: 0.3160, validation loss: 0.2994
2024-06-03 10:31:09 [INFO]: Epoch 057 - training loss: 0.3072, validation loss: 0.2899
2024-06-03 10:31:11 [INFO]: Epoch 058 - training loss: 0.3111, validation loss: 0.2616
2024-06-03 10:31:12 [INFO]: Epoch 059 - training loss: 0.3026, validation loss: 0.2690
2024-06-03 10:31:14 [INFO]: Epoch 060 - training loss: 0.2991, validation loss: 0.2785
2024-06-03 10:31:16 [INFO]: Epoch 061 - training loss: 0.3009, validation loss: 0.2899
2024-06-03 10:31:17 [INFO]: Epoch 062 - training loss: 0.2958, validation loss: 0.2769
2024-06-03 10:31:19 [INFO]: Epoch 063 - training loss: 0.3017, validation loss: 0.2656
2024-06-03 10:31:20 [INFO]: Epoch 064 - training loss: 0.2947, validation loss: 0.2917
2024-06-03 10:31:22 [INFO]: Epoch 065 - training loss: 0.2943, validation loss: 0.2694
2024-06-03 10:31:24 [INFO]: Epoch 066 - training loss: 0.2891, validation loss: 0.2779
2024-06-03 10:31:25 [INFO]: Epoch 067 - training loss: 0.2713, validation loss: 0.2624
2024-06-03 10:31:27 [INFO]: Epoch 068 - training loss: 0.2933, validation loss: 0.3001
2024-06-03 10:31:27 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:31:27 [INFO]: Finished training. The best model is from epoch#58.
2024-06-03 10:31:27 [INFO]: Saved the model to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_3/20240603_T102929/Pyraformer.pypots
2024-06-03 10:31:28 [INFO]: Successfully saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_3/imputation.pkl
2024-06-03 10:31:28 [INFO]: Round3 - Pyraformer on ETT_h1: MAE=0.4428, MSE=0.4551, MRE=0.5499
2024-06-03 10:31:28 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:31:28 [INFO]: Using the given device: cuda:0
2024-06-03 10:31:29 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_4/20240603_T103128
2024-06-03 10:31:29 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_4/20240603_T103128/tensorboard
2024-06-03 10:31:29 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 15,262,215
2024-06-03 10:31:30 [INFO]: Epoch 001 - training loss: 2.0095, validation loss: 0.8450
2024-06-03 10:31:32 [INFO]: Epoch 002 - training loss: 0.9765, validation loss: 0.5721
2024-06-03 10:31:34 [INFO]: Epoch 003 - training loss: 0.7979, validation loss: 0.5846
2024-06-03 10:31:35 [INFO]: Epoch 004 - training loss: 0.7057, validation loss: 0.5215
2024-06-03 10:31:37 [INFO]: Epoch 005 - training loss: 0.6126, validation loss: 0.4913
2024-06-03 10:31:39 [INFO]: Epoch 006 - training loss: 0.5823, validation loss: 0.4556
2024-06-03 10:31:40 [INFO]: Epoch 007 - training loss: 0.6010, validation loss: 0.4617
2024-06-03 10:31:42 [INFO]: Epoch 008 - training loss: 0.5631, validation loss: 0.4880
2024-06-03 10:31:43 [INFO]: Epoch 009 - training loss: 0.5549, validation loss: 0.4841
2024-06-03 10:31:45 [INFO]: Epoch 010 - training loss: 0.5183, validation loss: 0.5028
2024-06-03 10:31:47 [INFO]: Epoch 011 - training loss: 0.5373, validation loss: 0.4666
2024-06-03 10:31:48 [INFO]: Epoch 012 - training loss: 0.5066, validation loss: 0.4463
2024-06-03 10:31:50 [INFO]: Epoch 013 - training loss: 0.4921, validation loss: 0.4273
2024-06-03 10:31:51 [INFO]: Epoch 014 - training loss: 0.4890, validation loss: 0.4218
2024-06-03 10:31:53 [INFO]: Epoch 015 - training loss: 0.4801, validation loss: 0.4403
2024-06-03 10:31:55 [INFO]: Epoch 016 - training loss: 0.4447, validation loss: 0.4336
2024-06-03 10:31:56 [INFO]: Epoch 017 - training loss: 0.4404, validation loss: 0.4304
2024-06-03 10:31:58 [INFO]: Epoch 018 - training loss: 0.4478, validation loss: 0.4086
2024-06-03 10:31:59 [INFO]: Epoch 019 - training loss: 0.4241, validation loss: 0.3991
2024-06-03 10:32:01 [INFO]: Epoch 020 - training loss: 0.4169, validation loss: 0.3814
2024-06-03 10:32:03 [INFO]: Epoch 021 - training loss: 0.4241, validation loss: 0.3652
2024-06-03 10:32:04 [INFO]: Epoch 022 - training loss: 0.4224, validation loss: 0.3728
2024-06-03 10:32:06 [INFO]: Epoch 023 - training loss: 0.4154, validation loss: 0.3692
2024-06-03 10:32:07 [INFO]: Epoch 024 - training loss: 0.4149, validation loss: 0.3868
2024-06-03 10:32:09 [INFO]: Epoch 025 - training loss: 0.4117, validation loss: 0.3861
2024-06-03 10:32:11 [INFO]: Epoch 026 - training loss: 0.4077, validation loss: 0.3558
2024-06-03 10:32:12 [INFO]: Epoch 027 - training loss: 0.4036, validation loss: 0.3545
2024-06-03 10:32:14 [INFO]: Epoch 028 - training loss: 0.3831, validation loss: 0.3466
2024-06-03 10:32:16 [INFO]: Epoch 029 - training loss: 0.3779, validation loss: 0.3640
2024-06-03 10:32:18 [INFO]: Epoch 030 - training loss: 0.4089, validation loss: 0.3645
2024-06-03 10:32:19 [INFO]: Epoch 031 - training loss: 0.4058, validation loss: 0.3354
2024-06-03 10:32:20 [INFO]: Epoch 032 - training loss: 0.4048, validation loss: 0.3154
2024-06-03 10:32:22 [INFO]: Epoch 033 - training loss: 0.4087, validation loss: 0.3288
2024-06-03 10:32:23 [INFO]: Epoch 034 - training loss: 0.3867, validation loss: 0.3363
2024-06-03 10:32:24 [INFO]: Epoch 035 - training loss: 0.3767, validation loss: 0.3225
2024-06-03 10:32:25 [INFO]: Epoch 036 - training loss: 0.3773, validation loss: 0.3091
2024-06-03 10:32:26 [INFO]: Epoch 037 - training loss: 0.3552, validation loss: 0.3062
2024-06-03 10:32:27 [INFO]: Epoch 038 - training loss: 0.3650, validation loss: 0.3302
2024-06-03 10:32:28 [INFO]: Epoch 039 - training loss: 0.3532, validation loss: 0.3087
2024-06-03 10:32:29 [INFO]: Epoch 040 - training loss: 0.3644, validation loss: 0.3149
2024-06-03 10:32:30 [INFO]: Epoch 041 - training loss: 0.3627, validation loss: 0.3064
2024-06-03 10:32:31 [INFO]: Epoch 042 - training loss: 0.3551, validation loss: 0.3182
2024-06-03 10:32:32 [INFO]: Epoch 043 - training loss: 0.3347, validation loss: 0.2968
2024-06-03 10:32:33 [INFO]: Epoch 044 - training loss: 0.3191, validation loss: 0.2827
2024-06-03 10:32:34 [INFO]: Epoch 045 - training loss: 0.3335, validation loss: 0.2899
2024-06-03 10:32:35 [INFO]: Epoch 046 - training loss: 0.3528, validation loss: 0.2919
2024-06-03 10:32:36 [INFO]: Epoch 047 - training loss: 0.3521, validation loss: 0.2993
2024-06-03 10:32:37 [INFO]: Epoch 048 - training loss: 0.3566, validation loss: 0.2955
2024-06-03 10:32:38 [INFO]: Epoch 049 - training loss: 0.3506, validation loss: 0.2771
2024-06-03 10:32:39 [INFO]: Epoch 050 - training loss: 0.3304, validation loss: 0.2836
2024-06-03 10:32:40 [INFO]: Epoch 051 - training loss: 0.3176, validation loss: 0.3094
2024-06-03 10:32:41 [INFO]: Epoch 052 - training loss: 0.3153, validation loss: 0.2744
2024-06-03 10:32:42 [INFO]: Epoch 053 - training loss: 0.3181, validation loss: 0.3012
2024-06-03 10:32:43 [INFO]: Epoch 054 - training loss: 0.3123, validation loss: 0.2848
2024-06-03 10:32:44 [INFO]: Epoch 055 - training loss: 0.3053, validation loss: 0.2900
2024-06-03 10:32:45 [INFO]: Epoch 056 - training loss: 0.3039, validation loss: 0.2883
2024-06-03 10:32:46 [INFO]: Epoch 057 - training loss: 0.3036, validation loss: 0.2683
2024-06-03 10:32:47 [INFO]: Epoch 058 - training loss: 0.3075, validation loss: 0.2720
2024-06-03 10:32:48 [INFO]: Epoch 059 - training loss: 0.3029, validation loss: 0.3078
2024-06-03 10:32:49 [INFO]: Epoch 060 - training loss: 0.3120, validation loss: 0.2735
2024-06-03 10:32:50 [INFO]: Epoch 061 - training loss: 0.3098, validation loss: 0.2586
2024-06-03 10:32:51 [INFO]: Epoch 062 - training loss: 0.2995, validation loss: 0.2568
2024-06-03 10:32:52 [INFO]: Epoch 063 - training loss: 0.2970, validation loss: 0.2699
2024-06-03 10:32:53 [INFO]: Epoch 064 - training loss: 0.2799, validation loss: 0.2524
2024-06-03 10:32:54 [INFO]: Epoch 065 - training loss: 0.2962, validation loss: 0.2657
2024-06-03 10:32:55 [INFO]: Epoch 066 - training loss: 0.2929, validation loss: 0.2781
2024-06-03 10:32:56 [INFO]: Epoch 067 - training loss: 0.3067, validation loss: 0.2642
2024-06-03 10:32:57 [INFO]: Epoch 068 - training loss: 0.2935, validation loss: 0.2776
2024-06-03 10:32:58 [INFO]: Epoch 069 - training loss: 0.2903, validation loss: 0.2569
2024-06-03 10:32:59 [INFO]: Epoch 070 - training loss: 0.2726, validation loss: 0.2523
2024-06-03 10:33:00 [INFO]: Epoch 071 - training loss: 0.2808, validation loss: 0.2632
2024-06-03 10:33:01 [INFO]: Epoch 072 - training loss: 0.2703, validation loss: 0.2610
2024-06-03 10:33:02 [INFO]: Epoch 073 - training loss: 0.3004, validation loss: 0.2453
2024-06-03 10:33:03 [INFO]: Epoch 074 - training loss: 0.3101, validation loss: 0.2477
2024-06-03 10:33:04 [INFO]: Epoch 075 - training loss: 0.3089, validation loss: 0.2522
2024-06-03 10:33:05 [INFO]: Epoch 076 - training loss: 0.3180, validation loss: 0.2580
2024-06-03 10:33:06 [INFO]: Epoch 077 - training loss: 0.3074, validation loss: 0.2587
2024-06-03 10:33:07 [INFO]: Epoch 078 - training loss: 0.2795, validation loss: 0.2565
2024-06-03 10:33:08 [INFO]: Epoch 079 - training loss: 0.2872, validation loss: 0.2644
2024-06-03 10:33:09 [INFO]: Epoch 080 - training loss: 0.2848, validation loss: 0.2605
2024-06-03 10:33:10 [INFO]: Epoch 081 - training loss: 0.2932, validation loss: 0.2375
2024-06-03 10:33:11 [INFO]: Epoch 082 - training loss: 0.2889, validation loss: 0.2509
2024-06-03 10:33:12 [INFO]: Epoch 083 - training loss: 0.2684, validation loss: 0.2493
2024-06-03 10:33:13 [INFO]: Epoch 084 - training loss: 0.2555, validation loss: 0.2539
2024-06-03 10:33:14 [INFO]: Epoch 085 - training loss: 0.2659, validation loss: 0.2479
2024-06-03 10:33:15 [INFO]: Epoch 086 - training loss: 0.2737, validation loss: 0.2434
2024-06-03 10:33:16 [INFO]: Epoch 087 - training loss: 0.2622, validation loss: 0.2452
2024-06-03 10:33:17 [INFO]: Epoch 088 - training loss: 0.2484, validation loss: 0.2478
2024-06-03 10:33:18 [INFO]: Epoch 089 - training loss: 0.2591, validation loss: 0.2633
2024-06-03 10:33:19 [INFO]: Epoch 090 - training loss: 0.2583, validation loss: 0.2467
2024-06-03 10:33:20 [INFO]: Epoch 091 - training loss: 0.2482, validation loss: 0.2319
2024-06-03 10:33:21 [INFO]: Epoch 092 - training loss: 0.2442, validation loss: 0.2453
2024-06-03 10:33:22 [INFO]: Epoch 093 - training loss: 0.2545, validation loss: 0.2506
2024-06-03 10:33:23 [INFO]: Epoch 094 - training loss: 0.2716, validation loss: 0.2555
2024-06-03 10:33:23 [INFO]: Epoch 095 - training loss: 0.2610, validation loss: 0.2743
2024-06-03 10:33:24 [INFO]: Epoch 096 - training loss: 0.2615, validation loss: 0.2376
2024-06-03 10:33:25 [INFO]: Epoch 097 - training loss: 0.2596, validation loss: 0.2376
2024-06-03 10:33:26 [INFO]: Epoch 098 - training loss: 0.2482, validation loss: 0.2654
2024-06-03 10:33:27 [INFO]: Epoch 099 - training loss: 0.2525, validation loss: 0.2453
2024-06-03 10:33:28 [INFO]: Epoch 100 - training loss: 0.2551, validation loss: 0.2351
2024-06-03 10:33:28 [INFO]: Finished training. The best model is from epoch#91.
2024-06-03 10:33:28 [INFO]: Saved the model to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_4/20240603_T103128/Pyraformer.pypots
2024-06-03 10:33:29 [INFO]: Successfully saved to results_block_rate05/ETT_h1/Pyraformer_ETT_h1/round_4/imputation.pkl
2024-06-03 10:33:29 [INFO]: Round4 - Pyraformer on ETT_h1: MAE=0.3912, MSE=0.3368, MRE=0.4858
2024-06-03 10:33:29 [INFO]: Done! Final results:
Averaged Pyraformer (15,262,215 params) on ETT_h1: MAE=0.4397 ± 0.026281960150951484, MSE=0.4272 ± 0.04737650973385066, MRE=0.5460 ± 0.03263620260699453, average inference time=0.41
