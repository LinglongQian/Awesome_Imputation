2024-06-03 10:17:23 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:17:23 [INFO]: Using the given device: cuda:0
2024-06-03 10:17:25 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_0/20240603_T101725
2024-06-03 10:17:25 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_0/20240603_T101725/tensorboard
2024-06-03 10:17:25 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=128
2024-06-03 10:17:25 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (8) * d_k (128)
2024-06-03 10:17:31 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 23,723,056
2024-06-03 10:17:40 [INFO]: Epoch 001 - training loss: 1.3746, validation loss: 0.7213
2024-06-03 10:17:42 [INFO]: Epoch 002 - training loss: 0.9333, validation loss: 0.5749
2024-06-03 10:17:44 [INFO]: Epoch 003 - training loss: 0.7929, validation loss: 0.5660
2024-06-03 10:17:46 [INFO]: Epoch 004 - training loss: 0.7201, validation loss: 0.5518
2024-06-03 10:17:49 [INFO]: Epoch 005 - training loss: 0.6712, validation loss: 0.5215
2024-06-03 10:17:51 [INFO]: Epoch 006 - training loss: 0.6279, validation loss: 0.5300
2024-06-03 10:17:54 [INFO]: Epoch 007 - training loss: 0.6028, validation loss: 0.5176
2024-06-03 10:17:57 [INFO]: Epoch 008 - training loss: 0.5630, validation loss: 0.5257
2024-06-03 10:18:00 [INFO]: Epoch 009 - training loss: 0.5386, validation loss: 0.5162
2024-06-03 10:18:03 [INFO]: Epoch 010 - training loss: 0.5317, validation loss: 0.5095
2024-06-03 10:18:05 [INFO]: Epoch 011 - training loss: 0.5191, validation loss: 0.5025
2024-06-03 10:18:08 [INFO]: Epoch 012 - training loss: 0.5050, validation loss: 0.4987
2024-06-03 10:18:10 [INFO]: Epoch 013 - training loss: 0.4918, validation loss: 0.4925
2024-06-03 10:18:12 [INFO]: Epoch 014 - training loss: 0.4843, validation loss: 0.5057
2024-06-03 10:18:15 [INFO]: Epoch 015 - training loss: 0.4825, validation loss: 0.5010
2024-06-03 10:18:17 [INFO]: Epoch 016 - training loss: 0.4830, validation loss: 0.4889
2024-06-03 10:18:20 [INFO]: Epoch 017 - training loss: 0.4683, validation loss: 0.4875
2024-06-03 10:18:23 [INFO]: Epoch 018 - training loss: 0.4593, validation loss: 0.4843
2024-06-03 10:18:25 [INFO]: Epoch 019 - training loss: 0.4652, validation loss: 0.4797
2024-06-03 10:18:28 [INFO]: Epoch 020 - training loss: 0.4560, validation loss: 0.4834
2024-06-03 10:18:31 [INFO]: Epoch 021 - training loss: 0.4570, validation loss: 0.4903
2024-06-03 10:18:34 [INFO]: Epoch 022 - training loss: 0.4537, validation loss: 0.4769
2024-06-03 10:18:36 [INFO]: Epoch 023 - training loss: 0.4467, validation loss: 0.4792
2024-06-03 10:18:39 [INFO]: Epoch 024 - training loss: 0.4431, validation loss: 0.4845
2024-06-03 10:18:42 [INFO]: Epoch 025 - training loss: 0.4428, validation loss: 0.4548
2024-06-03 10:18:45 [INFO]: Epoch 026 - training loss: 0.4407, validation loss: 0.4760
2024-06-03 10:18:47 [INFO]: Epoch 027 - training loss: 0.4342, validation loss: 0.4853
2024-06-03 10:18:50 [INFO]: Epoch 028 - training loss: 0.4239, validation loss: 0.4610
2024-06-03 10:18:52 [INFO]: Epoch 029 - training loss: 0.4298, validation loss: 0.4719
2024-06-03 10:18:56 [INFO]: Epoch 030 - training loss: 0.4274, validation loss: 0.4544
2024-06-03 10:18:58 [INFO]: Epoch 031 - training loss: 0.4212, validation loss: 0.4620
2024-06-03 10:19:00 [INFO]: Epoch 032 - training loss: 0.4231, validation loss: 0.4651
2024-06-03 10:19:02 [INFO]: Epoch 033 - training loss: 0.4214, validation loss: 0.4516
2024-06-03 10:19:05 [INFO]: Epoch 034 - training loss: 0.4164, validation loss: 0.4589
2024-06-03 10:19:07 [INFO]: Epoch 035 - training loss: 0.4151, validation loss: 0.4541
2024-06-03 10:19:09 [INFO]: Epoch 036 - training loss: 0.4102, validation loss: 0.4580
2024-06-03 10:19:11 [INFO]: Epoch 037 - training loss: 0.4035, validation loss: 0.4474
2024-06-03 10:19:14 [INFO]: Epoch 038 - training loss: 0.4022, validation loss: 0.4555
2024-06-03 10:19:16 [INFO]: Epoch 039 - training loss: 0.4113, validation loss: 0.4703
2024-06-03 10:19:18 [INFO]: Epoch 040 - training loss: 0.4032, validation loss: 0.4292
2024-06-03 10:19:20 [INFO]: Epoch 041 - training loss: 0.3997, validation loss: 0.4472
2024-06-03 10:19:23 [INFO]: Epoch 042 - training loss: 0.4026, validation loss: 0.4572
2024-06-03 10:19:25 [INFO]: Epoch 043 - training loss: 0.3955, validation loss: 0.4380
2024-06-03 10:19:28 [INFO]: Epoch 044 - training loss: 0.3895, validation loss: 0.4356
2024-06-03 10:19:31 [INFO]: Epoch 045 - training loss: 0.3852, validation loss: 0.4442
2024-06-03 10:19:33 [INFO]: Epoch 046 - training loss: 0.3901, validation loss: 0.4395
2024-06-03 10:19:36 [INFO]: Epoch 047 - training loss: 0.3936, validation loss: 0.4447
2024-06-03 10:19:39 [INFO]: Epoch 048 - training loss: 0.3928, validation loss: 0.4329
2024-06-03 10:19:42 [INFO]: Epoch 049 - training loss: 0.3889, validation loss: 0.4329
2024-06-03 10:19:45 [INFO]: Epoch 050 - training loss: 0.3910, validation loss: 0.4269
2024-06-03 10:19:47 [INFO]: Epoch 051 - training loss: 0.3827, validation loss: 0.4495
2024-06-03 10:19:50 [INFO]: Epoch 052 - training loss: 0.3876, validation loss: 0.4435
2024-06-03 10:19:52 [INFO]: Epoch 053 - training loss: 0.3823, validation loss: 0.4389
2024-06-03 10:19:55 [INFO]: Epoch 054 - training loss: 0.3763, validation loss: 0.4271
2024-06-03 10:19:57 [INFO]: Epoch 055 - training loss: 0.3803, validation loss: 0.4177
2024-06-03 10:20:00 [INFO]: Epoch 056 - training loss: 0.3702, validation loss: 0.4213
2024-06-03 10:20:03 [INFO]: Epoch 057 - training loss: 0.3702, validation loss: 0.4284
2024-06-03 10:20:06 [INFO]: Epoch 058 - training loss: 0.3713, validation loss: 0.4314
2024-06-03 10:20:08 [INFO]: Epoch 059 - training loss: 0.3638, validation loss: 0.4333
2024-06-03 10:20:11 [INFO]: Epoch 060 - training loss: 0.3646, validation loss: 0.4195
2024-06-03 10:20:13 [INFO]: Epoch 061 - training loss: 0.3564, validation loss: 0.4182
2024-06-03 10:20:16 [INFO]: Epoch 062 - training loss: 0.3668, validation loss: 0.4269
2024-06-03 10:20:18 [INFO]: Epoch 063 - training loss: 0.3627, validation loss: 0.4284
2024-06-03 10:20:21 [INFO]: Epoch 064 - training loss: 0.3521, validation loss: 0.4203
2024-06-03 10:20:24 [INFO]: Epoch 065 - training loss: 0.3611, validation loss: 0.4187
2024-06-03 10:20:24 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:20:24 [INFO]: Finished training. The best model is from epoch#55.
2024-06-03 10:20:25 [INFO]: Saved the model to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_0/20240603_T101725/iTransformer.pypots
2024-06-03 10:20:27 [INFO]: Successfully saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_0/imputation.pkl
2024-06-03 10:20:27 [INFO]: Round0 - iTransformer on ETT_h1: MAE=0.5133, MSE=0.5333, MRE=0.6374
2024-06-03 10:20:27 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:20:27 [INFO]: Using the given device: cuda:0
2024-06-03 10:20:27 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_1/20240603_T102027
2024-06-03 10:20:27 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_1/20240603_T102027/tensorboard
2024-06-03 10:20:27 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=128
2024-06-03 10:20:27 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (8) * d_k (128)
2024-06-03 10:20:30 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 23,723,056
2024-06-03 10:20:32 [INFO]: Epoch 001 - training loss: 1.3986, validation loss: 0.6976
2024-06-03 10:20:34 [INFO]: Epoch 002 - training loss: 0.9492, validation loss: 0.5677
2024-06-03 10:20:37 [INFO]: Epoch 003 - training loss: 0.8033, validation loss: 0.5708
2024-06-03 10:20:39 [INFO]: Epoch 004 - training loss: 0.7367, validation loss: 0.5677
2024-06-03 10:20:42 [INFO]: Epoch 005 - training loss: 0.6829, validation loss: 0.5141
2024-06-03 10:20:44 [INFO]: Epoch 006 - training loss: 0.6331, validation loss: 0.5303
2024-06-03 10:20:47 [INFO]: Epoch 007 - training loss: 0.6071, validation loss: 0.5286
2024-06-03 10:20:49 [INFO]: Epoch 008 - training loss: 0.5770, validation loss: 0.5215
2024-06-03 10:20:51 [INFO]: Epoch 009 - training loss: 0.5596, validation loss: 0.5235
2024-06-03 10:20:54 [INFO]: Epoch 010 - training loss: 0.5342, validation loss: 0.5122
2024-06-03 10:20:57 [INFO]: Epoch 011 - training loss: 0.5215, validation loss: 0.5224
2024-06-03 10:20:59 [INFO]: Epoch 012 - training loss: 0.5224, validation loss: 0.5065
2024-06-03 10:21:02 [INFO]: Epoch 013 - training loss: 0.4985, validation loss: 0.5111
2024-06-03 10:21:04 [INFO]: Epoch 014 - training loss: 0.4901, validation loss: 0.4948
2024-06-03 10:21:07 [INFO]: Epoch 015 - training loss: 0.4807, validation loss: 0.5007
2024-06-03 10:21:09 [INFO]: Epoch 016 - training loss: 0.4774, validation loss: 0.4804
2024-06-03 10:21:12 [INFO]: Epoch 017 - training loss: 0.4724, validation loss: 0.5023
2024-06-03 10:21:14 [INFO]: Epoch 018 - training loss: 0.4834, validation loss: 0.4759
2024-06-03 10:21:17 [INFO]: Epoch 019 - training loss: 0.4690, validation loss: 0.5000
2024-06-03 10:21:19 [INFO]: Epoch 020 - training loss: 0.4599, validation loss: 0.4782
2024-06-03 10:21:21 [INFO]: Epoch 021 - training loss: 0.4636, validation loss: 0.4833
2024-06-03 10:21:23 [INFO]: Epoch 022 - training loss: 0.4489, validation loss: 0.4946
2024-06-03 10:21:25 [INFO]: Epoch 023 - training loss: 0.4615, validation loss: 0.4775
2024-06-03 10:21:27 [INFO]: Epoch 024 - training loss: 0.4446, validation loss: 0.4857
2024-06-03 10:21:29 [INFO]: Epoch 025 - training loss: 0.4439, validation loss: 0.4740
2024-06-03 10:21:31 [INFO]: Epoch 026 - training loss: 0.4354, validation loss: 0.4588
2024-06-03 10:21:33 [INFO]: Epoch 027 - training loss: 0.4381, validation loss: 0.4843
2024-06-03 10:21:35 [INFO]: Epoch 028 - training loss: 0.4286, validation loss: 0.4673
2024-06-03 10:21:37 [INFO]: Epoch 029 - training loss: 0.4321, validation loss: 0.4581
2024-06-03 10:21:39 [INFO]: Epoch 030 - training loss: 0.4348, validation loss: 0.4678
2024-06-03 10:21:40 [INFO]: Epoch 031 - training loss: 0.4241, validation loss: 0.4641
2024-06-03 10:21:43 [INFO]: Epoch 032 - training loss: 0.4158, validation loss: 0.4605
2024-06-03 10:21:45 [INFO]: Epoch 033 - training loss: 0.4237, validation loss: 0.4643
2024-06-03 10:21:48 [INFO]: Epoch 034 - training loss: 0.4187, validation loss: 0.4583
2024-06-03 10:21:50 [INFO]: Epoch 035 - training loss: 0.4121, validation loss: 0.4657
2024-06-03 10:21:52 [INFO]: Epoch 036 - training loss: 0.4102, validation loss: 0.4554
2024-06-03 10:21:55 [INFO]: Epoch 037 - training loss: 0.4122, validation loss: 0.4574
2024-06-03 10:21:57 [INFO]: Epoch 038 - training loss: 0.4140, validation loss: 0.4567
2024-06-03 10:21:59 [INFO]: Epoch 039 - training loss: 0.4071, validation loss: 0.4561
2024-06-03 10:22:01 [INFO]: Epoch 040 - training loss: 0.4130, validation loss: 0.4453
2024-06-03 10:22:03 [INFO]: Epoch 041 - training loss: 0.3974, validation loss: 0.4384
2024-06-03 10:22:06 [INFO]: Epoch 042 - training loss: 0.3962, validation loss: 0.4441
2024-06-03 10:22:08 [INFO]: Epoch 043 - training loss: 0.3969, validation loss: 0.4535
2024-06-03 10:22:10 [INFO]: Epoch 044 - training loss: 0.3982, validation loss: 0.4522
2024-06-03 10:22:13 [INFO]: Epoch 045 - training loss: 0.3906, validation loss: 0.4387
2024-06-03 10:22:15 [INFO]: Epoch 046 - training loss: 0.3879, validation loss: 0.4441
2024-06-03 10:22:17 [INFO]: Epoch 047 - training loss: 0.3862, validation loss: 0.4588
2024-06-03 10:22:19 [INFO]: Epoch 048 - training loss: 0.3862, validation loss: 0.4421
2024-06-03 10:22:21 [INFO]: Epoch 049 - training loss: 0.3800, validation loss: 0.4289
2024-06-03 10:22:23 [INFO]: Epoch 050 - training loss: 0.3901, validation loss: 0.4436
2024-06-03 10:22:25 [INFO]: Epoch 051 - training loss: 0.3748, validation loss: 0.4291
2024-06-03 10:22:28 [INFO]: Epoch 052 - training loss: 0.3821, validation loss: 0.4330
2024-06-03 10:22:29 [INFO]: Epoch 053 - training loss: 0.3839, validation loss: 0.4412
2024-06-03 10:22:32 [INFO]: Epoch 054 - training loss: 0.3702, validation loss: 0.4291
2024-06-03 10:22:34 [INFO]: Epoch 055 - training loss: 0.3817, validation loss: 0.4366
2024-06-03 10:22:36 [INFO]: Epoch 056 - training loss: 0.3731, validation loss: 0.4255
2024-06-03 10:22:38 [INFO]: Epoch 057 - training loss: 0.3681, validation loss: 0.4309
2024-06-03 10:22:39 [INFO]: Epoch 058 - training loss: 0.3727, validation loss: 0.4317
2024-06-03 10:22:41 [INFO]: Epoch 059 - training loss: 0.3717, validation loss: 0.4429
2024-06-03 10:22:44 [INFO]: Epoch 060 - training loss: 0.3669, validation loss: 0.4267
2024-06-03 10:22:45 [INFO]: Epoch 061 - training loss: 0.3682, validation loss: 0.4327
2024-06-03 10:22:47 [INFO]: Epoch 062 - training loss: 0.3645, validation loss: 0.4364
2024-06-03 10:22:50 [INFO]: Epoch 063 - training loss: 0.3644, validation loss: 0.4244
2024-06-03 10:22:52 [INFO]: Epoch 064 - training loss: 0.3577, validation loss: 0.4307
2024-06-03 10:22:54 [INFO]: Epoch 065 - training loss: 0.3541, validation loss: 0.4331
2024-06-03 10:22:56 [INFO]: Epoch 066 - training loss: 0.3531, validation loss: 0.4314
2024-06-03 10:22:58 [INFO]: Epoch 067 - training loss: 0.3529, validation loss: 0.4267
2024-06-03 10:23:00 [INFO]: Epoch 068 - training loss: 0.3563, validation loss: 0.4200
2024-06-03 10:23:02 [INFO]: Epoch 069 - training loss: 0.3539, validation loss: 0.4233
2024-06-03 10:23:04 [INFO]: Epoch 070 - training loss: 0.3522, validation loss: 0.4162
2024-06-03 10:23:07 [INFO]: Epoch 071 - training loss: 0.3439, validation loss: 0.4245
2024-06-03 10:23:08 [INFO]: Epoch 072 - training loss: 0.3444, validation loss: 0.4193
2024-06-03 10:23:11 [INFO]: Epoch 073 - training loss: 0.3414, validation loss: 0.4218
2024-06-03 10:23:13 [INFO]: Epoch 074 - training loss: 0.3448, validation loss: 0.4271
2024-06-03 10:23:15 [INFO]: Epoch 075 - training loss: 0.3431, validation loss: 0.4217
2024-06-03 10:23:17 [INFO]: Epoch 076 - training loss: 0.3427, validation loss: 0.4161
2024-06-03 10:23:19 [INFO]: Epoch 077 - training loss: 0.3437, validation loss: 0.4251
2024-06-03 10:23:21 [INFO]: Epoch 078 - training loss: 0.3405, validation loss: 0.4206
2024-06-03 10:23:23 [INFO]: Epoch 079 - training loss: 0.3368, validation loss: 0.4279
2024-06-03 10:23:25 [INFO]: Epoch 080 - training loss: 0.3420, validation loss: 0.4274
2024-06-03 10:23:27 [INFO]: Epoch 081 - training loss: 0.3368, validation loss: 0.4166
2024-06-03 10:23:29 [INFO]: Epoch 082 - training loss: 0.3377, validation loss: 0.4077
2024-06-03 10:23:31 [INFO]: Epoch 083 - training loss: 0.3326, validation loss: 0.4063
2024-06-03 10:23:33 [INFO]: Epoch 084 - training loss: 0.3387, validation loss: 0.4198
2024-06-03 10:23:35 [INFO]: Epoch 085 - training loss: 0.3345, validation loss: 0.4338
2024-06-03 10:23:37 [INFO]: Epoch 086 - training loss: 0.3332, validation loss: 0.4145
2024-06-03 10:23:39 [INFO]: Epoch 087 - training loss: 0.3274, validation loss: 0.4109
2024-06-03 10:23:40 [INFO]: Epoch 088 - training loss: 0.3321, validation loss: 0.4119
2024-06-03 10:23:43 [INFO]: Epoch 089 - training loss: 0.3296, validation loss: 0.4085
2024-06-03 10:23:45 [INFO]: Epoch 090 - training loss: 0.3239, validation loss: 0.4126
2024-06-03 10:23:47 [INFO]: Epoch 091 - training loss: 0.3279, validation loss: 0.4109
2024-06-03 10:23:49 [INFO]: Epoch 092 - training loss: 0.3284, validation loss: 0.4106
2024-06-03 10:23:51 [INFO]: Epoch 093 - training loss: 0.3256, validation loss: 0.4095
2024-06-03 10:23:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:23:51 [INFO]: Finished training. The best model is from epoch#83.
2024-06-03 10:23:52 [INFO]: Saved the model to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_1/20240603_T102027/iTransformer.pypots
2024-06-03 10:23:53 [INFO]: Successfully saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_1/imputation.pkl
2024-06-03 10:23:53 [INFO]: Round1 - iTransformer on ETT_h1: MAE=0.5078, MSE=0.5254, MRE=0.6305
2024-06-03 10:23:53 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:23:53 [INFO]: Using the given device: cuda:0
2024-06-03 10:23:53 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_2/20240603_T102353
2024-06-03 10:23:53 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_2/20240603_T102353/tensorboard
2024-06-03 10:23:53 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=128
2024-06-03 10:23:53 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (8) * d_k (128)
2024-06-03 10:23:56 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 23,723,056
2024-06-03 10:23:58 [INFO]: Epoch 001 - training loss: 1.3704, validation loss: 0.6908
2024-06-03 10:24:00 [INFO]: Epoch 002 - training loss: 0.9366, validation loss: 0.5568
2024-06-03 10:24:02 [INFO]: Epoch 003 - training loss: 0.8043, validation loss: 0.5537
2024-06-03 10:24:04 [INFO]: Epoch 004 - training loss: 0.7349, validation loss: 0.5454
2024-06-03 10:24:06 [INFO]: Epoch 005 - training loss: 0.6813, validation loss: 0.5273
2024-06-03 10:24:08 [INFO]: Epoch 006 - training loss: 0.6334, validation loss: 0.5206
2024-06-03 10:24:10 [INFO]: Epoch 007 - training loss: 0.5914, validation loss: 0.5298
2024-06-03 10:24:11 [INFO]: Epoch 008 - training loss: 0.5769, validation loss: 0.5178
2024-06-03 10:24:13 [INFO]: Epoch 009 - training loss: 0.5504, validation loss: 0.5184
2024-06-03 10:24:15 [INFO]: Epoch 010 - training loss: 0.5401, validation loss: 0.5126
2024-06-03 10:24:17 [INFO]: Epoch 011 - training loss: 0.5167, validation loss: 0.5100
2024-06-03 10:24:19 [INFO]: Epoch 012 - training loss: 0.5065, validation loss: 0.5098
2024-06-03 10:24:20 [INFO]: Epoch 013 - training loss: 0.4969, validation loss: 0.4928
2024-06-03 10:24:22 [INFO]: Epoch 014 - training loss: 0.4977, validation loss: 0.5067
2024-06-03 10:24:24 [INFO]: Epoch 015 - training loss: 0.4871, validation loss: 0.4836
2024-06-03 10:24:26 [INFO]: Epoch 016 - training loss: 0.4680, validation loss: 0.5209
2024-06-03 10:24:28 [INFO]: Epoch 017 - training loss: 0.4724, validation loss: 0.4944
2024-06-03 10:24:29 [INFO]: Epoch 018 - training loss: 0.4677, validation loss: 0.5020
2024-06-03 10:24:31 [INFO]: Epoch 019 - training loss: 0.4656, validation loss: 0.4949
2024-06-03 10:24:33 [INFO]: Epoch 020 - training loss: 0.4589, validation loss: 0.4727
2024-06-03 10:24:36 [INFO]: Epoch 021 - training loss: 0.4511, validation loss: 0.4783
2024-06-03 10:24:37 [INFO]: Epoch 022 - training loss: 0.4573, validation loss: 0.4860
2024-06-03 10:24:39 [INFO]: Epoch 023 - training loss: 0.4509, validation loss: 0.4862
2024-06-03 10:24:41 [INFO]: Epoch 024 - training loss: 0.4396, validation loss: 0.4838
2024-06-03 10:24:43 [INFO]: Epoch 025 - training loss: 0.4434, validation loss: 0.4776
2024-06-03 10:24:44 [INFO]: Epoch 026 - training loss: 0.4437, validation loss: 0.4535
2024-06-03 10:24:46 [INFO]: Epoch 027 - training loss: 0.4428, validation loss: 0.4705
2024-06-03 10:24:48 [INFO]: Epoch 028 - training loss: 0.4367, validation loss: 0.4585
2024-06-03 10:24:49 [INFO]: Epoch 029 - training loss: 0.4248, validation loss: 0.4657
2024-06-03 10:24:51 [INFO]: Epoch 030 - training loss: 0.4270, validation loss: 0.4668
2024-06-03 10:24:52 [INFO]: Epoch 031 - training loss: 0.4217, validation loss: 0.4540
2024-06-03 10:24:54 [INFO]: Epoch 032 - training loss: 0.4189, validation loss: 0.4546
2024-06-03 10:24:56 [INFO]: Epoch 033 - training loss: 0.4281, validation loss: 0.4591
2024-06-03 10:24:58 [INFO]: Epoch 034 - training loss: 0.4187, validation loss: 0.4566
2024-06-03 10:24:59 [INFO]: Epoch 035 - training loss: 0.4130, validation loss: 0.4577
2024-06-03 10:25:01 [INFO]: Epoch 036 - training loss: 0.4109, validation loss: 0.4403
2024-06-03 10:25:03 [INFO]: Epoch 037 - training loss: 0.4079, validation loss: 0.4517
2024-06-03 10:25:05 [INFO]: Epoch 038 - training loss: 0.4033, validation loss: 0.4579
2024-06-03 10:25:07 [INFO]: Epoch 039 - training loss: 0.4071, validation loss: 0.4461
2024-06-03 10:25:08 [INFO]: Epoch 040 - training loss: 0.4017, validation loss: 0.4331
2024-06-03 10:25:11 [INFO]: Epoch 041 - training loss: 0.4052, validation loss: 0.4650
2024-06-03 10:25:13 [INFO]: Epoch 042 - training loss: 0.4023, validation loss: 0.4371
2024-06-03 10:25:14 [INFO]: Epoch 043 - training loss: 0.3958, validation loss: 0.4390
2024-06-03 10:25:16 [INFO]: Epoch 044 - training loss: 0.3887, validation loss: 0.4579
2024-06-03 10:25:18 [INFO]: Epoch 045 - training loss: 0.3948, validation loss: 0.4473
2024-06-03 10:25:20 [INFO]: Epoch 046 - training loss: 0.3950, validation loss: 0.4261
2024-06-03 10:25:22 [INFO]: Epoch 047 - training loss: 0.3876, validation loss: 0.4351
2024-06-03 10:25:24 [INFO]: Epoch 048 - training loss: 0.3797, validation loss: 0.4272
2024-06-03 10:25:26 [INFO]: Epoch 049 - training loss: 0.3804, validation loss: 0.4355
2024-06-03 10:25:28 [INFO]: Epoch 050 - training loss: 0.3787, validation loss: 0.4248
2024-06-03 10:25:30 [INFO]: Epoch 051 - training loss: 0.3764, validation loss: 0.4173
2024-06-03 10:25:31 [INFO]: Epoch 052 - training loss: 0.3764, validation loss: 0.4343
2024-06-03 10:25:33 [INFO]: Epoch 053 - training loss: 0.3709, validation loss: 0.4347
2024-06-03 10:25:35 [INFO]: Epoch 054 - training loss: 0.3777, validation loss: 0.4283
2024-06-03 10:25:37 [INFO]: Epoch 055 - training loss: 0.3703, validation loss: 0.4117
2024-06-03 10:25:39 [INFO]: Epoch 056 - training loss: 0.3751, validation loss: 0.4240
2024-06-03 10:25:40 [INFO]: Epoch 057 - training loss: 0.3694, validation loss: 0.4249
2024-06-03 10:25:42 [INFO]: Epoch 058 - training loss: 0.3728, validation loss: 0.4439
2024-06-03 10:25:44 [INFO]: Epoch 059 - training loss: 0.3679, validation loss: 0.4199
2024-06-03 10:25:46 [INFO]: Epoch 060 - training loss: 0.3619, validation loss: 0.4218
2024-06-03 10:25:48 [INFO]: Epoch 061 - training loss: 0.3682, validation loss: 0.4269
2024-06-03 10:25:49 [INFO]: Epoch 062 - training loss: 0.3612, validation loss: 0.4224
2024-06-03 10:25:52 [INFO]: Epoch 063 - training loss: 0.3687, validation loss: 0.4219
2024-06-03 10:25:53 [INFO]: Epoch 064 - training loss: 0.3651, validation loss: 0.4188
2024-06-03 10:25:55 [INFO]: Epoch 065 - training loss: 0.3550, validation loss: 0.4091
2024-06-03 10:25:57 [INFO]: Epoch 066 - training loss: 0.3532, validation loss: 0.4113
2024-06-03 10:25:59 [INFO]: Epoch 067 - training loss: 0.3512, validation loss: 0.4138
2024-06-03 10:26:01 [INFO]: Epoch 068 - training loss: 0.3583, validation loss: 0.4191
2024-06-03 10:26:03 [INFO]: Epoch 069 - training loss: 0.3492, validation loss: 0.4167
2024-06-03 10:26:05 [INFO]: Epoch 070 - training loss: 0.3441, validation loss: 0.4071
2024-06-03 10:26:07 [INFO]: Epoch 071 - training loss: 0.3483, validation loss: 0.4049
2024-06-03 10:26:08 [INFO]: Epoch 072 - training loss: 0.3415, validation loss: 0.4071
2024-06-03 10:26:10 [INFO]: Epoch 073 - training loss: 0.3438, validation loss: 0.4139
2024-06-03 10:26:12 [INFO]: Epoch 074 - training loss: 0.3467, validation loss: 0.4044
2024-06-03 10:26:13 [INFO]: Epoch 075 - training loss: 0.3411, validation loss: 0.4101
2024-06-03 10:26:15 [INFO]: Epoch 076 - training loss: 0.3425, validation loss: 0.4077
2024-06-03 10:26:17 [INFO]: Epoch 077 - training loss: 0.3455, validation loss: 0.4062
2024-06-03 10:26:18 [INFO]: Epoch 078 - training loss: 0.3429, validation loss: 0.4134
2024-06-03 10:26:20 [INFO]: Epoch 079 - training loss: 0.3335, validation loss: 0.4111
2024-06-03 10:26:22 [INFO]: Epoch 080 - training loss: 0.3415, validation loss: 0.4106
2024-06-03 10:26:24 [INFO]: Epoch 081 - training loss: 0.3352, validation loss: 0.4106
2024-06-03 10:26:26 [INFO]: Epoch 082 - training loss: 0.3341, validation loss: 0.4059
2024-06-03 10:26:28 [INFO]: Epoch 083 - training loss: 0.3279, validation loss: 0.4025
2024-06-03 10:26:29 [INFO]: Epoch 084 - training loss: 0.3325, validation loss: 0.4118
2024-06-03 10:26:31 [INFO]: Epoch 085 - training loss: 0.3324, validation loss: 0.4005
2024-06-03 10:26:33 [INFO]: Epoch 086 - training loss: 0.3315, validation loss: 0.4014
2024-06-03 10:26:35 [INFO]: Epoch 087 - training loss: 0.3316, validation loss: 0.3904
2024-06-03 10:26:36 [INFO]: Epoch 088 - training loss: 0.3312, validation loss: 0.3971
2024-06-03 10:26:38 [INFO]: Epoch 089 - training loss: 0.3333, validation loss: 0.3925
2024-06-03 10:26:40 [INFO]: Epoch 090 - training loss: 0.3293, validation loss: 0.3957
2024-06-03 10:26:42 [INFO]: Epoch 091 - training loss: 0.3273, validation loss: 0.4036
2024-06-03 10:26:44 [INFO]: Epoch 092 - training loss: 0.3328, validation loss: 0.4004
2024-06-03 10:26:46 [INFO]: Epoch 093 - training loss: 0.3218, validation loss: 0.3986
2024-06-03 10:26:48 [INFO]: Epoch 094 - training loss: 0.3265, validation loss: 0.3957
2024-06-03 10:26:49 [INFO]: Epoch 095 - training loss: 0.3311, validation loss: 0.4099
2024-06-03 10:26:51 [INFO]: Epoch 096 - training loss: 0.3266, validation loss: 0.3862
2024-06-03 10:26:53 [INFO]: Epoch 097 - training loss: 0.3223, validation loss: 0.3877
2024-06-03 10:26:54 [INFO]: Epoch 098 - training loss: 0.3104, validation loss: 0.3926
2024-06-03 10:26:56 [INFO]: Epoch 099 - training loss: 0.3184, validation loss: 0.3965
2024-06-03 10:26:58 [INFO]: Epoch 100 - training loss: 0.3153, validation loss: 0.4018
2024-06-03 10:26:58 [INFO]: Finished training. The best model is from epoch#96.
2024-06-03 10:26:59 [INFO]: Saved the model to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_2/20240603_T102353/iTransformer.pypots
2024-06-03 10:27:00 [INFO]: Successfully saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_2/imputation.pkl
2024-06-03 10:27:00 [INFO]: Round2 - iTransformer on ETT_h1: MAE=0.5048, MSE=0.5109, MRE=0.6269
2024-06-03 10:27:00 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:27:00 [INFO]: Using the given device: cuda:0
2024-06-03 10:27:00 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_3/20240603_T102700
2024-06-03 10:27:00 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_3/20240603_T102700/tensorboard
2024-06-03 10:27:00 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=128
2024-06-03 10:27:00 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (8) * d_k (128)
2024-06-03 10:27:01 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 23,723,056
2024-06-03 10:27:03 [INFO]: Epoch 001 - training loss: 1.3714, validation loss: 0.6703
2024-06-03 10:27:05 [INFO]: Epoch 002 - training loss: 0.9375, validation loss: 0.5475
2024-06-03 10:27:06 [INFO]: Epoch 003 - training loss: 0.7974, validation loss: 0.5496
2024-06-03 10:27:08 [INFO]: Epoch 004 - training loss: 0.7119, validation loss: 0.5389
2024-06-03 10:27:10 [INFO]: Epoch 005 - training loss: 0.6655, validation loss: 0.5095
2024-06-03 10:27:12 [INFO]: Epoch 006 - training loss: 0.6241, validation loss: 0.5257
2024-06-03 10:27:13 [INFO]: Epoch 007 - training loss: 0.5992, validation loss: 0.5155
2024-06-03 10:27:15 [INFO]: Epoch 008 - training loss: 0.5733, validation loss: 0.5037
2024-06-03 10:27:17 [INFO]: Epoch 009 - training loss: 0.5552, validation loss: 0.5188
2024-06-03 10:27:19 [INFO]: Epoch 010 - training loss: 0.5256, validation loss: 0.5019
2024-06-03 10:27:21 [INFO]: Epoch 011 - training loss: 0.5188, validation loss: 0.5073
2024-06-03 10:27:22 [INFO]: Epoch 012 - training loss: 0.5060, validation loss: 0.4834
2024-06-03 10:27:24 [INFO]: Epoch 013 - training loss: 0.5026, validation loss: 0.5043
2024-06-03 10:27:26 [INFO]: Epoch 014 - training loss: 0.5002, validation loss: 0.4925
2024-06-03 10:27:28 [INFO]: Epoch 015 - training loss: 0.4787, validation loss: 0.4891
2024-06-03 10:27:29 [INFO]: Epoch 016 - training loss: 0.4743, validation loss: 0.4826
2024-06-03 10:27:31 [INFO]: Epoch 017 - training loss: 0.4736, validation loss: 0.4850
2024-06-03 10:27:33 [INFO]: Epoch 018 - training loss: 0.4684, validation loss: 0.4868
2024-06-03 10:27:35 [INFO]: Epoch 019 - training loss: 0.4567, validation loss: 0.4858
2024-06-03 10:27:37 [INFO]: Epoch 020 - training loss: 0.4600, validation loss: 0.4731
2024-06-03 10:27:39 [INFO]: Epoch 021 - training loss: 0.4618, validation loss: 0.4834
2024-06-03 10:27:40 [INFO]: Epoch 022 - training loss: 0.4410, validation loss: 0.4840
2024-06-03 10:27:42 [INFO]: Epoch 023 - training loss: 0.4513, validation loss: 0.4818
2024-06-03 10:27:44 [INFO]: Epoch 024 - training loss: 0.4411, validation loss: 0.4646
2024-06-03 10:27:46 [INFO]: Epoch 025 - training loss: 0.4395, validation loss: 0.4723
2024-06-03 10:27:47 [INFO]: Epoch 026 - training loss: 0.4311, validation loss: 0.4693
2024-06-03 10:27:49 [INFO]: Epoch 027 - training loss: 0.4383, validation loss: 0.4558
2024-06-03 10:27:50 [INFO]: Epoch 028 - training loss: 0.4333, validation loss: 0.4730
2024-06-03 10:27:52 [INFO]: Epoch 029 - training loss: 0.4316, validation loss: 0.4632
2024-06-03 10:27:54 [INFO]: Epoch 030 - training loss: 0.4255, validation loss: 0.4579
2024-06-03 10:27:56 [INFO]: Epoch 031 - training loss: 0.4267, validation loss: 0.4670
2024-06-03 10:27:58 [INFO]: Epoch 032 - training loss: 0.4206, validation loss: 0.4593
2024-06-03 10:27:59 [INFO]: Epoch 033 - training loss: 0.4176, validation loss: 0.4535
2024-06-03 10:28:01 [INFO]: Epoch 034 - training loss: 0.4116, validation loss: 0.4556
2024-06-03 10:28:03 [INFO]: Epoch 035 - training loss: 0.4136, validation loss: 0.4562
2024-06-03 10:28:05 [INFO]: Epoch 036 - training loss: 0.4110, validation loss: 0.4573
2024-06-03 10:28:06 [INFO]: Epoch 037 - training loss: 0.4074, validation loss: 0.4675
2024-06-03 10:28:08 [INFO]: Epoch 038 - training loss: 0.4073, validation loss: 0.4523
2024-06-03 10:28:10 [INFO]: Epoch 039 - training loss: 0.4047, validation loss: 0.4449
2024-06-03 10:28:11 [INFO]: Epoch 040 - training loss: 0.4041, validation loss: 0.4504
2024-06-03 10:28:13 [INFO]: Epoch 041 - training loss: 0.3890, validation loss: 0.4500
2024-06-03 10:28:15 [INFO]: Epoch 042 - training loss: 0.3939, validation loss: 0.4437
2024-06-03 10:28:16 [INFO]: Epoch 043 - training loss: 0.4021, validation loss: 0.4363
2024-06-03 10:28:18 [INFO]: Epoch 044 - training loss: 0.3915, validation loss: 0.4301
2024-06-03 10:28:20 [INFO]: Epoch 045 - training loss: 0.3941, validation loss: 0.4328
2024-06-03 10:28:22 [INFO]: Epoch 046 - training loss: 0.3918, validation loss: 0.4365
2024-06-03 10:28:23 [INFO]: Epoch 047 - training loss: 0.3955, validation loss: 0.4334
2024-06-03 10:28:25 [INFO]: Epoch 048 - training loss: 0.3862, validation loss: 0.4361
2024-06-03 10:28:27 [INFO]: Epoch 049 - training loss: 0.3900, validation loss: 0.4429
2024-06-03 10:28:28 [INFO]: Epoch 050 - training loss: 0.3832, validation loss: 0.4278
2024-06-03 10:28:29 [INFO]: Epoch 051 - training loss: 0.3886, validation loss: 0.4167
2024-06-03 10:28:31 [INFO]: Epoch 052 - training loss: 0.3862, validation loss: 0.4229
2024-06-03 10:28:32 [INFO]: Epoch 053 - training loss: 0.3863, validation loss: 0.4282
2024-06-03 10:28:33 [INFO]: Epoch 054 - training loss: 0.3827, validation loss: 0.4314
2024-06-03 10:28:35 [INFO]: Epoch 055 - training loss: 0.3784, validation loss: 0.4251
2024-06-03 10:28:36 [INFO]: Epoch 056 - training loss: 0.3766, validation loss: 0.4254
2024-06-03 10:28:37 [INFO]: Epoch 057 - training loss: 0.3716, validation loss: 0.4150
2024-06-03 10:28:38 [INFO]: Epoch 058 - training loss: 0.3712, validation loss: 0.4141
2024-06-03 10:28:40 [INFO]: Epoch 059 - training loss: 0.3702, validation loss: 0.4227
2024-06-03 10:28:41 [INFO]: Epoch 060 - training loss: 0.3643, validation loss: 0.4283
2024-06-03 10:28:42 [INFO]: Epoch 061 - training loss: 0.3623, validation loss: 0.4297
2024-06-03 10:28:43 [INFO]: Epoch 062 - training loss: 0.3601, validation loss: 0.4119
2024-06-03 10:28:44 [INFO]: Epoch 063 - training loss: 0.3693, validation loss: 0.4156
2024-06-03 10:28:46 [INFO]: Epoch 064 - training loss: 0.3600, validation loss: 0.4317
2024-06-03 10:28:47 [INFO]: Epoch 065 - training loss: 0.3650, validation loss: 0.4210
2024-06-03 10:28:48 [INFO]: Epoch 066 - training loss: 0.3541, validation loss: 0.4181
2024-06-03 10:28:49 [INFO]: Epoch 067 - training loss: 0.3561, validation loss: 0.4152
2024-06-03 10:28:50 [INFO]: Epoch 068 - training loss: 0.3485, validation loss: 0.4142
2024-06-03 10:28:51 [INFO]: Epoch 069 - training loss: 0.3542, validation loss: 0.4186
2024-06-03 10:28:53 [INFO]: Epoch 070 - training loss: 0.3514, validation loss: 0.4196
2024-06-03 10:28:54 [INFO]: Epoch 071 - training loss: 0.3525, validation loss: 0.4216
2024-06-03 10:28:55 [INFO]: Epoch 072 - training loss: 0.3460, validation loss: 0.4047
2024-06-03 10:28:56 [INFO]: Epoch 073 - training loss: 0.3461, validation loss: 0.4070
2024-06-03 10:28:57 [INFO]: Epoch 074 - training loss: 0.3491, validation loss: 0.4033
2024-06-03 10:28:58 [INFO]: Epoch 075 - training loss: 0.3440, validation loss: 0.4117
2024-06-03 10:29:00 [INFO]: Epoch 076 - training loss: 0.3496, validation loss: 0.4059
2024-06-03 10:29:00 [INFO]: Epoch 077 - training loss: 0.3395, validation loss: 0.4087
2024-06-03 10:29:02 [INFO]: Epoch 078 - training loss: 0.3417, validation loss: 0.4116
2024-06-03 10:29:03 [INFO]: Epoch 079 - training loss: 0.3430, validation loss: 0.4046
2024-06-03 10:29:04 [INFO]: Epoch 080 - training loss: 0.3327, validation loss: 0.4043
2024-06-03 10:29:05 [INFO]: Epoch 081 - training loss: 0.3348, validation loss: 0.3975
2024-06-03 10:29:06 [INFO]: Epoch 082 - training loss: 0.3312, validation loss: 0.4166
2024-06-03 10:29:07 [INFO]: Epoch 083 - training loss: 0.3316, validation loss: 0.4077
2024-06-03 10:29:08 [INFO]: Epoch 084 - training loss: 0.3328, validation loss: 0.4098
2024-06-03 10:29:09 [INFO]: Epoch 085 - training loss: 0.3272, validation loss: 0.4047
2024-06-03 10:29:10 [INFO]: Epoch 086 - training loss: 0.3289, validation loss: 0.4089
2024-06-03 10:29:11 [INFO]: Epoch 087 - training loss: 0.3267, validation loss: 0.4182
2024-06-03 10:29:12 [INFO]: Epoch 088 - training loss: 0.3313, validation loss: 0.3971
2024-06-03 10:29:13 [INFO]: Epoch 089 - training loss: 0.3308, validation loss: 0.4154
2024-06-03 10:29:14 [INFO]: Epoch 090 - training loss: 0.3301, validation loss: 0.4028
2024-06-03 10:29:16 [INFO]: Epoch 091 - training loss: 0.3267, validation loss: 0.4007
2024-06-03 10:29:17 [INFO]: Epoch 092 - training loss: 0.3243, validation loss: 0.3976
2024-06-03 10:29:18 [INFO]: Epoch 093 - training loss: 0.3258, validation loss: 0.4010
2024-06-03 10:29:19 [INFO]: Epoch 094 - training loss: 0.3198, validation loss: 0.4164
2024-06-03 10:29:20 [INFO]: Epoch 095 - training loss: 0.3176, validation loss: 0.4048
2024-06-03 10:29:21 [INFO]: Epoch 096 - training loss: 0.3209, validation loss: 0.4060
2024-06-03 10:29:22 [INFO]: Epoch 097 - training loss: 0.3158, validation loss: 0.4016
2024-06-03 10:29:23 [INFO]: Epoch 098 - training loss: 0.3182, validation loss: 0.3967
2024-06-03 10:29:24 [INFO]: Epoch 099 - training loss: 0.3176, validation loss: 0.3931
2024-06-03 10:29:25 [INFO]: Epoch 100 - training loss: 0.3155, validation loss: 0.3975
2024-06-03 10:29:25 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 10:29:26 [INFO]: Saved the model to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_3/20240603_T102700/iTransformer.pypots
2024-06-03 10:29:26 [INFO]: Successfully saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_3/imputation.pkl
2024-06-03 10:29:26 [INFO]: Round3 - iTransformer on ETT_h1: MAE=0.5001, MSE=0.5069, MRE=0.6210
2024-06-03 10:29:26 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:29:26 [INFO]: Using the given device: cuda:0
2024-06-03 10:29:26 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_4/20240603_T102926
2024-06-03 10:29:26 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_4/20240603_T102926/tensorboard
2024-06-03 10:29:26 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=256, n_heads=8, d_k=128
2024-06-03 10:29:26 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (8) * d_k (128)
2024-06-03 10:29:27 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 23,723,056
2024-06-03 10:29:28 [INFO]: Epoch 001 - training loss: 1.3817, validation loss: 0.7152
2024-06-03 10:29:29 [INFO]: Epoch 002 - training loss: 0.9311, validation loss: 0.5620
2024-06-03 10:29:30 [INFO]: Epoch 003 - training loss: 0.8066, validation loss: 0.5562
2024-06-03 10:29:31 [INFO]: Epoch 004 - training loss: 0.7406, validation loss: 0.5576
2024-06-03 10:29:32 [INFO]: Epoch 005 - training loss: 0.6822, validation loss: 0.5300
2024-06-03 10:29:34 [INFO]: Epoch 006 - training loss: 0.6445, validation loss: 0.5319
2024-06-03 10:29:34 [INFO]: Epoch 007 - training loss: 0.6082, validation loss: 0.5345
2024-06-03 10:29:36 [INFO]: Epoch 008 - training loss: 0.5866, validation loss: 0.5318
2024-06-03 10:29:37 [INFO]: Epoch 009 - training loss: 0.5597, validation loss: 0.5234
2024-06-03 10:29:38 [INFO]: Epoch 010 - training loss: 0.5426, validation loss: 0.5124
2024-06-03 10:29:39 [INFO]: Epoch 011 - training loss: 0.5215, validation loss: 0.5148
2024-06-03 10:29:40 [INFO]: Epoch 012 - training loss: 0.5126, validation loss: 0.5035
2024-06-03 10:29:41 [INFO]: Epoch 013 - training loss: 0.5087, validation loss: 0.5049
2024-06-03 10:29:42 [INFO]: Epoch 014 - training loss: 0.4905, validation loss: 0.5045
2024-06-03 10:29:43 [INFO]: Epoch 015 - training loss: 0.4878, validation loss: 0.4961
2024-06-03 10:29:44 [INFO]: Epoch 016 - training loss: 0.4892, validation loss: 0.5053
2024-06-03 10:29:45 [INFO]: Epoch 017 - training loss: 0.4799, validation loss: 0.4896
2024-06-03 10:29:46 [INFO]: Epoch 018 - training loss: 0.4744, validation loss: 0.5068
2024-06-03 10:29:47 [INFO]: Epoch 019 - training loss: 0.4711, validation loss: 0.4922
2024-06-03 10:29:48 [INFO]: Epoch 020 - training loss: 0.4644, validation loss: 0.5009
2024-06-03 10:29:49 [INFO]: Epoch 021 - training loss: 0.4557, validation loss: 0.5029
2024-06-03 10:29:50 [INFO]: Epoch 022 - training loss: 0.4568, validation loss: 0.4823
2024-06-03 10:29:50 [INFO]: Epoch 023 - training loss: 0.4491, validation loss: 0.4856
2024-06-03 10:29:51 [INFO]: Epoch 024 - training loss: 0.4395, validation loss: 0.4718
2024-06-03 10:29:52 [INFO]: Epoch 025 - training loss: 0.4434, validation loss: 0.4835
2024-06-03 10:29:53 [INFO]: Epoch 026 - training loss: 0.4484, validation loss: 0.4790
2024-06-03 10:29:54 [INFO]: Epoch 027 - training loss: 0.4340, validation loss: 0.4626
2024-06-03 10:29:55 [INFO]: Epoch 028 - training loss: 0.4317, validation loss: 0.4746
2024-06-03 10:29:56 [INFO]: Epoch 029 - training loss: 0.4199, validation loss: 0.4686
2024-06-03 10:29:57 [INFO]: Epoch 030 - training loss: 0.4203, validation loss: 0.4802
2024-06-03 10:29:58 [INFO]: Epoch 031 - training loss: 0.4275, validation loss: 0.4605
2024-06-03 10:29:59 [INFO]: Epoch 032 - training loss: 0.4196, validation loss: 0.4729
2024-06-03 10:30:00 [INFO]: Epoch 033 - training loss: 0.4206, validation loss: 0.4734
2024-06-03 10:30:01 [INFO]: Epoch 034 - training loss: 0.4222, validation loss: 0.4677
2024-06-03 10:30:02 [INFO]: Epoch 035 - training loss: 0.4096, validation loss: 0.4724
2024-06-03 10:30:03 [INFO]: Epoch 036 - training loss: 0.4151, validation loss: 0.4566
2024-06-03 10:30:04 [INFO]: Epoch 037 - training loss: 0.4076, validation loss: 0.4493
2024-06-03 10:30:05 [INFO]: Epoch 038 - training loss: 0.4174, validation loss: 0.4725
2024-06-03 10:30:06 [INFO]: Epoch 039 - training loss: 0.4081, validation loss: 0.4634
2024-06-03 10:30:07 [INFO]: Epoch 040 - training loss: 0.4050, validation loss: 0.4487
2024-06-03 10:30:08 [INFO]: Epoch 041 - training loss: 0.3981, validation loss: 0.4597
2024-06-03 10:30:09 [INFO]: Epoch 042 - training loss: 0.4022, validation loss: 0.4559
2024-06-03 10:30:10 [INFO]: Epoch 043 - training loss: 0.3958, validation loss: 0.4434
2024-06-03 10:30:11 [INFO]: Epoch 044 - training loss: 0.3857, validation loss: 0.4516
2024-06-03 10:30:12 [INFO]: Epoch 045 - training loss: 0.3979, validation loss: 0.4466
2024-06-03 10:30:13 [INFO]: Epoch 046 - training loss: 0.3814, validation loss: 0.4561
2024-06-03 10:30:14 [INFO]: Epoch 047 - training loss: 0.3833, validation loss: 0.4401
2024-06-03 10:30:15 [INFO]: Epoch 048 - training loss: 0.3863, validation loss: 0.4399
2024-06-03 10:30:16 [INFO]: Epoch 049 - training loss: 0.3805, validation loss: 0.4415
2024-06-03 10:30:17 [INFO]: Epoch 050 - training loss: 0.3862, validation loss: 0.4402
2024-06-03 10:30:18 [INFO]: Epoch 051 - training loss: 0.3909, validation loss: 0.4490
2024-06-03 10:30:19 [INFO]: Epoch 052 - training loss: 0.3824, validation loss: 0.4407
2024-06-03 10:30:20 [INFO]: Epoch 053 - training loss: 0.3858, validation loss: 0.4428
2024-06-03 10:30:21 [INFO]: Epoch 054 - training loss: 0.3717, validation loss: 0.4346
2024-06-03 10:30:22 [INFO]: Epoch 055 - training loss: 0.3733, validation loss: 0.4299
2024-06-03 10:30:23 [INFO]: Epoch 056 - training loss: 0.3635, validation loss: 0.4310
2024-06-03 10:30:24 [INFO]: Epoch 057 - training loss: 0.3688, validation loss: 0.4408
2024-06-03 10:30:25 [INFO]: Epoch 058 - training loss: 0.3671, validation loss: 0.4391
2024-06-03 10:30:26 [INFO]: Epoch 059 - training loss: 0.3652, validation loss: 0.4292
2024-06-03 10:30:27 [INFO]: Epoch 060 - training loss: 0.3640, validation loss: 0.4212
2024-06-03 10:30:28 [INFO]: Epoch 061 - training loss: 0.3634, validation loss: 0.4200
2024-06-03 10:30:28 [INFO]: Epoch 062 - training loss: 0.3550, validation loss: 0.4250
2024-06-03 10:30:29 [INFO]: Epoch 063 - training loss: 0.3594, validation loss: 0.4228
2024-06-03 10:30:30 [INFO]: Epoch 064 - training loss: 0.3619, validation loss: 0.4240
2024-06-03 10:30:31 [INFO]: Epoch 065 - training loss: 0.3550, validation loss: 0.4251
2024-06-03 10:30:32 [INFO]: Epoch 066 - training loss: 0.3560, validation loss: 0.4217
2024-06-03 10:30:33 [INFO]: Epoch 067 - training loss: 0.3524, validation loss: 0.4150
2024-06-03 10:30:34 [INFO]: Epoch 068 - training loss: 0.3537, validation loss: 0.4206
2024-06-03 10:30:35 [INFO]: Epoch 069 - training loss: 0.3465, validation loss: 0.4266
2024-06-03 10:30:36 [INFO]: Epoch 070 - training loss: 0.3466, validation loss: 0.4245
2024-06-03 10:30:37 [INFO]: Epoch 071 - training loss: 0.3532, validation loss: 0.4235
2024-06-03 10:30:38 [INFO]: Epoch 072 - training loss: 0.3533, validation loss: 0.4228
2024-06-03 10:30:39 [INFO]: Epoch 073 - training loss: 0.3445, validation loss: 0.4176
2024-06-03 10:30:40 [INFO]: Epoch 074 - training loss: 0.3454, validation loss: 0.4162
2024-06-03 10:30:41 [INFO]: Epoch 075 - training loss: 0.3400, validation loss: 0.4280
2024-06-03 10:30:42 [INFO]: Epoch 076 - training loss: 0.3359, validation loss: 0.4248
2024-06-03 10:30:43 [INFO]: Epoch 077 - training loss: 0.3463, validation loss: 0.4223
2024-06-03 10:30:43 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:30:43 [INFO]: Finished training. The best model is from epoch#67.
2024-06-03 10:30:44 [INFO]: Saved the model to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_4/20240603_T102926/iTransformer.pypots
2024-06-03 10:30:44 [INFO]: Successfully saved to results_block_rate05/ETT_h1/iTransformer_ETT_h1/round_4/imputation.pkl
2024-06-03 10:30:44 [INFO]: Round4 - iTransformer on ETT_h1: MAE=0.5216, MSE=0.5473, MRE=0.6477
2024-06-03 10:30:44 [INFO]: Done! Final results:
Averaged iTransformer (23,723,056 params) on ETT_h1: MAE=0.5095 ± 0.007402462062265183, MSE=0.5247 ± 0.014790884112949982, MRE=0.6327 ± 0.009192170228822528, average inference time=0.16
