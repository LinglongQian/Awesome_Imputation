2024-06-03 10:17:21 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:17:21 [INFO]: Using the given device: cuda:0
2024-06-03 10:17:25 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_0/20240603_T101725
2024-06-03 10:17:25 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_0/20240603_T101725/tensorboard
2024-06-03 10:17:29 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 409,407
2024-06-03 10:17:42 [INFO]: Epoch 001 - training loss: 0.6875, validation loss: 0.7997
2024-06-03 10:17:46 [INFO]: Epoch 002 - training loss: 0.3888, validation loss: 0.6966
2024-06-03 10:17:50 [INFO]: Epoch 003 - training loss: 0.3041, validation loss: 0.7930
2024-06-03 10:17:54 [INFO]: Epoch 004 - training loss: 0.2520, validation loss: 0.7328
2024-06-03 10:17:58 [INFO]: Epoch 005 - training loss: 0.2256, validation loss: 0.7077
2024-06-03 10:18:02 [INFO]: Epoch 006 - training loss: 0.2059, validation loss: 0.6933
2024-06-03 10:18:06 [INFO]: Epoch 007 - training loss: 0.1924, validation loss: 0.6527
2024-06-03 10:18:10 [INFO]: Epoch 008 - training loss: 0.1839, validation loss: 0.6404
2024-06-03 10:18:14 [INFO]: Epoch 009 - training loss: 0.1787, validation loss: 0.6441
2024-06-03 10:18:18 [INFO]: Epoch 010 - training loss: 0.1724, validation loss: 0.6568
2024-06-03 10:18:23 [INFO]: Epoch 011 - training loss: 0.1678, validation loss: 0.6161
2024-06-03 10:18:26 [INFO]: Epoch 012 - training loss: 0.1603, validation loss: 0.6084
2024-06-03 10:18:30 [INFO]: Epoch 013 - training loss: 0.1586, validation loss: 0.6305
2024-06-03 10:18:35 [INFO]: Epoch 014 - training loss: 0.1542, validation loss: 0.6228
2024-06-03 10:18:38 [INFO]: Epoch 015 - training loss: 0.1515, validation loss: 0.6245
2024-06-03 10:18:43 [INFO]: Epoch 016 - training loss: 0.1479, validation loss: 0.6221
2024-06-03 10:18:47 [INFO]: Epoch 017 - training loss: 0.1447, validation loss: 0.6213
2024-06-03 10:18:52 [INFO]: Epoch 018 - training loss: 0.1418, validation loss: 0.5827
2024-06-03 10:18:56 [INFO]: Epoch 019 - training loss: 0.1438, validation loss: 0.5629
2024-06-03 10:19:01 [INFO]: Epoch 020 - training loss: 0.1383, validation loss: 0.5768
2024-06-03 10:19:05 [INFO]: Epoch 021 - training loss: 0.1345, validation loss: 0.6067
2024-06-03 10:19:09 [INFO]: Epoch 022 - training loss: 0.1326, validation loss: 0.5566
2024-06-03 10:19:14 [INFO]: Epoch 023 - training loss: 0.1295, validation loss: 0.5509
2024-06-03 10:19:17 [INFO]: Epoch 024 - training loss: 0.1273, validation loss: 0.5943
2024-06-03 10:19:21 [INFO]: Epoch 025 - training loss: 0.1299, validation loss: 0.5572
2024-06-03 10:19:25 [INFO]: Epoch 026 - training loss: 0.1341, validation loss: 0.5798
2024-06-03 10:19:30 [INFO]: Epoch 027 - training loss: 0.1291, validation loss: 0.5477
2024-06-03 10:19:34 [INFO]: Epoch 028 - training loss: 0.1231, validation loss: 0.5520
2024-06-03 10:19:38 [INFO]: Epoch 029 - training loss: 0.1201, validation loss: 0.5222
2024-06-03 10:19:42 [INFO]: Epoch 030 - training loss: 0.1205, validation loss: 0.5802
2024-06-03 10:19:46 [INFO]: Epoch 031 - training loss: 0.1203, validation loss: 0.5133
2024-06-03 10:19:49 [INFO]: Epoch 032 - training loss: 0.1195, validation loss: 0.5036
2024-06-03 10:19:54 [INFO]: Epoch 033 - training loss: 0.1173, validation loss: 0.5169
2024-06-03 10:19:58 [INFO]: Epoch 034 - training loss: 0.1151, validation loss: 0.5405
2024-06-03 10:20:02 [INFO]: Epoch 035 - training loss: 0.1171, validation loss: 0.5443
2024-06-03 10:20:06 [INFO]: Epoch 036 - training loss: 0.1214, validation loss: 0.5237
2024-06-03 10:20:10 [INFO]: Epoch 037 - training loss: 0.1127, validation loss: 0.5177
2024-06-03 10:20:13 [INFO]: Epoch 038 - training loss: 0.1119, validation loss: 0.5099
2024-06-03 10:20:17 [INFO]: Epoch 039 - training loss: 0.1119, validation loss: 0.4928
2024-06-03 10:20:21 [INFO]: Epoch 040 - training loss: 0.1086, validation loss: 0.5210
2024-06-03 10:20:25 [INFO]: Epoch 041 - training loss: 0.1071, validation loss: 0.5401
2024-06-03 10:20:28 [INFO]: Epoch 042 - training loss: 0.1064, validation loss: 0.4968
2024-06-03 10:20:32 [INFO]: Epoch 043 - training loss: 0.1056, validation loss: 0.5406
2024-06-03 10:20:36 [INFO]: Epoch 044 - training loss: 0.1029, validation loss: 0.4944
2024-06-03 10:20:39 [INFO]: Epoch 045 - training loss: 0.1014, validation loss: 0.5020
2024-06-03 10:20:43 [INFO]: Epoch 046 - training loss: 0.1011, validation loss: 0.5100
2024-06-03 10:20:47 [INFO]: Epoch 047 - training loss: 0.0985, validation loss: 0.5354
2024-06-03 10:20:51 [INFO]: Epoch 048 - training loss: 0.0971, validation loss: 0.5044
2024-06-03 10:20:55 [INFO]: Epoch 049 - training loss: 0.0978, validation loss: 0.5224
2024-06-03 10:20:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:20:55 [INFO]: Finished training. The best model is from epoch#39.
2024-06-03 10:20:55 [INFO]: Saved the model to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_0/20240603_T101725/GRUD.pypots
2024-06-03 10:21:03 [INFO]: Successfully saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_0/imputation.pkl
2024-06-03 10:21:03 [INFO]: Round0 - GRUD on ETT_h1: MAE=0.5929, MSE=0.7220, MRE=0.7362
2024-06-03 10:21:03 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:21:03 [INFO]: Using the given device: cuda:0
2024-06-03 10:21:03 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_1/20240603_T102103
2024-06-03 10:21:03 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_1/20240603_T102103/tensorboard
2024-06-03 10:21:03 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 409,407
2024-06-03 10:21:09 [INFO]: Epoch 001 - training loss: 0.7370, validation loss: 0.7389
2024-06-03 10:21:12 [INFO]: Epoch 002 - training loss: 0.4397, validation loss: 0.7389
2024-06-03 10:21:16 [INFO]: Epoch 003 - training loss: 0.3448, validation loss: 0.6515
2024-06-03 10:21:19 [INFO]: Epoch 004 - training loss: 0.2785, validation loss: 0.7290
2024-06-03 10:21:23 [INFO]: Epoch 005 - training loss: 0.2427, validation loss: 0.6514
2024-06-03 10:21:26 [INFO]: Epoch 006 - training loss: 0.2177, validation loss: 0.6779
2024-06-03 10:21:29 [INFO]: Epoch 007 - training loss: 0.2011, validation loss: 0.6243
2024-06-03 10:21:32 [INFO]: Epoch 008 - training loss: 0.1935, validation loss: 0.6063
2024-06-03 10:21:35 [INFO]: Epoch 009 - training loss: 0.1795, validation loss: 0.5985
2024-06-03 10:21:38 [INFO]: Epoch 010 - training loss: 0.1758, validation loss: 0.5828
2024-06-03 10:21:41 [INFO]: Epoch 011 - training loss: 0.1690, validation loss: 0.5976
2024-06-03 10:21:44 [INFO]: Epoch 012 - training loss: 0.1658, validation loss: 0.5781
2024-06-03 10:21:48 [INFO]: Epoch 013 - training loss: 0.1607, validation loss: 0.5593
2024-06-03 10:21:51 [INFO]: Epoch 014 - training loss: 0.1569, validation loss: 0.5697
2024-06-03 10:21:54 [INFO]: Epoch 015 - training loss: 0.1560, validation loss: 0.5485
2024-06-03 10:21:57 [INFO]: Epoch 016 - training loss: 0.1544, validation loss: 0.5943
2024-06-03 10:22:00 [INFO]: Epoch 017 - training loss: 0.1513, validation loss: 0.5592
2024-06-03 10:22:03 [INFO]: Epoch 018 - training loss: 0.1474, validation loss: 0.5531
2024-06-03 10:22:07 [INFO]: Epoch 019 - training loss: 0.1441, validation loss: 0.5451
2024-06-03 10:22:10 [INFO]: Epoch 020 - training loss: 0.1410, validation loss: 0.5067
2024-06-03 10:22:13 [INFO]: Epoch 021 - training loss: 0.1417, validation loss: 0.5535
2024-06-03 10:22:16 [INFO]: Epoch 022 - training loss: 0.1402, validation loss: 0.5102
2024-06-03 10:22:20 [INFO]: Epoch 023 - training loss: 0.1365, validation loss: 0.5018
2024-06-03 10:22:23 [INFO]: Epoch 024 - training loss: 0.1366, validation loss: 0.5449
2024-06-03 10:22:26 [INFO]: Epoch 025 - training loss: 0.1319, validation loss: 0.5167
2024-06-03 10:22:29 [INFO]: Epoch 026 - training loss: 0.1314, validation loss: 0.5110
2024-06-03 10:22:32 [INFO]: Epoch 027 - training loss: 0.1302, validation loss: 0.5455
2024-06-03 10:22:35 [INFO]: Epoch 028 - training loss: 0.1272, validation loss: 0.5179
2024-06-03 10:22:38 [INFO]: Epoch 029 - training loss: 0.1259, validation loss: 0.5026
2024-06-03 10:22:41 [INFO]: Epoch 030 - training loss: 0.1226, validation loss: 0.5453
2024-06-03 10:22:44 [INFO]: Epoch 031 - training loss: 0.1243, validation loss: 0.5285
2024-06-03 10:22:46 [INFO]: Epoch 032 - training loss: 0.1210, validation loss: 0.5480
2024-06-03 10:22:49 [INFO]: Epoch 033 - training loss: 0.1224, validation loss: 0.5097
2024-06-03 10:22:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:22:49 [INFO]: Finished training. The best model is from epoch#23.
2024-06-03 10:22:50 [INFO]: Saved the model to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_1/20240603_T102103/GRUD.pypots
2024-06-03 10:22:55 [INFO]: Successfully saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_1/imputation.pkl
2024-06-03 10:22:55 [INFO]: Round1 - GRUD on ETT_h1: MAE=0.5782, MSE=0.7192, MRE=0.7180
2024-06-03 10:22:55 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:22:55 [INFO]: Using the given device: cuda:0
2024-06-03 10:22:55 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_2/20240603_T102255
2024-06-03 10:22:55 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_2/20240603_T102255/tensorboard
2024-06-03 10:22:55 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 409,407
2024-06-03 10:23:00 [INFO]: Epoch 001 - training loss: 0.7071, validation loss: 0.8011
2024-06-03 10:23:03 [INFO]: Epoch 002 - training loss: 0.4478, validation loss: 0.7075
2024-06-03 10:23:06 [INFO]: Epoch 003 - training loss: 0.3211, validation loss: 0.6898
2024-06-03 10:23:09 [INFO]: Epoch 004 - training loss: 0.2680, validation loss: 0.7036
2024-06-03 10:23:12 [INFO]: Epoch 005 - training loss: 0.2362, validation loss: 0.7239
2024-06-03 10:23:15 [INFO]: Epoch 006 - training loss: 0.2150, validation loss: 0.7257
2024-06-03 10:23:19 [INFO]: Epoch 007 - training loss: 0.2004, validation loss: 0.6880
2024-06-03 10:23:21 [INFO]: Epoch 008 - training loss: 0.1887, validation loss: 0.6713
2024-06-03 10:23:24 [INFO]: Epoch 009 - training loss: 0.1802, validation loss: 0.6511
2024-06-03 10:23:28 [INFO]: Epoch 010 - training loss: 0.1741, validation loss: 0.7046
2024-06-03 10:23:30 [INFO]: Epoch 011 - training loss: 0.1674, validation loss: 0.6317
2024-06-03 10:23:33 [INFO]: Epoch 012 - training loss: 0.1604, validation loss: 0.6075
2024-06-03 10:23:36 [INFO]: Epoch 013 - training loss: 0.1558, validation loss: 0.6529
2024-06-03 10:23:39 [INFO]: Epoch 014 - training loss: 0.1551, validation loss: 0.6571
2024-06-03 10:23:42 [INFO]: Epoch 015 - training loss: 0.1514, validation loss: 0.6408
2024-06-03 10:23:45 [INFO]: Epoch 016 - training loss: 0.1495, validation loss: 0.6026
2024-06-03 10:23:48 [INFO]: Epoch 017 - training loss: 0.1477, validation loss: 0.6212
2024-06-03 10:23:51 [INFO]: Epoch 018 - training loss: 0.1438, validation loss: 0.5477
2024-06-03 10:23:53 [INFO]: Epoch 019 - training loss: 0.1429, validation loss: 0.6114
2024-06-03 10:23:56 [INFO]: Epoch 020 - training loss: 0.1404, validation loss: 0.5573
2024-06-03 10:23:59 [INFO]: Epoch 021 - training loss: 0.1373, validation loss: 0.5532
2024-06-03 10:24:02 [INFO]: Epoch 022 - training loss: 0.1364, validation loss: 0.5123
2024-06-03 10:24:05 [INFO]: Epoch 023 - training loss: 0.1325, validation loss: 0.5399
2024-06-03 10:24:07 [INFO]: Epoch 024 - training loss: 0.1299, validation loss: 0.6040
2024-06-03 10:24:10 [INFO]: Epoch 025 - training loss: 0.1315, validation loss: 0.5247
2024-06-03 10:24:13 [INFO]: Epoch 026 - training loss: 0.1334, validation loss: 0.5937
2024-06-03 10:24:15 [INFO]: Epoch 027 - training loss: 0.1283, validation loss: 0.4921
2024-06-03 10:24:17 [INFO]: Epoch 028 - training loss: 0.1279, validation loss: 0.5316
2024-06-03 10:24:20 [INFO]: Epoch 029 - training loss: 0.1277, validation loss: 0.5094
2024-06-03 10:24:23 [INFO]: Epoch 030 - training loss: 0.1274, validation loss: 0.4835
2024-06-03 10:24:25 [INFO]: Epoch 031 - training loss: 0.1252, validation loss: 0.5374
2024-06-03 10:24:28 [INFO]: Epoch 032 - training loss: 0.1248, validation loss: 0.5726
2024-06-03 10:24:30 [INFO]: Epoch 033 - training loss: 0.1228, validation loss: 0.5458
2024-06-03 10:24:32 [INFO]: Epoch 034 - training loss: 0.1187, validation loss: 0.5088
2024-06-03 10:24:35 [INFO]: Epoch 035 - training loss: 0.1163, validation loss: 0.5425
2024-06-03 10:24:37 [INFO]: Epoch 036 - training loss: 0.1147, validation loss: 0.5068
2024-06-03 10:24:40 [INFO]: Epoch 037 - training loss: 0.1126, validation loss: 0.5423
2024-06-03 10:24:43 [INFO]: Epoch 038 - training loss: 0.1108, validation loss: 0.5147
2024-06-03 10:24:45 [INFO]: Epoch 039 - training loss: 0.1078, validation loss: 0.5132
2024-06-03 10:24:48 [INFO]: Epoch 040 - training loss: 0.1096, validation loss: 0.4745
2024-06-03 10:24:50 [INFO]: Epoch 041 - training loss: 0.1081, validation loss: 0.5783
2024-06-03 10:24:52 [INFO]: Epoch 042 - training loss: 0.1078, validation loss: 0.5194
2024-06-03 10:24:54 [INFO]: Epoch 043 - training loss: 0.1077, validation loss: 0.5069
2024-06-03 10:24:57 [INFO]: Epoch 044 - training loss: 0.1067, validation loss: 0.5246
2024-06-03 10:24:59 [INFO]: Epoch 045 - training loss: 0.1072, validation loss: 0.4625
2024-06-03 10:25:02 [INFO]: Epoch 046 - training loss: 0.1069, validation loss: 0.5440
2024-06-03 10:25:04 [INFO]: Epoch 047 - training loss: 0.1045, validation loss: 0.5265
2024-06-03 10:25:07 [INFO]: Epoch 048 - training loss: 0.1030, validation loss: 0.4753
2024-06-03 10:25:09 [INFO]: Epoch 049 - training loss: 0.1005, validation loss: 0.5046
2024-06-03 10:25:12 [INFO]: Epoch 050 - training loss: 0.1008, validation loss: 0.4861
2024-06-03 10:25:15 [INFO]: Epoch 051 - training loss: 0.0969, validation loss: 0.4859
2024-06-03 10:25:17 [INFO]: Epoch 052 - training loss: 0.0950, validation loss: 0.4897
2024-06-03 10:25:20 [INFO]: Epoch 053 - training loss: 0.0930, validation loss: 0.4939
2024-06-03 10:25:23 [INFO]: Epoch 054 - training loss: 0.0951, validation loss: 0.5234
2024-06-03 10:25:25 [INFO]: Epoch 055 - training loss: 0.0969, validation loss: 0.5345
2024-06-03 10:25:25 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:25:25 [INFO]: Finished training. The best model is from epoch#45.
2024-06-03 10:25:25 [INFO]: Saved the model to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_2/20240603_T102255/GRUD.pypots
2024-06-03 10:25:29 [INFO]: Successfully saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_2/imputation.pkl
2024-06-03 10:25:29 [INFO]: Round2 - GRUD on ETT_h1: MAE=0.5736, MSE=0.7092, MRE=0.7123
2024-06-03 10:25:29 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:25:29 [INFO]: Using the given device: cuda:0
2024-06-03 10:25:30 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_3/20240603_T102529
2024-06-03 10:25:30 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_3/20240603_T102529/tensorboard
2024-06-03 10:25:30 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 409,407
2024-06-03 10:25:34 [INFO]: Epoch 001 - training loss: 0.7452, validation loss: 0.8324
2024-06-03 10:25:37 [INFO]: Epoch 002 - training loss: 0.4382, validation loss: 0.6775
2024-06-03 10:25:39 [INFO]: Epoch 003 - training loss: 0.3228, validation loss: 0.8026
2024-06-03 10:25:42 [INFO]: Epoch 004 - training loss: 0.2645, validation loss: 0.7009
2024-06-03 10:25:45 [INFO]: Epoch 005 - training loss: 0.2329, validation loss: 0.7321
2024-06-03 10:25:47 [INFO]: Epoch 006 - training loss: 0.2149, validation loss: 0.7108
2024-06-03 10:25:50 [INFO]: Epoch 007 - training loss: 0.1989, validation loss: 0.6812
2024-06-03 10:25:52 [INFO]: Epoch 008 - training loss: 0.1847, validation loss: 0.6601
2024-06-03 10:25:55 [INFO]: Epoch 009 - training loss: 0.1788, validation loss: 0.6239
2024-06-03 10:25:57 [INFO]: Epoch 010 - training loss: 0.1754, validation loss: 0.6952
2024-06-03 10:26:00 [INFO]: Epoch 011 - training loss: 0.1676, validation loss: 0.5867
2024-06-03 10:26:02 [INFO]: Epoch 012 - training loss: 0.1643, validation loss: 0.6080
2024-06-03 10:26:04 [INFO]: Epoch 013 - training loss: 0.1586, validation loss: 0.6098
2024-06-03 10:26:07 [INFO]: Epoch 014 - training loss: 0.1538, validation loss: 0.6311
2024-06-03 10:26:09 [INFO]: Epoch 015 - training loss: 0.1545, validation loss: 0.5987
2024-06-03 10:26:11 [INFO]: Epoch 016 - training loss: 0.1534, validation loss: 0.5811
2024-06-03 10:26:14 [INFO]: Epoch 017 - training loss: 0.1469, validation loss: 0.6084
2024-06-03 10:26:16 [INFO]: Epoch 018 - training loss: 0.1423, validation loss: 0.6051
2024-06-03 10:26:18 [INFO]: Epoch 019 - training loss: 0.1409, validation loss: 0.5722
2024-06-03 10:26:21 [INFO]: Epoch 020 - training loss: 0.1375, validation loss: 0.6012
2024-06-03 10:26:24 [INFO]: Epoch 021 - training loss: 0.1379, validation loss: 0.5184
2024-06-03 10:26:26 [INFO]: Epoch 022 - training loss: 0.1355, validation loss: 0.5470
2024-06-03 10:26:28 [INFO]: Epoch 023 - training loss: 0.1346, validation loss: 0.5304
2024-06-03 10:26:30 [INFO]: Epoch 024 - training loss: 0.1332, validation loss: 0.5646
2024-06-03 10:26:33 [INFO]: Epoch 025 - training loss: 0.1314, validation loss: 0.5643
2024-06-03 10:26:35 [INFO]: Epoch 026 - training loss: 0.1286, validation loss: 0.5361
2024-06-03 10:26:37 [INFO]: Epoch 027 - training loss: 0.1267, validation loss: 0.5323
2024-06-03 10:26:40 [INFO]: Epoch 028 - training loss: 0.1245, validation loss: 0.5159
2024-06-03 10:26:42 [INFO]: Epoch 029 - training loss: 0.1218, validation loss: 0.5443
2024-06-03 10:26:45 [INFO]: Epoch 030 - training loss: 0.1204, validation loss: 0.5824
2024-06-03 10:26:47 [INFO]: Epoch 031 - training loss: 0.1195, validation loss: 0.5001
2024-06-03 10:26:50 [INFO]: Epoch 032 - training loss: 0.1199, validation loss: 0.5715
2024-06-03 10:26:52 [INFO]: Epoch 033 - training loss: 0.1178, validation loss: 0.5486
2024-06-03 10:26:54 [INFO]: Epoch 034 - training loss: 0.1219, validation loss: 0.5831
2024-06-03 10:26:57 [INFO]: Epoch 035 - training loss: 0.1189, validation loss: 0.5415
2024-06-03 10:26:59 [INFO]: Epoch 036 - training loss: 0.1158, validation loss: 0.5654
2024-06-03 10:27:01 [INFO]: Epoch 037 - training loss: 0.1160, validation loss: 0.5064
2024-06-03 10:27:03 [INFO]: Epoch 038 - training loss: 0.1142, validation loss: 0.5908
2024-06-03 10:27:06 [INFO]: Epoch 039 - training loss: 0.1133, validation loss: 0.4734
2024-06-03 10:27:08 [INFO]: Epoch 040 - training loss: 0.1101, validation loss: 0.5159
2024-06-03 10:27:10 [INFO]: Epoch 041 - training loss: 0.1072, validation loss: 0.5575
2024-06-03 10:27:12 [INFO]: Epoch 042 - training loss: 0.1048, validation loss: 0.4942
2024-06-03 10:27:15 [INFO]: Epoch 043 - training loss: 0.1109, validation loss: 0.4667
2024-06-03 10:27:17 [INFO]: Epoch 044 - training loss: 0.1111, validation loss: 0.5062
2024-06-03 10:27:19 [INFO]: Epoch 045 - training loss: 0.1055, validation loss: 0.4899
2024-06-03 10:27:21 [INFO]: Epoch 046 - training loss: 0.1025, validation loss: 0.5172
2024-06-03 10:27:24 [INFO]: Epoch 047 - training loss: 0.1013, validation loss: 0.5155
2024-06-03 10:27:26 [INFO]: Epoch 048 - training loss: 0.1001, validation loss: 0.5211
2024-06-03 10:27:28 [INFO]: Epoch 049 - training loss: 0.0987, validation loss: 0.4984
2024-06-03 10:27:30 [INFO]: Epoch 050 - training loss: 0.0961, validation loss: 0.5256
2024-06-03 10:27:32 [INFO]: Epoch 051 - training loss: 0.0963, validation loss: 0.5326
2024-06-03 10:27:35 [INFO]: Epoch 052 - training loss: 0.0970, validation loss: 0.5412
2024-06-03 10:27:37 [INFO]: Epoch 053 - training loss: 0.0956, validation loss: 0.4840
2024-06-03 10:27:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:27:37 [INFO]: Finished training. The best model is from epoch#43.
2024-06-03 10:27:37 [INFO]: Saved the model to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_3/20240603_T102529/GRUD.pypots
2024-06-03 10:27:41 [INFO]: Successfully saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_3/imputation.pkl
2024-06-03 10:27:41 [INFO]: Round3 - GRUD on ETT_h1: MAE=0.5488, MSE=0.6169, MRE=0.6815
2024-06-03 10:27:41 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:27:41 [INFO]: Using the given device: cuda:0
2024-06-03 10:27:41 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_4/20240603_T102741
2024-06-03 10:27:41 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_4/20240603_T102741/tensorboard
2024-06-03 10:27:41 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 409,407
2024-06-03 10:27:44 [INFO]: Epoch 001 - training loss: 0.7038, validation loss: 0.8348
2024-06-03 10:27:47 [INFO]: Epoch 002 - training loss: 0.4152, validation loss: 0.7364
2024-06-03 10:27:49 [INFO]: Epoch 003 - training loss: 0.3142, validation loss: 0.7624
2024-06-03 10:27:51 [INFO]: Epoch 004 - training loss: 0.2628, validation loss: 0.6776
2024-06-03 10:27:53 [INFO]: Epoch 005 - training loss: 0.2305, validation loss: 0.7009
2024-06-03 10:27:56 [INFO]: Epoch 006 - training loss: 0.2078, validation loss: 0.5983
2024-06-03 10:27:58 [INFO]: Epoch 007 - training loss: 0.1938, validation loss: 0.6263
2024-06-03 10:28:00 [INFO]: Epoch 008 - training loss: 0.1819, validation loss: 0.6334
2024-06-03 10:28:03 [INFO]: Epoch 009 - training loss: 0.1740, validation loss: 0.5927
2024-06-03 10:28:05 [INFO]: Epoch 010 - training loss: 0.1677, validation loss: 0.6115
2024-06-03 10:28:07 [INFO]: Epoch 011 - training loss: 0.1660, validation loss: 0.5866
2024-06-03 10:28:09 [INFO]: Epoch 012 - training loss: 0.1589, validation loss: 0.5735
2024-06-03 10:28:12 [INFO]: Epoch 013 - training loss: 0.1560, validation loss: 0.5414
2024-06-03 10:28:14 [INFO]: Epoch 014 - training loss: 0.1526, validation loss: 0.5467
2024-06-03 10:28:16 [INFO]: Epoch 015 - training loss: 0.1530, validation loss: 0.5664
2024-06-03 10:28:18 [INFO]: Epoch 016 - training loss: 0.1517, validation loss: 0.5490
2024-06-03 10:28:20 [INFO]: Epoch 017 - training loss: 0.1460, validation loss: 0.5356
2024-06-03 10:28:22 [INFO]: Epoch 018 - training loss: 0.1458, validation loss: 0.5363
2024-06-03 10:28:24 [INFO]: Epoch 019 - training loss: 0.1389, validation loss: 0.5470
2024-06-03 10:28:26 [INFO]: Epoch 020 - training loss: 0.1356, validation loss: 0.5200
2024-06-03 10:28:28 [INFO]: Epoch 021 - training loss: 0.1366, validation loss: 0.5379
2024-06-03 10:28:30 [INFO]: Epoch 022 - training loss: 0.1357, validation loss: 0.5374
2024-06-03 10:28:32 [INFO]: Epoch 023 - training loss: 0.1305, validation loss: 0.4909
2024-06-03 10:28:34 [INFO]: Epoch 024 - training loss: 0.1311, validation loss: 0.5110
2024-06-03 10:28:35 [INFO]: Epoch 025 - training loss: 0.1346, validation loss: 0.5435
2024-06-03 10:28:37 [INFO]: Epoch 026 - training loss: 0.1322, validation loss: 0.5056
2024-06-03 10:28:39 [INFO]: Epoch 027 - training loss: 0.1305, validation loss: 0.5243
2024-06-03 10:28:41 [INFO]: Epoch 028 - training loss: 0.1246, validation loss: 0.5545
2024-06-03 10:28:43 [INFO]: Epoch 029 - training loss: 0.1243, validation loss: 0.5514
2024-06-03 10:28:44 [INFO]: Epoch 030 - training loss: 0.1255, validation loss: 0.4932
2024-06-03 10:28:46 [INFO]: Epoch 031 - training loss: 0.1218, validation loss: 0.4937
2024-06-03 10:28:48 [INFO]: Epoch 032 - training loss: 0.1202, validation loss: 0.4945
2024-06-03 10:28:50 [INFO]: Epoch 033 - training loss: 0.1179, validation loss: 0.5079
2024-06-03 10:28:50 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:28:50 [INFO]: Finished training. The best model is from epoch#23.
2024-06-03 10:28:50 [INFO]: Saved the model to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_4/20240603_T102741/GRUD.pypots
2024-06-03 10:28:53 [INFO]: Successfully saved to results_block_rate05/ETT_h1/GRUD_ETT_h1/round_4/imputation.pkl
2024-06-03 10:28:53 [INFO]: Round4 - GRUD on ETT_h1: MAE=0.5579, MSE=0.7138, MRE=0.6928
2024-06-03 10:28:53 [INFO]: Done! Final results:
Averaged GRUD (409,407 params) on ETT_h1: MAE=0.5703 ± 0.015476212517459714, MSE=0.6962 ± 0.03990458630727734, MRE=0.7082 ± 0.019217927597779867, average inference time=0.98
