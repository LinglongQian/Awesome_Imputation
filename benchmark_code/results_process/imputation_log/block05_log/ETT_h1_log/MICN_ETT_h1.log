2024-06-03 10:17:24 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:17:24 [INFO]: Using the given device: cuda:0
2024-06-03 10:17:25 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_0/20240603_T101725
2024-06-03 10:17:25 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_0/20240603_T101725/tensorboard
2024-06-03 10:17:31 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 10:17:44 [INFO]: Epoch 001 - training loss: 1.1061, validation loss: 0.8896
2024-06-03 10:17:46 [INFO]: Epoch 002 - training loss: 0.7021, validation loss: 0.8413
2024-06-03 10:17:49 [INFO]: Epoch 003 - training loss: 0.6594, validation loss: 0.8479
2024-06-03 10:17:51 [INFO]: Epoch 004 - training loss: 0.6379, validation loss: 0.8393
2024-06-03 10:17:55 [INFO]: Epoch 005 - training loss: 0.6296, validation loss: 0.8330
2024-06-03 10:17:57 [INFO]: Epoch 006 - training loss: 0.6153, validation loss: 0.8330
2024-06-03 10:18:00 [INFO]: Epoch 007 - training loss: 0.6157, validation loss: 0.8406
2024-06-03 10:18:03 [INFO]: Epoch 008 - training loss: 0.6107, validation loss: 0.8321
2024-06-03 10:18:05 [INFO]: Epoch 009 - training loss: 0.5988, validation loss: 0.8284
2024-06-03 10:18:08 [INFO]: Epoch 010 - training loss: 0.6010, validation loss: 0.8351
2024-06-03 10:18:11 [INFO]: Epoch 011 - training loss: 0.5896, validation loss: 0.8270
2024-06-03 10:18:14 [INFO]: Epoch 012 - training loss: 0.5996, validation loss: 0.8372
2024-06-03 10:18:17 [INFO]: Epoch 013 - training loss: 0.5919, validation loss: 0.8219
2024-06-03 10:18:20 [INFO]: Epoch 014 - training loss: 0.5867, validation loss: 0.8290
2024-06-03 10:18:23 [INFO]: Epoch 015 - training loss: 0.5779, validation loss: 0.8298
2024-06-03 10:18:25 [INFO]: Epoch 016 - training loss: 0.5812, validation loss: 0.8207
2024-06-03 10:18:28 [INFO]: Epoch 017 - training loss: 0.5756, validation loss: 0.8238
2024-06-03 10:18:31 [INFO]: Epoch 018 - training loss: 0.5710, validation loss: 0.8204
2024-06-03 10:18:34 [INFO]: Epoch 019 - training loss: 0.5694, validation loss: 0.8238
2024-06-03 10:18:36 [INFO]: Epoch 020 - training loss: 0.5824, validation loss: 0.8239
2024-06-03 10:18:39 [INFO]: Epoch 021 - training loss: 0.5741, validation loss: 0.8199
2024-06-03 10:18:42 [INFO]: Epoch 022 - training loss: 0.5679, validation loss: 0.8191
2024-06-03 10:18:45 [INFO]: Epoch 023 - training loss: 0.5714, validation loss: 0.8181
2024-06-03 10:18:47 [INFO]: Epoch 024 - training loss: 0.5630, validation loss: 0.8128
2024-06-03 10:18:50 [INFO]: Epoch 025 - training loss: 0.5636, validation loss: 0.8089
2024-06-03 10:18:52 [INFO]: Epoch 026 - training loss: 0.5534, validation loss: 0.8114
2024-06-03 10:18:55 [INFO]: Epoch 027 - training loss: 0.5391, validation loss: 0.8024
2024-06-03 10:18:58 [INFO]: Epoch 028 - training loss: 0.5527, validation loss: 0.8095
2024-06-03 10:19:00 [INFO]: Epoch 029 - training loss: 0.5495, validation loss: 0.8084
2024-06-03 10:19:03 [INFO]: Epoch 030 - training loss: 0.5512, validation loss: 0.8086
2024-06-03 10:19:05 [INFO]: Epoch 031 - training loss: 0.5534, validation loss: 0.8102
2024-06-03 10:19:08 [INFO]: Epoch 032 - training loss: 0.5597, validation loss: 0.7972
2024-06-03 10:19:10 [INFO]: Epoch 033 - training loss: 0.5423, validation loss: 0.8000
2024-06-03 10:19:13 [INFO]: Epoch 034 - training loss: 0.5431, validation loss: 0.7902
2024-06-03 10:19:15 [INFO]: Epoch 035 - training loss: 0.5309, validation loss: 0.7906
2024-06-03 10:19:18 [INFO]: Epoch 036 - training loss: 0.5370, validation loss: 0.7932
2024-06-03 10:19:20 [INFO]: Epoch 037 - training loss: 0.5240, validation loss: 0.7866
2024-06-03 10:19:23 [INFO]: Epoch 038 - training loss: 0.5318, validation loss: 0.7948
2024-06-03 10:19:25 [INFO]: Epoch 039 - training loss: 0.5393, validation loss: 0.7850
2024-06-03 10:19:28 [INFO]: Epoch 040 - training loss: 0.5383, validation loss: 0.7905
2024-06-03 10:19:31 [INFO]: Epoch 041 - training loss: 0.5223, validation loss: 0.7844
2024-06-03 10:19:34 [INFO]: Epoch 042 - training loss: 0.5201, validation loss: 0.7780
2024-06-03 10:19:37 [INFO]: Epoch 043 - training loss: 0.5260, validation loss: 0.7939
2024-06-03 10:19:40 [INFO]: Epoch 044 - training loss: 0.5210, validation loss: 0.7956
2024-06-03 10:19:43 [INFO]: Epoch 045 - training loss: 0.5351, validation loss: 0.7747
2024-06-03 10:19:45 [INFO]: Epoch 046 - training loss: 0.5198, validation loss: 0.7850
2024-06-03 10:19:49 [INFO]: Epoch 047 - training loss: 0.5180, validation loss: 0.7814
2024-06-03 10:19:51 [INFO]: Epoch 048 - training loss: 0.5068, validation loss: 0.7814
2024-06-03 10:19:54 [INFO]: Epoch 049 - training loss: 0.5141, validation loss: 0.7838
2024-06-03 10:19:56 [INFO]: Epoch 050 - training loss: 0.5145, validation loss: 0.7679
2024-06-03 10:19:59 [INFO]: Epoch 051 - training loss: 0.5139, validation loss: 0.7637
2024-06-03 10:20:01 [INFO]: Epoch 052 - training loss: 0.5068, validation loss: 0.7728
2024-06-03 10:20:04 [INFO]: Epoch 053 - training loss: 0.5019, validation loss: 0.7675
2024-06-03 10:20:07 [INFO]: Epoch 054 - training loss: 0.5004, validation loss: 0.7728
2024-06-03 10:20:09 [INFO]: Epoch 055 - training loss: 0.5005, validation loss: 0.7497
2024-06-03 10:20:12 [INFO]: Epoch 056 - training loss: 0.4964, validation loss: 0.7687
2024-06-03 10:20:15 [INFO]: Epoch 057 - training loss: 0.5019, validation loss: 0.7609
2024-06-03 10:20:18 [INFO]: Epoch 058 - training loss: 0.4957, validation loss: 0.7501
2024-06-03 10:20:21 [INFO]: Epoch 059 - training loss: 0.4901, validation loss: 0.7554
2024-06-03 10:20:23 [INFO]: Epoch 060 - training loss: 0.4980, validation loss: 0.7573
2024-06-03 10:20:26 [INFO]: Epoch 061 - training loss: 0.4966, validation loss: 0.7663
2024-06-03 10:20:28 [INFO]: Epoch 062 - training loss: 0.4928, validation loss: 0.7531
2024-06-03 10:20:31 [INFO]: Epoch 063 - training loss: 0.4875, validation loss: 0.7362
2024-06-03 10:20:34 [INFO]: Epoch 064 - training loss: 0.4890, validation loss: 0.7444
2024-06-03 10:20:36 [INFO]: Epoch 065 - training loss: 0.4887, validation loss: 0.7346
2024-06-03 10:20:39 [INFO]: Epoch 066 - training loss: 0.4867, validation loss: 0.7593
2024-06-03 10:20:42 [INFO]: Epoch 067 - training loss: 0.4807, validation loss: 0.7454
2024-06-03 10:20:44 [INFO]: Epoch 068 - training loss: 0.4757, validation loss: 0.7342
2024-06-03 10:20:47 [INFO]: Epoch 069 - training loss: 0.4801, validation loss: 0.7484
2024-06-03 10:20:50 [INFO]: Epoch 070 - training loss: 0.4780, validation loss: 0.7573
2024-06-03 10:20:53 [INFO]: Epoch 071 - training loss: 0.4700, validation loss: 0.7545
2024-06-03 10:20:55 [INFO]: Epoch 072 - training loss: 0.4811, validation loss: 0.7432
2024-06-03 10:20:58 [INFO]: Epoch 073 - training loss: 0.4801, validation loss: 0.7562
2024-06-03 10:21:00 [INFO]: Epoch 074 - training loss: 0.4765, validation loss: 0.7420
2024-06-03 10:21:02 [INFO]: Epoch 075 - training loss: 0.4662, validation loss: 0.7147
2024-06-03 10:21:05 [INFO]: Epoch 076 - training loss: 0.4668, validation loss: 0.7430
2024-06-03 10:21:07 [INFO]: Epoch 077 - training loss: 0.4664, validation loss: 0.7458
2024-06-03 10:21:10 [INFO]: Epoch 078 - training loss: 0.4664, validation loss: 0.7304
2024-06-03 10:21:13 [INFO]: Epoch 079 - training loss: 0.4703, validation loss: 0.7379
2024-06-03 10:21:15 [INFO]: Epoch 080 - training loss: 0.4705, validation loss: 0.7421
2024-06-03 10:21:17 [INFO]: Epoch 081 - training loss: 0.4606, validation loss: 0.7413
2024-06-03 10:21:19 [INFO]: Epoch 082 - training loss: 0.4650, validation loss: 0.7324
2024-06-03 10:21:22 [INFO]: Epoch 083 - training loss: 0.4622, validation loss: 0.7218
2024-06-03 10:21:24 [INFO]: Epoch 084 - training loss: 0.4560, validation loss: 0.7410
2024-06-03 10:21:27 [INFO]: Epoch 085 - training loss: 0.4557, validation loss: 0.7359
2024-06-03 10:21:27 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:21:27 [INFO]: Finished training. The best model is from epoch#75.
2024-06-03 10:21:27 [INFO]: Saved the model to results_block_rate05/ETT_h1/MICN_ETT_h1/round_0/20240603_T101725/MICN.pypots
2024-06-03 10:21:28 [INFO]: Successfully saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_0/imputation.pkl
2024-06-03 10:21:28 [INFO]: Round0 - MICN on ETT_h1: MAE=0.6861, MSE=0.9124, MRE=0.8519
2024-06-03 10:21:28 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:21:28 [INFO]: Using the given device: cuda:0
2024-06-03 10:21:28 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_1/20240603_T102128
2024-06-03 10:21:28 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_1/20240603_T102128/tensorboard
2024-06-03 10:21:29 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 10:21:31 [INFO]: Epoch 001 - training loss: 1.1391, validation loss: 0.8540
2024-06-03 10:21:33 [INFO]: Epoch 002 - training loss: 0.7000, validation loss: 0.8342
2024-06-03 10:21:35 [INFO]: Epoch 003 - training loss: 0.6746, validation loss: 0.8280
2024-06-03 10:21:37 [INFO]: Epoch 004 - training loss: 0.6420, validation loss: 0.8235
2024-06-03 10:21:39 [INFO]: Epoch 005 - training loss: 0.6416, validation loss: 0.8194
2024-06-03 10:21:40 [INFO]: Epoch 006 - training loss: 0.6285, validation loss: 0.8241
2024-06-03 10:21:43 [INFO]: Epoch 007 - training loss: 0.6296, validation loss: 0.8151
2024-06-03 10:21:45 [INFO]: Epoch 008 - training loss: 0.6230, validation loss: 0.8191
2024-06-03 10:21:48 [INFO]: Epoch 009 - training loss: 0.6143, validation loss: 0.8198
2024-06-03 10:21:50 [INFO]: Epoch 010 - training loss: 0.6077, validation loss: 0.8191
2024-06-03 10:21:53 [INFO]: Epoch 011 - training loss: 0.6059, validation loss: 0.8142
2024-06-03 10:21:55 [INFO]: Epoch 012 - training loss: 0.6114, validation loss: 0.8180
2024-06-03 10:21:58 [INFO]: Epoch 013 - training loss: 0.5971, validation loss: 0.8162
2024-06-03 10:22:00 [INFO]: Epoch 014 - training loss: 0.6000, validation loss: 0.8106
2024-06-03 10:22:02 [INFO]: Epoch 015 - training loss: 0.5953, validation loss: 0.8207
2024-06-03 10:22:05 [INFO]: Epoch 016 - training loss: 0.5902, validation loss: 0.8090
2024-06-03 10:22:07 [INFO]: Epoch 017 - training loss: 0.5875, validation loss: 0.8166
2024-06-03 10:22:09 [INFO]: Epoch 018 - training loss: 0.6017, validation loss: 0.8096
2024-06-03 10:22:11 [INFO]: Epoch 019 - training loss: 0.5907, validation loss: 0.8106
2024-06-03 10:22:13 [INFO]: Epoch 020 - training loss: 0.5929, validation loss: 0.8152
2024-06-03 10:22:15 [INFO]: Epoch 021 - training loss: 0.5844, validation loss: 0.8116
2024-06-03 10:22:18 [INFO]: Epoch 022 - training loss: 0.5944, validation loss: 0.8077
2024-06-03 10:22:21 [INFO]: Epoch 023 - training loss: 0.5943, validation loss: 0.8128
2024-06-03 10:22:23 [INFO]: Epoch 024 - training loss: 0.5897, validation loss: 0.8057
2024-06-03 10:22:25 [INFO]: Epoch 025 - training loss: 0.5955, validation loss: 0.8130
2024-06-03 10:22:27 [INFO]: Epoch 026 - training loss: 0.5860, validation loss: 0.8096
2024-06-03 10:22:29 [INFO]: Epoch 027 - training loss: 0.5742, validation loss: 0.8072
2024-06-03 10:22:31 [INFO]: Epoch 028 - training loss: 0.5826, validation loss: 0.8120
2024-06-03 10:22:33 [INFO]: Epoch 029 - training loss: 0.5699, validation loss: 0.7996
2024-06-03 10:22:36 [INFO]: Epoch 030 - training loss: 0.5676, validation loss: 0.8133
2024-06-03 10:22:38 [INFO]: Epoch 031 - training loss: 0.5621, validation loss: 0.8062
2024-06-03 10:22:40 [INFO]: Epoch 032 - training loss: 0.5687, validation loss: 0.8033
2024-06-03 10:22:42 [INFO]: Epoch 033 - training loss: 0.5617, validation loss: 0.8085
2024-06-03 10:22:44 [INFO]: Epoch 034 - training loss: 0.5537, validation loss: 0.8029
2024-06-03 10:22:46 [INFO]: Epoch 035 - training loss: 0.5584, validation loss: 0.8072
2024-06-03 10:22:48 [INFO]: Epoch 036 - training loss: 0.5630, validation loss: 0.8005
2024-06-03 10:22:50 [INFO]: Epoch 037 - training loss: 0.5560, validation loss: 0.8074
2024-06-03 10:22:52 [INFO]: Epoch 038 - training loss: 0.5495, validation loss: 0.8127
2024-06-03 10:22:54 [INFO]: Epoch 039 - training loss: 0.5552, validation loss: 0.8028
2024-06-03 10:22:54 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:22:54 [INFO]: Finished training. The best model is from epoch#29.
2024-06-03 10:22:54 [INFO]: Saved the model to results_block_rate05/ETT_h1/MICN_ETT_h1/round_1/20240603_T102128/MICN.pypots
2024-06-03 10:22:55 [INFO]: Successfully saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_1/imputation.pkl
2024-06-03 10:22:55 [INFO]: Round1 - MICN on ETT_h1: MAE=0.7229, MSE=0.9899, MRE=0.8977
2024-06-03 10:22:55 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:22:55 [INFO]: Using the given device: cuda:0
2024-06-03 10:22:56 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_2/20240603_T102255
2024-06-03 10:22:56 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_2/20240603_T102255/tensorboard
2024-06-03 10:22:56 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 10:22:58 [INFO]: Epoch 001 - training loss: 1.1632, validation loss: 0.8968
2024-06-03 10:23:00 [INFO]: Epoch 002 - training loss: 0.7170, validation loss: 0.8413
2024-06-03 10:23:02 [INFO]: Epoch 003 - training loss: 0.6680, validation loss: 0.8495
2024-06-03 10:23:05 [INFO]: Epoch 004 - training loss: 0.6506, validation loss: 0.8363
2024-06-03 10:23:07 [INFO]: Epoch 005 - training loss: 0.6407, validation loss: 0.8347
2024-06-03 10:23:09 [INFO]: Epoch 006 - training loss: 0.6237, validation loss: 0.8372
2024-06-03 10:23:11 [INFO]: Epoch 007 - training loss: 0.6266, validation loss: 0.8276
2024-06-03 10:23:14 [INFO]: Epoch 008 - training loss: 0.6314, validation loss: 0.8347
2024-06-03 10:23:16 [INFO]: Epoch 009 - training loss: 0.6163, validation loss: 0.8315
2024-06-03 10:23:18 [INFO]: Epoch 010 - training loss: 0.6172, validation loss: 0.8264
2024-06-03 10:23:21 [INFO]: Epoch 011 - training loss: 0.6110, validation loss: 0.8327
2024-06-03 10:23:23 [INFO]: Epoch 012 - training loss: 0.6082, validation loss: 0.8275
2024-06-03 10:23:25 [INFO]: Epoch 013 - training loss: 0.6065, validation loss: 0.8281
2024-06-03 10:23:27 [INFO]: Epoch 014 - training loss: 0.6079, validation loss: 0.8286
2024-06-03 10:23:29 [INFO]: Epoch 015 - training loss: 0.5891, validation loss: 0.8301
2024-06-03 10:23:31 [INFO]: Epoch 016 - training loss: 0.5973, validation loss: 0.8269
2024-06-03 10:23:33 [INFO]: Epoch 017 - training loss: 0.5943, validation loss: 0.8259
2024-06-03 10:23:35 [INFO]: Epoch 018 - training loss: 0.6018, validation loss: 0.8245
2024-06-03 10:23:37 [INFO]: Epoch 019 - training loss: 0.5865, validation loss: 0.8184
2024-06-03 10:23:39 [INFO]: Epoch 020 - training loss: 0.5900, validation loss: 0.8261
2024-06-03 10:23:41 [INFO]: Epoch 021 - training loss: 0.5950, validation loss: 0.8217
2024-06-03 10:23:43 [INFO]: Epoch 022 - training loss: 0.5947, validation loss: 0.8236
2024-06-03 10:23:45 [INFO]: Epoch 023 - training loss: 0.5971, validation loss: 0.8231
2024-06-03 10:23:48 [INFO]: Epoch 024 - training loss: 0.5899, validation loss: 0.8184
2024-06-03 10:23:50 [INFO]: Epoch 025 - training loss: 0.5861, validation loss: 0.8186
2024-06-03 10:23:52 [INFO]: Epoch 026 - training loss: 0.5934, validation loss: 0.8193
2024-06-03 10:23:54 [INFO]: Epoch 027 - training loss: 0.5770, validation loss: 0.8100
2024-06-03 10:23:56 [INFO]: Epoch 028 - training loss: 0.5767, validation loss: 0.8175
2024-06-03 10:23:58 [INFO]: Epoch 029 - training loss: 0.5774, validation loss: 0.8115
2024-06-03 10:24:00 [INFO]: Epoch 030 - training loss: 0.5799, validation loss: 0.8185
2024-06-03 10:24:02 [INFO]: Epoch 031 - training loss: 0.5789, validation loss: 0.8201
2024-06-03 10:24:04 [INFO]: Epoch 032 - training loss: 0.5705, validation loss: 0.8216
2024-06-03 10:24:05 [INFO]: Epoch 033 - training loss: 0.5743, validation loss: 0.8229
2024-06-03 10:24:07 [INFO]: Epoch 034 - training loss: 0.5718, validation loss: 0.8129
2024-06-03 10:24:09 [INFO]: Epoch 035 - training loss: 0.5708, validation loss: 0.8183
2024-06-03 10:24:11 [INFO]: Epoch 036 - training loss: 0.5778, validation loss: 0.8200
2024-06-03 10:24:13 [INFO]: Epoch 037 - training loss: 0.5589, validation loss: 0.8153
2024-06-03 10:24:13 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:24:13 [INFO]: Finished training. The best model is from epoch#27.
2024-06-03 10:24:13 [INFO]: Saved the model to results_block_rate05/ETT_h1/MICN_ETT_h1/round_2/20240603_T102255/MICN.pypots
2024-06-03 10:24:14 [INFO]: Successfully saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_2/imputation.pkl
2024-06-03 10:24:14 [INFO]: Round2 - MICN on ETT_h1: MAE=0.7364, MSE=1.0239, MRE=0.9144
2024-06-03 10:24:14 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:24:14 [INFO]: Using the given device: cuda:0
2024-06-03 10:24:15 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_3/20240603_T102414
2024-06-03 10:24:15 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_3/20240603_T102414/tensorboard
2024-06-03 10:24:15 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 10:24:16 [INFO]: Epoch 001 - training loss: 1.0900, validation loss: 0.8773
2024-06-03 10:24:18 [INFO]: Epoch 002 - training loss: 0.6816, validation loss: 0.8500
2024-06-03 10:24:20 [INFO]: Epoch 003 - training loss: 0.6377, validation loss: 0.8413
2024-06-03 10:24:22 [INFO]: Epoch 004 - training loss: 0.6270, validation loss: 0.8459
2024-06-03 10:24:24 [INFO]: Epoch 005 - training loss: 0.6351, validation loss: 0.8460
2024-06-03 10:24:26 [INFO]: Epoch 006 - training loss: 0.6204, validation loss: 0.8383
2024-06-03 10:24:28 [INFO]: Epoch 007 - training loss: 0.6112, validation loss: 0.8385
2024-06-03 10:24:30 [INFO]: Epoch 008 - training loss: 0.6004, validation loss: 0.8392
2024-06-03 10:24:32 [INFO]: Epoch 009 - training loss: 0.6010, validation loss: 0.8415
2024-06-03 10:24:34 [INFO]: Epoch 010 - training loss: 0.6067, validation loss: 0.8371
2024-06-03 10:24:36 [INFO]: Epoch 011 - training loss: 0.5977, validation loss: 0.8403
2024-06-03 10:24:37 [INFO]: Epoch 012 - training loss: 0.5936, validation loss: 0.8484
2024-06-03 10:24:39 [INFO]: Epoch 013 - training loss: 0.5973, validation loss: 0.8346
2024-06-03 10:24:41 [INFO]: Epoch 014 - training loss: 0.6004, validation loss: 0.8333
2024-06-03 10:24:43 [INFO]: Epoch 015 - training loss: 0.5882, validation loss: 0.8419
2024-06-03 10:24:44 [INFO]: Epoch 016 - training loss: 0.5876, validation loss: 0.8320
2024-06-03 10:24:46 [INFO]: Epoch 017 - training loss: 0.5960, validation loss: 0.8367
2024-06-03 10:24:48 [INFO]: Epoch 018 - training loss: 0.5799, validation loss: 0.8336
2024-06-03 10:24:50 [INFO]: Epoch 019 - training loss: 0.5870, validation loss: 0.8369
2024-06-03 10:24:51 [INFO]: Epoch 020 - training loss: 0.5788, validation loss: 0.8381
2024-06-03 10:24:53 [INFO]: Epoch 021 - training loss: 0.5731, validation loss: 0.8286
2024-06-03 10:24:54 [INFO]: Epoch 022 - training loss: 0.5736, validation loss: 0.8320
2024-06-03 10:24:56 [INFO]: Epoch 023 - training loss: 0.5816, validation loss: 0.8219
2024-06-03 10:24:58 [INFO]: Epoch 024 - training loss: 0.5756, validation loss: 0.8277
2024-06-03 10:25:00 [INFO]: Epoch 025 - training loss: 0.5728, validation loss: 0.8241
2024-06-03 10:25:01 [INFO]: Epoch 026 - training loss: 0.5711, validation loss: 0.8205
2024-06-03 10:25:03 [INFO]: Epoch 027 - training loss: 0.5659, validation loss: 0.8316
2024-06-03 10:25:05 [INFO]: Epoch 028 - training loss: 0.5573, validation loss: 0.8161
2024-06-03 10:25:07 [INFO]: Epoch 029 - training loss: 0.5589, validation loss: 0.8196
2024-06-03 10:25:09 [INFO]: Epoch 030 - training loss: 0.5648, validation loss: 0.8224
2024-06-03 10:25:12 [INFO]: Epoch 031 - training loss: 0.5657, validation loss: 0.8253
2024-06-03 10:25:14 [INFO]: Epoch 032 - training loss: 0.5604, validation loss: 0.8351
2024-06-03 10:25:16 [INFO]: Epoch 033 - training loss: 0.5529, validation loss: 0.8178
2024-06-03 10:25:17 [INFO]: Epoch 034 - training loss: 0.5457, validation loss: 0.8222
2024-06-03 10:25:19 [INFO]: Epoch 035 - training loss: 0.5505, validation loss: 0.8242
2024-06-03 10:25:21 [INFO]: Epoch 036 - training loss: 0.5551, validation loss: 0.8190
2024-06-03 10:25:23 [INFO]: Epoch 037 - training loss: 0.5416, validation loss: 0.8045
2024-06-03 10:25:25 [INFO]: Epoch 038 - training loss: 0.5425, validation loss: 0.8195
2024-06-03 10:25:27 [INFO]: Epoch 039 - training loss: 0.5420, validation loss: 0.8220
2024-06-03 10:25:28 [INFO]: Epoch 040 - training loss: 0.5431, validation loss: 0.8112
2024-06-03 10:25:30 [INFO]: Epoch 041 - training loss: 0.5455, validation loss: 0.8256
2024-06-03 10:25:33 [INFO]: Epoch 042 - training loss: 0.5354, validation loss: 0.8158
2024-06-03 10:25:35 [INFO]: Epoch 043 - training loss: 0.5476, validation loss: 0.7988
2024-06-03 10:25:36 [INFO]: Epoch 044 - training loss: 0.5295, validation loss: 0.8008
2024-06-03 10:25:38 [INFO]: Epoch 045 - training loss: 0.5306, validation loss: 0.7972
2024-06-03 10:25:40 [INFO]: Epoch 046 - training loss: 0.5271, validation loss: 0.8051
2024-06-03 10:25:42 [INFO]: Epoch 047 - training loss: 0.5232, validation loss: 0.7987
2024-06-03 10:25:43 [INFO]: Epoch 048 - training loss: 0.5128, validation loss: 0.8044
2024-06-03 10:25:45 [INFO]: Epoch 049 - training loss: 0.5187, validation loss: 0.7998
2024-06-03 10:25:47 [INFO]: Epoch 050 - training loss: 0.5210, validation loss: 0.7930
2024-06-03 10:25:49 [INFO]: Epoch 051 - training loss: 0.5152, validation loss: 0.8066
2024-06-03 10:25:51 [INFO]: Epoch 052 - training loss: 0.5121, validation loss: 0.8040
2024-06-03 10:25:53 [INFO]: Epoch 053 - training loss: 0.5142, validation loss: 0.7921
2024-06-03 10:25:55 [INFO]: Epoch 054 - training loss: 0.5198, validation loss: 0.7979
2024-06-03 10:25:57 [INFO]: Epoch 055 - training loss: 0.5004, validation loss: 0.7833
2024-06-03 10:25:58 [INFO]: Epoch 056 - training loss: 0.4968, validation loss: 0.7939
2024-06-03 10:26:00 [INFO]: Epoch 057 - training loss: 0.5000, validation loss: 0.8046
2024-06-03 10:26:02 [INFO]: Epoch 058 - training loss: 0.4973, validation loss: 0.7975
2024-06-03 10:26:04 [INFO]: Epoch 059 - training loss: 0.5053, validation loss: 0.7897
2024-06-03 10:26:06 [INFO]: Epoch 060 - training loss: 0.4901, validation loss: 0.7835
2024-06-03 10:26:08 [INFO]: Epoch 061 - training loss: 0.4992, validation loss: 0.7808
2024-06-03 10:26:10 [INFO]: Epoch 062 - training loss: 0.4966, validation loss: 0.7954
2024-06-03 10:26:12 [INFO]: Epoch 063 - training loss: 0.4898, validation loss: 0.7954
2024-06-03 10:26:14 [INFO]: Epoch 064 - training loss: 0.4914, validation loss: 0.7964
2024-06-03 10:26:16 [INFO]: Epoch 065 - training loss: 0.4765, validation loss: 0.7855
2024-06-03 10:26:17 [INFO]: Epoch 066 - training loss: 0.4853, validation loss: 0.7608
2024-06-03 10:26:19 [INFO]: Epoch 067 - training loss: 0.4955, validation loss: 0.7651
2024-06-03 10:26:21 [INFO]: Epoch 068 - training loss: 0.4745, validation loss: 0.7863
2024-06-03 10:26:23 [INFO]: Epoch 069 - training loss: 0.4947, validation loss: 0.7640
2024-06-03 10:26:25 [INFO]: Epoch 070 - training loss: 0.4771, validation loss: 0.7851
2024-06-03 10:26:27 [INFO]: Epoch 071 - training loss: 0.4808, validation loss: 0.7775
2024-06-03 10:26:29 [INFO]: Epoch 072 - training loss: 0.4743, validation loss: 0.7838
2024-06-03 10:26:30 [INFO]: Epoch 073 - training loss: 0.4678, validation loss: 0.7768
2024-06-03 10:26:32 [INFO]: Epoch 074 - training loss: 0.4746, validation loss: 0.7765
2024-06-03 10:26:34 [INFO]: Epoch 075 - training loss: 0.4693, validation loss: 0.7685
2024-06-03 10:26:35 [INFO]: Epoch 076 - training loss: 0.4644, validation loss: 0.7606
2024-06-03 10:26:37 [INFO]: Epoch 077 - training loss: 0.4718, validation loss: 0.7649
2024-06-03 10:26:39 [INFO]: Epoch 078 - training loss: 0.4613, validation loss: 0.7637
2024-06-03 10:26:40 [INFO]: Epoch 079 - training loss: 0.4678, validation loss: 0.7641
2024-06-03 10:26:42 [INFO]: Epoch 080 - training loss: 0.4681, validation loss: 0.7679
2024-06-03 10:26:44 [INFO]: Epoch 081 - training loss: 0.4541, validation loss: 0.7585
2024-06-03 10:26:46 [INFO]: Epoch 082 - training loss: 0.4525, validation loss: 0.7558
2024-06-03 10:26:48 [INFO]: Epoch 083 - training loss: 0.4525, validation loss: 0.7623
2024-06-03 10:26:49 [INFO]: Epoch 084 - training loss: 0.4560, validation loss: 0.7514
2024-06-03 10:26:51 [INFO]: Epoch 085 - training loss: 0.4558, validation loss: 0.7611
2024-06-03 10:26:53 [INFO]: Epoch 086 - training loss: 0.4586, validation loss: 0.7597
2024-06-03 10:26:55 [INFO]: Epoch 087 - training loss: 0.4520, validation loss: 0.7769
2024-06-03 10:26:56 [INFO]: Epoch 088 - training loss: 0.4582, validation loss: 0.7496
2024-06-03 10:26:58 [INFO]: Epoch 089 - training loss: 0.4457, validation loss: 0.7496
2024-06-03 10:27:00 [INFO]: Epoch 090 - training loss: 0.4496, validation loss: 0.7476
2024-06-03 10:27:02 [INFO]: Epoch 091 - training loss: 0.4522, validation loss: 0.7641
2024-06-03 10:27:04 [INFO]: Epoch 092 - training loss: 0.4474, validation loss: 0.7652
2024-06-03 10:27:06 [INFO]: Epoch 093 - training loss: 0.4447, validation loss: 0.7631
2024-06-03 10:27:07 [INFO]: Epoch 094 - training loss: 0.4524, validation loss: 0.7765
2024-06-03 10:27:09 [INFO]: Epoch 095 - training loss: 0.4450, validation loss: 0.7476
2024-06-03 10:27:11 [INFO]: Epoch 096 - training loss: 0.4424, validation loss: 0.7572
2024-06-03 10:27:12 [INFO]: Epoch 097 - training loss: 0.4440, validation loss: 0.7385
2024-06-03 10:27:14 [INFO]: Epoch 098 - training loss: 0.4373, validation loss: 0.7547
2024-06-03 10:27:16 [INFO]: Epoch 099 - training loss: 0.4433, validation loss: 0.7377
2024-06-03 10:27:18 [INFO]: Epoch 100 - training loss: 0.4495, validation loss: 0.7457
2024-06-03 10:27:18 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 10:27:18 [INFO]: Saved the model to results_block_rate05/ETT_h1/MICN_ETT_h1/round_3/20240603_T102414/MICN.pypots
2024-06-03 10:27:19 [INFO]: Successfully saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_3/imputation.pkl
2024-06-03 10:27:19 [INFO]: Round3 - MICN on ETT_h1: MAE=0.7003, MSE=0.9227, MRE=0.8696
2024-06-03 10:27:19 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:27:19 [INFO]: Using the given device: cuda:0
2024-06-03 10:27:19 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_4/20240603_T102719
2024-06-03 10:27:19 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_4/20240603_T102719/tensorboard
2024-06-03 10:27:19 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 10:27:21 [INFO]: Epoch 001 - training loss: 1.1021, validation loss: 0.8783
2024-06-03 10:27:23 [INFO]: Epoch 002 - training loss: 0.7011, validation loss: 0.8365
2024-06-03 10:27:24 [INFO]: Epoch 003 - training loss: 0.6495, validation loss: 0.8313
2024-06-03 10:27:26 [INFO]: Epoch 004 - training loss: 0.6359, validation loss: 0.8336
2024-06-03 10:27:28 [INFO]: Epoch 005 - training loss: 0.6280, validation loss: 0.8257
2024-06-03 10:27:30 [INFO]: Epoch 006 - training loss: 0.6220, validation loss: 0.8290
2024-06-03 10:27:32 [INFO]: Epoch 007 - training loss: 0.6165, validation loss: 0.8314
2024-06-03 10:27:34 [INFO]: Epoch 008 - training loss: 0.6200, validation loss: 0.8291
2024-06-03 10:27:36 [INFO]: Epoch 009 - training loss: 0.6133, validation loss: 0.8276
2024-06-03 10:27:37 [INFO]: Epoch 010 - training loss: 0.6100, validation loss: 0.8258
2024-06-03 10:27:39 [INFO]: Epoch 011 - training loss: 0.6106, validation loss: 0.8278
2024-06-03 10:27:41 [INFO]: Epoch 012 - training loss: 0.6016, validation loss: 0.8222
2024-06-03 10:27:43 [INFO]: Epoch 013 - training loss: 0.5992, validation loss: 0.8293
2024-06-03 10:27:45 [INFO]: Epoch 014 - training loss: 0.5936, validation loss: 0.8290
2024-06-03 10:27:46 [INFO]: Epoch 015 - training loss: 0.5949, validation loss: 0.8289
2024-06-03 10:27:48 [INFO]: Epoch 016 - training loss: 0.5865, validation loss: 0.8308
2024-06-03 10:27:50 [INFO]: Epoch 017 - training loss: 0.5895, validation loss: 0.8249
2024-06-03 10:27:52 [INFO]: Epoch 018 - training loss: 0.5872, validation loss: 0.8329
2024-06-03 10:27:54 [INFO]: Epoch 019 - training loss: 0.5840, validation loss: 0.8240
2024-06-03 10:27:55 [INFO]: Epoch 020 - training loss: 0.5815, validation loss: 0.8265
2024-06-03 10:27:57 [INFO]: Epoch 021 - training loss: 0.5767, validation loss: 0.8291
2024-06-03 10:27:58 [INFO]: Epoch 022 - training loss: 0.5810, validation loss: 0.8189
2024-06-03 10:28:00 [INFO]: Epoch 023 - training loss: 0.5807, validation loss: 0.8276
2024-06-03 10:28:02 [INFO]: Epoch 024 - training loss: 0.5954, validation loss: 0.8216
2024-06-03 10:28:03 [INFO]: Epoch 025 - training loss: 0.5741, validation loss: 0.8221
2024-06-03 10:28:05 [INFO]: Epoch 026 - training loss: 0.5727, validation loss: 0.8205
2024-06-03 10:28:06 [INFO]: Epoch 027 - training loss: 0.5727, validation loss: 0.8189
2024-06-03 10:28:08 [INFO]: Epoch 028 - training loss: 0.5771, validation loss: 0.8161
2024-06-03 10:28:10 [INFO]: Epoch 029 - training loss: 0.5756, validation loss: 0.8179
2024-06-03 10:28:12 [INFO]: Epoch 030 - training loss: 0.5760, validation loss: 0.8219
2024-06-03 10:28:14 [INFO]: Epoch 031 - training loss: 0.5589, validation loss: 0.8185
2024-06-03 10:28:15 [INFO]: Epoch 032 - training loss: 0.5632, validation loss: 0.8177
2024-06-03 10:28:17 [INFO]: Epoch 033 - training loss: 0.5606, validation loss: 0.8221
2024-06-03 10:28:19 [INFO]: Epoch 034 - training loss: 0.5556, validation loss: 0.8148
2024-06-03 10:28:21 [INFO]: Epoch 035 - training loss: 0.5641, validation loss: 0.8220
2024-06-03 10:28:23 [INFO]: Epoch 036 - training loss: 0.5528, validation loss: 0.8254
2024-06-03 10:28:24 [INFO]: Epoch 037 - training loss: 0.5442, validation loss: 0.8107
2024-06-03 10:28:25 [INFO]: Epoch 038 - training loss: 0.5507, validation loss: 0.8090
2024-06-03 10:28:27 [INFO]: Epoch 039 - training loss: 0.5520, validation loss: 0.8071
2024-06-03 10:28:29 [INFO]: Epoch 040 - training loss: 0.5383, validation loss: 0.8039
2024-06-03 10:28:30 [INFO]: Epoch 041 - training loss: 0.5381, validation loss: 0.8080
2024-06-03 10:28:31 [INFO]: Epoch 042 - training loss: 0.5380, validation loss: 0.8006
2024-06-03 10:28:33 [INFO]: Epoch 043 - training loss: 0.5289, validation loss: 0.7975
2024-06-03 10:28:34 [INFO]: Epoch 044 - training loss: 0.5347, validation loss: 0.8044
2024-06-03 10:28:35 [INFO]: Epoch 045 - training loss: 0.5212, validation loss: 0.7920
2024-06-03 10:28:36 [INFO]: Epoch 046 - training loss: 0.5392, validation loss: 0.7856
2024-06-03 10:28:38 [INFO]: Epoch 047 - training loss: 0.5248, validation loss: 0.7927
2024-06-03 10:28:39 [INFO]: Epoch 048 - training loss: 0.5258, validation loss: 0.7898
2024-06-03 10:28:41 [INFO]: Epoch 049 - training loss: 0.5241, validation loss: 0.7880
2024-06-03 10:28:42 [INFO]: Epoch 050 - training loss: 0.5287, validation loss: 0.7816
2024-06-03 10:28:43 [INFO]: Epoch 051 - training loss: 0.5193, validation loss: 0.7755
2024-06-03 10:28:44 [INFO]: Epoch 052 - training loss: 0.5151, validation loss: 0.7732
2024-06-03 10:28:46 [INFO]: Epoch 053 - training loss: 0.5146, validation loss: 0.7925
2024-06-03 10:28:47 [INFO]: Epoch 054 - training loss: 0.5112, validation loss: 0.7841
2024-06-03 10:28:49 [INFO]: Epoch 055 - training loss: 0.5062, validation loss: 0.7694
2024-06-03 10:28:50 [INFO]: Epoch 056 - training loss: 0.5115, validation loss: 0.7849
2024-06-03 10:28:51 [INFO]: Epoch 057 - training loss: 0.4998, validation loss: 0.7643
2024-06-03 10:28:53 [INFO]: Epoch 058 - training loss: 0.5049, validation loss: 0.7671
2024-06-03 10:28:53 [INFO]: Epoch 059 - training loss: 0.4999, validation loss: 0.7608
2024-06-03 10:28:55 [INFO]: Epoch 060 - training loss: 0.5011, validation loss: 0.7804
2024-06-03 10:28:56 [INFO]: Epoch 061 - training loss: 0.4951, validation loss: 0.7693
2024-06-03 10:28:57 [INFO]: Epoch 062 - training loss: 0.5029, validation loss: 0.7530
2024-06-03 10:28:58 [INFO]: Epoch 063 - training loss: 0.4917, validation loss: 0.7727
2024-06-03 10:29:00 [INFO]: Epoch 064 - training loss: 0.4932, validation loss: 0.7703
2024-06-03 10:29:01 [INFO]: Epoch 065 - training loss: 0.4913, validation loss: 0.7535
2024-06-03 10:29:02 [INFO]: Epoch 066 - training loss: 0.4899, validation loss: 0.7529
2024-06-03 10:29:03 [INFO]: Epoch 067 - training loss: 0.4848, validation loss: 0.7742
2024-06-03 10:29:04 [INFO]: Epoch 068 - training loss: 0.4814, validation loss: 0.7593
2024-06-03 10:29:05 [INFO]: Epoch 069 - training loss: 0.4836, validation loss: 0.7654
2024-06-03 10:29:06 [INFO]: Epoch 070 - training loss: 0.4768, validation loss: 0.7620
2024-06-03 10:29:07 [INFO]: Epoch 071 - training loss: 0.4767, validation loss: 0.7561
2024-06-03 10:29:08 [INFO]: Epoch 072 - training loss: 0.4736, validation loss: 0.7518
2024-06-03 10:29:10 [INFO]: Epoch 073 - training loss: 0.4770, validation loss: 0.7543
2024-06-03 10:29:11 [INFO]: Epoch 074 - training loss: 0.4768, validation loss: 0.7577
2024-06-03 10:29:12 [INFO]: Epoch 075 - training loss: 0.4790, validation loss: 0.7643
2024-06-03 10:29:13 [INFO]: Epoch 076 - training loss: 0.4684, validation loss: 0.7555
2024-06-03 10:29:14 [INFO]: Epoch 077 - training loss: 0.4673, validation loss: 0.7473
2024-06-03 10:29:15 [INFO]: Epoch 078 - training loss: 0.4686, validation loss: 0.7532
2024-06-03 10:29:16 [INFO]: Epoch 079 - training loss: 0.4657, validation loss: 0.7439
2024-06-03 10:29:18 [INFO]: Epoch 080 - training loss: 0.4648, validation loss: 0.7498
2024-06-03 10:29:19 [INFO]: Epoch 081 - training loss: 0.4652, validation loss: 0.7580
2024-06-03 10:29:20 [INFO]: Epoch 082 - training loss: 0.4591, validation loss: 0.7389
2024-06-03 10:29:21 [INFO]: Epoch 083 - training loss: 0.4660, validation loss: 0.7262
2024-06-03 10:29:22 [INFO]: Epoch 084 - training loss: 0.4589, validation loss: 0.7601
2024-06-03 10:29:23 [INFO]: Epoch 085 - training loss: 0.4573, validation loss: 0.7578
2024-06-03 10:29:25 [INFO]: Epoch 086 - training loss: 0.4611, validation loss: 0.7408
2024-06-03 10:29:26 [INFO]: Epoch 087 - training loss: 0.4533, validation loss: 0.7290
2024-06-03 10:29:27 [INFO]: Epoch 088 - training loss: 0.4538, validation loss: 0.7283
2024-06-03 10:29:28 [INFO]: Epoch 089 - training loss: 0.4462, validation loss: 0.7487
2024-06-03 10:29:29 [INFO]: Epoch 090 - training loss: 0.4584, validation loss: 0.7441
2024-06-03 10:29:30 [INFO]: Epoch 091 - training loss: 0.4502, validation loss: 0.7185
2024-06-03 10:29:31 [INFO]: Epoch 092 - training loss: 0.4405, validation loss: 0.7295
2024-06-03 10:29:32 [INFO]: Epoch 093 - training loss: 0.4501, validation loss: 0.7273
2024-06-03 10:29:33 [INFO]: Epoch 094 - training loss: 0.4460, validation loss: 0.7372
2024-06-03 10:29:34 [INFO]: Epoch 095 - training loss: 0.4481, validation loss: 0.7119
2024-06-03 10:29:35 [INFO]: Epoch 096 - training loss: 0.4514, validation loss: 0.7345
2024-06-03 10:29:36 [INFO]: Epoch 097 - training loss: 0.4500, validation loss: 0.7330
2024-06-03 10:29:37 [INFO]: Epoch 098 - training loss: 0.4551, validation loss: 0.7279
2024-06-03 10:29:38 [INFO]: Epoch 099 - training loss: 0.4370, validation loss: 0.7199
2024-06-03 10:29:39 [INFO]: Epoch 100 - training loss: 0.4409, validation loss: 0.7176
2024-06-03 10:29:39 [INFO]: Finished training. The best model is from epoch#95.
2024-06-03 10:29:39 [INFO]: Saved the model to results_block_rate05/ETT_h1/MICN_ETT_h1/round_4/20240603_T102719/MICN.pypots
2024-06-03 10:29:40 [INFO]: Successfully saved to results_block_rate05/ETT_h1/MICN_ETT_h1/round_4/imputation.pkl
2024-06-03 10:29:40 [INFO]: Round4 - MICN on ETT_h1: MAE=0.6855, MSE=0.9006, MRE=0.8513
2024-06-03 10:29:40 [INFO]: Done! Final results:
Averaged MICN (3,153,163 params) on ETT_h1: MAE=0.7062 ± 0.02029230683128641, MSE=0.9499 ± 0.04829251484884147, MRE=0.8770 ± 0.025198418736860687, average inference time=0.30
