2024-06-03 10:17:20 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:17:20 [INFO]: Using the given device: cuda:0
2024-06-03 10:17:25 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_0/20240603_T101725
2024-06-03 10:17:25 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_0/20240603_T101725/tensorboard
2024-06-03 10:17:25 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 10:18:08 [INFO]: Epoch 001 - training loss: 0.7491, validation loss: 0.5184
2024-06-03 10:18:29 [INFO]: Epoch 002 - training loss: 0.4669, validation loss: 0.4862
2024-06-03 10:18:48 [INFO]: Epoch 003 - training loss: 0.4813, validation loss: 0.4527
2024-06-03 10:19:08 [INFO]: Epoch 004 - training loss: 0.4101, validation loss: 0.4030
2024-06-03 10:19:28 [INFO]: Epoch 005 - training loss: 0.3290, validation loss: 0.3961
2024-06-03 10:19:48 [INFO]: Epoch 006 - training loss: 0.3511, validation loss: 0.3861
2024-06-03 10:20:08 [INFO]: Epoch 007 - training loss: 0.3036, validation loss: 0.3709
2024-06-03 10:20:27 [INFO]: Epoch 008 - training loss: 0.3255, validation loss: 0.4087
2024-06-03 10:20:47 [INFO]: Epoch 009 - training loss: 0.3136, validation loss: 0.3903
2024-06-03 10:21:07 [INFO]: Epoch 010 - training loss: 0.3770, validation loss: 0.3249
2024-06-03 10:21:25 [INFO]: Epoch 011 - training loss: 0.3222, validation loss: 0.3384
2024-06-03 10:21:41 [INFO]: Epoch 012 - training loss: 0.3379, validation loss: 0.3367
2024-06-03 10:21:57 [INFO]: Epoch 013 - training loss: 0.2746, validation loss: 0.3248
2024-06-03 10:22:13 [INFO]: Epoch 014 - training loss: 0.3015, validation loss: 0.3385
2024-06-03 10:22:28 [INFO]: Epoch 015 - training loss: 0.3432, validation loss: 0.3034
2024-06-03 10:22:42 [INFO]: Epoch 016 - training loss: 0.3279, validation loss: 0.3170
2024-06-03 10:22:56 [INFO]: Epoch 017 - training loss: 0.3234, validation loss: 0.3283
2024-06-03 10:23:11 [INFO]: Epoch 018 - training loss: 0.3213, validation loss: 0.3140
2024-06-03 10:23:26 [INFO]: Epoch 019 - training loss: 0.2679, validation loss: 0.2959
2024-06-03 10:23:40 [INFO]: Epoch 020 - training loss: 0.3093, validation loss: 0.3039
2024-06-03 10:23:55 [INFO]: Epoch 021 - training loss: 0.2694, validation loss: 0.3038
2024-06-03 10:24:08 [INFO]: Epoch 022 - training loss: 0.3266, validation loss: 0.3084
2024-06-03 10:24:20 [INFO]: Epoch 023 - training loss: 0.2871, validation loss: 0.3015
2024-06-03 10:24:32 [INFO]: Epoch 024 - training loss: 0.3065, validation loss: 0.2860
2024-06-03 10:24:44 [INFO]: Epoch 025 - training loss: 0.2906, validation loss: 0.2753
2024-06-03 10:24:56 [INFO]: Epoch 026 - training loss: 0.2064, validation loss: 0.2859
2024-06-03 10:25:07 [INFO]: Epoch 027 - training loss: 0.3218, validation loss: 0.2698
2024-06-03 10:25:20 [INFO]: Epoch 028 - training loss: 0.2197, validation loss: 0.2662
2024-06-03 10:25:32 [INFO]: Epoch 029 - training loss: 0.2684, validation loss: 0.2715
2024-06-03 10:25:44 [INFO]: Epoch 030 - training loss: 0.2107, validation loss: 0.2593
2024-06-03 10:25:56 [INFO]: Epoch 031 - training loss: 0.2397, validation loss: 0.2603
2024-06-03 10:26:07 [INFO]: Epoch 032 - training loss: 0.2204, validation loss: 0.2554
2024-06-03 10:26:19 [INFO]: Epoch 033 - training loss: 0.2028, validation loss: 0.2472
2024-06-03 10:26:30 [INFO]: Epoch 034 - training loss: 0.2455, validation loss: 0.2564
2024-06-03 10:26:41 [INFO]: Epoch 035 - training loss: 0.2687, validation loss: 0.2657
2024-06-03 10:26:52 [INFO]: Epoch 036 - training loss: 0.2412, validation loss: 0.2506
2024-06-03 10:27:03 [INFO]: Epoch 037 - training loss: 0.2314, validation loss: 0.2586
2024-06-03 10:27:13 [INFO]: Epoch 038 - training loss: 0.1983, validation loss: 0.2636
2024-06-03 10:27:24 [INFO]: Epoch 039 - training loss: 0.2100, validation loss: 0.2426
2024-06-03 10:27:35 [INFO]: Epoch 040 - training loss: 0.2125, validation loss: 0.2428
2024-06-03 10:27:45 [INFO]: Epoch 041 - training loss: 0.2837, validation loss: 0.2456
2024-06-03 10:27:56 [INFO]: Epoch 042 - training loss: 0.2674, validation loss: 0.2438
2024-06-03 10:28:07 [INFO]: Epoch 043 - training loss: 0.2220, validation loss: 0.2412
2024-06-03 10:28:16 [INFO]: Epoch 044 - training loss: 0.2503, validation loss: 0.2311
2024-06-03 10:28:26 [INFO]: Epoch 045 - training loss: 0.2412, validation loss: 0.2428
2024-06-03 10:28:35 [INFO]: Epoch 046 - training loss: 0.2202, validation loss: 0.2332
2024-06-03 10:28:44 [INFO]: Epoch 047 - training loss: 0.2135, validation loss: 0.2413
2024-06-03 10:28:52 [INFO]: Epoch 048 - training loss: 0.2469, validation loss: 0.2303
2024-06-03 10:29:00 [INFO]: Epoch 049 - training loss: 0.2579, validation loss: 0.2290
2024-06-03 10:29:06 [INFO]: Epoch 050 - training loss: 0.2128, validation loss: 0.2360
2024-06-03 10:29:13 [INFO]: Epoch 051 - training loss: 0.1913, validation loss: 0.2359
2024-06-03 10:29:20 [INFO]: Epoch 052 - training loss: 0.2834, validation loss: 0.2200
2024-06-03 10:29:26 [INFO]: Epoch 053 - training loss: 0.2072, validation loss: 0.2228
2024-06-03 10:29:33 [INFO]: Epoch 054 - training loss: 0.2182, validation loss: 0.2149
2024-06-03 10:29:40 [INFO]: Epoch 055 - training loss: 0.1465, validation loss: 0.2213
2024-06-03 10:29:46 [INFO]: Epoch 056 - training loss: 0.2163, validation loss: 0.2197
2024-06-03 10:29:51 [INFO]: Epoch 057 - training loss: 0.2300, validation loss: 0.2240
2024-06-03 10:29:57 [INFO]: Epoch 058 - training loss: 0.2020, validation loss: 0.2264
2024-06-03 10:30:03 [INFO]: Epoch 059 - training loss: 0.2100, validation loss: 0.2194
2024-06-03 10:30:09 [INFO]: Epoch 060 - training loss: 0.2226, validation loss: 0.2177
2024-06-03 10:30:14 [INFO]: Epoch 061 - training loss: 0.2200, validation loss: 0.2248
2024-06-03 10:30:20 [INFO]: Epoch 062 - training loss: 0.2156, validation loss: 0.2148
2024-06-03 10:30:26 [INFO]: Epoch 063 - training loss: 0.1765, validation loss: 0.2201
2024-06-03 10:30:32 [INFO]: Epoch 064 - training loss: 0.2210, validation loss: 0.2184
2024-06-03 10:30:38 [INFO]: Epoch 065 - training loss: 0.1964, validation loss: 0.2184
2024-06-03 10:30:44 [INFO]: Epoch 066 - training loss: 0.1649, validation loss: 0.2212
2024-06-03 10:30:49 [INFO]: Epoch 067 - training loss: 0.1897, validation loss: 0.2149
2024-06-03 10:30:54 [INFO]: Epoch 068 - training loss: 0.2138, validation loss: 0.2197
2024-06-03 10:31:00 [INFO]: Epoch 069 - training loss: 0.1613, validation loss: 0.2158
2024-06-03 10:31:05 [INFO]: Epoch 070 - training loss: 0.1747, validation loss: 0.2177
2024-06-03 10:31:10 [INFO]: Epoch 071 - training loss: 0.1914, validation loss: 0.2224
2024-06-03 10:31:15 [INFO]: Epoch 072 - training loss: 0.1934, validation loss: 0.2285
2024-06-03 10:31:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:31:15 [INFO]: Finished training. The best model is from epoch#62.
2024-06-03 10:31:16 [INFO]: Saved the model to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_0/20240603_T101725/CSDI.pypots
2024-06-03 10:33:27 [INFO]: Successfully saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_0/imputation.pkl
2024-06-03 10:33:27 [INFO]: Round0 - CSDI on ETT_h1: MAE=0.3706, MSE=0.2917, MRE=0.4602
2024-06-03 10:33:27 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:33:27 [INFO]: Using the given device: cuda:0
2024-06-03 10:33:27 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_1/20240603_T103327
2024-06-03 10:33:27 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_1/20240603_T103327/tensorboard
2024-06-03 10:33:27 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 10:33:30 [INFO]: Epoch 001 - training loss: 0.7691, validation loss: 0.5271
2024-06-03 10:33:33 [INFO]: Epoch 002 - training loss: 0.4261, validation loss: 0.4056
2024-06-03 10:33:35 [INFO]: Epoch 003 - training loss: 0.4110, validation loss: 0.4051
2024-06-03 10:33:38 [INFO]: Epoch 004 - training loss: 0.3982, validation loss: 0.3643
2024-06-03 10:33:40 [INFO]: Epoch 005 - training loss: 0.3760, validation loss: 0.3781
2024-06-03 10:33:43 [INFO]: Epoch 006 - training loss: 0.3156, validation loss: 0.3575
2024-06-03 10:33:45 [INFO]: Epoch 007 - training loss: 0.3812, validation loss: 0.3741
2024-06-03 10:33:48 [INFO]: Epoch 008 - training loss: 0.3524, validation loss: 0.3671
2024-06-03 10:33:50 [INFO]: Epoch 009 - training loss: 0.2922, validation loss: 0.3442
2024-06-03 10:33:52 [INFO]: Epoch 010 - training loss: 0.2955, validation loss: 0.3555
2024-06-03 10:33:55 [INFO]: Epoch 011 - training loss: 0.3097, validation loss: 0.3593
2024-06-03 10:33:57 [INFO]: Epoch 012 - training loss: 0.2723, validation loss: 0.3374
2024-06-03 10:34:00 [INFO]: Epoch 013 - training loss: 0.3157, validation loss: 0.3261
2024-06-03 10:34:02 [INFO]: Epoch 014 - training loss: 0.2578, validation loss: 0.3430
2024-06-03 10:34:05 [INFO]: Epoch 015 - training loss: 0.2603, validation loss: 0.3102
2024-06-03 10:34:07 [INFO]: Epoch 016 - training loss: 0.3167, validation loss: 0.3101
2024-06-03 10:34:10 [INFO]: Epoch 017 - training loss: 0.3271, validation loss: 0.3033
2024-06-03 10:34:12 [INFO]: Epoch 018 - training loss: 0.2493, validation loss: 0.2971
2024-06-03 10:34:14 [INFO]: Epoch 019 - training loss: 0.2512, validation loss: 0.3150
2024-06-03 10:34:17 [INFO]: Epoch 020 - training loss: 0.2855, validation loss: 0.3070
2024-06-03 10:34:19 [INFO]: Epoch 021 - training loss: 0.3094, validation loss: 0.2927
2024-06-03 10:34:22 [INFO]: Epoch 022 - training loss: 0.2664, validation loss: 0.2869
2024-06-03 10:34:25 [INFO]: Epoch 023 - training loss: 0.2867, validation loss: 0.2843
2024-06-03 10:34:27 [INFO]: Epoch 024 - training loss: 0.2945, validation loss: 0.2968
2024-06-03 10:34:30 [INFO]: Epoch 025 - training loss: 0.2574, validation loss: 0.2807
2024-06-03 10:34:32 [INFO]: Epoch 026 - training loss: 0.3095, validation loss: 0.2705
2024-06-03 10:34:35 [INFO]: Epoch 027 - training loss: 0.2410, validation loss: 0.2771
2024-06-03 10:34:37 [INFO]: Epoch 028 - training loss: 0.2441, validation loss: 0.2750
2024-06-03 10:34:40 [INFO]: Epoch 029 - training loss: 0.2637, validation loss: 0.2610
2024-06-03 10:34:42 [INFO]: Epoch 030 - training loss: 0.2385, validation loss: 0.2657
2024-06-03 10:34:45 [INFO]: Epoch 031 - training loss: 0.2193, validation loss: 0.2580
2024-06-03 10:34:47 [INFO]: Epoch 032 - training loss: 0.2540, validation loss: 0.2514
2024-06-03 10:34:50 [INFO]: Epoch 033 - training loss: 0.2547, validation loss: 0.2632
2024-06-03 10:34:52 [INFO]: Epoch 034 - training loss: 0.2148, validation loss: 0.2628
2024-06-03 10:34:55 [INFO]: Epoch 035 - training loss: 0.2388, validation loss: 0.2655
2024-06-03 10:34:57 [INFO]: Epoch 036 - training loss: 0.2805, validation loss: 0.2575
2024-06-03 10:35:00 [INFO]: Epoch 037 - training loss: 0.1981, validation loss: 0.2536
2024-06-03 10:35:02 [INFO]: Epoch 038 - training loss: 0.2664, validation loss: 0.2468
2024-06-03 10:35:05 [INFO]: Epoch 039 - training loss: 0.2159, validation loss: 0.2506
2024-06-03 10:35:07 [INFO]: Epoch 040 - training loss: 0.2650, validation loss: 0.2411
2024-06-03 10:35:10 [INFO]: Epoch 041 - training loss: 0.2500, validation loss: 0.2510
2024-06-03 10:35:12 [INFO]: Epoch 042 - training loss: 0.2097, validation loss: 0.2474
2024-06-03 10:35:15 [INFO]: Epoch 043 - training loss: 0.2344, validation loss: 0.2474
2024-06-03 10:35:17 [INFO]: Epoch 044 - training loss: 0.2534, validation loss: 0.2488
2024-06-03 10:35:19 [INFO]: Epoch 045 - training loss: 0.2702, validation loss: 0.2366
2024-06-03 10:35:22 [INFO]: Epoch 046 - training loss: 0.2297, validation loss: 0.2443
2024-06-03 10:35:24 [INFO]: Epoch 047 - training loss: 0.1930, validation loss: 0.2354
2024-06-03 10:35:27 [INFO]: Epoch 048 - training loss: 0.2234, validation loss: 0.2336
2024-06-03 10:35:29 [INFO]: Epoch 049 - training loss: 0.2731, validation loss: 0.2340
2024-06-03 10:35:32 [INFO]: Epoch 050 - training loss: 0.2374, validation loss: 0.2390
2024-06-03 10:35:34 [INFO]: Epoch 051 - training loss: 0.1939, validation loss: 0.2317
2024-06-03 10:35:37 [INFO]: Epoch 052 - training loss: 0.1761, validation loss: 0.2277
2024-06-03 10:35:39 [INFO]: Epoch 053 - training loss: 0.1868, validation loss: 0.2351
2024-06-03 10:35:42 [INFO]: Epoch 054 - training loss: 0.2155, validation loss: 0.2278
2024-06-03 10:35:44 [INFO]: Epoch 055 - training loss: 0.2475, validation loss: 0.2234
2024-06-03 10:35:46 [INFO]: Epoch 056 - training loss: 0.1966, validation loss: 0.2283
2024-06-03 10:35:49 [INFO]: Epoch 057 - training loss: 0.2172, validation loss: 0.2205
2024-06-03 10:35:51 [INFO]: Epoch 058 - training loss: 0.2354, validation loss: 0.2311
2024-06-03 10:35:54 [INFO]: Epoch 059 - training loss: 0.1964, validation loss: 0.2247
2024-06-03 10:35:56 [INFO]: Epoch 060 - training loss: 0.1673, validation loss: 0.2271
2024-06-03 10:35:59 [INFO]: Epoch 061 - training loss: 0.1868, validation loss: 0.2222
2024-06-03 10:36:01 [INFO]: Epoch 062 - training loss: 0.1839, validation loss: 0.2206
2024-06-03 10:36:04 [INFO]: Epoch 063 - training loss: 0.1688, validation loss: 0.2213
2024-06-03 10:36:06 [INFO]: Epoch 064 - training loss: 0.1896, validation loss: 0.2187
2024-06-03 10:36:09 [INFO]: Epoch 065 - training loss: 0.2173, validation loss: 0.2229
2024-06-03 10:36:11 [INFO]: Epoch 066 - training loss: 0.2090, validation loss: 0.2227
2024-06-03 10:36:13 [INFO]: Epoch 067 - training loss: 0.1792, validation loss: 0.2181
2024-06-03 10:36:16 [INFO]: Epoch 068 - training loss: 0.2138, validation loss: 0.2242
2024-06-03 10:36:18 [INFO]: Epoch 069 - training loss: 0.2428, validation loss: 0.2382
2024-06-03 10:36:20 [INFO]: Epoch 070 - training loss: 0.1947, validation loss: 0.2155
2024-06-03 10:36:23 [INFO]: Epoch 071 - training loss: 0.2037, validation loss: 0.2229
2024-06-03 10:36:25 [INFO]: Epoch 072 - training loss: 0.2163, validation loss: 0.2240
2024-06-03 10:36:28 [INFO]: Epoch 073 - training loss: 0.2011, validation loss: 0.2261
2024-06-03 10:36:30 [INFO]: Epoch 074 - training loss: 0.2327, validation loss: 0.2195
2024-06-03 10:36:33 [INFO]: Epoch 075 - training loss: 0.1959, validation loss: 0.2170
2024-06-03 10:36:35 [INFO]: Epoch 076 - training loss: 0.2512, validation loss: 0.2158
2024-06-03 10:36:37 [INFO]: Epoch 077 - training loss: 0.2170, validation loss: 0.2208
2024-06-03 10:36:40 [INFO]: Epoch 078 - training loss: 0.1831, validation loss: 0.2112
2024-06-03 10:36:43 [INFO]: Epoch 079 - training loss: 0.2183, validation loss: 0.2087
2024-06-03 10:36:45 [INFO]: Epoch 080 - training loss: 0.2318, validation loss: 0.2097
2024-06-03 10:36:48 [INFO]: Epoch 081 - training loss: 0.2318, validation loss: 0.2128
2024-06-03 10:36:50 [INFO]: Epoch 082 - training loss: 0.1897, validation loss: 0.2185
2024-06-03 10:36:53 [INFO]: Epoch 083 - training loss: 0.1515, validation loss: 0.2165
2024-06-03 10:36:55 [INFO]: Epoch 084 - training loss: 0.1854, validation loss: 0.2072
2024-06-03 10:36:58 [INFO]: Epoch 085 - training loss: 0.2346, validation loss: 0.2042
2024-06-03 10:37:00 [INFO]: Epoch 086 - training loss: 0.2267, validation loss: 0.2043
2024-06-03 10:37:03 [INFO]: Epoch 087 - training loss: 0.2149, validation loss: 0.2167
2024-06-03 10:37:05 [INFO]: Epoch 088 - training loss: 0.1777, validation loss: 0.2117
2024-06-03 10:37:08 [INFO]: Epoch 089 - training loss: 0.2271, validation loss: 0.2062
2024-06-03 10:37:10 [INFO]: Epoch 090 - training loss: 0.1903, validation loss: 0.2182
2024-06-03 10:37:13 [INFO]: Epoch 091 - training loss: 0.2031, validation loss: 0.2071
2024-06-03 10:37:15 [INFO]: Epoch 092 - training loss: 0.2005, validation loss: 0.2056
2024-06-03 10:37:17 [INFO]: Epoch 093 - training loss: 0.1831, validation loss: 0.2138
2024-06-03 10:37:20 [INFO]: Epoch 094 - training loss: 0.1830, validation loss: 0.2116
2024-06-03 10:37:22 [INFO]: Epoch 095 - training loss: 0.2665, validation loss: 0.2117
2024-06-03 10:37:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:37:22 [INFO]: Finished training. The best model is from epoch#85.
2024-06-03 10:37:22 [INFO]: Saved the model to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_1/20240603_T103327/CSDI.pypots
2024-06-03 10:38:48 [INFO]: Successfully saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_1/imputation.pkl
2024-06-03 10:38:48 [INFO]: Round1 - CSDI on ETT_h1: MAE=0.3534, MSE=0.2718, MRE=0.4388
2024-06-03 10:38:48 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:38:48 [INFO]: Using the given device: cuda:0
2024-06-03 10:38:48 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_2/20240603_T103848
2024-06-03 10:38:48 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_2/20240603_T103848/tensorboard
2024-06-03 10:38:48 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 10:38:51 [INFO]: Epoch 001 - training loss: 0.7327, validation loss: 0.4546
2024-06-03 10:38:53 [INFO]: Epoch 002 - training loss: 0.5350, validation loss: 0.3995
2024-06-03 10:38:56 [INFO]: Epoch 003 - training loss: 0.4019, validation loss: 0.4286
2024-06-03 10:38:58 [INFO]: Epoch 004 - training loss: 0.4128, validation loss: 0.3955
2024-06-03 10:39:00 [INFO]: Epoch 005 - training loss: 0.3310, validation loss: 0.3857
2024-06-03 10:39:03 [INFO]: Epoch 006 - training loss: 0.3440, validation loss: 0.3629
2024-06-03 10:39:05 [INFO]: Epoch 007 - training loss: 0.3432, validation loss: 0.3830
2024-06-03 10:39:08 [INFO]: Epoch 008 - training loss: 0.3488, validation loss: 0.3596
2024-06-03 10:39:10 [INFO]: Epoch 009 - training loss: 0.3316, validation loss: 0.3556
2024-06-03 10:39:13 [INFO]: Epoch 010 - training loss: 0.2947, validation loss: 0.3688
2024-06-03 10:39:15 [INFO]: Epoch 011 - training loss: 0.3679, validation loss: 0.3325
2024-06-03 10:39:17 [INFO]: Epoch 012 - training loss: 0.3147, validation loss: 0.3707
2024-06-03 10:39:20 [INFO]: Epoch 013 - training loss: 0.3331, validation loss: 0.3283
2024-06-03 10:39:22 [INFO]: Epoch 014 - training loss: 0.2198, validation loss: 0.3336
2024-06-03 10:39:25 [INFO]: Epoch 015 - training loss: 0.2910, validation loss: 0.3194
2024-06-03 10:39:27 [INFO]: Epoch 016 - training loss: 0.2684, validation loss: 0.3042
2024-06-03 10:39:30 [INFO]: Epoch 017 - training loss: 0.2597, validation loss: 0.3152
2024-06-03 10:39:32 [INFO]: Epoch 018 - training loss: 0.3183, validation loss: 0.3114
2024-06-03 10:39:35 [INFO]: Epoch 019 - training loss: 0.3078, validation loss: 0.3038
2024-06-03 10:39:37 [INFO]: Epoch 020 - training loss: 0.2900, validation loss: 0.2899
2024-06-03 10:39:40 [INFO]: Epoch 021 - training loss: 0.2889, validation loss: 0.3186
2024-06-03 10:39:43 [INFO]: Epoch 022 - training loss: 0.2581, validation loss: 0.2982
2024-06-03 10:39:45 [INFO]: Epoch 023 - training loss: 0.3080, validation loss: 0.2953
2024-06-03 10:39:47 [INFO]: Epoch 024 - training loss: 0.2317, validation loss: 0.2834
2024-06-03 10:39:50 [INFO]: Epoch 025 - training loss: 0.2342, validation loss: 0.2760
2024-06-03 10:39:53 [INFO]: Epoch 026 - training loss: 0.2420, validation loss: 0.2836
2024-06-03 10:39:55 [INFO]: Epoch 027 - training loss: 0.2065, validation loss: 0.2775
2024-06-03 10:39:58 [INFO]: Epoch 028 - training loss: 0.2744, validation loss: 0.2704
2024-06-03 10:40:00 [INFO]: Epoch 029 - training loss: 0.2934, validation loss: 0.2642
2024-06-03 10:40:03 [INFO]: Epoch 030 - training loss: 0.2074, validation loss: 0.2884
2024-06-03 10:40:05 [INFO]: Epoch 031 - training loss: 0.2637, validation loss: 0.2587
2024-06-03 10:40:08 [INFO]: Epoch 032 - training loss: 0.2409, validation loss: 0.2631
2024-06-03 10:40:10 [INFO]: Epoch 033 - training loss: 0.2605, validation loss: 0.2506
2024-06-03 10:40:13 [INFO]: Epoch 034 - training loss: 0.2386, validation loss: 0.2491
2024-06-03 10:40:15 [INFO]: Epoch 035 - training loss: 0.2190, validation loss: 0.2534
2024-06-03 10:40:18 [INFO]: Epoch 036 - training loss: 0.2643, validation loss: 0.2484
2024-06-03 10:40:20 [INFO]: Epoch 037 - training loss: 0.2394, validation loss: 0.2492
2024-06-03 10:40:23 [INFO]: Epoch 038 - training loss: 0.2614, validation loss: 0.2440
2024-06-03 10:40:25 [INFO]: Epoch 039 - training loss: 0.2646, validation loss: 0.2485
2024-06-03 10:40:27 [INFO]: Epoch 040 - training loss: 0.2384, validation loss: 0.2422
2024-06-03 10:40:30 [INFO]: Epoch 041 - training loss: 0.1749, validation loss: 0.2473
2024-06-03 10:40:32 [INFO]: Epoch 042 - training loss: 0.2027, validation loss: 0.2350
2024-06-03 10:40:35 [INFO]: Epoch 043 - training loss: 0.2260, validation loss: 0.2293
2024-06-03 10:40:37 [INFO]: Epoch 044 - training loss: 0.2115, validation loss: 0.2335
2024-06-03 10:40:40 [INFO]: Epoch 045 - training loss: 0.2317, validation loss: 0.2391
2024-06-03 10:40:42 [INFO]: Epoch 046 - training loss: 0.2731, validation loss: 0.2425
2024-06-03 10:40:45 [INFO]: Epoch 047 - training loss: 0.2079, validation loss: 0.2400
2024-06-03 10:40:47 [INFO]: Epoch 048 - training loss: 0.1828, validation loss: 0.2477
2024-06-03 10:40:50 [INFO]: Epoch 049 - training loss: 0.1990, validation loss: 0.2385
2024-06-03 10:40:52 [INFO]: Epoch 050 - training loss: 0.1854, validation loss: 0.2333
2024-06-03 10:40:55 [INFO]: Epoch 051 - training loss: 0.1961, validation loss: 0.2374
2024-06-03 10:40:57 [INFO]: Epoch 052 - training loss: 0.2327, validation loss: 0.2313
2024-06-03 10:41:00 [INFO]: Epoch 053 - training loss: 0.2080, validation loss: 0.2334
2024-06-03 10:41:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:41:00 [INFO]: Finished training. The best model is from epoch#43.
2024-06-03 10:41:00 [INFO]: Saved the model to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_2/20240603_T103848/CSDI.pypots
2024-06-03 10:42:25 [INFO]: Successfully saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_2/imputation.pkl
2024-06-03 10:42:25 [INFO]: Round2 - CSDI on ETT_h1: MAE=0.4003, MSE=0.3435, MRE=0.4970
2024-06-03 10:42:25 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:42:25 [INFO]: Using the given device: cuda:0
2024-06-03 10:42:25 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_3/20240603_T104225
2024-06-03 10:42:25 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_3/20240603_T104225/tensorboard
2024-06-03 10:42:25 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 10:42:28 [INFO]: Epoch 001 - training loss: 0.7695, validation loss: 0.5139
2024-06-03 10:42:30 [INFO]: Epoch 002 - training loss: 0.5447, validation loss: 0.4445
2024-06-03 10:42:33 [INFO]: Epoch 003 - training loss: 0.4203, validation loss: 0.3870
2024-06-03 10:42:35 [INFO]: Epoch 004 - training loss: 0.3271, validation loss: 0.4036
2024-06-03 10:42:38 [INFO]: Epoch 005 - training loss: 0.3355, validation loss: 0.3719
2024-06-03 10:42:40 [INFO]: Epoch 006 - training loss: 0.3343, validation loss: 0.3942
2024-06-03 10:42:43 [INFO]: Epoch 007 - training loss: 0.3370, validation loss: 0.3588
2024-06-03 10:42:45 [INFO]: Epoch 008 - training loss: 0.3657, validation loss: 0.3302
2024-06-03 10:42:48 [INFO]: Epoch 009 - training loss: 0.3316, validation loss: 0.3392
2024-06-03 10:42:50 [INFO]: Epoch 010 - training loss: 0.3298, validation loss: 0.3286
2024-06-03 10:42:53 [INFO]: Epoch 011 - training loss: 0.3531, validation loss: 0.3419
2024-06-03 10:42:55 [INFO]: Epoch 012 - training loss: 0.3042, validation loss: 0.3649
2024-06-03 10:42:58 [INFO]: Epoch 013 - training loss: 0.3268, validation loss: 0.3571
2024-06-03 10:43:00 [INFO]: Epoch 014 - training loss: 0.3376, validation loss: 0.3205
2024-06-03 10:43:03 [INFO]: Epoch 015 - training loss: 0.2975, validation loss: 0.3447
2024-06-03 10:43:05 [INFO]: Epoch 016 - training loss: 0.3065, validation loss: 0.3294
2024-06-03 10:43:07 [INFO]: Epoch 017 - training loss: 0.2895, validation loss: 0.3111
2024-06-03 10:43:10 [INFO]: Epoch 018 - training loss: 0.2882, validation loss: 0.3020
2024-06-03 10:43:12 [INFO]: Epoch 019 - training loss: 0.3116, validation loss: 0.2958
2024-06-03 10:43:15 [INFO]: Epoch 020 - training loss: 0.2991, validation loss: 0.2967
2024-06-03 10:43:17 [INFO]: Epoch 021 - training loss: 0.2648, validation loss: 0.2903
2024-06-03 10:43:20 [INFO]: Epoch 022 - training loss: 0.3049, validation loss: 0.2934
2024-06-03 10:43:22 [INFO]: Epoch 023 - training loss: 0.2044, validation loss: 0.2855
2024-06-03 10:43:24 [INFO]: Epoch 024 - training loss: 0.2548, validation loss: 0.2699
2024-06-03 10:43:27 [INFO]: Epoch 025 - training loss: 0.2238, validation loss: 0.2826
2024-06-03 10:43:29 [INFO]: Epoch 026 - training loss: 0.2421, validation loss: 0.2850
2024-06-03 10:43:32 [INFO]: Epoch 027 - training loss: 0.2546, validation loss: 0.2671
2024-06-03 10:43:35 [INFO]: Epoch 028 - training loss: 0.3111, validation loss: 0.2942
2024-06-03 10:43:37 [INFO]: Epoch 029 - training loss: 0.2259, validation loss: 0.2736
2024-06-03 10:43:39 [INFO]: Epoch 030 - training loss: 0.2583, validation loss: 0.2641
2024-06-03 10:43:42 [INFO]: Epoch 031 - training loss: 0.2419, validation loss: 0.2622
2024-06-03 10:43:44 [INFO]: Epoch 032 - training loss: 0.2929, validation loss: 0.2553
2024-06-03 10:43:47 [INFO]: Epoch 033 - training loss: 0.2263, validation loss: 0.2539
2024-06-03 10:43:49 [INFO]: Epoch 034 - training loss: 0.2257, validation loss: 0.2569
2024-06-03 10:43:52 [INFO]: Epoch 035 - training loss: 0.2190, validation loss: 0.2528
2024-06-03 10:43:54 [INFO]: Epoch 036 - training loss: 0.2677, validation loss: 0.2520
2024-06-03 10:43:57 [INFO]: Epoch 037 - training loss: 0.2786, validation loss: 0.2475
2024-06-03 10:43:59 [INFO]: Epoch 038 - training loss: 0.1961, validation loss: 0.2514
2024-06-03 10:44:01 [INFO]: Epoch 039 - training loss: 0.1989, validation loss: 0.2482
2024-06-03 10:44:04 [INFO]: Epoch 040 - training loss: 0.2487, validation loss: 0.2464
2024-06-03 10:44:06 [INFO]: Epoch 041 - training loss: 0.1868, validation loss: 0.2366
2024-06-03 10:44:09 [INFO]: Epoch 042 - training loss: 0.1964, validation loss: 0.2439
2024-06-03 10:44:11 [INFO]: Epoch 043 - training loss: 0.1666, validation loss: 0.2334
2024-06-03 10:44:14 [INFO]: Epoch 044 - training loss: 0.2086, validation loss: 0.2377
2024-06-03 10:44:17 [INFO]: Epoch 045 - training loss: 0.2165, validation loss: 0.2432
2024-06-03 10:44:19 [INFO]: Epoch 046 - training loss: 0.2240, validation loss: 0.2442
2024-06-03 10:44:22 [INFO]: Epoch 047 - training loss: 0.2426, validation loss: 0.2383
2024-06-03 10:44:24 [INFO]: Epoch 048 - training loss: 0.2158, validation loss: 0.2300
2024-06-03 10:44:26 [INFO]: Epoch 049 - training loss: 0.2361, validation loss: 0.2295
2024-06-03 10:44:29 [INFO]: Epoch 050 - training loss: 0.2058, validation loss: 0.2327
2024-06-03 10:44:31 [INFO]: Epoch 051 - training loss: 0.2331, validation loss: 0.2290
2024-06-03 10:44:34 [INFO]: Epoch 052 - training loss: 0.2390, validation loss: 0.2297
2024-06-03 10:44:36 [INFO]: Epoch 053 - training loss: 0.2257, validation loss: 0.2282
2024-06-03 10:44:39 [INFO]: Epoch 054 - training loss: 0.2408, validation loss: 0.2424
2024-06-03 10:44:41 [INFO]: Epoch 055 - training loss: 0.2085, validation loss: 0.2281
2024-06-03 10:44:44 [INFO]: Epoch 056 - training loss: 0.2255, validation loss: 0.2282
2024-06-03 10:44:46 [INFO]: Epoch 057 - training loss: 0.2123, validation loss: 0.2259
2024-06-03 10:44:49 [INFO]: Epoch 058 - training loss: 0.2240, validation loss: 0.2294
2024-06-03 10:44:51 [INFO]: Epoch 059 - training loss: 0.1932, validation loss: 0.2170
2024-06-03 10:44:54 [INFO]: Epoch 060 - training loss: 0.2273, validation loss: 0.2204
2024-06-03 10:44:56 [INFO]: Epoch 061 - training loss: 0.1928, validation loss: 0.2151
2024-06-03 10:44:59 [INFO]: Epoch 062 - training loss: 0.2293, validation loss: 0.2177
2024-06-03 10:45:01 [INFO]: Epoch 063 - training loss: 0.2052, validation loss: 0.2209
2024-06-03 10:45:04 [INFO]: Epoch 064 - training loss: 0.2000, validation loss: 0.2198
2024-06-03 10:45:05 [INFO]: Epoch 065 - training loss: 0.2431, validation loss: 0.2185
2024-06-03 10:45:07 [INFO]: Epoch 066 - training loss: 0.1712, validation loss: 0.2188
2024-06-03 10:45:09 [INFO]: Epoch 067 - training loss: 0.1842, validation loss: 0.2182
2024-06-03 10:45:11 [INFO]: Epoch 068 - training loss: 0.2441, validation loss: 0.2147
2024-06-03 10:45:13 [INFO]: Epoch 069 - training loss: 0.1917, validation loss: 0.2176
2024-06-03 10:45:15 [INFO]: Epoch 070 - training loss: 0.2093, validation loss: 0.2180
2024-06-03 10:45:16 [INFO]: Epoch 071 - training loss: 0.1893, validation loss: 0.2163
2024-06-03 10:45:18 [INFO]: Epoch 072 - training loss: 0.2144, validation loss: 0.2271
2024-06-03 10:45:20 [INFO]: Epoch 073 - training loss: 0.2165, validation loss: 0.2205
2024-06-03 10:45:22 [INFO]: Epoch 074 - training loss: 0.1993, validation loss: 0.2163
2024-06-03 10:45:24 [INFO]: Epoch 075 - training loss: 0.2131, validation loss: 0.2333
2024-06-03 10:45:26 [INFO]: Epoch 076 - training loss: 0.2304, validation loss: 0.2183
2024-06-03 10:45:28 [INFO]: Epoch 077 - training loss: 0.1924, validation loss: 0.2141
2024-06-03 10:45:30 [INFO]: Epoch 078 - training loss: 0.2354, validation loss: 0.2191
2024-06-03 10:45:32 [INFO]: Epoch 079 - training loss: 0.2093, validation loss: 0.2110
2024-06-03 10:45:34 [INFO]: Epoch 080 - training loss: 0.1911, validation loss: 0.2107
2024-06-03 10:45:36 [INFO]: Epoch 081 - training loss: 0.2107, validation loss: 0.2129
2024-06-03 10:45:38 [INFO]: Epoch 082 - training loss: 0.2411, validation loss: 0.2153
2024-06-03 10:45:39 [INFO]: Epoch 083 - training loss: 0.1885, validation loss: 0.2314
2024-06-03 10:45:41 [INFO]: Epoch 084 - training loss: 0.2104, validation loss: 0.2116
2024-06-03 10:45:43 [INFO]: Epoch 085 - training loss: 0.2173, validation loss: 0.2047
2024-06-03 10:45:45 [INFO]: Epoch 086 - training loss: 0.2382, validation loss: 0.2141
2024-06-03 10:45:47 [INFO]: Epoch 087 - training loss: 0.2058, validation loss: 0.2052
2024-06-03 10:45:49 [INFO]: Epoch 088 - training loss: 0.2121, validation loss: 0.2147
2024-06-03 10:45:51 [INFO]: Epoch 089 - training loss: 0.2237, validation loss: 0.2130
2024-06-03 10:45:53 [INFO]: Epoch 090 - training loss: 0.1740, validation loss: 0.2055
2024-06-03 10:45:55 [INFO]: Epoch 091 - training loss: 0.2201, validation loss: 0.2059
2024-06-03 10:45:57 [INFO]: Epoch 092 - training loss: 0.2261, validation loss: 0.2119
2024-06-03 10:45:59 [INFO]: Epoch 093 - training loss: 0.1744, validation loss: 0.2130
2024-06-03 10:46:00 [INFO]: Epoch 094 - training loss: 0.2145, validation loss: 0.2033
2024-06-03 10:46:02 [INFO]: Epoch 095 - training loss: 0.1995, validation loss: 0.2079
2024-06-03 10:46:04 [INFO]: Epoch 096 - training loss: 0.2015, validation loss: 0.2126
2024-06-03 10:46:06 [INFO]: Epoch 097 - training loss: 0.1700, validation loss: 0.2120
2024-06-03 10:46:08 [INFO]: Epoch 098 - training loss: 0.1799, validation loss: 0.2038
2024-06-03 10:46:10 [INFO]: Epoch 099 - training loss: 0.2169, validation loss: 0.2117
2024-06-03 10:46:12 [INFO]: Epoch 100 - training loss: 0.2162, validation loss: 0.2051
2024-06-03 10:46:12 [INFO]: Finished training. The best model is from epoch#94.
2024-06-03 10:46:12 [INFO]: Saved the model to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_3/20240603_T104225/CSDI.pypots
2024-06-03 10:47:15 [INFO]: Successfully saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_3/imputation.pkl
2024-06-03 10:47:15 [INFO]: Round3 - CSDI on ETT_h1: MAE=0.3498, MSE=0.2606, MRE=0.4344
2024-06-03 10:47:15 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:47:15 [INFO]: Using the given device: cuda:0
2024-06-03 10:47:15 [INFO]: Model files will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_4/20240603_T104715
2024-06-03 10:47:15 [INFO]: Tensorboard file will be saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_4/20240603_T104715/tensorboard
2024-06-03 10:47:15 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 10:47:17 [INFO]: Epoch 001 - training loss: 0.7454, validation loss: 0.5387
2024-06-03 10:47:19 [INFO]: Epoch 002 - training loss: 0.4770, validation loss: 0.4622
2024-06-03 10:47:21 [INFO]: Epoch 003 - training loss: 0.4319, validation loss: 0.4378
2024-06-03 10:47:23 [INFO]: Epoch 004 - training loss: 0.3607, validation loss: 0.3792
2024-06-03 10:47:25 [INFO]: Epoch 005 - training loss: 0.3976, validation loss: 0.3742
2024-06-03 10:47:27 [INFO]: Epoch 006 - training loss: 0.3143, validation loss: 0.3524
2024-06-03 10:47:29 [INFO]: Epoch 007 - training loss: 0.3082, validation loss: 0.3734
2024-06-03 10:47:30 [INFO]: Epoch 008 - training loss: 0.3371, validation loss: 0.3573
2024-06-03 10:47:32 [INFO]: Epoch 009 - training loss: 0.3532, validation loss: 0.3524
2024-06-03 10:47:34 [INFO]: Epoch 010 - training loss: 0.3336, validation loss: 0.3336
2024-06-03 10:47:36 [INFO]: Epoch 011 - training loss: 0.3431, validation loss: 0.3312
2024-06-03 10:47:38 [INFO]: Epoch 012 - training loss: 0.3333, validation loss: 0.3227
2024-06-03 10:47:40 [INFO]: Epoch 013 - training loss: 0.3467, validation loss: 0.3322
2024-06-03 10:47:42 [INFO]: Epoch 014 - training loss: 0.2894, validation loss: 0.3187
2024-06-03 10:47:44 [INFO]: Epoch 015 - training loss: 0.3312, validation loss: 0.3143
2024-06-03 10:47:45 [INFO]: Epoch 016 - training loss: 0.3542, validation loss: 0.3338
2024-06-03 10:47:47 [INFO]: Epoch 017 - training loss: 0.2868, validation loss: 0.3036
2024-06-03 10:47:49 [INFO]: Epoch 018 - training loss: 0.3089, validation loss: 0.2979
2024-06-03 10:47:51 [INFO]: Epoch 019 - training loss: 0.2525, validation loss: 0.3138
2024-06-03 10:47:53 [INFO]: Epoch 020 - training loss: 0.3215, validation loss: 0.3025
2024-06-03 10:47:55 [INFO]: Epoch 021 - training loss: 0.2899, validation loss: 0.2989
2024-06-03 10:47:57 [INFO]: Epoch 022 - training loss: 0.2820, validation loss: 0.2886
2024-06-03 10:47:59 [INFO]: Epoch 023 - training loss: 0.2536, validation loss: 0.2979
2024-06-03 10:48:01 [INFO]: Epoch 024 - training loss: 0.2861, validation loss: 0.2959
2024-06-03 10:48:03 [INFO]: Epoch 025 - training loss: 0.2921, validation loss: 0.2791
2024-06-03 10:48:05 [INFO]: Epoch 026 - training loss: 0.2179, validation loss: 0.2653
2024-06-03 10:48:06 [INFO]: Epoch 027 - training loss: 0.3127, validation loss: 0.2839
2024-06-03 10:48:08 [INFO]: Epoch 028 - training loss: 0.2547, validation loss: 0.2780
2024-06-03 10:48:10 [INFO]: Epoch 029 - training loss: 0.3068, validation loss: 0.2714
2024-06-03 10:48:12 [INFO]: Epoch 030 - training loss: 0.2411, validation loss: 0.2666
2024-06-03 10:48:14 [INFO]: Epoch 031 - training loss: 0.2790, validation loss: 0.2572
2024-06-03 10:48:16 [INFO]: Epoch 032 - training loss: 0.2190, validation loss: 0.2714
2024-06-03 10:48:18 [INFO]: Epoch 033 - training loss: 0.2395, validation loss: 0.2536
2024-06-03 10:48:20 [INFO]: Epoch 034 - training loss: 0.2415, validation loss: 0.2524
2024-06-03 10:48:21 [INFO]: Epoch 035 - training loss: 0.2627, validation loss: 0.2516
2024-06-03 10:48:23 [INFO]: Epoch 036 - training loss: 0.2449, validation loss: 0.2455
2024-06-03 10:48:25 [INFO]: Epoch 037 - training loss: 0.2189, validation loss: 0.2432
2024-06-03 10:48:27 [INFO]: Epoch 038 - training loss: 0.2360, validation loss: 0.2420
2024-06-03 10:48:29 [INFO]: Epoch 039 - training loss: 0.2137, validation loss: 0.2416
2024-06-03 10:48:31 [INFO]: Epoch 040 - training loss: 0.2506, validation loss: 0.2587
2024-06-03 10:48:33 [INFO]: Epoch 041 - training loss: 0.2355, validation loss: 0.2492
2024-06-03 10:48:35 [INFO]: Epoch 042 - training loss: 0.3269, validation loss: 0.2481
2024-06-03 10:48:37 [INFO]: Epoch 043 - training loss: 0.2171, validation loss: 0.2515
2024-06-03 10:48:39 [INFO]: Epoch 044 - training loss: 0.2324, validation loss: 0.2385
2024-06-03 10:48:41 [INFO]: Epoch 045 - training loss: 0.2423, validation loss: 0.2472
2024-06-03 10:48:42 [INFO]: Epoch 046 - training loss: 0.2454, validation loss: 0.2393
2024-06-03 10:48:44 [INFO]: Epoch 047 - training loss: 0.2399, validation loss: 0.2400
2024-06-03 10:48:46 [INFO]: Epoch 048 - training loss: 0.2510, validation loss: 0.2353
2024-06-03 10:48:48 [INFO]: Epoch 049 - training loss: 0.2034, validation loss: 0.2259
2024-06-03 10:48:50 [INFO]: Epoch 050 - training loss: 0.2356, validation loss: 0.2301
2024-06-03 10:48:52 [INFO]: Epoch 051 - training loss: 0.2428, validation loss: 0.2236
2024-06-03 10:48:54 [INFO]: Epoch 052 - training loss: 0.1622, validation loss: 0.2293
2024-06-03 10:48:56 [INFO]: Epoch 053 - training loss: 0.2384, validation loss: 0.2251
2024-06-03 10:48:58 [INFO]: Epoch 054 - training loss: 0.1992, validation loss: 0.2245
2024-06-03 10:49:00 [INFO]: Epoch 055 - training loss: 0.1929, validation loss: 0.2292
2024-06-03 10:49:02 [INFO]: Epoch 056 - training loss: 0.1987, validation loss: 0.2305
2024-06-03 10:49:04 [INFO]: Epoch 057 - training loss: 0.2155, validation loss: 0.2249
2024-06-03 10:49:06 [INFO]: Epoch 058 - training loss: 0.1867, validation loss: 0.2299
2024-06-03 10:49:07 [INFO]: Epoch 059 - training loss: 0.2652, validation loss: 0.2328
2024-06-03 10:49:09 [INFO]: Epoch 060 - training loss: 0.2557, validation loss: 0.2395
2024-06-03 10:49:11 [INFO]: Epoch 061 - training loss: 0.2215, validation loss: 0.2311
2024-06-03 10:49:11 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:49:11 [INFO]: Finished training. The best model is from epoch#51.
2024-06-03 10:49:11 [INFO]: Saved the model to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_4/20240603_T104715/CSDI.pypots
2024-06-03 10:50:12 [INFO]: Successfully saved to results_block_rate05/ETT_h1/CSDI_ETT_h1/round_4/imputation.pkl
2024-06-03 10:50:12 [INFO]: Round4 - CSDI on ETT_h1: MAE=0.5291, MSE=0.5328, MRE=0.6570
2024-06-03 10:50:12 [INFO]: Done! Final results:
Averaged CSDI (1,194,993 params) on ETT_h1: MAE=0.4006 ± 0.06667141714639302, MSE=0.3401 ± 0.10045765003506042, MRE=0.4975 ± 0.08279069999298959, average inference time=20.91
