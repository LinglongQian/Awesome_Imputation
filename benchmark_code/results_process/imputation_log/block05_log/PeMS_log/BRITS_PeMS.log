2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_0/20240603_T100954
2024-06-03 10:09:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_0/20240603_T100954/tensorboard
2024-06-03 10:10:01 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 10:14:13 [INFO]: Epoch 001 - training loss: 1.0320, validation loss: 0.7648
2024-06-03 10:18:18 [INFO]: Epoch 002 - training loss: 0.7079, validation loss: 0.6357
2024-06-03 10:22:15 [INFO]: Epoch 003 - training loss: 0.6160, validation loss: 0.5924
2024-06-03 10:26:18 [INFO]: Epoch 004 - training loss: 0.5754, validation loss: 0.5692
2024-06-03 10:30:06 [INFO]: Epoch 005 - training loss: 0.5437, validation loss: 0.5524
2024-06-03 10:33:53 [INFO]: Epoch 006 - training loss: 0.5262, validation loss: 0.5437
2024-06-03 10:37:37 [INFO]: Epoch 007 - training loss: 0.5065, validation loss: 0.5384
2024-06-03 10:41:16 [INFO]: Epoch 008 - training loss: 0.4928, validation loss: 0.5331
2024-06-03 10:44:50 [INFO]: Epoch 009 - training loss: 0.4789, validation loss: 0.5295
2024-06-03 10:48:06 [INFO]: Epoch 010 - training loss: 0.4671, validation loss: 0.5266
2024-06-03 10:51:16 [INFO]: Epoch 011 - training loss: 0.4602, validation loss: 0.5237
2024-06-03 10:54:28 [INFO]: Epoch 012 - training loss: 0.4528, validation loss: 0.5219
2024-06-03 10:57:29 [INFO]: Epoch 013 - training loss: 0.4445, validation loss: 0.5183
2024-06-03 11:00:21 [INFO]: Epoch 014 - training loss: 0.4362, validation loss: 0.5147
2024-06-03 11:02:42 [INFO]: Epoch 015 - training loss: 0.4301, validation loss: 0.5167
2024-06-03 11:05:04 [INFO]: Epoch 016 - training loss: 0.4259, validation loss: 0.5124
2024-06-03 11:07:20 [INFO]: Epoch 017 - training loss: 0.4185, validation loss: 0.5116
2024-06-03 11:09:41 [INFO]: Epoch 018 - training loss: 0.4171, validation loss: 0.5095
2024-06-03 11:12:02 [INFO]: Epoch 019 - training loss: 0.4085, validation loss: 0.5109
2024-06-03 11:14:20 [INFO]: Epoch 020 - training loss: 0.4025, validation loss: 0.5109
2024-06-03 11:16:40 [INFO]: Epoch 021 - training loss: 0.4010, validation loss: 0.5070
2024-06-03 11:18:56 [INFO]: Epoch 022 - training loss: 0.3981, validation loss: 0.5060
2024-06-03 11:21:15 [INFO]: Epoch 023 - training loss: 0.3909, validation loss: 0.5046
2024-06-03 11:23:17 [INFO]: Epoch 024 - training loss: 0.3877, validation loss: 0.5001
2024-06-03 11:25:04 [INFO]: Epoch 025 - training loss: 0.3847, validation loss: 0.5011
2024-06-03 11:26:56 [INFO]: Epoch 026 - training loss: 0.3798, validation loss: 0.5008
2024-06-03 11:28:39 [INFO]: Epoch 027 - training loss: 0.3785, validation loss: 0.4996
2024-06-03 11:30:19 [INFO]: Epoch 028 - training loss: 0.3752, validation loss: 0.4970
2024-06-03 11:31:53 [INFO]: Epoch 029 - training loss: 0.3724, validation loss: 0.4965
2024-06-03 11:33:34 [INFO]: Epoch 030 - training loss: 0.3674, validation loss: 0.4961
2024-06-03 11:35:00 [INFO]: Epoch 031 - training loss: 0.3655, validation loss: 0.4955
2024-06-03 11:36:30 [INFO]: Epoch 032 - training loss: 0.3589, validation loss: 0.4967
2024-06-03 11:37:47 [INFO]: Epoch 033 - training loss: 0.3600, validation loss: 0.4933
2024-06-03 11:39:07 [INFO]: Epoch 034 - training loss: 0.3569, validation loss: 0.4946
2024-06-03 11:40:28 [INFO]: Epoch 035 - training loss: 0.3555, validation loss: 0.4941
2024-06-03 11:41:48 [INFO]: Epoch 036 - training loss: 0.3516, validation loss: 0.4912
2024-06-03 11:43:04 [INFO]: Epoch 037 - training loss: 0.3501, validation loss: 0.4925
2024-06-03 11:44:23 [INFO]: Epoch 038 - training loss: 0.3463, validation loss: 0.4918
2024-06-03 11:45:41 [INFO]: Epoch 039 - training loss: 0.3442, validation loss: 0.4889
2024-06-03 11:46:59 [INFO]: Epoch 040 - training loss: 0.3457, validation loss: 0.4885
2024-06-03 11:48:04 [INFO]: Epoch 041 - training loss: 0.3382, validation loss: 0.4886
2024-06-03 11:48:44 [INFO]: Epoch 042 - training loss: 0.3406, validation loss: 0.4865
2024-06-03 11:49:31 [INFO]: Epoch 043 - training loss: 0.3381, validation loss: 0.4872
2024-06-03 11:50:19 [INFO]: Epoch 044 - training loss: 0.3377, validation loss: 0.4861
2024-06-03 11:51:07 [INFO]: Epoch 045 - training loss: 0.3327, validation loss: 0.4858
2024-06-03 11:51:54 [INFO]: Epoch 046 - training loss: 0.3336, validation loss: 0.4844
2024-06-03 11:52:41 [INFO]: Epoch 047 - training loss: 0.3290, validation loss: 0.4865
2024-06-03 11:53:29 [INFO]: Epoch 048 - training loss: 0.3292, validation loss: 0.4845
2024-06-03 11:54:17 [INFO]: Epoch 049 - training loss: 0.3282, validation loss: 0.4810
2024-06-03 11:55:01 [INFO]: Epoch 050 - training loss: 0.3264, validation loss: 0.4833
2024-06-03 11:55:43 [INFO]: Epoch 051 - training loss: 0.3264, validation loss: 0.4784
2024-06-03 11:56:28 [INFO]: Epoch 052 - training loss: 0.3253, validation loss: 0.4850
2024-06-03 11:57:15 [INFO]: Epoch 053 - training loss: 0.3216, validation loss: 0.4798
2024-06-03 11:58:02 [INFO]: Epoch 054 - training loss: 0.3218, validation loss: 0.4814
2024-06-03 11:58:48 [INFO]: Epoch 055 - training loss: 0.3171, validation loss: 0.4797
2024-06-03 11:59:36 [INFO]: Epoch 056 - training loss: 0.3168, validation loss: 0.4782
2024-06-03 12:00:21 [INFO]: Epoch 057 - training loss: 0.3133, validation loss: 0.4801
2024-06-03 12:01:09 [INFO]: Epoch 058 - training loss: 0.3138, validation loss: 0.4781
2024-06-03 12:01:56 [INFO]: Epoch 059 - training loss: 0.3119, validation loss: 0.4783
2024-06-03 12:02:42 [INFO]: Epoch 060 - training loss: 0.3100, validation loss: 0.4769
2024-06-03 12:03:29 [INFO]: Epoch 061 - training loss: 0.3096, validation loss: 0.4805
2024-06-03 12:04:16 [INFO]: Epoch 062 - training loss: 0.3067, validation loss: 0.4780
2024-06-03 12:05:05 [INFO]: Epoch 063 - training loss: 0.3067, validation loss: 0.4806
2024-06-03 12:05:53 [INFO]: Epoch 064 - training loss: 0.3055, validation loss: 0.4765
2024-06-03 12:06:39 [INFO]: Epoch 065 - training loss: 0.3046, validation loss: 0.4770
2024-06-03 12:07:27 [INFO]: Epoch 066 - training loss: 0.3018, validation loss: 0.4758
2024-06-03 12:08:12 [INFO]: Epoch 067 - training loss: 0.3015, validation loss: 0.4784
2024-06-03 12:09:00 [INFO]: Epoch 068 - training loss: 0.2978, validation loss: 0.4749
2024-06-03 12:09:47 [INFO]: Epoch 069 - training loss: 0.3044, validation loss: 0.4723
2024-06-03 12:10:35 [INFO]: Epoch 070 - training loss: 0.2964, validation loss: 0.4746
2024-06-03 12:11:24 [INFO]: Epoch 071 - training loss: 0.2961, validation loss: 0.4724
2024-06-03 12:12:11 [INFO]: Epoch 072 - training loss: 0.2947, validation loss: 0.4743
2024-06-03 12:12:57 [INFO]: Epoch 073 - training loss: 0.2939, validation loss: 0.4741
2024-06-03 12:13:44 [INFO]: Epoch 074 - training loss: 0.2953, validation loss: 0.4734
2024-06-03 12:14:32 [INFO]: Epoch 075 - training loss: 0.2917, validation loss: 0.4736
2024-06-03 12:15:16 [INFO]: Epoch 076 - training loss: 0.2909, validation loss: 0.4723
2024-06-03 12:15:59 [INFO]: Epoch 077 - training loss: 0.2915, validation loss: 0.4714
2024-06-03 12:16:42 [INFO]: Epoch 078 - training loss: 0.2892, validation loss: 0.4708
2024-06-03 12:17:11 [INFO]: Epoch 079 - training loss: 0.2890, validation loss: 0.4710
2024-06-03 12:17:51 [INFO]: Epoch 080 - training loss: 0.2866, validation loss: 0.4746
2024-06-03 12:18:33 [INFO]: Epoch 081 - training loss: 0.2875, validation loss: 0.4709
2024-06-03 12:19:14 [INFO]: Epoch 082 - training loss: 0.2830, validation loss: 0.4705
2024-06-03 12:19:56 [INFO]: Epoch 083 - training loss: 0.2826, validation loss: 0.4714
2024-06-03 12:20:37 [INFO]: Epoch 084 - training loss: 0.2838, validation loss: 0.4708
2024-06-03 12:21:19 [INFO]: Epoch 085 - training loss: 0.2799, validation loss: 0.4721
2024-06-03 12:21:58 [INFO]: Epoch 086 - training loss: 0.2786, validation loss: 0.4710
2024-06-03 12:22:38 [INFO]: Epoch 087 - training loss: 0.2809, validation loss: 0.4711
2024-06-03 12:23:08 [INFO]: Epoch 088 - training loss: 0.2829, validation loss: 0.4693
2024-06-03 12:23:44 [INFO]: Epoch 089 - training loss: 0.2830, validation loss: 0.4705
2024-06-03 12:24:22 [INFO]: Epoch 090 - training loss: 0.2801, validation loss: 0.4693
2024-06-03 12:24:59 [INFO]: Epoch 091 - training loss: 0.2805, validation loss: 0.4695
2024-06-03 12:25:35 [INFO]: Epoch 092 - training loss: 0.2773, validation loss: 0.4717
2024-06-03 12:26:12 [INFO]: Epoch 093 - training loss: 0.2762, validation loss: 0.4685
2024-06-03 12:26:48 [INFO]: Epoch 094 - training loss: 0.2784, validation loss: 0.4682
2024-06-03 12:27:21 [INFO]: Epoch 095 - training loss: 0.2737, validation loss: 0.4695
2024-06-03 12:27:59 [INFO]: Epoch 096 - training loss: 0.2742, validation loss: 0.4684
2024-06-03 12:28:36 [INFO]: Epoch 097 - training loss: 0.2734, validation loss: 0.4674
2024-06-03 12:29:11 [INFO]: Epoch 098 - training loss: 0.2729, validation loss: 0.4682
2024-06-03 12:29:48 [INFO]: Epoch 099 - training loss: 0.2710, validation loss: 0.4685
2024-06-03 12:30:24 [INFO]: Epoch 100 - training loss: 0.2692, validation loss: 0.4691
2024-06-03 12:30:24 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 12:30:24 [INFO]: Saved the model to results_block_rate05/PeMS/BRITS_PeMS/round_0/20240603_T100954/BRITS.pypots
2024-06-03 12:31:06 [INFO]: Successfully saved to results_block_rate05/PeMS/BRITS_PeMS/round_0/imputation.pkl
2024-06-03 12:31:06 [INFO]: Round0 - BRITS on PeMS: MAE=0.3216, MSE=0.6993, MRE=0.3850
2024-06-03 12:31:06 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 12:31:06 [INFO]: Using the given device: cuda:0
2024-06-03 12:31:06 [INFO]: Model files will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_1/20240603_T123106
2024-06-03 12:31:06 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_1/20240603_T123106/tensorboard
2024-06-03 12:31:06 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 12:31:42 [INFO]: Epoch 001 - training loss: 1.0290, validation loss: 0.7575
2024-06-03 12:32:17 [INFO]: Epoch 002 - training loss: 0.7071, validation loss: 0.6391
2024-06-03 12:32:53 [INFO]: Epoch 003 - training loss: 0.6242, validation loss: 0.5875
2024-06-03 12:33:28 [INFO]: Epoch 004 - training loss: 0.5785, validation loss: 0.5658
2024-06-03 12:34:05 [INFO]: Epoch 005 - training loss: 0.5508, validation loss: 0.5542
2024-06-03 12:34:40 [INFO]: Epoch 006 - training loss: 0.5245, validation loss: 0.5434
2024-06-03 12:35:13 [INFO]: Epoch 007 - training loss: 0.5090, validation loss: 0.5386
2024-06-03 12:35:52 [INFO]: Epoch 008 - training loss: 0.4914, validation loss: 0.5374
2024-06-03 12:36:30 [INFO]: Epoch 009 - training loss: 0.4817, validation loss: 0.5334
2024-06-03 12:37:08 [INFO]: Epoch 010 - training loss: 0.4666, validation loss: 0.5247
2024-06-03 12:37:43 [INFO]: Epoch 011 - training loss: 0.4620, validation loss: 0.5209
2024-06-03 12:38:17 [INFO]: Epoch 012 - training loss: 0.4512, validation loss: 0.5239
2024-06-03 12:38:55 [INFO]: Epoch 013 - training loss: 0.4460, validation loss: 0.5202
2024-06-03 12:39:31 [INFO]: Epoch 014 - training loss: 0.4399, validation loss: 0.5171
2024-06-03 12:40:07 [INFO]: Epoch 015 - training loss: 0.4310, validation loss: 0.5163
2024-06-03 12:40:44 [INFO]: Epoch 016 - training loss: 0.4292, validation loss: 0.5148
2024-06-03 12:41:20 [INFO]: Epoch 017 - training loss: 0.4184, validation loss: 0.5099
2024-06-03 12:41:52 [INFO]: Epoch 018 - training loss: 0.4156, validation loss: 0.5103
2024-06-03 12:42:28 [INFO]: Epoch 019 - training loss: 0.4120, validation loss: 0.5083
2024-06-03 12:43:03 [INFO]: Epoch 020 - training loss: 0.4033, validation loss: 0.5071
2024-06-03 12:43:36 [INFO]: Epoch 021 - training loss: 0.3970, validation loss: 0.5058
2024-06-03 12:44:05 [INFO]: Epoch 022 - training loss: 0.3959, validation loss: 0.5033
2024-06-03 12:44:33 [INFO]: Epoch 023 - training loss: 0.3939, validation loss: 0.5024
2024-06-03 12:45:01 [INFO]: Epoch 024 - training loss: 0.3846, validation loss: 0.5028
2024-06-03 12:45:29 [INFO]: Epoch 025 - training loss: 0.3884, validation loss: 0.4985
2024-06-03 12:45:58 [INFO]: Epoch 026 - training loss: 0.3815, validation loss: 0.4990
2024-06-03 12:46:26 [INFO]: Epoch 027 - training loss: 0.3757, validation loss: 0.4992
2024-06-03 12:46:53 [INFO]: Epoch 028 - training loss: 0.3771, validation loss: 0.4942
2024-06-03 12:47:20 [INFO]: Epoch 029 - training loss: 0.3724, validation loss: 0.4959
2024-06-03 12:47:48 [INFO]: Epoch 030 - training loss: 0.3654, validation loss: 0.4988
2024-06-03 12:48:16 [INFO]: Epoch 031 - training loss: 0.3669, validation loss: 0.4932
2024-06-03 12:48:43 [INFO]: Epoch 032 - training loss: 0.3633, validation loss: 0.4925
2024-06-03 12:49:08 [INFO]: Epoch 033 - training loss: 0.3608, validation loss: 0.4927
2024-06-03 12:49:35 [INFO]: Epoch 034 - training loss: 0.3549, validation loss: 0.4906
2024-06-03 12:50:04 [INFO]: Epoch 035 - training loss: 0.3545, validation loss: 0.4883
2024-06-03 12:50:32 [INFO]: Epoch 036 - training loss: 0.3536, validation loss: 0.4891
2024-06-03 12:50:59 [INFO]: Epoch 037 - training loss: 0.3526, validation loss: 0.4895
2024-06-03 12:51:26 [INFO]: Epoch 038 - training loss: 0.3460, validation loss: 0.4875
2024-06-03 12:51:54 [INFO]: Epoch 039 - training loss: 0.3457, validation loss: 0.4887
2024-06-03 12:52:22 [INFO]: Epoch 040 - training loss: 0.3471, validation loss: 0.4875
2024-06-03 12:52:50 [INFO]: Epoch 041 - training loss: 0.3440, validation loss: 0.4853
2024-06-03 12:53:19 [INFO]: Epoch 042 - training loss: 0.3389, validation loss: 0.4846
2024-06-03 12:53:47 [INFO]: Epoch 043 - training loss: 0.3370, validation loss: 0.4871
2024-06-03 12:54:14 [INFO]: Epoch 044 - training loss: 0.3376, validation loss: 0.4834
2024-06-03 12:54:43 [INFO]: Epoch 045 - training loss: 0.3349, validation loss: 0.4851
2024-06-03 12:55:10 [INFO]: Epoch 046 - training loss: 0.3341, validation loss: 0.4828
2024-06-03 12:55:37 [INFO]: Epoch 047 - training loss: 0.3322, validation loss: 0.4825
2024-06-03 12:56:02 [INFO]: Epoch 048 - training loss: 0.3287, validation loss: 0.4810
2024-06-03 12:56:31 [INFO]: Epoch 049 - training loss: 0.3253, validation loss: 0.4809
2024-06-03 12:56:59 [INFO]: Epoch 050 - training loss: 0.3254, validation loss: 0.4811
2024-06-03 12:57:27 [INFO]: Epoch 051 - training loss: 0.3242, validation loss: 0.4802
2024-06-03 12:57:56 [INFO]: Epoch 052 - training loss: 0.3240, validation loss: 0.4796
2024-06-03 12:58:24 [INFO]: Epoch 053 - training loss: 0.3252, validation loss: 0.4779
2024-06-03 12:58:52 [INFO]: Epoch 054 - training loss: 0.3196, validation loss: 0.4793
2024-06-03 12:59:20 [INFO]: Epoch 055 - training loss: 0.3185, validation loss: 0.4779
2024-06-03 12:59:48 [INFO]: Epoch 056 - training loss: 0.3190, validation loss: 0.4786
2024-06-03 13:00:16 [INFO]: Epoch 057 - training loss: 0.3144, validation loss: 0.4778
2024-06-03 13:00:45 [INFO]: Epoch 058 - training loss: 0.3139, validation loss: 0.4775
2024-06-03 13:01:13 [INFO]: Epoch 059 - training loss: 0.3148, validation loss: 0.4758
2024-06-03 13:01:41 [INFO]: Epoch 060 - training loss: 0.3115, validation loss: 0.4758
2024-06-03 13:02:10 [INFO]: Epoch 061 - training loss: 0.3091, validation loss: 0.4771
2024-06-03 13:02:38 [INFO]: Epoch 062 - training loss: 0.3093, validation loss: 0.4755
2024-06-03 13:03:05 [INFO]: Epoch 063 - training loss: 0.3073, validation loss: 0.4750
2024-06-03 13:03:32 [INFO]: Epoch 064 - training loss: 0.3072, validation loss: 0.4763
2024-06-03 13:04:01 [INFO]: Epoch 065 - training loss: 0.3046, validation loss: 0.4770
2024-06-03 13:04:30 [INFO]: Epoch 066 - training loss: 0.3049, validation loss: 0.4737
2024-06-03 13:04:58 [INFO]: Epoch 067 - training loss: 0.3010, validation loss: 0.4749
2024-06-03 13:05:22 [INFO]: Epoch 068 - training loss: 0.3015, validation loss: 0.4731
2024-06-03 13:05:47 [INFO]: Epoch 069 - training loss: 0.3014, validation loss: 0.4719
2024-06-03 13:06:16 [INFO]: Epoch 070 - training loss: 0.2987, validation loss: 0.4733
2024-06-03 13:06:44 [INFO]: Epoch 071 - training loss: 0.2952, validation loss: 0.4731
2024-06-03 13:07:12 [INFO]: Epoch 072 - training loss: 0.2947, validation loss: 0.4719
2024-06-03 13:07:40 [INFO]: Epoch 073 - training loss: 0.2947, validation loss: 0.4701
2024-06-03 13:08:08 [INFO]: Epoch 074 - training loss: 0.2916, validation loss: 0.4727
2024-06-03 13:08:37 [INFO]: Epoch 075 - training loss: 0.2954, validation loss: 0.4714
2024-06-03 13:09:05 [INFO]: Epoch 076 - training loss: 0.2929, validation loss: 0.4719
2024-06-03 13:09:33 [INFO]: Epoch 077 - training loss: 0.2901, validation loss: 0.4723
2024-06-03 13:09:55 [INFO]: Epoch 078 - training loss: 0.2894, validation loss: 0.4700
2024-06-03 13:10:21 [INFO]: Epoch 079 - training loss: 0.2890, validation loss: 0.4702
2024-06-03 13:10:43 [INFO]: Epoch 080 - training loss: 0.2834, validation loss: 0.4701
2024-06-03 13:11:06 [INFO]: Epoch 081 - training loss: 0.2873, validation loss: 0.4687
2024-06-03 13:11:30 [INFO]: Epoch 082 - training loss: 0.2844, validation loss: 0.4701
2024-06-03 13:11:54 [INFO]: Epoch 083 - training loss: 0.2863, validation loss: 0.4702
2024-06-03 13:12:17 [INFO]: Epoch 084 - training loss: 0.2844, validation loss: 0.4702
2024-06-03 13:12:40 [INFO]: Epoch 085 - training loss: 0.2819, validation loss: 0.4693
2024-06-03 13:13:07 [INFO]: Epoch 086 - training loss: 0.2807, validation loss: 0.4697
2024-06-03 13:13:29 [INFO]: Epoch 087 - training loss: 0.2789, validation loss: 0.4705
2024-06-03 13:13:49 [INFO]: Epoch 088 - training loss: 0.2820, validation loss: 0.4689
2024-06-03 13:14:15 [INFO]: Epoch 089 - training loss: 0.2803, validation loss: 0.4683
2024-06-03 13:14:40 [INFO]: Epoch 090 - training loss: 0.2815, validation loss: 0.4666
2024-06-03 13:15:00 [INFO]: Epoch 091 - training loss: 0.2785, validation loss: 0.4667
2024-06-03 13:15:23 [INFO]: Epoch 092 - training loss: 0.2742, validation loss: 0.4681
2024-06-03 13:15:50 [INFO]: Epoch 093 - training loss: 0.2752, validation loss: 0.4681
2024-06-03 13:16:12 [INFO]: Epoch 094 - training loss: 0.2753, validation loss: 0.4671
2024-06-03 13:16:33 [INFO]: Epoch 095 - training loss: 0.2726, validation loss: 0.4666
2024-06-03 13:16:58 [INFO]: Epoch 096 - training loss: 0.2746, validation loss: 0.4657
2024-06-03 13:17:21 [INFO]: Epoch 097 - training loss: 0.2755, validation loss: 0.4653
2024-06-03 13:17:42 [INFO]: Epoch 098 - training loss: 0.2718, validation loss: 0.4660
2024-06-03 13:18:04 [INFO]: Epoch 099 - training loss: 0.2743, validation loss: 0.4658
2024-06-03 13:18:28 [INFO]: Epoch 100 - training loss: 0.2736, validation loss: 0.4665
2024-06-03 13:18:28 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 13:18:29 [INFO]: Saved the model to results_block_rate05/PeMS/BRITS_PeMS/round_1/20240603_T123106/BRITS.pypots
2024-06-03 13:18:55 [INFO]: Successfully saved to results_block_rate05/PeMS/BRITS_PeMS/round_1/imputation.pkl
2024-06-03 13:18:55 [INFO]: Round1 - BRITS on PeMS: MAE=0.3197, MSE=0.6955, MRE=0.3828
2024-06-03 13:18:55 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 13:18:55 [INFO]: Using the given device: cuda:0
2024-06-03 13:18:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_2/20240603_T131855
2024-06-03 13:18:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_2/20240603_T131855/tensorboard
2024-06-03 13:18:55 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 13:19:20 [INFO]: Epoch 001 - training loss: 1.0308, validation loss: 0.7559
2024-06-03 13:19:43 [INFO]: Epoch 002 - training loss: 0.7074, validation loss: 0.6313
2024-06-03 13:20:09 [INFO]: Epoch 003 - training loss: 0.6215, validation loss: 0.5852
2024-06-03 13:20:32 [INFO]: Epoch 004 - training loss: 0.5717, validation loss: 0.5633
2024-06-03 13:20:53 [INFO]: Epoch 005 - training loss: 0.5474, validation loss: 0.5486
2024-06-03 13:21:19 [INFO]: Epoch 006 - training loss: 0.5230, validation loss: 0.5435
2024-06-03 13:21:45 [INFO]: Epoch 007 - training loss: 0.5070, validation loss: 0.5389
2024-06-03 13:22:06 [INFO]: Epoch 008 - training loss: 0.4963, validation loss: 0.5361
2024-06-03 13:22:27 [INFO]: Epoch 009 - training loss: 0.4784, validation loss: 0.5295
2024-06-03 13:22:54 [INFO]: Epoch 010 - training loss: 0.4701, validation loss: 0.5244
2024-06-03 13:23:17 [INFO]: Epoch 011 - training loss: 0.4592, validation loss: 0.5230
2024-06-03 13:23:38 [INFO]: Epoch 012 - training loss: 0.4500, validation loss: 0.5206
2024-06-03 13:24:03 [INFO]: Epoch 013 - training loss: 0.4443, validation loss: 0.5197
2024-06-03 13:24:29 [INFO]: Epoch 014 - training loss: 0.4381, validation loss: 0.5163
2024-06-03 13:24:52 [INFO]: Epoch 015 - training loss: 0.4294, validation loss: 0.5158
2024-06-03 13:25:15 [INFO]: Epoch 016 - training loss: 0.4288, validation loss: 0.5140
2024-06-03 13:25:40 [INFO]: Epoch 017 - training loss: 0.4201, validation loss: 0.5156
2024-06-03 13:26:05 [INFO]: Epoch 018 - training loss: 0.4137, validation loss: 0.5065
2024-06-03 13:26:27 [INFO]: Epoch 019 - training loss: 0.4094, validation loss: 0.5100
2024-06-03 13:26:49 [INFO]: Epoch 020 - training loss: 0.4044, validation loss: 0.5094
2024-06-03 13:27:16 [INFO]: Epoch 021 - training loss: 0.4019, validation loss: 0.5032
2024-06-03 13:27:39 [INFO]: Epoch 022 - training loss: 0.3972, validation loss: 0.5049
2024-06-03 13:28:01 [INFO]: Epoch 023 - training loss: 0.3948, validation loss: 0.5017
2024-06-03 13:28:25 [INFO]: Epoch 024 - training loss: 0.3915, validation loss: 0.5005
2024-06-03 13:28:50 [INFO]: Epoch 025 - training loss: 0.3868, validation loss: 0.4989
2024-06-03 13:29:12 [INFO]: Epoch 026 - training loss: 0.3808, validation loss: 0.4984
2024-06-03 13:29:34 [INFO]: Epoch 027 - training loss: 0.3746, validation loss: 0.4977
2024-06-03 13:30:00 [INFO]: Epoch 028 - training loss: 0.3775, validation loss: 0.4966
2024-06-03 13:30:24 [INFO]: Epoch 029 - training loss: 0.3700, validation loss: 0.4966
2024-06-03 13:30:46 [INFO]: Epoch 030 - training loss: 0.3671, validation loss: 0.4964
2024-06-03 13:31:10 [INFO]: Epoch 031 - training loss: 0.3666, validation loss: 0.4929
2024-06-03 13:31:36 [INFO]: Epoch 032 - training loss: 0.3655, validation loss: 0.4930
2024-06-03 13:32:00 [INFO]: Epoch 033 - training loss: 0.3625, validation loss: 0.4907
2024-06-03 13:32:22 [INFO]: Epoch 034 - training loss: 0.3585, validation loss: 0.4921
2024-06-03 13:32:46 [INFO]: Epoch 035 - training loss: 0.3547, validation loss: 0.4899
2024-06-03 13:33:10 [INFO]: Epoch 036 - training loss: 0.3510, validation loss: 0.4925
2024-06-03 13:33:33 [INFO]: Epoch 037 - training loss: 0.3515, validation loss: 0.4884
2024-06-03 13:33:56 [INFO]: Epoch 038 - training loss: 0.3470, validation loss: 0.4891
2024-06-03 13:34:22 [INFO]: Epoch 039 - training loss: 0.3494, validation loss: 0.4876
2024-06-03 13:34:48 [INFO]: Epoch 040 - training loss: 0.3457, validation loss: 0.4854
2024-06-03 13:35:08 [INFO]: Epoch 041 - training loss: 0.3460, validation loss: 0.4865
2024-06-03 13:35:31 [INFO]: Epoch 042 - training loss: 0.3399, validation loss: 0.4864
2024-06-03 13:35:57 [INFO]: Epoch 043 - training loss: 0.3362, validation loss: 0.4834
2024-06-03 13:36:21 [INFO]: Epoch 044 - training loss: 0.3371, validation loss: 0.4872
2024-06-03 13:36:44 [INFO]: Epoch 045 - training loss: 0.3319, validation loss: 0.4870
2024-06-03 13:37:06 [INFO]: Epoch 046 - training loss: 0.3312, validation loss: 0.4844
2024-06-03 13:37:24 [INFO]: Epoch 047 - training loss: 0.3295, validation loss: 0.4824
2024-06-03 13:37:39 [INFO]: Epoch 048 - training loss: 0.3318, validation loss: 0.4826
2024-06-03 13:38:00 [INFO]: Epoch 049 - training loss: 0.3262, validation loss: 0.4809
2024-06-03 13:38:16 [INFO]: Epoch 050 - training loss: 0.3265, validation loss: 0.4816
2024-06-03 13:38:36 [INFO]: Epoch 051 - training loss: 0.3255, validation loss: 0.4807
2024-06-03 13:38:52 [INFO]: Epoch 052 - training loss: 0.3221, validation loss: 0.4816
2024-06-03 13:39:09 [INFO]: Epoch 053 - training loss: 0.3239, validation loss: 0.4815
2024-06-03 13:39:28 [INFO]: Epoch 054 - training loss: 0.3194, validation loss: 0.4788
2024-06-03 13:39:44 [INFO]: Epoch 055 - training loss: 0.3164, validation loss: 0.4791
2024-06-03 13:40:05 [INFO]: Epoch 056 - training loss: 0.3164, validation loss: 0.4778
2024-06-03 13:40:21 [INFO]: Epoch 057 - training loss: 0.3158, validation loss: 0.4785
2024-06-03 13:40:39 [INFO]: Epoch 058 - training loss: 0.3141, validation loss: 0.4787
2024-06-03 13:40:58 [INFO]: Epoch 059 - training loss: 0.3079, validation loss: 0.4783
2024-06-03 13:41:13 [INFO]: Epoch 060 - training loss: 0.3115, validation loss: 0.4783
2024-06-03 13:41:34 [INFO]: Epoch 061 - training loss: 0.3080, validation loss: 0.4776
2024-06-03 13:41:50 [INFO]: Epoch 062 - training loss: 0.3097, validation loss: 0.4777
2024-06-03 13:42:09 [INFO]: Epoch 063 - training loss: 0.3089, validation loss: 0.4759
2024-06-03 13:42:26 [INFO]: Epoch 064 - training loss: 0.3060, validation loss: 0.4745
2024-06-03 13:42:43 [INFO]: Epoch 065 - training loss: 0.3037, validation loss: 0.4763
2024-06-03 13:43:02 [INFO]: Epoch 066 - training loss: 0.3007, validation loss: 0.4746
2024-06-03 13:43:17 [INFO]: Epoch 067 - training loss: 0.3008, validation loss: 0.4767
2024-06-03 13:43:38 [INFO]: Epoch 068 - training loss: 0.2990, validation loss: 0.4762
2024-06-03 13:43:54 [INFO]: Epoch 069 - training loss: 0.2986, validation loss: 0.4743
2024-06-03 13:44:13 [INFO]: Epoch 070 - training loss: 0.2979, validation loss: 0.4735
2024-06-03 13:44:30 [INFO]: Epoch 071 - training loss: 0.2970, validation loss: 0.4727
2024-06-03 13:44:46 [INFO]: Epoch 072 - training loss: 0.2971, validation loss: 0.4747
2024-06-03 13:45:06 [INFO]: Epoch 073 - training loss: 0.2965, validation loss: 0.4726
2024-06-03 13:45:21 [INFO]: Epoch 074 - training loss: 0.2939, validation loss: 0.4717
2024-06-03 13:45:42 [INFO]: Epoch 075 - training loss: 0.2937, validation loss: 0.4713
2024-06-03 13:46:04 [INFO]: Epoch 076 - training loss: 0.2906, validation loss: 0.4722
2024-06-03 13:46:26 [INFO]: Epoch 077 - training loss: 0.2943, validation loss: 0.4705
2024-06-03 13:46:48 [INFO]: Epoch 078 - training loss: 0.2903, validation loss: 0.4699
2024-06-03 13:47:10 [INFO]: Epoch 079 - training loss: 0.2929, validation loss: 0.4710
2024-06-03 13:47:32 [INFO]: Epoch 080 - training loss: 0.2887, validation loss: 0.4727
2024-06-03 13:47:53 [INFO]: Epoch 081 - training loss: 0.2866, validation loss: 0.4730
2024-06-03 13:48:15 [INFO]: Epoch 082 - training loss: 0.2845, validation loss: 0.4713
2024-06-03 13:48:37 [INFO]: Epoch 083 - training loss: 0.2850, validation loss: 0.4702
2024-06-03 13:48:59 [INFO]: Epoch 084 - training loss: 0.2829, validation loss: 0.4701
2024-06-03 13:49:21 [INFO]: Epoch 085 - training loss: 0.2813, validation loss: 0.4693
2024-06-03 13:49:43 [INFO]: Epoch 086 - training loss: 0.2839, validation loss: 0.4706
2024-06-03 13:50:04 [INFO]: Epoch 087 - training loss: 0.2805, validation loss: 0.4699
2024-06-03 13:50:26 [INFO]: Epoch 088 - training loss: 0.2817, validation loss: 0.4686
2024-06-03 13:50:47 [INFO]: Epoch 089 - training loss: 0.2787, validation loss: 0.4693
2024-06-03 13:51:08 [INFO]: Epoch 090 - training loss: 0.2771, validation loss: 0.4696
2024-06-03 13:51:30 [INFO]: Epoch 091 - training loss: 0.2775, validation loss: 0.4692
2024-06-03 13:51:51 [INFO]: Epoch 092 - training loss: 0.2765, validation loss: 0.4704
2024-06-03 13:52:13 [INFO]: Epoch 093 - training loss: 0.2763, validation loss: 0.4681
2024-06-03 13:52:35 [INFO]: Epoch 094 - training loss: 0.2759, validation loss: 0.4699
2024-06-03 13:52:57 [INFO]: Epoch 095 - training loss: 0.2745, validation loss: 0.4716
2024-06-03 13:53:19 [INFO]: Epoch 096 - training loss: 0.2738, validation loss: 0.4693
2024-06-03 13:53:41 [INFO]: Epoch 097 - training loss: 0.2727, validation loss: 0.4704
2024-06-03 13:54:03 [INFO]: Epoch 098 - training loss: 0.2730, validation loss: 0.4696
2024-06-03 13:54:24 [INFO]: Epoch 099 - training loss: 0.2716, validation loss: 0.4656
2024-06-03 13:54:46 [INFO]: Epoch 100 - training loss: 0.2720, validation loss: 0.4661
2024-06-03 13:54:46 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 13:54:46 [INFO]: Saved the model to results_block_rate05/PeMS/BRITS_PeMS/round_2/20240603_T131855/BRITS.pypots
2024-06-03 13:55:13 [INFO]: Successfully saved to results_block_rate05/PeMS/BRITS_PeMS/round_2/imputation.pkl
2024-06-03 13:55:13 [INFO]: Round2 - BRITS on PeMS: MAE=0.3188, MSE=0.6952, MRE=0.3818
2024-06-03 13:55:13 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 13:55:13 [INFO]: Using the given device: cuda:0
2024-06-03 13:55:13 [INFO]: Model files will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_3/20240603_T135513
2024-06-03 13:55:13 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_3/20240603_T135513/tensorboard
2024-06-03 13:55:14 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 13:55:36 [INFO]: Epoch 001 - training loss: 1.0327, validation loss: 0.7619
2024-06-03 13:55:58 [INFO]: Epoch 002 - training loss: 0.7066, validation loss: 0.6389
2024-06-03 13:56:20 [INFO]: Epoch 003 - training loss: 0.6253, validation loss: 0.5857
2024-06-03 13:56:42 [INFO]: Epoch 004 - training loss: 0.5748, validation loss: 0.5628
2024-06-03 13:57:03 [INFO]: Epoch 005 - training loss: 0.5451, validation loss: 0.5507
2024-06-03 13:57:25 [INFO]: Epoch 006 - training loss: 0.5225, validation loss: 0.5431
2024-06-03 13:57:47 [INFO]: Epoch 007 - training loss: 0.5078, validation loss: 0.5349
2024-06-03 13:58:09 [INFO]: Epoch 008 - training loss: 0.4942, validation loss: 0.5297
2024-06-03 13:58:31 [INFO]: Epoch 009 - training loss: 0.4832, validation loss: 0.5270
2024-06-03 13:58:53 [INFO]: Epoch 010 - training loss: 0.4710, validation loss: 0.5271
2024-06-03 13:59:15 [INFO]: Epoch 011 - training loss: 0.4579, validation loss: 0.5250
2024-06-03 13:59:36 [INFO]: Epoch 012 - training loss: 0.4496, validation loss: 0.5225
2024-06-03 13:59:58 [INFO]: Epoch 013 - training loss: 0.4439, validation loss: 0.5198
2024-06-03 14:00:20 [INFO]: Epoch 014 - training loss: 0.4381, validation loss: 0.5181
2024-06-03 14:00:42 [INFO]: Epoch 015 - training loss: 0.4308, validation loss: 0.5142
2024-06-03 14:01:04 [INFO]: Epoch 016 - training loss: 0.4252, validation loss: 0.5119
2024-06-03 14:01:26 [INFO]: Epoch 017 - training loss: 0.4217, validation loss: 0.5111
2024-06-03 14:01:48 [INFO]: Epoch 018 - training loss: 0.4152, validation loss: 0.5109
2024-06-03 14:02:10 [INFO]: Epoch 019 - training loss: 0.4150, validation loss: 0.5057
2024-06-03 14:02:31 [INFO]: Epoch 020 - training loss: 0.4043, validation loss: 0.5081
2024-06-03 14:02:53 [INFO]: Epoch 021 - training loss: 0.3993, validation loss: 0.5054
2024-06-03 14:03:15 [INFO]: Epoch 022 - training loss: 0.3985, validation loss: 0.5046
2024-06-03 14:03:37 [INFO]: Epoch 023 - training loss: 0.3919, validation loss: 0.5025
2024-06-03 14:03:59 [INFO]: Epoch 024 - training loss: 0.3884, validation loss: 0.5029
2024-06-03 14:04:21 [INFO]: Epoch 025 - training loss: 0.3850, validation loss: 0.5006
2024-06-03 14:04:43 [INFO]: Epoch 026 - training loss: 0.3839, validation loss: 0.4978
2024-06-03 14:05:05 [INFO]: Epoch 027 - training loss: 0.3771, validation loss: 0.4974
2024-06-03 14:05:26 [INFO]: Epoch 028 - training loss: 0.3749, validation loss: 0.4972
2024-06-03 14:05:48 [INFO]: Epoch 029 - training loss: 0.3687, validation loss: 0.4973
2024-06-03 14:06:10 [INFO]: Epoch 030 - training loss: 0.3715, validation loss: 0.4944
2024-06-03 14:06:29 [INFO]: Epoch 031 - training loss: 0.3646, validation loss: 0.4942
2024-06-03 14:06:51 [INFO]: Epoch 032 - training loss: 0.3650, validation loss: 0.4945
2024-06-03 14:07:12 [INFO]: Epoch 033 - training loss: 0.3618, validation loss: 0.4916
2024-06-03 14:07:34 [INFO]: Epoch 034 - training loss: 0.3592, validation loss: 0.4933
2024-06-03 14:07:56 [INFO]: Epoch 035 - training loss: 0.3558, validation loss: 0.4912
2024-06-03 14:08:18 [INFO]: Epoch 036 - training loss: 0.3515, validation loss: 0.4914
2024-06-03 14:08:40 [INFO]: Epoch 037 - training loss: 0.3517, validation loss: 0.4888
2024-06-03 14:09:02 [INFO]: Epoch 038 - training loss: 0.3507, validation loss: 0.4872
2024-06-03 14:09:24 [INFO]: Epoch 039 - training loss: 0.3471, validation loss: 0.4908
2024-06-03 14:09:45 [INFO]: Epoch 040 - training loss: 0.3454, validation loss: 0.4890
2024-06-03 14:10:07 [INFO]: Epoch 041 - training loss: 0.3445, validation loss: 0.4863
2024-06-03 14:10:25 [INFO]: Epoch 042 - training loss: 0.3401, validation loss: 0.4865
2024-06-03 14:10:42 [INFO]: Epoch 043 - training loss: 0.3387, validation loss: 0.4848
2024-06-03 14:10:59 [INFO]: Epoch 044 - training loss: 0.3403, validation loss: 0.4858
2024-06-03 14:11:18 [INFO]: Epoch 045 - training loss: 0.3338, validation loss: 0.4859
2024-06-03 14:11:33 [INFO]: Epoch 046 - training loss: 0.3323, validation loss: 0.4840
2024-06-03 14:11:54 [INFO]: Epoch 047 - training loss: 0.3314, validation loss: 0.4841
2024-06-03 14:12:10 [INFO]: Epoch 048 - training loss: 0.3318, validation loss: 0.4819
2024-06-03 14:12:29 [INFO]: Epoch 049 - training loss: 0.3286, validation loss: 0.4834
2024-06-03 14:12:46 [INFO]: Epoch 050 - training loss: 0.3251, validation loss: 0.4831
2024-06-03 14:13:02 [INFO]: Epoch 051 - training loss: 0.3234, validation loss: 0.4822
2024-06-03 14:13:23 [INFO]: Epoch 052 - training loss: 0.3222, validation loss: 0.4811
2024-06-03 14:13:38 [INFO]: Epoch 053 - training loss: 0.3200, validation loss: 0.4813
2024-06-03 14:13:59 [INFO]: Epoch 054 - training loss: 0.3211, validation loss: 0.4801
2024-06-03 14:14:15 [INFO]: Epoch 055 - training loss: 0.3182, validation loss: 0.4789
2024-06-03 14:14:32 [INFO]: Epoch 056 - training loss: 0.3178, validation loss: 0.4795
2024-06-03 14:14:51 [INFO]: Epoch 057 - training loss: 0.3156, validation loss: 0.4796
2024-06-03 14:15:06 [INFO]: Epoch 058 - training loss: 0.3116, validation loss: 0.4787
2024-06-03 14:15:28 [INFO]: Epoch 059 - training loss: 0.3170, validation loss: 0.4769
2024-06-03 14:15:43 [INFO]: Epoch 060 - training loss: 0.3129, validation loss: 0.4788
2024-06-03 14:16:02 [INFO]: Epoch 061 - training loss: 0.3132, validation loss: 0.4771
2024-06-03 14:16:20 [INFO]: Epoch 062 - training loss: 0.3092, validation loss: 0.4759
2024-06-03 14:16:36 [INFO]: Epoch 063 - training loss: 0.3076, validation loss: 0.4765
2024-06-03 14:16:56 [INFO]: Epoch 064 - training loss: 0.3061, validation loss: 0.4770
2024-06-03 14:17:11 [INFO]: Epoch 065 - training loss: 0.3055, validation loss: 0.4765
2024-06-03 14:17:33 [INFO]: Epoch 066 - training loss: 0.3039, validation loss: 0.4760
2024-06-03 14:17:48 [INFO]: Epoch 067 - training loss: 0.3036, validation loss: 0.4739
2024-06-03 14:18:06 [INFO]: Epoch 068 - training loss: 0.3010, validation loss: 0.4759
2024-06-03 14:18:24 [INFO]: Epoch 069 - training loss: 0.2988, validation loss: 0.4746
2024-06-03 14:18:39 [INFO]: Epoch 070 - training loss: 0.2984, validation loss: 0.4743
2024-06-03 14:19:00 [INFO]: Epoch 071 - training loss: 0.2958, validation loss: 0.4749
2024-06-03 14:19:16 [INFO]: Epoch 072 - training loss: 0.2978, validation loss: 0.4725
2024-06-03 14:19:36 [INFO]: Epoch 073 - training loss: 0.2961, validation loss: 0.4714
2024-06-03 14:19:53 [INFO]: Epoch 074 - training loss: 0.2946, validation loss: 0.4736
2024-06-03 14:20:09 [INFO]: Epoch 075 - training loss: 0.2922, validation loss: 0.4738
2024-06-03 14:20:29 [INFO]: Epoch 076 - training loss: 0.2924, validation loss: 0.4726
2024-06-03 14:20:44 [INFO]: Epoch 077 - training loss: 0.2906, validation loss: 0.4715
2024-06-03 14:21:05 [INFO]: Epoch 078 - training loss: 0.2919, validation loss: 0.4716
2024-06-03 14:21:22 [INFO]: Epoch 079 - training loss: 0.2876, validation loss: 0.4727
2024-06-03 14:21:39 [INFO]: Epoch 080 - training loss: 0.2885, validation loss: 0.4727
2024-06-03 14:21:58 [INFO]: Epoch 081 - training loss: 0.2872, validation loss: 0.4704
2024-06-03 14:22:13 [INFO]: Epoch 082 - training loss: 0.2865, validation loss: 0.4701
2024-06-03 14:22:34 [INFO]: Epoch 083 - training loss: 0.2852, validation loss: 0.4723
2024-06-03 14:22:50 [INFO]: Epoch 084 - training loss: 0.2829, validation loss: 0.4700
2024-06-03 14:23:10 [INFO]: Epoch 085 - training loss: 0.2838, validation loss: 0.4698
2024-06-03 14:23:26 [INFO]: Epoch 086 - training loss: 0.2816, validation loss: 0.4699
2024-06-03 14:23:43 [INFO]: Epoch 087 - training loss: 0.2807, validation loss: 0.4695
2024-06-03 14:24:03 [INFO]: Epoch 088 - training loss: 0.2798, validation loss: 0.4715
2024-06-03 14:24:18 [INFO]: Epoch 089 - training loss: 0.2795, validation loss: 0.4700
2024-06-03 14:24:39 [INFO]: Epoch 090 - training loss: 0.2774, validation loss: 0.4698
2024-06-03 14:24:54 [INFO]: Epoch 091 - training loss: 0.2754, validation loss: 0.4695
2024-06-03 14:25:13 [INFO]: Epoch 092 - training loss: 0.2775, validation loss: 0.4705
2024-06-03 14:25:31 [INFO]: Epoch 093 - training loss: 0.2730, validation loss: 0.4683
2024-06-03 14:25:46 [INFO]: Epoch 094 - training loss: 0.2748, validation loss: 0.4702
2024-06-03 14:26:07 [INFO]: Epoch 095 - training loss: 0.2714, validation loss: 0.4690
2024-06-03 14:26:23 [INFO]: Epoch 096 - training loss: 0.2733, validation loss: 0.4695
2024-06-03 14:26:43 [INFO]: Epoch 097 - training loss: 0.2722, validation loss: 0.4685
2024-06-03 14:26:59 [INFO]: Epoch 098 - training loss: 0.2719, validation loss: 0.4681
2024-06-03 14:27:16 [INFO]: Epoch 099 - training loss: 0.2735, validation loss: 0.4681
2024-06-03 14:27:35 [INFO]: Epoch 100 - training loss: 0.2701, validation loss: 0.4687
2024-06-03 14:27:35 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 14:27:35 [INFO]: Saved the model to results_block_rate05/PeMS/BRITS_PeMS/round_3/20240603_T135513/BRITS.pypots
2024-06-03 14:27:53 [INFO]: Successfully saved to results_block_rate05/PeMS/BRITS_PeMS/round_3/imputation.pkl
2024-06-03 14:27:53 [INFO]: Round3 - BRITS on PeMS: MAE=0.3207, MSE=0.6993, MRE=0.3840
2024-06-03 14:27:53 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 14:27:53 [INFO]: Using the given device: cuda:0
2024-06-03 14:27:53 [INFO]: Model files will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_4/20240603_T142753
2024-06-03 14:27:53 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/BRITS_PeMS/round_4/20240603_T142753/tensorboard
2024-06-03 14:27:54 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 14:28:14 [INFO]: Epoch 001 - training loss: 1.0256, validation loss: 0.7574
2024-06-03 14:28:31 [INFO]: Epoch 002 - training loss: 0.7103, validation loss: 0.6376
2024-06-03 14:28:48 [INFO]: Epoch 003 - training loss: 0.6297, validation loss: 0.5849
2024-06-03 14:29:07 [INFO]: Epoch 004 - training loss: 0.5758, validation loss: 0.5623
2024-06-03 14:29:23 [INFO]: Epoch 005 - training loss: 0.5397, validation loss: 0.5511
2024-06-03 14:29:44 [INFO]: Epoch 006 - training loss: 0.5274, validation loss: 0.5452
2024-06-03 14:30:00 [INFO]: Epoch 007 - training loss: 0.5019, validation loss: 0.5388
2024-06-03 14:30:18 [INFO]: Epoch 008 - training loss: 0.4900, validation loss: 0.5317
2024-06-03 14:30:35 [INFO]: Epoch 009 - training loss: 0.4783, validation loss: 0.5297
2024-06-03 14:30:51 [INFO]: Epoch 010 - training loss: 0.4701, validation loss: 0.5238
2024-06-03 14:31:11 [INFO]: Epoch 011 - training loss: 0.4593, validation loss: 0.5219
2024-06-03 14:31:27 [INFO]: Epoch 012 - training loss: 0.4509, validation loss: 0.5194
2024-06-03 14:31:48 [INFO]: Epoch 013 - training loss: 0.4457, validation loss: 0.5187
2024-06-03 14:32:04 [INFO]: Epoch 014 - training loss: 0.4364, validation loss: 0.5157
2024-06-03 14:32:21 [INFO]: Epoch 015 - training loss: 0.4284, validation loss: 0.5138
2024-06-03 14:32:40 [INFO]: Epoch 016 - training loss: 0.4208, validation loss: 0.5133
2024-06-03 14:32:55 [INFO]: Epoch 017 - training loss: 0.4184, validation loss: 0.5087
2024-06-03 14:33:17 [INFO]: Epoch 018 - training loss: 0.4129, validation loss: 0.5068
2024-06-03 14:33:32 [INFO]: Epoch 019 - training loss: 0.4079, validation loss: 0.5091
2024-06-03 14:33:51 [INFO]: Epoch 020 - training loss: 0.4019, validation loss: 0.5077
2024-06-03 14:34:09 [INFO]: Epoch 021 - training loss: 0.3994, validation loss: 0.5059
2024-06-03 14:34:25 [INFO]: Epoch 022 - training loss: 0.3931, validation loss: 0.5045
2024-06-03 14:34:45 [INFO]: Epoch 023 - training loss: 0.3914, validation loss: 0.5017
2024-06-03 14:35:00 [INFO]: Epoch 024 - training loss: 0.3881, validation loss: 0.5023
2024-06-03 14:35:21 [INFO]: Epoch 025 - training loss: 0.3865, validation loss: 0.4989
2024-06-03 14:35:37 [INFO]: Epoch 026 - training loss: 0.3822, validation loss: 0.4989
2024-06-03 14:35:54 [INFO]: Epoch 027 - training loss: 0.3788, validation loss: 0.4971
2024-06-03 14:36:13 [INFO]: Epoch 028 - training loss: 0.3729, validation loss: 0.4958
2024-06-03 14:36:28 [INFO]: Epoch 029 - training loss: 0.3715, validation loss: 0.4946
2024-06-03 14:36:49 [INFO]: Epoch 030 - training loss: 0.3698, validation loss: 0.4946
2024-06-03 14:37:05 [INFO]: Epoch 031 - training loss: 0.3661, validation loss: 0.4943
2024-06-03 14:37:25 [INFO]: Epoch 032 - training loss: 0.3607, validation loss: 0.4923
2024-06-03 14:37:41 [INFO]: Epoch 033 - training loss: 0.3591, validation loss: 0.4929
2024-06-03 14:37:58 [INFO]: Epoch 034 - training loss: 0.3575, validation loss: 0.4908
2024-06-03 14:38:17 [INFO]: Epoch 035 - training loss: 0.3554, validation loss: 0.4900
2024-06-03 14:38:31 [INFO]: Epoch 036 - training loss: 0.3525, validation loss: 0.4895
2024-06-03 14:38:53 [INFO]: Epoch 037 - training loss: 0.3532, validation loss: 0.4880
2024-06-03 14:39:07 [INFO]: Epoch 038 - training loss: 0.3499, validation loss: 0.4863
2024-06-03 14:39:27 [INFO]: Epoch 039 - training loss: 0.3435, validation loss: 0.4888
2024-06-03 14:39:43 [INFO]: Epoch 040 - training loss: 0.3415, validation loss: 0.4880
2024-06-03 14:39:59 [INFO]: Epoch 041 - training loss: 0.3430, validation loss: 0.4880
2024-06-03 14:40:17 [INFO]: Epoch 042 - training loss: 0.3396, validation loss: 0.4864
2024-06-03 14:40:31 [INFO]: Epoch 043 - training loss: 0.3356, validation loss: 0.4844
2024-06-03 14:40:51 [INFO]: Epoch 044 - training loss: 0.3356, validation loss: 0.4844
2024-06-03 14:41:06 [INFO]: Epoch 045 - training loss: 0.3307, validation loss: 0.4828
2024-06-03 14:41:27 [INFO]: Epoch 046 - training loss: 0.3314, validation loss: 0.4828
2024-06-03 14:41:43 [INFO]: Epoch 047 - training loss: 0.3310, validation loss: 0.4811
2024-06-03 14:42:01 [INFO]: Epoch 048 - training loss: 0.3292, validation loss: 0.4828
2024-06-03 14:42:19 [INFO]: Epoch 049 - training loss: 0.3311, validation loss: 0.4809
2024-06-03 14:42:35 [INFO]: Epoch 050 - training loss: 0.3296, validation loss: 0.4790
2024-06-03 14:42:55 [INFO]: Epoch 051 - training loss: 0.3256, validation loss: 0.4804
2024-06-03 14:43:11 [INFO]: Epoch 052 - training loss: 0.3202, validation loss: 0.4804
2024-06-03 14:43:32 [INFO]: Epoch 053 - training loss: 0.3207, validation loss: 0.4812
2024-06-03 14:43:48 [INFO]: Epoch 054 - training loss: 0.3166, validation loss: 0.4801
2024-06-03 14:44:05 [INFO]: Epoch 055 - training loss: 0.3140, validation loss: 0.4787
2024-06-03 14:44:25 [INFO]: Epoch 056 - training loss: 0.3152, validation loss: 0.4789
2024-06-03 14:44:41 [INFO]: Epoch 057 - training loss: 0.3140, validation loss: 0.4793
2024-06-03 14:45:02 [INFO]: Epoch 058 - training loss: 0.3149, validation loss: 0.4781
2024-06-03 14:45:18 [INFO]: Epoch 059 - training loss: 0.3095, validation loss: 0.4784
2024-06-03 14:45:36 [INFO]: Epoch 060 - training loss: 0.3112, validation loss: 0.4765
2024-06-03 14:45:54 [INFO]: Epoch 061 - training loss: 0.3061, validation loss: 0.4763
2024-06-03 14:46:10 [INFO]: Epoch 062 - training loss: 0.3073, validation loss: 0.4765
2024-06-03 14:46:31 [INFO]: Epoch 063 - training loss: 0.3065, validation loss: 0.4750
2024-06-03 14:46:47 [INFO]: Epoch 064 - training loss: 0.3040, validation loss: 0.4775
2024-06-03 14:47:06 [INFO]: Epoch 065 - training loss: 0.3028, validation loss: 0.4757
2024-06-03 14:47:23 [INFO]: Epoch 066 - training loss: 0.3010, validation loss: 0.4771
2024-06-03 14:47:40 [INFO]: Epoch 067 - training loss: 0.3024, validation loss: 0.4770
2024-06-03 14:48:00 [INFO]: Epoch 068 - training loss: 0.3015, validation loss: 0.4758
2024-06-03 14:48:15 [INFO]: Epoch 069 - training loss: 0.2985, validation loss: 0.4747
2024-06-03 14:48:37 [INFO]: Epoch 070 - training loss: 0.2973, validation loss: 0.4737
2024-06-03 14:48:53 [INFO]: Epoch 071 - training loss: 0.2946, validation loss: 0.4747
2024-06-03 14:49:10 [INFO]: Epoch 072 - training loss: 0.2953, validation loss: 0.4738
2024-06-03 14:49:29 [INFO]: Epoch 073 - training loss: 0.2929, validation loss: 0.4725
2024-06-03 14:49:44 [INFO]: Epoch 074 - training loss: 0.2913, validation loss: 0.4729
2024-06-03 14:50:05 [INFO]: Epoch 075 - training loss: 0.2932, validation loss: 0.4732
2024-06-03 14:50:21 [INFO]: Epoch 076 - training loss: 0.2945, validation loss: 0.4712
2024-06-03 14:50:40 [INFO]: Epoch 077 - training loss: 0.2913, validation loss: 0.4725
2024-06-03 14:50:57 [INFO]: Epoch 078 - training loss: 0.2897, validation loss: 0.4710
2024-06-03 14:51:14 [INFO]: Epoch 079 - training loss: 0.2894, validation loss: 0.4694
2024-06-03 14:51:34 [INFO]: Epoch 080 - training loss: 0.2879, validation loss: 0.4730
2024-06-03 14:51:49 [INFO]: Epoch 081 - training loss: 0.2880, validation loss: 0.4733
2024-06-03 14:52:10 [INFO]: Epoch 082 - training loss: 0.2846, validation loss: 0.4704
2024-06-03 14:52:26 [INFO]: Epoch 083 - training loss: 0.2844, validation loss: 0.4708
2024-06-03 14:52:44 [INFO]: Epoch 084 - training loss: 0.2844, validation loss: 0.4692
2024-06-03 14:53:03 [INFO]: Epoch 085 - training loss: 0.2825, validation loss: 0.4709
2024-06-03 14:53:18 [INFO]: Epoch 086 - training loss: 0.2849, validation loss: 0.4699
2024-06-03 14:53:39 [INFO]: Epoch 087 - training loss: 0.2811, validation loss: 0.4702
2024-06-03 14:53:55 [INFO]: Epoch 088 - training loss: 0.2805, validation loss: 0.4698
2024-06-03 14:54:14 [INFO]: Epoch 089 - training loss: 0.2798, validation loss: 0.4695
2024-06-03 14:54:31 [INFO]: Epoch 090 - training loss: 0.2797, validation loss: 0.4678
2024-06-03 14:54:47 [INFO]: Epoch 091 - training loss: 0.2759, validation loss: 0.4712
2024-06-03 14:55:07 [INFO]: Epoch 092 - training loss: 0.2748, validation loss: 0.4691
2024-06-03 14:55:23 [INFO]: Epoch 093 - training loss: 0.2754, validation loss: 0.4704
2024-06-03 14:55:44 [INFO]: Epoch 094 - training loss: 0.2754, validation loss: 0.4692
2024-06-03 14:56:00 [INFO]: Epoch 095 - training loss: 0.2728, validation loss: 0.4686
2024-06-03 14:56:17 [INFO]: Epoch 096 - training loss: 0.2700, validation loss: 0.4687
2024-06-03 14:56:36 [INFO]: Epoch 097 - training loss: 0.2685, validation loss: 0.4681
2024-06-03 14:56:51 [INFO]: Epoch 098 - training loss: 0.2723, validation loss: 0.4680
2024-06-03 14:57:12 [INFO]: Epoch 099 - training loss: 0.2699, validation loss: 0.4672
2024-06-03 14:57:28 [INFO]: Epoch 100 - training loss: 0.2703, validation loss: 0.4660
2024-06-03 14:57:28 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 14:57:28 [INFO]: Saved the model to results_block_rate05/PeMS/BRITS_PeMS/round_4/20240603_T142753/BRITS.pypots
2024-06-03 14:57:49 [INFO]: Successfully saved to results_block_rate05/PeMS/BRITS_PeMS/round_4/imputation.pkl
2024-06-03 14:57:49 [INFO]: Round4 - BRITS on PeMS: MAE=0.3201, MSE=0.6956, MRE=0.3833
2024-06-03 14:57:49 [INFO]: Done! Final results:
Averaged BRITS (32,012,048 params) on PeMS: MAE=0.3202 ± 0.0009214695462642368, MSE=0.6970 ± 0.0018925965984931562, MRE=0.3834 ± 0.001103332680501688, average inference time=6.47
