2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_0/20240603_T100955
2024-06-03 10:09:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_0/20240603_T100955/tensorboard
2024-06-03 10:09:55 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 10:09:55 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 10:09:59 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 10:10:19 [INFO]: Epoch 001 - training loss: 1.0675, validation loss: 1.0204
2024-06-03 10:10:34 [INFO]: Epoch 002 - training loss: 0.5883, validation loss: 1.0311
2024-06-03 10:10:49 [INFO]: Epoch 003 - training loss: 0.4782, validation loss: 0.9754
2024-06-03 10:11:07 [INFO]: Epoch 004 - training loss: 0.4296, validation loss: 0.9453
2024-06-03 10:11:23 [INFO]: Epoch 005 - training loss: 0.4236, validation loss: 0.9515
2024-06-03 10:11:41 [INFO]: Epoch 006 - training loss: 0.3906, validation loss: 0.9538
2024-06-03 10:11:57 [INFO]: Epoch 007 - training loss: 0.3468, validation loss: 0.9477
2024-06-03 10:12:12 [INFO]: Epoch 008 - training loss: 0.3290, validation loss: 0.9677
2024-06-03 10:12:28 [INFO]: Epoch 009 - training loss: 0.3172, validation loss: 0.9627
2024-06-03 10:12:46 [INFO]: Epoch 010 - training loss: 0.3058, validation loss: 0.9644
2024-06-03 10:13:02 [INFO]: Epoch 011 - training loss: 0.2966, validation loss: 0.9505
2024-06-03 10:13:19 [INFO]: Epoch 012 - training loss: 0.2883, validation loss: 0.9600
2024-06-03 10:13:35 [INFO]: Epoch 013 - training loss: 0.2862, validation loss: 0.9358
2024-06-03 10:13:50 [INFO]: Epoch 014 - training loss: 0.2855, validation loss: 0.9256
2024-06-03 10:14:06 [INFO]: Epoch 015 - training loss: 0.2839, validation loss: 0.9134
2024-06-03 10:14:22 [INFO]: Epoch 016 - training loss: 0.2778, validation loss: 0.9056
2024-06-03 10:14:39 [INFO]: Epoch 017 - training loss: 0.2692, validation loss: 0.9224
2024-06-03 10:14:57 [INFO]: Epoch 018 - training loss: 0.2650, validation loss: 0.9023
2024-06-03 10:15:13 [INFO]: Epoch 019 - training loss: 0.2589, validation loss: 0.9128
2024-06-03 10:15:29 [INFO]: Epoch 020 - training loss: 0.2612, validation loss: 0.9108
2024-06-03 10:15:44 [INFO]: Epoch 021 - training loss: 0.2606, validation loss: 0.9073
2024-06-03 10:16:00 [INFO]: Epoch 022 - training loss: 0.2611, validation loss: 0.9092
2024-06-03 10:16:17 [INFO]: Epoch 023 - training loss: 0.2507, validation loss: 0.8922
2024-06-03 10:16:35 [INFO]: Epoch 024 - training loss: 0.2410, validation loss: 0.8694
2024-06-03 10:16:51 [INFO]: Epoch 025 - training loss: 0.2369, validation loss: 0.8751
2024-06-03 10:17:07 [INFO]: Epoch 026 - training loss: 0.2310, validation loss: 0.8833
2024-06-03 10:17:24 [INFO]: Epoch 027 - training loss: 0.2410, validation loss: 0.8715
2024-06-03 10:17:40 [INFO]: Epoch 028 - training loss: 0.2352, validation loss: 0.8738
2024-06-03 10:17:57 [INFO]: Epoch 029 - training loss: 0.2380, validation loss: 0.8401
2024-06-03 10:18:15 [INFO]: Epoch 030 - training loss: 0.2420, validation loss: 0.8528
2024-06-03 10:18:32 [INFO]: Epoch 031 - training loss: 0.2389, validation loss: 0.8398
2024-06-03 10:18:46 [INFO]: Epoch 032 - training loss: 0.2271, validation loss: 0.8372
2024-06-03 10:19:01 [INFO]: Epoch 033 - training loss: 0.2262, validation loss: 0.8454
2024-06-03 10:19:19 [INFO]: Epoch 034 - training loss: 0.2300, validation loss: 0.8625
2024-06-03 10:19:35 [INFO]: Epoch 035 - training loss: 0.2303, validation loss: 0.8160
2024-06-03 10:19:52 [INFO]: Epoch 036 - training loss: 0.2206, validation loss: 0.8039
2024-06-03 10:20:10 [INFO]: Epoch 037 - training loss: 0.2205, validation loss: 0.8237
2024-06-03 10:20:24 [INFO]: Epoch 038 - training loss: 0.2123, validation loss: 0.7899
2024-06-03 10:20:40 [INFO]: Epoch 039 - training loss: 0.2150, validation loss: 0.8183
2024-06-03 10:20:56 [INFO]: Epoch 040 - training loss: 0.2189, validation loss: 0.7937
2024-06-03 10:21:13 [INFO]: Epoch 041 - training loss: 0.2124, validation loss: 0.7853
2024-06-03 10:21:29 [INFO]: Epoch 042 - training loss: 0.2121, validation loss: 0.7692
2024-06-03 10:21:46 [INFO]: Epoch 043 - training loss: 0.2289, validation loss: 0.7498
2024-06-03 10:22:00 [INFO]: Epoch 044 - training loss: 0.2199, validation loss: 0.8106
2024-06-03 10:22:16 [INFO]: Epoch 045 - training loss: 0.2132, validation loss: 0.7807
2024-06-03 10:22:32 [INFO]: Epoch 046 - training loss: 0.2061, validation loss: 0.7814
2024-06-03 10:22:50 [INFO]: Epoch 047 - training loss: 0.2052, validation loss: 0.7892
2024-06-03 10:23:07 [INFO]: Epoch 048 - training loss: 0.2091, validation loss: 0.7484
2024-06-03 10:23:23 [INFO]: Epoch 049 - training loss: 0.2104, validation loss: 0.7607
2024-06-03 10:23:37 [INFO]: Epoch 050 - training loss: 0.2086, validation loss: 0.7611
2024-06-03 10:23:53 [INFO]: Epoch 051 - training loss: 0.2028, validation loss: 0.7618
2024-06-03 10:24:10 [INFO]: Epoch 052 - training loss: 0.1971, validation loss: 0.7517
2024-06-03 10:24:27 [INFO]: Epoch 053 - training loss: 0.1993, validation loss: 0.7237
2024-06-03 10:24:43 [INFO]: Epoch 054 - training loss: 0.2049, validation loss: 0.7502
2024-06-03 10:25:00 [INFO]: Epoch 055 - training loss: 0.2019, validation loss: 0.7499
2024-06-03 10:25:14 [INFO]: Epoch 056 - training loss: 0.2110, validation loss: 0.7325
2024-06-03 10:25:30 [INFO]: Epoch 057 - training loss: 0.2016, validation loss: 0.7391
2024-06-03 10:25:46 [INFO]: Epoch 058 - training loss: 0.2009, validation loss: 0.7327
2024-06-03 10:26:03 [INFO]: Epoch 059 - training loss: 0.1956, validation loss: 0.7489
2024-06-03 10:26:19 [INFO]: Epoch 060 - training loss: 0.1925, validation loss: 0.7019
2024-06-03 10:26:36 [INFO]: Epoch 061 - training loss: 0.1950, validation loss: 0.7261
2024-06-03 10:26:50 [INFO]: Epoch 062 - training loss: 0.1974, validation loss: 0.7252
2024-06-03 10:27:05 [INFO]: Epoch 063 - training loss: 0.1991, validation loss: 0.7284
2024-06-03 10:27:22 [INFO]: Epoch 064 - training loss: 0.1974, validation loss: 0.7152
2024-06-03 10:27:39 [INFO]: Epoch 065 - training loss: 0.1989, validation loss: 0.7435
2024-06-03 10:27:55 [INFO]: Epoch 066 - training loss: 0.1908, validation loss: 0.7164
2024-06-03 10:28:11 [INFO]: Epoch 067 - training loss: 0.1922, validation loss: 0.7140
2024-06-03 10:28:27 [INFO]: Epoch 068 - training loss: 0.1912, validation loss: 0.7024
2024-06-03 10:28:41 [INFO]: Epoch 069 - training loss: 0.1975, validation loss: 0.7000
2024-06-03 10:28:58 [INFO]: Epoch 070 - training loss: 0.1950, validation loss: 0.7361
2024-06-03 10:29:14 [INFO]: Epoch 071 - training loss: 0.1874, validation loss: 0.6953
2024-06-03 10:29:30 [INFO]: Epoch 072 - training loss: 0.1849, validation loss: 0.6918
2024-06-03 10:29:46 [INFO]: Epoch 073 - training loss: 0.1882, validation loss: 0.6995
2024-06-03 10:30:02 [INFO]: Epoch 074 - training loss: 0.1850, validation loss: 0.6930
2024-06-03 10:30:16 [INFO]: Epoch 075 - training loss: 0.1940, validation loss: 0.6822
2024-06-03 10:30:33 [INFO]: Epoch 076 - training loss: 0.1926, validation loss: 0.7419
2024-06-03 10:30:49 [INFO]: Epoch 077 - training loss: 0.1954, validation loss: 0.7162
2024-06-03 10:31:06 [INFO]: Epoch 078 - training loss: 0.1855, validation loss: 0.7055
2024-06-03 10:31:22 [INFO]: Epoch 079 - training loss: 0.1832, validation loss: 0.6664
2024-06-03 10:31:38 [INFO]: Epoch 080 - training loss: 0.1800, validation loss: 0.6959
2024-06-03 10:31:53 [INFO]: Epoch 081 - training loss: 0.1854, validation loss: 0.6547
2024-06-03 10:32:09 [INFO]: Epoch 082 - training loss: 0.1911, validation loss: 0.7173
2024-06-03 10:32:26 [INFO]: Epoch 083 - training loss: 0.1938, validation loss: 0.6831
2024-06-03 10:32:41 [INFO]: Epoch 084 - training loss: 0.1870, validation loss: 0.6835
2024-06-03 10:32:57 [INFO]: Epoch 085 - training loss: 0.1830, validation loss: 0.7191
2024-06-03 10:33:13 [INFO]: Epoch 086 - training loss: 0.1808, validation loss: 0.6796
2024-06-03 10:33:27 [INFO]: Epoch 087 - training loss: 0.1846, validation loss: 0.6620
2024-06-03 10:33:43 [INFO]: Epoch 088 - training loss: 0.1813, validation loss: 0.6877
2024-06-03 10:33:59 [INFO]: Epoch 089 - training loss: 0.1784, validation loss: 0.6690
2024-06-03 10:34:16 [INFO]: Epoch 090 - training loss: 0.1886, validation loss: 0.6782
2024-06-03 10:34:32 [INFO]: Epoch 091 - training loss: 0.1861, validation loss: 0.7118
2024-06-03 10:34:32 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:34:32 [INFO]: Finished training. The best model is from epoch#81.
2024-06-03 10:34:32 [INFO]: Saved the model to results_block_rate05/PeMS/iTransformer_PeMS/round_0/20240603_T100955/iTransformer.pypots
2024-06-03 10:34:39 [INFO]: Successfully saved to results_block_rate05/PeMS/iTransformer_PeMS/round_0/imputation.pkl
2024-06-03 10:34:39 [INFO]: Round0 - iTransformer on PeMS: MAE=0.4743, MSE=1.0153, MRE=0.5679
2024-06-03 10:34:39 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:34:39 [INFO]: Using the given device: cuda:0
2024-06-03 10:34:39 [INFO]: Model files will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_1/20240603_T103439
2024-06-03 10:34:39 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_1/20240603_T103439/tensorboard
2024-06-03 10:34:39 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 10:34:39 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 10:34:40 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 10:34:56 [INFO]: Epoch 001 - training loss: 1.0516, validation loss: 1.0579
2024-06-03 10:35:10 [INFO]: Epoch 002 - training loss: 0.5893, validation loss: 0.9679
2024-06-03 10:35:26 [INFO]: Epoch 003 - training loss: 0.4747, validation loss: 0.9297
2024-06-03 10:35:42 [INFO]: Epoch 004 - training loss: 0.4211, validation loss: 0.9726
2024-06-03 10:35:58 [INFO]: Epoch 005 - training loss: 0.3950, validation loss: 0.9440
2024-06-03 10:36:15 [INFO]: Epoch 006 - training loss: 0.3540, validation loss: 0.9508
2024-06-03 10:36:30 [INFO]: Epoch 007 - training loss: 0.3552, validation loss: 0.9503
2024-06-03 10:36:45 [INFO]: Epoch 008 - training loss: 0.3310, validation loss: 0.9788
2024-06-03 10:37:00 [INFO]: Epoch 009 - training loss: 0.3206, validation loss: 0.9360
2024-06-03 10:37:16 [INFO]: Epoch 010 - training loss: 0.3184, validation loss: 0.9144
2024-06-03 10:37:32 [INFO]: Epoch 011 - training loss: 0.3064, validation loss: 0.9214
2024-06-03 10:37:48 [INFO]: Epoch 012 - training loss: 0.2872, validation loss: 0.9172
2024-06-03 10:38:04 [INFO]: Epoch 013 - training loss: 0.2886, validation loss: 0.9174
2024-06-03 10:38:19 [INFO]: Epoch 014 - training loss: 0.2808, validation loss: 0.9199
2024-06-03 10:38:34 [INFO]: Epoch 015 - training loss: 0.2702, validation loss: 0.8861
2024-06-03 10:38:50 [INFO]: Epoch 016 - training loss: 0.2660, validation loss: 0.8882
2024-06-03 10:39:07 [INFO]: Epoch 017 - training loss: 0.2578, validation loss: 0.8871
2024-06-03 10:39:23 [INFO]: Epoch 018 - training loss: 0.2553, validation loss: 0.8762
2024-06-03 10:39:39 [INFO]: Epoch 019 - training loss: 0.2629, validation loss: 0.8665
2024-06-03 10:39:53 [INFO]: Epoch 020 - training loss: 0.2545, validation loss: 0.8935
2024-06-03 10:40:06 [INFO]: Epoch 021 - training loss: 0.2453, validation loss: 0.8721
2024-06-03 10:40:22 [INFO]: Epoch 022 - training loss: 0.2430, validation loss: 0.8760
2024-06-03 10:40:38 [INFO]: Epoch 023 - training loss: 0.2425, validation loss: 0.8702
2024-06-03 10:40:54 [INFO]: Epoch 024 - training loss: 0.2416, validation loss: 0.8772
2024-06-03 10:41:11 [INFO]: Epoch 025 - training loss: 0.2393, validation loss: 0.8392
2024-06-03 10:41:26 [INFO]: Epoch 026 - training loss: 0.2410, validation loss: 0.8563
2024-06-03 10:41:40 [INFO]: Epoch 027 - training loss: 0.2365, validation loss: 0.8595
2024-06-03 10:41:55 [INFO]: Epoch 028 - training loss: 0.2360, validation loss: 0.8418
2024-06-03 10:42:11 [INFO]: Epoch 029 - training loss: 0.2265, validation loss: 0.8516
2024-06-03 10:42:28 [INFO]: Epoch 030 - training loss: 0.2307, validation loss: 0.8347
2024-06-03 10:42:44 [INFO]: Epoch 031 - training loss: 0.2319, validation loss: 0.8196
2024-06-03 10:42:59 [INFO]: Epoch 032 - training loss: 0.2244, validation loss: 0.8152
2024-06-03 10:43:14 [INFO]: Epoch 033 - training loss: 0.2351, validation loss: 0.8487
2024-06-03 10:43:29 [INFO]: Epoch 034 - training loss: 0.2267, validation loss: 0.8095
2024-06-03 10:43:45 [INFO]: Epoch 035 - training loss: 0.2192, validation loss: 0.7964
2024-06-03 10:44:01 [INFO]: Epoch 036 - training loss: 0.2251, validation loss: 0.7976
2024-06-03 10:44:17 [INFO]: Epoch 037 - training loss: 0.2209, validation loss: 0.7821
2024-06-03 10:44:33 [INFO]: Epoch 038 - training loss: 0.2214, validation loss: 0.8033
2024-06-03 10:44:49 [INFO]: Epoch 039 - training loss: 0.2071, validation loss: 0.7981
2024-06-03 10:45:03 [INFO]: Epoch 040 - training loss: 0.2217, validation loss: 0.7904
2024-06-03 10:45:17 [INFO]: Epoch 041 - training loss: 0.2143, validation loss: 0.8118
2024-06-03 10:45:32 [INFO]: Epoch 042 - training loss: 0.2214, validation loss: 0.8008
2024-06-03 10:45:47 [INFO]: Epoch 043 - training loss: 0.2122, validation loss: 0.7850
2024-06-03 10:46:02 [INFO]: Epoch 044 - training loss: 0.2093, validation loss: 0.7881
2024-06-03 10:46:18 [INFO]: Epoch 045 - training loss: 0.2075, validation loss: 0.7996
2024-06-03 10:46:32 [INFO]: Epoch 046 - training loss: 0.2115, validation loss: 0.7805
2024-06-03 10:46:46 [INFO]: Epoch 047 - training loss: 0.2050, validation loss: 0.7793
2024-06-03 10:47:01 [INFO]: Epoch 048 - training loss: 0.2010, validation loss: 0.7573
2024-06-03 10:47:16 [INFO]: Epoch 049 - training loss: 0.2007, validation loss: 0.7606
2024-06-03 10:47:31 [INFO]: Epoch 050 - training loss: 0.2001, validation loss: 0.7764
2024-06-03 10:47:46 [INFO]: Epoch 051 - training loss: 0.2044, validation loss: 0.7376
2024-06-03 10:48:01 [INFO]: Epoch 052 - training loss: 0.2029, validation loss: 0.7593
2024-06-03 10:48:15 [INFO]: Epoch 053 - training loss: 0.1962, validation loss: 0.7772
2024-06-03 10:48:29 [INFO]: Epoch 054 - training loss: 0.1958, validation loss: 0.7474
2024-06-03 10:48:44 [INFO]: Epoch 055 - training loss: 0.1983, validation loss: 0.7312
2024-06-03 10:49:00 [INFO]: Epoch 056 - training loss: 0.2000, validation loss: 0.7308
2024-06-03 10:49:15 [INFO]: Epoch 057 - training loss: 0.1977, validation loss: 0.7214
2024-06-03 10:49:30 [INFO]: Epoch 058 - training loss: 0.2026, validation loss: 0.7197
2024-06-03 10:49:45 [INFO]: Epoch 059 - training loss: 0.1935, validation loss: 0.7239
2024-06-03 10:49:58 [INFO]: Epoch 060 - training loss: 0.1934, validation loss: 0.7244
2024-06-03 10:50:12 [INFO]: Epoch 061 - training loss: 0.1928, validation loss: 0.7108
2024-06-03 10:50:28 [INFO]: Epoch 062 - training loss: 0.1906, validation loss: 0.7171
2024-06-03 10:50:42 [INFO]: Epoch 063 - training loss: 0.1938, validation loss: 0.7062
2024-06-03 10:50:57 [INFO]: Epoch 064 - training loss: 0.1936, validation loss: 0.7011
2024-06-03 10:51:13 [INFO]: Epoch 065 - training loss: 0.1907, validation loss: 0.6976
2024-06-03 10:51:27 [INFO]: Epoch 066 - training loss: 0.1922, validation loss: 0.7127
2024-06-03 10:51:40 [INFO]: Epoch 067 - training loss: 0.1953, validation loss: 0.7271
2024-06-03 10:51:55 [INFO]: Epoch 068 - training loss: 0.1948, validation loss: 0.7177
2024-06-03 10:52:10 [INFO]: Epoch 069 - training loss: 0.1962, validation loss: 0.7036
2024-06-03 10:52:25 [INFO]: Epoch 070 - training loss: 0.1934, validation loss: 0.7118
2024-06-03 10:52:40 [INFO]: Epoch 071 - training loss: 0.1929, validation loss: 0.7068
2024-06-03 10:52:55 [INFO]: Epoch 072 - training loss: 0.2024, validation loss: 0.7163
2024-06-03 10:53:08 [INFO]: Epoch 073 - training loss: 0.1984, validation loss: 0.6950
2024-06-03 10:53:23 [INFO]: Epoch 074 - training loss: 0.1894, validation loss: 0.6928
2024-06-03 10:53:38 [INFO]: Epoch 075 - training loss: 0.1933, validation loss: 0.6786
2024-06-03 10:53:52 [INFO]: Epoch 076 - training loss: 0.1982, validation loss: 0.7232
2024-06-03 10:54:07 [INFO]: Epoch 077 - training loss: 0.1871, validation loss: 0.7156
2024-06-03 10:54:22 [INFO]: Epoch 078 - training loss: 0.1850, validation loss: 0.7030
2024-06-03 10:54:35 [INFO]: Epoch 079 - training loss: 0.1858, validation loss: 0.6932
2024-06-03 10:54:48 [INFO]: Epoch 080 - training loss: 0.1929, validation loss: 0.7107
2024-06-03 10:55:02 [INFO]: Epoch 081 - training loss: 0.1851, validation loss: 0.6649
2024-06-03 10:55:17 [INFO]: Epoch 082 - training loss: 0.1828, validation loss: 0.6841
2024-06-03 10:55:30 [INFO]: Epoch 083 - training loss: 0.1924, validation loss: 0.7029
2024-06-03 10:55:44 [INFO]: Epoch 084 - training loss: 0.1863, validation loss: 0.6776
2024-06-03 10:55:59 [INFO]: Epoch 085 - training loss: 0.1837, validation loss: 0.7055
2024-06-03 10:56:12 [INFO]: Epoch 086 - training loss: 0.1907, validation loss: 0.6880
2024-06-03 10:56:25 [INFO]: Epoch 087 - training loss: 0.1832, validation loss: 0.6718
2024-06-03 10:56:39 [INFO]: Epoch 088 - training loss: 0.1782, validation loss: 0.6571
2024-06-03 10:56:53 [INFO]: Epoch 089 - training loss: 0.1820, validation loss: 0.6782
2024-06-03 10:57:07 [INFO]: Epoch 090 - training loss: 0.1808, validation loss: 0.6449
2024-06-03 10:57:22 [INFO]: Epoch 091 - training loss: 0.1966, validation loss: 0.6874
2024-06-03 10:57:36 [INFO]: Epoch 092 - training loss: 0.1857, validation loss: 0.6713
2024-06-03 10:57:48 [INFO]: Epoch 093 - training loss: 0.1837, validation loss: 0.6918
2024-06-03 10:58:02 [INFO]: Epoch 094 - training loss: 0.1788, validation loss: 0.6732
2024-06-03 10:58:17 [INFO]: Epoch 095 - training loss: 0.1803, validation loss: 0.6665
2024-06-03 10:58:31 [INFO]: Epoch 096 - training loss: 0.1771, validation loss: 0.6609
2024-06-03 10:58:46 [INFO]: Epoch 097 - training loss: 0.1849, validation loss: 0.6915
2024-06-03 10:59:00 [INFO]: Epoch 098 - training loss: 0.1784, validation loss: 0.6853
2024-06-03 10:59:14 [INFO]: Epoch 099 - training loss: 0.1770, validation loss: 0.6870
2024-06-03 10:59:26 [INFO]: Epoch 100 - training loss: 0.1733, validation loss: 0.6619
2024-06-03 10:59:26 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:59:26 [INFO]: Finished training. The best model is from epoch#90.
2024-06-03 10:59:26 [INFO]: Saved the model to results_block_rate05/PeMS/iTransformer_PeMS/round_1/20240603_T103439/iTransformer.pypots
2024-06-03 10:59:32 [INFO]: Successfully saved to results_block_rate05/PeMS/iTransformer_PeMS/round_1/imputation.pkl
2024-06-03 10:59:32 [INFO]: Round1 - iTransformer on PeMS: MAE=0.4535, MSE=0.9680, MRE=0.5430
2024-06-03 10:59:32 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:59:32 [INFO]: Using the given device: cuda:0
2024-06-03 10:59:32 [INFO]: Model files will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_2/20240603_T105932
2024-06-03 10:59:32 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_2/20240603_T105932/tensorboard
2024-06-03 10:59:32 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 10:59:32 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 10:59:33 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 10:59:46 [INFO]: Epoch 001 - training loss: 1.0289, validation loss: 1.1248
2024-06-03 10:59:59 [INFO]: Epoch 002 - training loss: 0.5608, validation loss: 0.9565
2024-06-03 11:00:11 [INFO]: Epoch 003 - training loss: 0.4680, validation loss: 0.9465
2024-06-03 11:00:23 [INFO]: Epoch 004 - training loss: 0.4268, validation loss: 0.9238
2024-06-03 11:00:35 [INFO]: Epoch 005 - training loss: 0.3872, validation loss: 0.9348
2024-06-03 11:00:45 [INFO]: Epoch 006 - training loss: 0.3613, validation loss: 0.9702
2024-06-03 11:00:57 [INFO]: Epoch 007 - training loss: 0.3459, validation loss: 0.9333
2024-06-03 11:01:09 [INFO]: Epoch 008 - training loss: 0.3235, validation loss: 0.9714
2024-06-03 11:01:22 [INFO]: Epoch 009 - training loss: 0.3185, validation loss: 0.9464
2024-06-03 11:01:34 [INFO]: Epoch 010 - training loss: 0.3046, validation loss: 0.9253
2024-06-03 11:01:46 [INFO]: Epoch 011 - training loss: 0.2989, validation loss: 0.9234
2024-06-03 11:01:57 [INFO]: Epoch 012 - training loss: 0.3193, validation loss: 0.9152
2024-06-03 11:02:07 [INFO]: Epoch 013 - training loss: 0.3024, validation loss: 0.9018
2024-06-03 11:02:19 [INFO]: Epoch 014 - training loss: 0.2775, validation loss: 0.8991
2024-06-03 11:02:30 [INFO]: Epoch 015 - training loss: 0.2800, validation loss: 0.9203
2024-06-03 11:02:41 [INFO]: Epoch 016 - training loss: 0.2769, validation loss: 0.9117
2024-06-03 11:02:53 [INFO]: Epoch 017 - training loss: 0.2649, validation loss: 0.9036
2024-06-03 11:03:05 [INFO]: Epoch 018 - training loss: 0.2595, validation loss: 0.8962
2024-06-03 11:03:16 [INFO]: Epoch 019 - training loss: 0.2617, validation loss: 0.8973
2024-06-03 11:03:26 [INFO]: Epoch 020 - training loss: 0.2602, validation loss: 0.8680
2024-06-03 11:03:37 [INFO]: Epoch 021 - training loss: 0.2582, validation loss: 0.8976
2024-06-03 11:03:49 [INFO]: Epoch 022 - training loss: 0.2613, validation loss: 0.8625
2024-06-03 11:04:01 [INFO]: Epoch 023 - training loss: 0.2485, validation loss: 0.8515
2024-06-03 11:04:12 [INFO]: Epoch 024 - training loss: 0.2452, validation loss: 0.8526
2024-06-03 11:04:24 [INFO]: Epoch 025 - training loss: 0.2410, validation loss: 0.8599
2024-06-03 11:04:34 [INFO]: Epoch 026 - training loss: 0.2383, validation loss: 0.8532
2024-06-03 11:04:45 [INFO]: Epoch 027 - training loss: 0.2375, validation loss: 0.8588
2024-06-03 11:04:57 [INFO]: Epoch 028 - training loss: 0.2272, validation loss: 0.8656
2024-06-03 11:05:08 [INFO]: Epoch 029 - training loss: 0.2255, validation loss: 0.8462
2024-06-03 11:05:20 [INFO]: Epoch 030 - training loss: 0.2251, validation loss: 0.8717
2024-06-03 11:05:31 [INFO]: Epoch 031 - training loss: 0.2309, validation loss: 0.8242
2024-06-03 11:05:43 [INFO]: Epoch 032 - training loss: 0.2261, validation loss: 0.8213
2024-06-03 11:05:54 [INFO]: Epoch 033 - training loss: 0.2214, validation loss: 0.8130
2024-06-03 11:06:04 [INFO]: Epoch 034 - training loss: 0.2223, validation loss: 0.8305
2024-06-03 11:06:16 [INFO]: Epoch 035 - training loss: 0.2127, validation loss: 0.7991
2024-06-03 11:06:28 [INFO]: Epoch 036 - training loss: 0.2202, validation loss: 0.8060
2024-06-03 11:06:39 [INFO]: Epoch 037 - training loss: 0.2168, validation loss: 0.8401
2024-06-03 11:06:51 [INFO]: Epoch 038 - training loss: 0.2118, validation loss: 0.7633
2024-06-03 11:07:03 [INFO]: Epoch 039 - training loss: 0.2093, validation loss: 0.7722
2024-06-03 11:07:13 [INFO]: Epoch 040 - training loss: 0.2158, validation loss: 0.7765
2024-06-03 11:07:24 [INFO]: Epoch 041 - training loss: 0.2120, validation loss: 0.7945
2024-06-03 11:07:35 [INFO]: Epoch 042 - training loss: 0.2206, validation loss: 0.7760
2024-06-03 11:07:47 [INFO]: Epoch 043 - training loss: 0.2145, validation loss: 0.7975
2024-06-03 11:07:59 [INFO]: Epoch 044 - training loss: 0.2078, validation loss: 0.7821
2024-06-03 11:08:10 [INFO]: Epoch 045 - training loss: 0.2128, validation loss: 0.7720
2024-06-03 11:08:21 [INFO]: Epoch 046 - training loss: 0.2110, validation loss: 0.7570
2024-06-03 11:08:31 [INFO]: Epoch 047 - training loss: 0.2091, validation loss: 0.7448
2024-06-03 11:08:43 [INFO]: Epoch 048 - training loss: 0.2047, validation loss: 0.7443
2024-06-03 11:08:55 [INFO]: Epoch 049 - training loss: 0.2025, validation loss: 0.7467
2024-06-03 11:09:06 [INFO]: Epoch 050 - training loss: 0.2008, validation loss: 0.7474
2024-06-03 11:09:17 [INFO]: Epoch 051 - training loss: 0.2024, validation loss: 0.7390
2024-06-03 11:09:29 [INFO]: Epoch 052 - training loss: 0.2040, validation loss: 0.7375
2024-06-03 11:09:40 [INFO]: Epoch 053 - training loss: 0.2008, validation loss: 0.7542
2024-06-03 11:09:50 [INFO]: Epoch 054 - training loss: 0.1970, validation loss: 0.7399
2024-06-03 11:10:01 [INFO]: Epoch 055 - training loss: 0.2001, validation loss: 0.7628
2024-06-03 11:10:13 [INFO]: Epoch 056 - training loss: 0.2070, validation loss: 0.7568
2024-06-03 11:10:24 [INFO]: Epoch 057 - training loss: 0.2037, validation loss: 0.7232
2024-06-03 11:10:36 [INFO]: Epoch 058 - training loss: 0.2005, validation loss: 0.7430
2024-06-03 11:10:47 [INFO]: Epoch 059 - training loss: 0.1987, validation loss: 0.7362
2024-06-03 11:10:58 [INFO]: Epoch 060 - training loss: 0.1989, validation loss: 0.7394
2024-06-03 11:11:08 [INFO]: Epoch 061 - training loss: 0.2074, validation loss: 0.7229
2024-06-03 11:11:19 [INFO]: Epoch 062 - training loss: 0.1969, validation loss: 0.7293
2024-06-03 11:11:31 [INFO]: Epoch 063 - training loss: 0.1936, validation loss: 0.7416
2024-06-03 11:11:42 [INFO]: Epoch 064 - training loss: 0.1909, validation loss: 0.7629
2024-06-03 11:11:54 [INFO]: Epoch 065 - training loss: 0.1985, validation loss: 0.7365
2024-06-03 11:12:05 [INFO]: Epoch 066 - training loss: 0.1956, validation loss: 0.7185
2024-06-03 11:12:16 [INFO]: Epoch 067 - training loss: 0.1884, validation loss: 0.7277
2024-06-03 11:12:26 [INFO]: Epoch 068 - training loss: 0.1973, validation loss: 0.7041
2024-06-03 11:12:38 [INFO]: Epoch 069 - training loss: 0.1895, validation loss: 0.7145
2024-06-03 11:12:49 [INFO]: Epoch 070 - training loss: 0.1837, validation loss: 0.7099
2024-06-03 11:13:01 [INFO]: Epoch 071 - training loss: 0.1886, validation loss: 0.7336
2024-06-03 11:13:13 [INFO]: Epoch 072 - training loss: 0.1874, validation loss: 0.7444
2024-06-03 11:13:24 [INFO]: Epoch 073 - training loss: 0.1909, validation loss: 0.7369
2024-06-03 11:13:35 [INFO]: Epoch 074 - training loss: 0.1924, validation loss: 0.7349
2024-06-03 11:13:45 [INFO]: Epoch 075 - training loss: 0.1907, validation loss: 0.7029
2024-06-03 11:13:57 [INFO]: Epoch 076 - training loss: 0.1872, validation loss: 0.6986
2024-06-03 11:14:08 [INFO]: Epoch 077 - training loss: 0.1886, validation loss: 0.7089
2024-06-03 11:14:20 [INFO]: Epoch 078 - training loss: 0.1923, validation loss: 0.7009
2024-06-03 11:14:31 [INFO]: Epoch 079 - training loss: 0.1861, validation loss: 0.7201
2024-06-03 11:14:43 [INFO]: Epoch 080 - training loss: 0.1852, validation loss: 0.7081
2024-06-03 11:14:54 [INFO]: Epoch 081 - training loss: 0.1853, validation loss: 0.6948
2024-06-03 11:15:04 [INFO]: Epoch 082 - training loss: 0.1821, validation loss: 0.7068
2024-06-03 11:15:15 [INFO]: Epoch 083 - training loss: 0.1829, validation loss: 0.6722
2024-06-03 11:15:27 [INFO]: Epoch 084 - training loss: 0.1809, validation loss: 0.7142
2024-06-03 11:15:39 [INFO]: Epoch 085 - training loss: 0.1854, validation loss: 0.6875
2024-06-03 11:15:50 [INFO]: Epoch 086 - training loss: 0.1765, validation loss: 0.6865
2024-06-03 11:16:01 [INFO]: Epoch 087 - training loss: 0.1802, validation loss: 0.7114
2024-06-03 11:16:12 [INFO]: Epoch 088 - training loss: 0.1828, validation loss: 0.6831
2024-06-03 11:16:23 [INFO]: Epoch 089 - training loss: 0.1839, validation loss: 0.6956
2024-06-03 11:16:34 [INFO]: Epoch 090 - training loss: 0.1854, validation loss: 0.7045
2024-06-03 11:16:46 [INFO]: Epoch 091 - training loss: 0.1969, validation loss: 0.6819
2024-06-03 11:16:57 [INFO]: Epoch 092 - training loss: 0.1870, validation loss: 0.7034
2024-06-03 11:17:09 [INFO]: Epoch 093 - training loss: 0.1891, validation loss: 0.6640
2024-06-03 11:17:20 [INFO]: Epoch 094 - training loss: 0.1852, validation loss: 0.7159
2024-06-03 11:17:31 [INFO]: Epoch 095 - training loss: 0.1817, validation loss: 0.6582
2024-06-03 11:17:41 [INFO]: Epoch 096 - training loss: 0.1809, validation loss: 0.6955
2024-06-03 11:17:53 [INFO]: Epoch 097 - training loss: 0.1809, validation loss: 0.6806
2024-06-03 11:18:04 [INFO]: Epoch 098 - training loss: 0.1874, validation loss: 0.6852
2024-06-03 11:18:16 [INFO]: Epoch 099 - training loss: 0.1783, validation loss: 0.7021
2024-06-03 11:18:28 [INFO]: Epoch 100 - training loss: 0.1753, validation loss: 0.6663
2024-06-03 11:18:28 [INFO]: Finished training. The best model is from epoch#95.
2024-06-03 11:18:28 [INFO]: Saved the model to results_block_rate05/PeMS/iTransformer_PeMS/round_2/20240603_T105932/iTransformer.pypots
2024-06-03 11:18:32 [INFO]: Successfully saved to results_block_rate05/PeMS/iTransformer_PeMS/round_2/imputation.pkl
2024-06-03 11:18:32 [INFO]: Round2 - iTransformer on PeMS: MAE=0.4634, MSE=0.9806, MRE=0.5548
2024-06-03 11:18:32 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 11:18:32 [INFO]: Using the given device: cuda:0
2024-06-03 11:18:32 [INFO]: Model files will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_3/20240603_T111832
2024-06-03 11:18:32 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_3/20240603_T111832/tensorboard
2024-06-03 11:18:32 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 11:18:32 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 11:18:33 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 11:18:44 [INFO]: Epoch 001 - training loss: 1.0350, validation loss: 1.0194
2024-06-03 11:18:55 [INFO]: Epoch 002 - training loss: 0.5849, validation loss: 0.9802
2024-06-03 11:19:06 [INFO]: Epoch 003 - training loss: 0.4797, validation loss: 1.0063
2024-06-03 11:19:17 [INFO]: Epoch 004 - training loss: 0.4296, validation loss: 0.9874
2024-06-03 11:19:29 [INFO]: Epoch 005 - training loss: 0.3957, validation loss: 0.9380
2024-06-03 11:19:41 [INFO]: Epoch 006 - training loss: 0.3734, validation loss: 0.9472
2024-06-03 11:19:52 [INFO]: Epoch 007 - training loss: 0.3426, validation loss: 0.9583
2024-06-03 11:20:03 [INFO]: Epoch 008 - training loss: 0.3308, validation loss: 0.9281
2024-06-03 11:20:14 [INFO]: Epoch 009 - training loss: 0.3238, validation loss: 0.9560
2024-06-03 11:20:25 [INFO]: Epoch 010 - training loss: 0.3115, validation loss: 0.9281
2024-06-03 11:20:36 [INFO]: Epoch 011 - training loss: 0.3042, validation loss: 0.9333
2024-06-03 11:20:48 [INFO]: Epoch 012 - training loss: 0.2927, validation loss: 0.9559
2024-06-03 11:21:00 [INFO]: Epoch 013 - training loss: 0.2872, validation loss: 0.8996
2024-06-03 11:21:11 [INFO]: Epoch 014 - training loss: 0.2803, validation loss: 0.9137
2024-06-03 11:21:22 [INFO]: Epoch 015 - training loss: 0.2712, validation loss: 0.9094
2024-06-03 11:21:32 [INFO]: Epoch 016 - training loss: 0.2695, validation loss: 0.9106
2024-06-03 11:21:43 [INFO]: Epoch 017 - training loss: 0.2643, validation loss: 0.9044
2024-06-03 11:21:54 [INFO]: Epoch 018 - training loss: 0.2683, validation loss: 0.8896
2024-06-03 11:22:06 [INFO]: Epoch 019 - training loss: 0.2647, validation loss: 0.8821
2024-06-03 11:22:17 [INFO]: Epoch 020 - training loss: 0.2613, validation loss: 0.8973
2024-06-03 11:22:28 [INFO]: Epoch 021 - training loss: 0.2532, validation loss: 0.8776
2024-06-03 11:22:40 [INFO]: Epoch 022 - training loss: 0.2517, validation loss: 0.8637
2024-06-03 11:22:49 [INFO]: Epoch 023 - training loss: 0.2554, validation loss: 0.8611
2024-06-03 11:22:58 [INFO]: Epoch 024 - training loss: 0.2478, validation loss: 0.8759
2024-06-03 11:23:08 [INFO]: Epoch 025 - training loss: 0.2360, validation loss: 0.8686
2024-06-03 11:23:17 [INFO]: Epoch 026 - training loss: 0.2383, validation loss: 0.8550
2024-06-03 11:23:27 [INFO]: Epoch 027 - training loss: 0.2360, validation loss: 0.8758
2024-06-03 11:23:36 [INFO]: Epoch 028 - training loss: 0.2348, validation loss: 0.8455
2024-06-03 11:23:45 [INFO]: Epoch 029 - training loss: 0.2349, validation loss: 0.8606
2024-06-03 11:23:54 [INFO]: Epoch 030 - training loss: 0.2316, validation loss: 0.8568
2024-06-03 11:24:03 [INFO]: Epoch 031 - training loss: 0.2418, validation loss: 0.8837
2024-06-03 11:24:13 [INFO]: Epoch 032 - training loss: 0.2380, validation loss: 0.8246
2024-06-03 11:24:23 [INFO]: Epoch 033 - training loss: 0.2305, validation loss: 0.8324
2024-06-03 11:24:33 [INFO]: Epoch 034 - training loss: 0.2223, validation loss: 0.8245
2024-06-03 11:24:42 [INFO]: Epoch 035 - training loss: 0.2190, validation loss: 0.8259
2024-06-03 11:24:51 [INFO]: Epoch 036 - training loss: 0.2186, validation loss: 0.8107
2024-06-03 11:24:59 [INFO]: Epoch 037 - training loss: 0.2142, validation loss: 0.8210
2024-06-03 11:25:09 [INFO]: Epoch 038 - training loss: 0.2135, validation loss: 0.8005
2024-06-03 11:25:19 [INFO]: Epoch 039 - training loss: 0.2133, validation loss: 0.7873
2024-06-03 11:25:28 [INFO]: Epoch 040 - training loss: 0.2142, validation loss: 0.8136
2024-06-03 11:25:38 [INFO]: Epoch 041 - training loss: 0.2141, validation loss: 0.7835
2024-06-03 11:25:48 [INFO]: Epoch 042 - training loss: 0.2145, validation loss: 0.8076
2024-06-03 11:25:57 [INFO]: Epoch 043 - training loss: 0.2156, validation loss: 0.7920
2024-06-03 11:26:06 [INFO]: Epoch 044 - training loss: 0.2057, validation loss: 0.8051
2024-06-03 11:26:15 [INFO]: Epoch 045 - training loss: 0.2087, validation loss: 0.7571
2024-06-03 11:26:25 [INFO]: Epoch 046 - training loss: 0.2084, validation loss: 0.7894
2024-06-03 11:26:35 [INFO]: Epoch 047 - training loss: 0.2048, validation loss: 0.7784
2024-06-03 11:26:44 [INFO]: Epoch 048 - training loss: 0.2068, validation loss: 0.7672
2024-06-03 11:26:54 [INFO]: Epoch 049 - training loss: 0.2019, validation loss: 0.7242
2024-06-03 11:27:03 [INFO]: Epoch 050 - training loss: 0.2023, validation loss: 0.7577
2024-06-03 11:27:12 [INFO]: Epoch 051 - training loss: 0.1968, validation loss: 0.7713
2024-06-03 11:27:22 [INFO]: Epoch 052 - training loss: 0.2010, validation loss: 0.7474
2024-06-03 11:27:32 [INFO]: Epoch 053 - training loss: 0.2043, validation loss: 0.7459
2024-06-03 11:27:41 [INFO]: Epoch 054 - training loss: 0.2069, validation loss: 0.7726
2024-06-03 11:27:51 [INFO]: Epoch 055 - training loss: 0.2047, validation loss: 0.7619
2024-06-03 11:28:01 [INFO]: Epoch 056 - training loss: 0.1966, validation loss: 0.7406
2024-06-03 11:28:09 [INFO]: Epoch 057 - training loss: 0.1943, validation loss: 0.7534
2024-06-03 11:28:18 [INFO]: Epoch 058 - training loss: 0.1980, validation loss: 0.7405
2024-06-03 11:28:27 [INFO]: Epoch 059 - training loss: 0.1992, validation loss: 0.7235
2024-06-03 11:28:37 [INFO]: Epoch 060 - training loss: 0.1996, validation loss: 0.7304
2024-06-03 11:28:47 [INFO]: Epoch 061 - training loss: 0.1953, validation loss: 0.7493
2024-06-03 11:28:57 [INFO]: Epoch 062 - training loss: 0.1978, validation loss: 0.7166
2024-06-03 11:29:07 [INFO]: Epoch 063 - training loss: 0.2050, validation loss: 0.7447
2024-06-03 11:29:15 [INFO]: Epoch 064 - training loss: 0.1940, validation loss: 0.7194
2024-06-03 11:29:24 [INFO]: Epoch 065 - training loss: 0.1936, validation loss: 0.7117
2024-06-03 11:29:34 [INFO]: Epoch 066 - training loss: 0.1955, validation loss: 0.7197
2024-06-03 11:29:43 [INFO]: Epoch 067 - training loss: 0.1958, validation loss: 0.7253
2024-06-03 11:29:53 [INFO]: Epoch 068 - training loss: 0.2003, validation loss: 0.7167
2024-06-03 11:30:03 [INFO]: Epoch 069 - training loss: 0.1954, validation loss: 0.7305
2024-06-03 11:30:12 [INFO]: Epoch 070 - training loss: 0.1908, validation loss: 0.7060
2024-06-03 11:30:20 [INFO]: Epoch 071 - training loss: 0.1882, validation loss: 0.6991
2024-06-03 11:30:29 [INFO]: Epoch 072 - training loss: 0.1919, validation loss: 0.7119
2024-06-03 11:30:39 [INFO]: Epoch 073 - training loss: 0.1899, validation loss: 0.7153
2024-06-03 11:30:48 [INFO]: Epoch 074 - training loss: 0.1875, validation loss: 0.7234
2024-06-03 11:30:57 [INFO]: Epoch 075 - training loss: 0.1940, validation loss: 0.6950
2024-06-03 11:31:07 [INFO]: Epoch 076 - training loss: 0.1969, validation loss: 0.7393
2024-06-03 11:31:16 [INFO]: Epoch 077 - training loss: 0.1920, validation loss: 0.6837
2024-06-03 11:31:24 [INFO]: Epoch 078 - training loss: 0.1860, validation loss: 0.6965
2024-06-03 11:31:33 [INFO]: Epoch 079 - training loss: 0.1822, validation loss: 0.6852
2024-06-03 11:31:42 [INFO]: Epoch 080 - training loss: 0.1841, validation loss: 0.6961
2024-06-03 11:31:52 [INFO]: Epoch 081 - training loss: 0.1830, validation loss: 0.6855
2024-06-03 11:32:01 [INFO]: Epoch 082 - training loss: 0.1802, validation loss: 0.7038
2024-06-03 11:32:10 [INFO]: Epoch 083 - training loss: 0.1779, validation loss: 0.6830
2024-06-03 11:32:19 [INFO]: Epoch 084 - training loss: 0.1818, validation loss: 0.7030
2024-06-03 11:32:28 [INFO]: Epoch 085 - training loss: 0.1879, validation loss: 0.7040
2024-06-03 11:32:37 [INFO]: Epoch 086 - training loss: 0.1850, validation loss: 0.7112
2024-06-03 11:32:46 [INFO]: Epoch 087 - training loss: 0.1817, validation loss: 0.6907
2024-06-03 11:32:55 [INFO]: Epoch 088 - training loss: 0.1871, validation loss: 0.6729
2024-06-03 11:33:04 [INFO]: Epoch 089 - training loss: 0.1947, validation loss: 0.7130
2024-06-03 11:33:14 [INFO]: Epoch 090 - training loss: 0.1848, validation loss: 0.6837
2024-06-03 11:33:23 [INFO]: Epoch 091 - training loss: 0.1812, validation loss: 0.6867
2024-06-03 11:33:32 [INFO]: Epoch 092 - training loss: 0.1830, validation loss: 0.7072
2024-06-03 11:33:40 [INFO]: Epoch 093 - training loss: 0.1871, validation loss: 0.7018
2024-06-03 11:33:49 [INFO]: Epoch 094 - training loss: 0.1813, validation loss: 0.6918
2024-06-03 11:33:58 [INFO]: Epoch 095 - training loss: 0.1774, validation loss: 0.7015
2024-06-03 11:34:07 [INFO]: Epoch 096 - training loss: 0.1789, validation loss: 0.6772
2024-06-03 11:34:16 [INFO]: Epoch 097 - training loss: 0.1826, validation loss: 0.6733
2024-06-03 11:34:24 [INFO]: Epoch 098 - training loss: 0.1819, validation loss: 0.6867
2024-06-03 11:34:24 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:34:24 [INFO]: Finished training. The best model is from epoch#88.
2024-06-03 11:34:24 [INFO]: Saved the model to results_block_rate05/PeMS/iTransformer_PeMS/round_3/20240603_T111832/iTransformer.pypots
2024-06-03 11:34:28 [INFO]: Successfully saved to results_block_rate05/PeMS/iTransformer_PeMS/round_3/imputation.pkl
2024-06-03 11:34:28 [INFO]: Round3 - iTransformer on PeMS: MAE=0.4647, MSE=0.9985, MRE=0.5564
2024-06-03 11:34:28 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 11:34:28 [INFO]: Using the given device: cuda:0
2024-06-03 11:34:28 [INFO]: Model files will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_4/20240603_T113428
2024-06-03 11:34:28 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/iTransformer_PeMS/round_4/20240603_T113428/tensorboard
2024-06-03 11:34:28 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 11:34:28 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 11:34:28 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 11:34:36 [INFO]: Epoch 001 - training loss: 1.0452, validation loss: 1.0316
2024-06-03 11:34:44 [INFO]: Epoch 002 - training loss: 0.5800, validation loss: 0.9662
2024-06-03 11:34:52 [INFO]: Epoch 003 - training loss: 0.4710, validation loss: 0.9481
2024-06-03 11:35:01 [INFO]: Epoch 004 - training loss: 0.4248, validation loss: 0.9876
2024-06-03 11:35:09 [INFO]: Epoch 005 - training loss: 0.4033, validation loss: 0.9519
2024-06-03 11:35:18 [INFO]: Epoch 006 - training loss: 0.3667, validation loss: 0.9537
2024-06-03 11:35:26 [INFO]: Epoch 007 - training loss: 0.3497, validation loss: 0.9494
2024-06-03 11:35:35 [INFO]: Epoch 008 - training loss: 0.3265, validation loss: 0.9469
2024-06-03 11:35:43 [INFO]: Epoch 009 - training loss: 0.3153, validation loss: 0.9349
2024-06-03 11:35:50 [INFO]: Epoch 010 - training loss: 0.3066, validation loss: 0.9342
2024-06-03 11:35:59 [INFO]: Epoch 011 - training loss: 0.3084, validation loss: 0.9045
2024-06-03 11:36:07 [INFO]: Epoch 012 - training loss: 0.2925, validation loss: 0.8960
2024-06-03 11:36:16 [INFO]: Epoch 013 - training loss: 0.2852, validation loss: 0.9247
2024-06-03 11:36:24 [INFO]: Epoch 014 - training loss: 0.2811, validation loss: 0.9107
2024-06-03 11:36:33 [INFO]: Epoch 015 - training loss: 0.2728, validation loss: 0.8931
2024-06-03 11:36:41 [INFO]: Epoch 016 - training loss: 0.2691, validation loss: 0.8960
2024-06-03 11:36:48 [INFO]: Epoch 017 - training loss: 0.2714, validation loss: 0.8941
2024-06-03 11:36:56 [INFO]: Epoch 018 - training loss: 0.2672, validation loss: 0.8906
2024-06-03 11:37:05 [INFO]: Epoch 019 - training loss: 0.2657, validation loss: 0.9054
2024-06-03 11:37:13 [INFO]: Epoch 020 - training loss: 0.2539, validation loss: 0.8834
2024-06-03 11:37:21 [INFO]: Epoch 021 - training loss: 0.2523, validation loss: 0.8751
2024-06-03 11:37:29 [INFO]: Epoch 022 - training loss: 0.2537, validation loss: 0.8764
2024-06-03 11:37:38 [INFO]: Epoch 023 - training loss: 0.2498, validation loss: 0.8600
2024-06-03 11:37:45 [INFO]: Epoch 024 - training loss: 0.2471, validation loss: 0.8586
2024-06-03 11:37:53 [INFO]: Epoch 025 - training loss: 0.2458, validation loss: 0.8649
2024-06-03 11:38:01 [INFO]: Epoch 026 - training loss: 0.2441, validation loss: 0.8512
2024-06-03 11:38:09 [INFO]: Epoch 027 - training loss: 0.2564, validation loss: 0.8614
2024-06-03 11:38:17 [INFO]: Epoch 028 - training loss: 0.2409, validation loss: 0.8616
2024-06-03 11:38:26 [INFO]: Epoch 029 - training loss: 0.2373, validation loss: 0.8536
2024-06-03 11:38:34 [INFO]: Epoch 030 - training loss: 0.2338, validation loss: 0.8367
2024-06-03 11:38:42 [INFO]: Epoch 031 - training loss: 0.2340, validation loss: 0.8450
2024-06-03 11:38:49 [INFO]: Epoch 032 - training loss: 0.2357, validation loss: 0.8092
2024-06-03 11:38:57 [INFO]: Epoch 033 - training loss: 0.2314, validation loss: 0.8322
2024-06-03 11:39:05 [INFO]: Epoch 034 - training loss: 0.2247, validation loss: 0.8521
2024-06-03 11:39:13 [INFO]: Epoch 035 - training loss: 0.2231, validation loss: 0.8235
2024-06-03 11:39:21 [INFO]: Epoch 036 - training loss: 0.2172, validation loss: 0.8295
2024-06-03 11:39:29 [INFO]: Epoch 037 - training loss: 0.2139, validation loss: 0.8335
2024-06-03 11:39:38 [INFO]: Epoch 038 - training loss: 0.2165, validation loss: 0.8196
2024-06-03 11:39:46 [INFO]: Epoch 039 - training loss: 0.2232, validation loss: 0.8227
2024-06-03 11:39:53 [INFO]: Epoch 040 - training loss: 0.2209, validation loss: 0.7968
2024-06-03 11:40:01 [INFO]: Epoch 041 - training loss: 0.2168, validation loss: 0.7854
2024-06-03 11:40:10 [INFO]: Epoch 042 - training loss: 0.2130, validation loss: 0.7976
2024-06-03 11:40:18 [INFO]: Epoch 043 - training loss: 0.2102, validation loss: 0.7784
2024-06-03 11:40:26 [INFO]: Epoch 044 - training loss: 0.2032, validation loss: 0.7775
2024-06-03 11:40:34 [INFO]: Epoch 045 - training loss: 0.2162, validation loss: 0.8071
2024-06-03 11:40:43 [INFO]: Epoch 046 - training loss: 0.2156, validation loss: 0.7856
2024-06-03 11:40:50 [INFO]: Epoch 047 - training loss: 0.2172, validation loss: 0.7827
2024-06-03 11:40:58 [INFO]: Epoch 048 - training loss: 0.2101, validation loss: 0.7917
2024-06-03 11:41:05 [INFO]: Epoch 049 - training loss: 0.2123, validation loss: 0.7790
2024-06-03 11:41:13 [INFO]: Epoch 050 - training loss: 0.2120, validation loss: 0.7678
2024-06-03 11:41:21 [INFO]: Epoch 051 - training loss: 0.2060, validation loss: 0.7632
2024-06-03 11:41:29 [INFO]: Epoch 052 - training loss: 0.1998, validation loss: 0.7583
2024-06-03 11:41:37 [INFO]: Epoch 053 - training loss: 0.2000, validation loss: 0.7757
2024-06-03 11:41:45 [INFO]: Epoch 054 - training loss: 0.1945, validation loss: 0.7650
2024-06-03 11:41:53 [INFO]: Epoch 055 - training loss: 0.1973, validation loss: 0.7297
2024-06-03 11:42:00 [INFO]: Epoch 056 - training loss: 0.1990, validation loss: 0.7551
2024-06-03 11:42:07 [INFO]: Epoch 057 - training loss: 0.1944, validation loss: 0.7599
2024-06-03 11:42:15 [INFO]: Epoch 058 - training loss: 0.2037, validation loss: 0.7744
2024-06-03 11:42:24 [INFO]: Epoch 059 - training loss: 0.2116, validation loss: 0.7563
2024-06-03 11:42:32 [INFO]: Epoch 060 - training loss: 0.2019, validation loss: 0.7335
2024-06-03 11:42:40 [INFO]: Epoch 061 - training loss: 0.1937, validation loss: 0.7105
2024-06-03 11:42:48 [INFO]: Epoch 062 - training loss: 0.1935, validation loss: 0.7314
2024-06-03 11:42:56 [INFO]: Epoch 063 - training loss: 0.1995, validation loss: 0.7145
2024-06-03 11:43:03 [INFO]: Epoch 064 - training loss: 0.1923, validation loss: 0.7042
2024-06-03 11:43:10 [INFO]: Epoch 065 - training loss: 0.1905, validation loss: 0.7290
2024-06-03 11:43:18 [INFO]: Epoch 066 - training loss: 0.1985, validation loss: 0.7429
2024-06-03 11:43:26 [INFO]: Epoch 067 - training loss: 0.1971, validation loss: 0.7385
2024-06-03 11:43:34 [INFO]: Epoch 068 - training loss: 0.1911, validation loss: 0.7221
2024-06-03 11:43:43 [INFO]: Epoch 069 - training loss: 0.2020, validation loss: 0.7295
2024-06-03 11:43:51 [INFO]: Epoch 070 - training loss: 0.1930, validation loss: 0.7008
2024-06-03 11:43:58 [INFO]: Epoch 071 - training loss: 0.1933, validation loss: 0.7232
2024-06-03 11:44:06 [INFO]: Epoch 072 - training loss: 0.1947, validation loss: 0.6947
2024-06-03 11:44:13 [INFO]: Epoch 073 - training loss: 0.1933, validation loss: 0.7120
2024-06-03 11:44:21 [INFO]: Epoch 074 - training loss: 0.1942, validation loss: 0.7525
2024-06-03 11:44:29 [INFO]: Epoch 075 - training loss: 0.1887, validation loss: 0.6945
2024-06-03 11:44:37 [INFO]: Epoch 076 - training loss: 0.1889, validation loss: 0.7214
2024-06-03 11:44:45 [INFO]: Epoch 077 - training loss: 0.1923, validation loss: 0.7197
2024-06-03 11:44:53 [INFO]: Epoch 078 - training loss: 0.1957, validation loss: 0.6866
2024-06-03 11:45:01 [INFO]: Epoch 079 - training loss: 0.1888, validation loss: 0.6853
2024-06-03 11:45:09 [INFO]: Epoch 080 - training loss: 0.1885, validation loss: 0.7039
2024-06-03 11:45:16 [INFO]: Epoch 081 - training loss: 0.1836, validation loss: 0.7104
2024-06-03 11:45:24 [INFO]: Epoch 082 - training loss: 0.1823, validation loss: 0.7005
2024-06-03 11:45:32 [INFO]: Epoch 083 - training loss: 0.1936, validation loss: 0.6820
2024-06-03 11:45:40 [INFO]: Epoch 084 - training loss: 0.1931, validation loss: 0.7121
2024-06-03 11:45:48 [INFO]: Epoch 085 - training loss: 0.1898, validation loss: 0.7045
2024-06-03 11:45:56 [INFO]: Epoch 086 - training loss: 0.1854, validation loss: 0.6831
2024-06-03 11:46:04 [INFO]: Epoch 087 - training loss: 0.1823, validation loss: 0.7059
2024-06-03 11:46:11 [INFO]: Epoch 088 - training loss: 0.1869, validation loss: 0.6919
2024-06-03 11:46:19 [INFO]: Epoch 089 - training loss: 0.1815, validation loss: 0.6906
2024-06-03 11:46:27 [INFO]: Epoch 090 - training loss: 0.1798, validation loss: 0.6943
2024-06-03 11:46:35 [INFO]: Epoch 091 - training loss: 0.1818, validation loss: 0.6595
2024-06-03 11:46:43 [INFO]: Epoch 092 - training loss: 0.1838, validation loss: 0.7082
2024-06-03 11:46:51 [INFO]: Epoch 093 - training loss: 0.1840, validation loss: 0.7202
2024-06-03 11:46:59 [INFO]: Epoch 094 - training loss: 0.1872, validation loss: 0.7078
2024-06-03 11:47:07 [INFO]: Epoch 095 - training loss: 0.1781, validation loss: 0.6868
2024-06-03 11:47:14 [INFO]: Epoch 096 - training loss: 0.1759, validation loss: 0.6729
2024-06-03 11:47:22 [INFO]: Epoch 097 - training loss: 0.1810, validation loss: 0.6885
2024-06-03 11:47:30 [INFO]: Epoch 098 - training loss: 0.1820, validation loss: 0.6993
2024-06-03 11:47:38 [INFO]: Epoch 099 - training loss: 0.1895, validation loss: 0.6994
2024-06-03 11:47:46 [INFO]: Epoch 100 - training loss: 0.1802, validation loss: 0.6620
2024-06-03 11:47:46 [INFO]: Finished training. The best model is from epoch#91.
2024-06-03 11:47:46 [INFO]: Saved the model to results_block_rate05/PeMS/iTransformer_PeMS/round_4/20240603_T113428/iTransformer.pypots
2024-06-03 11:47:49 [INFO]: Successfully saved to results_block_rate05/PeMS/iTransformer_PeMS/round_4/imputation.pkl
2024-06-03 11:47:49 [INFO]: Round4 - iTransformer on PeMS: MAE=0.4617, MSE=0.9802, MRE=0.5528
2024-06-03 11:47:49 [INFO]: Done! Final results:
Averaged iTransformer (1,854,744 params) on PeMS: MAE=0.4635 ± 0.0066407926364585495, MSE=0.9885 ± 0.016554581854085034, MRE=0.5550 ± 0.007951433197053829, average inference time=0.91
