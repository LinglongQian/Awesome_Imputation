2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_0/20240603_T100955
2024-06-03 10:09:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_0/20240603_T100955/tensorboard
2024-06-03 10:10:00 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 2,396,536
2024-06-03 10:10:36 [INFO]: Epoch 001 - training loss: 632960.7000, validation loss: 1.3012
2024-06-03 10:10:59 [INFO]: Epoch 002 - training loss: 367730.6375, validation loss: 1.1303
2024-06-03 10:11:22 [INFO]: Epoch 003 - training loss: 275770.4000, validation loss: 0.8952
2024-06-03 10:11:46 [INFO]: Epoch 004 - training loss: 263408.0062, validation loss: 0.6880
2024-06-03 10:12:10 [INFO]: Epoch 005 - training loss: 260656.7854, validation loss: 0.6124
2024-06-03 10:12:32 [INFO]: Epoch 006 - training loss: 259905.6333, validation loss: 0.5834
2024-06-03 10:12:54 [INFO]: Epoch 007 - training loss: 259697.7594, validation loss: 0.5760
2024-06-03 10:13:16 [INFO]: Epoch 008 - training loss: 259360.0365, validation loss: 0.5704
2024-06-03 10:13:39 [INFO]: Epoch 009 - training loss: 259084.0115, validation loss: 0.5522
2024-06-03 10:14:02 [INFO]: Epoch 010 - training loss: 259020.3625, validation loss: 0.5529
2024-06-03 10:14:24 [INFO]: Epoch 011 - training loss: 258956.6042, validation loss: 0.5696
2024-06-03 10:14:47 [INFO]: Epoch 012 - training loss: 258924.7365, validation loss: 0.5568
2024-06-03 10:15:10 [INFO]: Epoch 013 - training loss: 258892.0521, validation loss: 0.5380
2024-06-03 10:15:32 [INFO]: Epoch 014 - training loss: 258865.2969, validation loss: 0.5358
2024-06-03 10:15:53 [INFO]: Epoch 015 - training loss: 258891.6938, validation loss: 0.5347
2024-06-03 10:16:14 [INFO]: Epoch 016 - training loss: 258874.5594, validation loss: 0.5328
2024-06-03 10:16:37 [INFO]: Epoch 017 - training loss: 258808.9906, validation loss: 0.5568
2024-06-03 10:16:58 [INFO]: Epoch 018 - training loss: 258810.8656, validation loss: 0.5352
2024-06-03 10:17:21 [INFO]: Epoch 019 - training loss: 258781.6354, validation loss: 0.5324
2024-06-03 10:17:43 [INFO]: Epoch 020 - training loss: 258782.5333, validation loss: 0.5306
2024-06-03 10:18:05 [INFO]: Epoch 021 - training loss: 258828.6292, validation loss: 0.5631
2024-06-03 10:18:27 [INFO]: Epoch 022 - training loss: 258798.9500, validation loss: 0.5403
2024-06-03 10:18:48 [INFO]: Epoch 023 - training loss: 258788.4927, validation loss: 0.5250
2024-06-03 10:19:09 [INFO]: Epoch 024 - training loss: 258800.1906, validation loss: 0.5379
2024-06-03 10:19:31 [INFO]: Epoch 025 - training loss: 258888.3062, validation loss: 0.5430
2024-06-03 10:19:53 [INFO]: Epoch 026 - training loss: 258913.1802, validation loss: 0.5233
2024-06-03 10:20:16 [INFO]: Epoch 027 - training loss: 258992.6552, validation loss: 0.5196
2024-06-03 10:20:39 [INFO]: Epoch 028 - training loss: 259019.2354, validation loss: 0.5303
2024-06-03 10:21:01 [INFO]: Epoch 029 - training loss: 258954.1688, validation loss: 0.5238
2024-06-03 10:21:23 [INFO]: Epoch 030 - training loss: 258931.1240, validation loss: 0.5212
2024-06-03 10:21:44 [INFO]: Epoch 031 - training loss: 258930.3615, validation loss: 0.5418
2024-06-03 10:22:07 [INFO]: Epoch 032 - training loss: 258942.0688, validation loss: 0.5260
2024-06-03 10:22:29 [INFO]: Epoch 033 - training loss: 258867.6552, validation loss: 0.5289
2024-06-03 10:22:52 [INFO]: Epoch 034 - training loss: 258848.0990, validation loss: 0.5130
2024-06-03 10:23:13 [INFO]: Epoch 035 - training loss: 258818.2771, validation loss: 0.5368
2024-06-03 10:23:35 [INFO]: Epoch 036 - training loss: 258881.3844, validation loss: 0.5178
2024-06-03 10:23:58 [INFO]: Epoch 037 - training loss: 258828.1938, validation loss: 0.5283
2024-06-03 10:24:20 [INFO]: Epoch 038 - training loss: 258804.8740, validation loss: 0.5257
2024-06-03 10:24:42 [INFO]: Epoch 039 - training loss: 258739.5854, validation loss: 0.5206
2024-06-03 10:25:05 [INFO]: Epoch 040 - training loss: 258758.9052, validation loss: 0.5353
2024-06-03 10:25:27 [INFO]: Epoch 041 - training loss: 258699.6646, validation loss: 0.5246
2024-06-03 10:25:50 [INFO]: Epoch 042 - training loss: 258657.5521, validation loss: 0.5259
2024-06-03 10:26:12 [INFO]: Epoch 043 - training loss: 258674.7073, validation loss: 0.5203
2024-06-03 10:26:34 [INFO]: Epoch 044 - training loss: 258703.6500, validation loss: 0.5313
2024-06-03 10:26:34 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:26:34 [INFO]: Finished training. The best model is from epoch#34.
2024-06-03 10:26:34 [INFO]: Saved the model to results_block_rate05/PeMS/GPVAE_PeMS/round_0/20240603_T100955/GPVAE.pypots
2024-06-03 10:29:09 [INFO]: Successfully saved to results_block_rate05/PeMS/GPVAE_PeMS/round_0/imputation.pkl
2024-06-03 10:29:09 [INFO]: Round0 - GPVAE on PeMS: MAE=0.3757, MSE=0.7560, MRE=0.4499
2024-06-03 10:29:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:29:09 [INFO]: Using the given device: cuda:0
2024-06-03 10:29:09 [INFO]: Model files will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_1/20240603_T102909
2024-06-03 10:29:09 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_1/20240603_T102909/tensorboard
2024-06-03 10:29:09 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 2,396,536
2024-06-03 10:29:30 [INFO]: Epoch 001 - training loss: 639856.7625, validation loss: 1.3008
2024-06-03 10:29:49 [INFO]: Epoch 002 - training loss: 374495.2167, validation loss: 1.1453
2024-06-03 10:30:09 [INFO]: Epoch 003 - training loss: 278098.1646, validation loss: 0.9430
2024-06-03 10:30:29 [INFO]: Epoch 004 - training loss: 263960.9208, validation loss: 0.7363
2024-06-03 10:30:48 [INFO]: Epoch 005 - training loss: 260870.6896, validation loss: 0.6511
2024-06-03 10:31:09 [INFO]: Epoch 006 - training loss: 260029.1187, validation loss: 0.6293
2024-06-03 10:31:28 [INFO]: Epoch 007 - training loss: 259702.1073, validation loss: 0.6114
2024-06-03 10:31:48 [INFO]: Epoch 008 - training loss: 259561.4375, validation loss: 0.5715
2024-06-03 10:32:08 [INFO]: Epoch 009 - training loss: 259428.4229, validation loss: 0.5701
2024-06-03 10:32:28 [INFO]: Epoch 010 - training loss: 259330.6156, validation loss: 0.5600
2024-06-03 10:32:47 [INFO]: Epoch 011 - training loss: 259282.6615, validation loss: 0.5575
2024-06-03 10:33:08 [INFO]: Epoch 012 - training loss: 259252.0281, validation loss: 0.5892
2024-06-03 10:33:27 [INFO]: Epoch 013 - training loss: 259259.7073, validation loss: 0.5473
2024-06-03 10:33:46 [INFO]: Epoch 014 - training loss: 259179.9865, validation loss: 0.5465
2024-06-03 10:34:05 [INFO]: Epoch 015 - training loss: 259163.0333, validation loss: 0.5601
2024-06-03 10:34:25 [INFO]: Epoch 016 - training loss: 259133.1896, validation loss: 0.5715
2024-06-03 10:34:43 [INFO]: Epoch 017 - training loss: 259108.0635, validation loss: 0.5365
2024-06-03 10:35:02 [INFO]: Epoch 018 - training loss: 259106.7354, validation loss: 0.5399
2024-06-03 10:35:21 [INFO]: Epoch 019 - training loss: 259096.6854, validation loss: 0.5774
2024-06-03 10:35:41 [INFO]: Epoch 020 - training loss: 259118.4292, validation loss: 0.5498
2024-06-03 10:36:01 [INFO]: Epoch 021 - training loss: 259084.1646, validation loss: 0.5764
2024-06-03 10:36:20 [INFO]: Epoch 022 - training loss: 259104.2427, validation loss: 0.5288
2024-06-03 10:36:38 [INFO]: Epoch 023 - training loss: 259078.6740, validation loss: 0.5262
2024-06-03 10:36:56 [INFO]: Epoch 024 - training loss: 259086.3917, validation loss: 0.5307
2024-06-03 10:37:15 [INFO]: Epoch 025 - training loss: 259078.6177, validation loss: 0.5396
2024-06-03 10:37:33 [INFO]: Epoch 026 - training loss: 259110.9500, validation loss: 0.5263
2024-06-03 10:37:51 [INFO]: Epoch 027 - training loss: 259124.6865, validation loss: 0.5369
2024-06-03 10:38:09 [INFO]: Epoch 028 - training loss: 259087.7750, validation loss: 0.5471
2024-06-03 10:38:28 [INFO]: Epoch 029 - training loss: 259130.6927, validation loss: 0.5288
2024-06-03 10:38:46 [INFO]: Epoch 030 - training loss: 259097.3521, validation loss: 0.5277
2024-06-03 10:39:04 [INFO]: Epoch 031 - training loss: 259128.6271, validation loss: 0.5424
2024-06-03 10:39:23 [INFO]: Epoch 032 - training loss: 259165.7760, validation loss: 0.5318
2024-06-03 10:39:41 [INFO]: Epoch 033 - training loss: 259151.9271, validation loss: 0.5333
2024-06-03 10:39:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:39:41 [INFO]: Finished training. The best model is from epoch#23.
2024-06-03 10:39:41 [INFO]: Saved the model to results_block_rate05/PeMS/GPVAE_PeMS/round_1/20240603_T102909/GPVAE.pypots
2024-06-03 10:42:01 [INFO]: Successfully saved to results_block_rate05/PeMS/GPVAE_PeMS/round_1/imputation.pkl
2024-06-03 10:42:01 [INFO]: Round1 - GPVAE on PeMS: MAE=0.3954, MSE=0.7511, MRE=0.4735
2024-06-03 10:42:01 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:42:01 [INFO]: Using the given device: cuda:0
2024-06-03 10:42:01 [INFO]: Model files will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_2/20240603_T104201
2024-06-03 10:42:01 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_2/20240603_T104201/tensorboard
2024-06-03 10:42:02 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 2,396,536
2024-06-03 10:42:20 [INFO]: Epoch 001 - training loss: 633964.6750, validation loss: 1.2838
2024-06-03 10:42:38 [INFO]: Epoch 002 - training loss: 375077.5396, validation loss: 1.0578
2024-06-03 10:42:55 [INFO]: Epoch 003 - training loss: 276672.0292, validation loss: 0.8422
2024-06-03 10:43:12 [INFO]: Epoch 004 - training loss: 263958.7396, validation loss: 0.7040
2024-06-03 10:43:28 [INFO]: Epoch 005 - training loss: 261336.7406, validation loss: 0.6183
2024-06-03 10:43:46 [INFO]: Epoch 006 - training loss: 260074.8937, validation loss: 0.6009
2024-06-03 10:44:03 [INFO]: Epoch 007 - training loss: 259450.3156, validation loss: 0.5798
2024-06-03 10:44:20 [INFO]: Epoch 008 - training loss: 259250.5406, validation loss: 0.5679
2024-06-03 10:44:37 [INFO]: Epoch 009 - training loss: 259098.9406, validation loss: 0.5645
2024-06-03 10:44:53 [INFO]: Epoch 010 - training loss: 259025.9479, validation loss: 0.5919
2024-06-03 10:45:11 [INFO]: Epoch 011 - training loss: 259068.8677, validation loss: 0.5680
2024-06-03 10:45:28 [INFO]: Epoch 012 - training loss: 258932.2333, validation loss: 0.5536
2024-06-03 10:45:45 [INFO]: Epoch 013 - training loss: 258888.9854, validation loss: 0.5418
2024-06-03 10:46:02 [INFO]: Epoch 014 - training loss: 258877.8719, validation loss: 0.5461
2024-06-03 10:46:19 [INFO]: Epoch 015 - training loss: 258833.6323, validation loss: 0.5448
2024-06-03 10:46:36 [INFO]: Epoch 016 - training loss: 258824.3708, validation loss: 0.5375
2024-06-03 10:46:53 [INFO]: Epoch 017 - training loss: 258776.9917, validation loss: 0.5350
2024-06-03 10:47:10 [INFO]: Epoch 018 - training loss: 258789.9729, validation loss: 0.5341
2024-06-03 10:47:27 [INFO]: Epoch 019 - training loss: 258803.9052, validation loss: 0.5317
2024-06-03 10:47:43 [INFO]: Epoch 020 - training loss: 258803.8708, validation loss: 0.5358
2024-06-03 10:47:59 [INFO]: Epoch 021 - training loss: 258809.0187, validation loss: 0.5325
2024-06-03 10:48:14 [INFO]: Epoch 022 - training loss: 258829.1208, validation loss: 0.5190
2024-06-03 10:48:30 [INFO]: Epoch 023 - training loss: 258899.5229, validation loss: 0.5424
2024-06-03 10:48:46 [INFO]: Epoch 024 - training loss: 258860.3542, validation loss: 0.5384
2024-06-03 10:49:02 [INFO]: Epoch 025 - training loss: 258822.1573, validation loss: 0.5258
2024-06-03 10:49:17 [INFO]: Epoch 026 - training loss: 258935.4854, validation loss: 0.5211
2024-06-03 10:49:33 [INFO]: Epoch 027 - training loss: 258856.5583, validation loss: 0.5314
2024-06-03 10:49:48 [INFO]: Epoch 028 - training loss: 258870.3740, validation loss: 0.5161
2024-06-03 10:50:04 [INFO]: Epoch 029 - training loss: 258884.7927, validation loss: 0.5297
2024-06-03 10:50:20 [INFO]: Epoch 030 - training loss: 258876.9625, validation loss: 0.5170
2024-06-03 10:50:36 [INFO]: Epoch 031 - training loss: 259011.6844, validation loss: 0.5159
2024-06-03 10:50:53 [INFO]: Epoch 032 - training loss: 258927.8260, validation loss: 0.5196
2024-06-03 10:51:08 [INFO]: Epoch 033 - training loss: 258854.2458, validation loss: 0.5288
2024-06-03 10:51:23 [INFO]: Epoch 034 - training loss: 258840.2344, validation loss: 0.5204
2024-06-03 10:51:38 [INFO]: Epoch 035 - training loss: 258826.3302, validation loss: 0.5356
2024-06-03 10:51:54 [INFO]: Epoch 036 - training loss: 258803.3656, validation loss: 0.5157
2024-06-03 10:52:10 [INFO]: Epoch 037 - training loss: 258774.5521, validation loss: 0.5275
2024-06-03 10:52:25 [INFO]: Epoch 038 - training loss: 258737.3156, validation loss: 0.5131
2024-06-03 10:52:42 [INFO]: Epoch 039 - training loss: 258696.3583, validation loss: 0.5133
2024-06-03 10:52:58 [INFO]: Epoch 040 - training loss: 258711.1917, validation loss: 0.5197
2024-06-03 10:53:13 [INFO]: Epoch 041 - training loss: 258764.8458, validation loss: 0.5192
2024-06-03 10:53:29 [INFO]: Epoch 042 - training loss: 258899.2271, validation loss: 0.5323
2024-06-03 10:53:43 [INFO]: Epoch 043 - training loss: 258844.1927, validation loss: 0.5226
2024-06-03 10:53:59 [INFO]: Epoch 044 - training loss: 258765.0969, validation loss: 0.5268
2024-06-03 10:54:15 [INFO]: Epoch 045 - training loss: 258707.1823, validation loss: 0.5331
2024-06-03 10:54:30 [INFO]: Epoch 046 - training loss: 258683.3469, validation loss: 0.5122
2024-06-03 10:54:45 [INFO]: Epoch 047 - training loss: 258644.6271, validation loss: 0.5098
2024-06-03 10:55:01 [INFO]: Epoch 048 - training loss: 258633.3948, validation loss: 0.5159
2024-06-03 10:55:16 [INFO]: Epoch 049 - training loss: 258647.8229, validation loss: 0.5139
2024-06-03 10:55:31 [INFO]: Epoch 050 - training loss: 258644.6271, validation loss: 0.5126
2024-06-03 10:55:46 [INFO]: Epoch 051 - training loss: 258601.9521, validation loss: 0.5165
2024-06-03 10:56:01 [INFO]: Epoch 052 - training loss: 258570.0302, validation loss: 0.5097
2024-06-03 10:56:16 [INFO]: Epoch 053 - training loss: 258596.0771, validation loss: 0.5102
2024-06-03 10:56:32 [INFO]: Epoch 054 - training loss: 258591.4542, validation loss: 0.5322
2024-06-03 10:56:46 [INFO]: Epoch 055 - training loss: 258606.9927, validation loss: 0.5385
2024-06-03 10:57:01 [INFO]: Epoch 056 - training loss: 258584.1406, validation loss: 0.5079
2024-06-03 10:57:16 [INFO]: Epoch 057 - training loss: 258568.8344, validation loss: 0.5029
2024-06-03 10:57:31 [INFO]: Epoch 058 - training loss: 258525.8667, validation loss: 0.5056
2024-06-03 10:57:45 [INFO]: Epoch 059 - training loss: 258541.6458, validation loss: 0.5057
2024-06-03 10:58:01 [INFO]: Epoch 060 - training loss: 258542.8813, validation loss: 0.5087
2024-06-03 10:58:17 [INFO]: Epoch 061 - training loss: 258560.5375, validation loss: 0.5062
2024-06-03 10:58:32 [INFO]: Epoch 062 - training loss: 258529.0521, validation loss: 0.5070
2024-06-03 10:58:48 [INFO]: Epoch 063 - training loss: 258533.9229, validation loss: 0.5066
2024-06-03 10:59:03 [INFO]: Epoch 064 - training loss: 258528.6396, validation loss: 0.5088
2024-06-03 10:59:17 [INFO]: Epoch 065 - training loss: 258570.0167, validation loss: 0.4993
2024-06-03 10:59:32 [INFO]: Epoch 066 - training loss: 258625.4167, validation loss: 0.5003
2024-06-03 10:59:47 [INFO]: Epoch 067 - training loss: 258634.4583, validation loss: 0.5215
2024-06-03 11:00:01 [INFO]: Epoch 068 - training loss: 258608.0417, validation loss: 0.5016
2024-06-03 11:00:14 [INFO]: Epoch 069 - training loss: 258559.5354, validation loss: 0.4985
2024-06-03 11:00:27 [INFO]: Epoch 070 - training loss: 258574.5479, validation loss: 0.5120
2024-06-03 11:00:40 [INFO]: Epoch 071 - training loss: 258541.3885, validation loss: 0.5257
2024-06-03 11:00:52 [INFO]: Epoch 072 - training loss: 258571.4969, validation loss: 0.5209
2024-06-03 11:01:05 [INFO]: Epoch 073 - training loss: 258526.7177, validation loss: 0.5010
2024-06-03 11:01:18 [INFO]: Epoch 074 - training loss: 258499.2146, validation loss: 0.5032
2024-06-03 11:01:30 [INFO]: Epoch 075 - training loss: 258511.0281, validation loss: 0.5081
2024-06-03 11:01:43 [INFO]: Epoch 076 - training loss: 258497.7271, validation loss: 0.5040
2024-06-03 11:01:55 [INFO]: Epoch 077 - training loss: 258513.8500, validation loss: 0.5138
2024-06-03 11:02:07 [INFO]: Epoch 078 - training loss: 258508.6031, validation loss: 0.5058
2024-06-03 11:02:20 [INFO]: Epoch 079 - training loss: 258530.7323, validation loss: 0.4985
2024-06-03 11:02:33 [INFO]: Epoch 080 - training loss: 258546.5510, validation loss: 0.5006
2024-06-03 11:02:45 [INFO]: Epoch 081 - training loss: 258539.7781, validation loss: 0.5010
2024-06-03 11:02:58 [INFO]: Epoch 082 - training loss: 258533.3667, validation loss: 0.5060
2024-06-03 11:03:10 [INFO]: Epoch 083 - training loss: 258520.1354, validation loss: 0.5131
2024-06-03 11:03:21 [INFO]: Epoch 084 - training loss: 258553.1344, validation loss: 0.5023
2024-06-03 11:03:34 [INFO]: Epoch 085 - training loss: 258563.0844, validation loss: 0.4997
2024-06-03 11:03:45 [INFO]: Epoch 086 - training loss: 258591.8865, validation loss: 0.5016
2024-06-03 11:03:57 [INFO]: Epoch 087 - training loss: 258566.3615, validation loss: 0.5027
2024-06-03 11:04:10 [INFO]: Epoch 088 - training loss: 258553.9688, validation loss: 0.5009
2024-06-03 11:04:21 [INFO]: Epoch 089 - training loss: 258583.0646, validation loss: 0.4977
2024-06-03 11:04:33 [INFO]: Epoch 090 - training loss: 258576.3448, validation loss: 0.4984
2024-06-03 11:04:45 [INFO]: Epoch 091 - training loss: 258548.2615, validation loss: 0.4993
2024-06-03 11:04:57 [INFO]: Epoch 092 - training loss: 258524.3646, validation loss: 0.5050
2024-06-03 11:05:09 [INFO]: Epoch 093 - training loss: 258546.9104, validation loss: 0.4965
2024-06-03 11:05:21 [INFO]: Epoch 094 - training loss: 258549.3271, validation loss: 0.4985
2024-06-03 11:05:33 [INFO]: Epoch 095 - training loss: 258562.1187, validation loss: 0.5346
2024-06-03 11:05:45 [INFO]: Epoch 096 - training loss: 258551.1375, validation loss: 0.5002
2024-06-03 11:05:56 [INFO]: Epoch 097 - training loss: 258570.7615, validation loss: 0.5003
2024-06-03 11:06:07 [INFO]: Epoch 098 - training loss: 258540.0594, validation loss: 0.4958
2024-06-03 11:06:19 [INFO]: Epoch 099 - training loss: 258487.9417, validation loss: 0.4958
2024-06-03 11:06:31 [INFO]: Epoch 100 - training loss: 258531.4708, validation loss: 0.4994
2024-06-03 11:06:31 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 11:06:31 [INFO]: Saved the model to results_block_rate05/PeMS/GPVAE_PeMS/round_2/20240603_T104201/GPVAE.pypots
2024-06-03 11:08:03 [INFO]: Successfully saved to results_block_rate05/PeMS/GPVAE_PeMS/round_2/imputation.pkl
2024-06-03 11:08:03 [INFO]: Round2 - GPVAE on PeMS: MAE=0.3617, MSE=0.7275, MRE=0.4331
2024-06-03 11:08:03 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 11:08:03 [INFO]: Using the given device: cuda:0
2024-06-03 11:08:03 [INFO]: Model files will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_3/20240603_T110803
2024-06-03 11:08:03 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_3/20240603_T110803/tensorboard
2024-06-03 11:08:03 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 2,396,536
2024-06-03 11:08:16 [INFO]: Epoch 001 - training loss: 637626.2333, validation loss: 1.2970
2024-06-03 11:08:27 [INFO]: Epoch 002 - training loss: 377946.2625, validation loss: 1.1111
2024-06-03 11:08:40 [INFO]: Epoch 003 - training loss: 276495.5312, validation loss: 0.8538
2024-06-03 11:08:52 [INFO]: Epoch 004 - training loss: 263066.2229, validation loss: 0.7323
2024-06-03 11:09:03 [INFO]: Epoch 005 - training loss: 260244.0198, validation loss: 0.6255
2024-06-03 11:09:15 [INFO]: Epoch 006 - training loss: 259588.0708, validation loss: 0.5883
2024-06-03 11:09:26 [INFO]: Epoch 007 - training loss: 259297.5469, validation loss: 0.6268
2024-06-03 11:09:38 [INFO]: Epoch 008 - training loss: 259165.3396, validation loss: 0.5679
2024-06-03 11:09:49 [INFO]: Epoch 009 - training loss: 259040.9094, validation loss: 0.5613
2024-06-03 11:10:00 [INFO]: Epoch 010 - training loss: 258995.8750, validation loss: 0.5553
2024-06-03 11:10:11 [INFO]: Epoch 011 - training loss: 258978.2229, validation loss: 0.5441
2024-06-03 11:10:22 [INFO]: Epoch 012 - training loss: 258895.6094, validation loss: 0.5544
2024-06-03 11:10:34 [INFO]: Epoch 013 - training loss: 258868.9937, validation loss: 0.5459
2024-06-03 11:10:45 [INFO]: Epoch 014 - training loss: 258862.6458, validation loss: 0.5655
2024-06-03 11:10:56 [INFO]: Epoch 015 - training loss: 258842.2958, validation loss: 0.5441
2024-06-03 11:11:06 [INFO]: Epoch 016 - training loss: 258816.4177, validation loss: 0.5329
2024-06-03 11:11:17 [INFO]: Epoch 017 - training loss: 258844.0323, validation loss: 0.5713
2024-06-03 11:11:29 [INFO]: Epoch 018 - training loss: 258865.0021, validation loss: 0.5310
2024-06-03 11:11:40 [INFO]: Epoch 019 - training loss: 258794.8052, validation loss: 0.5353
2024-06-03 11:11:52 [INFO]: Epoch 020 - training loss: 258780.0844, validation loss: 0.5376
2024-06-03 11:12:03 [INFO]: Epoch 021 - training loss: 258797.8010, validation loss: 0.5306
2024-06-03 11:12:15 [INFO]: Epoch 022 - training loss: 258806.0094, validation loss: 0.5310
2024-06-03 11:12:26 [INFO]: Epoch 023 - training loss: 258821.5438, validation loss: 0.5265
2024-06-03 11:12:37 [INFO]: Epoch 024 - training loss: 258789.7344, validation loss: 0.5279
2024-06-03 11:12:48 [INFO]: Epoch 025 - training loss: 258821.2323, validation loss: 0.5290
2024-06-03 11:13:00 [INFO]: Epoch 026 - training loss: 258869.2698, validation loss: 0.5349
2024-06-03 11:13:11 [INFO]: Epoch 027 - training loss: 258883.5708, validation loss: 0.5264
2024-06-03 11:13:22 [INFO]: Epoch 028 - training loss: 258907.7240, validation loss: 0.5277
2024-06-03 11:13:33 [INFO]: Epoch 029 - training loss: 259078.3312, validation loss: 0.5291
2024-06-03 11:13:44 [INFO]: Epoch 030 - training loss: 258970.2542, validation loss: 0.5264
2024-06-03 11:13:55 [INFO]: Epoch 031 - training loss: 258961.6625, validation loss: 0.5229
2024-06-03 11:14:07 [INFO]: Epoch 032 - training loss: 258848.0208, validation loss: 0.5211
2024-06-03 11:14:18 [INFO]: Epoch 033 - training loss: 258807.8844, validation loss: 0.5210
2024-06-03 11:14:29 [INFO]: Epoch 034 - training loss: 258771.9615, validation loss: 0.5160
2024-06-03 11:14:41 [INFO]: Epoch 035 - training loss: 258770.6542, validation loss: 0.5223
2024-06-03 11:14:52 [INFO]: Epoch 036 - training loss: 258718.5042, validation loss: 0.5222
2024-06-03 11:15:03 [INFO]: Epoch 037 - training loss: 258667.2115, validation loss: 0.5150
2024-06-03 11:15:14 [INFO]: Epoch 038 - training loss: 258686.9302, validation loss: 0.5151
2024-06-03 11:15:26 [INFO]: Epoch 039 - training loss: 258653.1688, validation loss: 0.5143
2024-06-03 11:15:37 [INFO]: Epoch 040 - training loss: 258651.5135, validation loss: 0.5323
2024-06-03 11:15:48 [INFO]: Epoch 041 - training loss: 258669.5625, validation loss: 0.5224
2024-06-03 11:15:59 [INFO]: Epoch 042 - training loss: 258648.7604, validation loss: 0.5229
2024-06-03 11:16:11 [INFO]: Epoch 043 - training loss: 258645.1896, validation loss: 0.5162
2024-06-03 11:16:22 [INFO]: Epoch 044 - training loss: 258634.8406, validation loss: 0.5163
2024-06-03 11:16:33 [INFO]: Epoch 045 - training loss: 258638.2062, validation loss: 0.5075
2024-06-03 11:16:44 [INFO]: Epoch 046 - training loss: 258665.3531, validation loss: 0.5424
2024-06-03 11:16:55 [INFO]: Epoch 047 - training loss: 258727.4760, validation loss: 0.5177
2024-06-03 11:17:06 [INFO]: Epoch 048 - training loss: 258682.3698, validation loss: 0.5149
2024-06-03 11:17:18 [INFO]: Epoch 049 - training loss: 258672.9448, validation loss: 0.5230
2024-06-03 11:17:29 [INFO]: Epoch 050 - training loss: 258701.0708, validation loss: 0.5077
2024-06-03 11:17:40 [INFO]: Epoch 051 - training loss: 258711.1292, validation loss: 0.5076
2024-06-03 11:17:52 [INFO]: Epoch 052 - training loss: 258688.7667, validation loss: 0.5107
2024-06-03 11:18:03 [INFO]: Epoch 053 - training loss: 258698.5635, validation loss: 0.5170
2024-06-03 11:18:14 [INFO]: Epoch 054 - training loss: 258720.5854, validation loss: 0.5055
2024-06-03 11:18:26 [INFO]: Epoch 055 - training loss: 258693.5354, validation loss: 0.5075
2024-06-03 11:18:37 [INFO]: Epoch 056 - training loss: 258684.0625, validation loss: 0.5091
2024-06-03 11:18:48 [INFO]: Epoch 057 - training loss: 258773.6552, validation loss: 0.5127
2024-06-03 11:18:59 [INFO]: Epoch 058 - training loss: 258795.2802, validation loss: 0.5160
2024-06-03 11:19:10 [INFO]: Epoch 059 - training loss: 258748.9771, validation loss: 0.5102
2024-06-03 11:19:22 [INFO]: Epoch 060 - training loss: 258739.4073, validation loss: 0.5070
2024-06-03 11:19:33 [INFO]: Epoch 061 - training loss: 258704.7219, validation loss: 0.5045
2024-06-03 11:19:44 [INFO]: Epoch 062 - training loss: 258673.1990, validation loss: 0.5052
2024-06-03 11:19:56 [INFO]: Epoch 063 - training loss: 258653.8687, validation loss: 0.5062
2024-06-03 11:20:07 [INFO]: Epoch 064 - training loss: 258620.4823, validation loss: 0.5195
2024-06-03 11:20:17 [INFO]: Epoch 065 - training loss: 258599.7667, validation loss: 0.5019
2024-06-03 11:20:29 [INFO]: Epoch 066 - training loss: 258598.4167, validation loss: 0.4999
2024-06-03 11:20:40 [INFO]: Epoch 067 - training loss: 258607.0698, validation loss: 0.5130
2024-06-03 11:20:52 [INFO]: Epoch 068 - training loss: 258585.4365, validation loss: 0.5072
2024-06-03 11:21:03 [INFO]: Epoch 069 - training loss: 258558.8260, validation loss: 0.5216
2024-06-03 11:21:14 [INFO]: Epoch 070 - training loss: 258627.9031, validation loss: 0.5024
2024-06-03 11:21:25 [INFO]: Epoch 071 - training loss: 258595.2229, validation loss: 0.5089
2024-06-03 11:21:36 [INFO]: Epoch 072 - training loss: 258567.0333, validation loss: 0.5041
2024-06-03 11:21:47 [INFO]: Epoch 073 - training loss: 258558.6083, validation loss: 0.5023
2024-06-03 11:21:58 [INFO]: Epoch 074 - training loss: 258518.5333, validation loss: 0.5003
2024-06-03 11:22:09 [INFO]: Epoch 075 - training loss: 258508.8813, validation loss: 0.5098
2024-06-03 11:22:21 [INFO]: Epoch 076 - training loss: 258518.5917, validation loss: 0.5048
2024-06-03 11:22:21 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:22:21 [INFO]: Finished training. The best model is from epoch#66.
2024-06-03 11:22:21 [INFO]: Saved the model to results_block_rate05/PeMS/GPVAE_PeMS/round_3/20240603_T110803/GPVAE.pypots
2024-06-03 11:23:41 [INFO]: Successfully saved to results_block_rate05/PeMS/GPVAE_PeMS/round_3/imputation.pkl
2024-06-03 11:23:41 [INFO]: Round3 - GPVAE on PeMS: MAE=0.3631, MSE=0.7319, MRE=0.4347
2024-06-03 11:23:41 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 11:23:41 [INFO]: Using the given device: cuda:0
2024-06-03 11:23:41 [INFO]: Model files will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_4/20240603_T112341
2024-06-03 11:23:41 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/GPVAE_PeMS/round_4/20240603_T112341/tensorboard
2024-06-03 11:23:41 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 2,396,536
2024-06-03 11:23:51 [INFO]: Epoch 001 - training loss: 642596.4146, validation loss: 1.3326
2024-06-03 11:24:01 [INFO]: Epoch 002 - training loss: 385578.4771, validation loss: 1.1231
2024-06-03 11:24:12 [INFO]: Epoch 003 - training loss: 280288.5896, validation loss: 0.9087
2024-06-03 11:24:23 [INFO]: Epoch 004 - training loss: 263477.4594, validation loss: 0.7627
2024-06-03 11:24:33 [INFO]: Epoch 005 - training loss: 260590.5667, validation loss: 0.6238
2024-06-03 11:24:43 [INFO]: Epoch 006 - training loss: 259734.2729, validation loss: 0.6132
2024-06-03 11:24:54 [INFO]: Epoch 007 - training loss: 259425.0344, validation loss: 0.5927
2024-06-03 11:25:04 [INFO]: Epoch 008 - training loss: 259216.4250, validation loss: 0.5744
2024-06-03 11:25:15 [INFO]: Epoch 009 - training loss: 259101.1792, validation loss: 0.5539
2024-06-03 11:25:25 [INFO]: Epoch 010 - training loss: 259023.5021, validation loss: 0.5591
2024-06-03 11:25:36 [INFO]: Epoch 011 - training loss: 258976.6979, validation loss: 0.5576
2024-06-03 11:25:47 [INFO]: Epoch 012 - training loss: 258903.8646, validation loss: 0.5413
2024-06-03 11:25:57 [INFO]: Epoch 013 - training loss: 258844.9396, validation loss: 0.5488
2024-06-03 11:26:08 [INFO]: Epoch 014 - training loss: 258854.9823, validation loss: 0.5416
2024-06-03 11:26:18 [INFO]: Epoch 015 - training loss: 258843.4688, validation loss: 0.5546
2024-06-03 11:26:28 [INFO]: Epoch 016 - training loss: 258810.9823, validation loss: 0.5488
2024-06-03 11:26:39 [INFO]: Epoch 017 - training loss: 258786.8948, validation loss: 0.5440
2024-06-03 11:26:49 [INFO]: Epoch 018 - training loss: 258777.9615, validation loss: 0.5338
2024-06-03 11:26:59 [INFO]: Epoch 019 - training loss: 258743.5385, validation loss: 0.5269
2024-06-03 11:27:10 [INFO]: Epoch 020 - training loss: 258747.1990, validation loss: 0.5286
2024-06-03 11:27:21 [INFO]: Epoch 021 - training loss: 258756.5552, validation loss: 0.5554
2024-06-03 11:27:31 [INFO]: Epoch 022 - training loss: 258765.1583, validation loss: 0.5289
2024-06-03 11:27:41 [INFO]: Epoch 023 - training loss: 258770.9406, validation loss: 0.5372
2024-06-03 11:27:52 [INFO]: Epoch 024 - training loss: 258762.0979, validation loss: 0.5200
2024-06-03 11:28:02 [INFO]: Epoch 025 - training loss: 258744.3802, validation loss: 0.5269
2024-06-03 11:28:13 [INFO]: Epoch 026 - training loss: 258795.4646, validation loss: 0.5412
2024-06-03 11:28:24 [INFO]: Epoch 027 - training loss: 258816.9438, validation loss: 0.5236
2024-06-03 11:28:35 [INFO]: Epoch 028 - training loss: 258840.7458, validation loss: 0.5376
2024-06-03 11:28:45 [INFO]: Epoch 029 - training loss: 258825.7865, validation loss: 0.5511
2024-06-03 11:28:56 [INFO]: Epoch 030 - training loss: 258790.4365, validation loss: 0.5285
2024-06-03 11:29:06 [INFO]: Epoch 031 - training loss: 258773.0187, validation loss: 0.5265
2024-06-03 11:29:15 [INFO]: Epoch 032 - training loss: 258781.2417, validation loss: 0.5183
2024-06-03 11:29:25 [INFO]: Epoch 033 - training loss: 258787.0854, validation loss: 0.5258
2024-06-03 11:29:34 [INFO]: Epoch 034 - training loss: 258783.7031, validation loss: 0.5226
2024-06-03 11:29:44 [INFO]: Epoch 035 - training loss: 258823.9427, validation loss: 0.5190
2024-06-03 11:29:54 [INFO]: Epoch 036 - training loss: 258805.6208, validation loss: 0.5124
2024-06-03 11:30:04 [INFO]: Epoch 037 - training loss: 258786.5781, validation loss: 0.5110
2024-06-03 11:30:13 [INFO]: Epoch 038 - training loss: 258805.7906, validation loss: 0.5292
2024-06-03 11:30:22 [INFO]: Epoch 039 - training loss: 258813.0802, validation loss: 0.5164
2024-06-03 11:30:31 [INFO]: Epoch 040 - training loss: 258890.7573, validation loss: 0.5133
2024-06-03 11:30:40 [INFO]: Epoch 041 - training loss: 258844.4937, validation loss: 0.5423
2024-06-03 11:30:48 [INFO]: Epoch 042 - training loss: 258741.2729, validation loss: 0.5132
2024-06-03 11:30:57 [INFO]: Epoch 043 - training loss: 258726.6427, validation loss: 0.5215
2024-06-03 11:31:06 [INFO]: Epoch 044 - training loss: 258661.6104, validation loss: 0.5100
2024-06-03 11:31:15 [INFO]: Epoch 045 - training loss: 258655.9365, validation loss: 0.5156
2024-06-03 11:31:23 [INFO]: Epoch 046 - training loss: 258642.3937, validation loss: 0.5177
2024-06-03 11:31:31 [INFO]: Epoch 047 - training loss: 258655.0729, validation loss: 0.5165
2024-06-03 11:31:40 [INFO]: Epoch 048 - training loss: 258626.5594, validation loss: 0.5240
2024-06-03 11:31:49 [INFO]: Epoch 049 - training loss: 258606.5135, validation loss: 0.5152
2024-06-03 11:31:58 [INFO]: Epoch 050 - training loss: 258616.4094, validation loss: 0.5076
2024-06-03 11:32:07 [INFO]: Epoch 051 - training loss: 258599.9823, validation loss: 0.5095
2024-06-03 11:32:15 [INFO]: Epoch 052 - training loss: 258634.4365, validation loss: 0.5071
2024-06-03 11:32:24 [INFO]: Epoch 053 - training loss: 258654.5396, validation loss: 0.5042
2024-06-03 11:32:32 [INFO]: Epoch 054 - training loss: 258594.6260, validation loss: 0.5132
2024-06-03 11:32:40 [INFO]: Epoch 055 - training loss: 258584.6583, validation loss: 0.5111
2024-06-03 11:32:49 [INFO]: Epoch 056 - training loss: 258599.4354, validation loss: 0.5088
2024-06-03 11:32:58 [INFO]: Epoch 057 - training loss: 258567.3302, validation loss: 0.5020
2024-06-03 11:33:06 [INFO]: Epoch 058 - training loss: 258584.2615, validation loss: 0.5164
2024-06-03 11:33:14 [INFO]: Epoch 059 - training loss: 258569.4750, validation loss: 0.5056
2024-06-03 11:33:22 [INFO]: Epoch 060 - training loss: 258588.5000, validation loss: 0.5035
2024-06-03 11:33:31 [INFO]: Epoch 061 - training loss: 258584.0490, validation loss: 0.5089
2024-06-03 11:33:39 [INFO]: Epoch 062 - training loss: 258565.3958, validation loss: 0.5176
2024-06-03 11:33:47 [INFO]: Epoch 063 - training loss: 258571.3542, validation loss: 0.5144
2024-06-03 11:33:55 [INFO]: Epoch 064 - training loss: 258552.0708, validation loss: 0.4997
2024-06-03 11:34:04 [INFO]: Epoch 065 - training loss: 258546.2521, validation loss: 0.5202
2024-06-03 11:34:12 [INFO]: Epoch 066 - training loss: 258654.2792, validation loss: 0.5094
2024-06-03 11:34:20 [INFO]: Epoch 067 - training loss: 258597.2687, validation loss: 0.5098
2024-06-03 11:34:27 [INFO]: Epoch 068 - training loss: 258556.5333, validation loss: 0.5015
2024-06-03 11:34:34 [INFO]: Epoch 069 - training loss: 258550.9281, validation loss: 0.4996
2024-06-03 11:34:42 [INFO]: Epoch 070 - training loss: 258537.1823, validation loss: 0.4995
2024-06-03 11:34:50 [INFO]: Epoch 071 - training loss: 258529.7896, validation loss: 0.5047
2024-06-03 11:34:57 [INFO]: Epoch 072 - training loss: 258529.4781, validation loss: 0.5068
2024-06-03 11:35:04 [INFO]: Epoch 073 - training loss: 258537.9375, validation loss: 0.4998
2024-06-03 11:35:12 [INFO]: Epoch 074 - training loss: 258599.8229, validation loss: 0.4981
2024-06-03 11:35:19 [INFO]: Epoch 075 - training loss: 258591.4969, validation loss: 0.5008
2024-06-03 11:35:26 [INFO]: Epoch 076 - training loss: 258556.3792, validation loss: 0.4998
2024-06-03 11:35:34 [INFO]: Epoch 077 - training loss: 258541.1302, validation loss: 0.5028
2024-06-03 11:35:41 [INFO]: Epoch 078 - training loss: 258547.0344, validation loss: 0.5153
2024-06-03 11:35:48 [INFO]: Epoch 079 - training loss: 258542.9354, validation loss: 0.4989
2024-06-03 11:35:56 [INFO]: Epoch 080 - training loss: 258514.4500, validation loss: 0.5032
2024-06-03 11:36:03 [INFO]: Epoch 081 - training loss: 258507.1042, validation loss: 0.5094
2024-06-03 11:36:11 [INFO]: Epoch 082 - training loss: 258520.7958, validation loss: 0.5056
2024-06-03 11:36:18 [INFO]: Epoch 083 - training loss: 258503.5833, validation loss: 0.5086
2024-06-03 11:36:26 [INFO]: Epoch 084 - training loss: 258493.2896, validation loss: 0.4988
2024-06-03 11:36:26 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:36:26 [INFO]: Finished training. The best model is from epoch#74.
2024-06-03 11:36:26 [INFO]: Saved the model to results_block_rate05/PeMS/GPVAE_PeMS/round_4/20240603_T112341/GPVAE.pypots
2024-06-03 11:37:21 [INFO]: Successfully saved to results_block_rate05/PeMS/GPVAE_PeMS/round_4/imputation.pkl
2024-06-03 11:37:21 [INFO]: Round4 - GPVAE on PeMS: MAE=0.3620, MSE=0.7228, MRE=0.4334
2024-06-03 11:37:21 [INFO]: Done! Final results:
Averaged GPVAE (2,396,536 params) on PeMS: MAE=0.3716 ± 0.013016107592956936, MSE=0.7379 ± 0.013238084590990587, MRE=0.4449 ± 0.015584993490514374, average inference time=21.80
