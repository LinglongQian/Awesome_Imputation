2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:54 [INFO]: Model files will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_0/20240603_T100954
2024-06-03 10:09:54 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_0/20240603_T100954/tensorboard
2024-06-03 10:09:54 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 10:09:54 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 10:09:58 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 10:10:17 [INFO]: Epoch 001 - training loss: 1.0870, validation loss: 1.0242
2024-06-03 10:10:28 [INFO]: Epoch 002 - training loss: 0.7232, validation loss: 0.9669
2024-06-03 10:10:39 [INFO]: Epoch 003 - training loss: 0.6141, validation loss: 0.8495
2024-06-03 10:10:53 [INFO]: Epoch 004 - training loss: 0.5345, validation loss: 0.7443
2024-06-03 10:11:07 [INFO]: Epoch 005 - training loss: 0.5037, validation loss: 0.7021
2024-06-03 10:11:21 [INFO]: Epoch 006 - training loss: 0.4882, validation loss: 0.6896
2024-06-03 10:11:34 [INFO]: Epoch 007 - training loss: 0.4691, validation loss: 0.6802
2024-06-03 10:11:47 [INFO]: Epoch 008 - training loss: 0.4597, validation loss: 0.6695
2024-06-03 10:12:00 [INFO]: Epoch 009 - training loss: 0.4494, validation loss: 0.6521
2024-06-03 10:12:12 [INFO]: Epoch 010 - training loss: 0.4353, validation loss: 0.6412
2024-06-03 10:12:26 [INFO]: Epoch 011 - training loss: 0.4229, validation loss: 0.6459
2024-06-03 10:12:39 [INFO]: Epoch 012 - training loss: 0.4165, validation loss: 0.6376
2024-06-03 10:12:51 [INFO]: Epoch 013 - training loss: 0.4118, validation loss: 0.6257
2024-06-03 10:13:04 [INFO]: Epoch 014 - training loss: 0.4037, validation loss: 0.6289
2024-06-03 10:13:18 [INFO]: Epoch 015 - training loss: 0.4035, validation loss: 0.6235
2024-06-03 10:13:31 [INFO]: Epoch 016 - training loss: 0.3935, validation loss: 0.6115
2024-06-03 10:13:43 [INFO]: Epoch 017 - training loss: 0.3882, validation loss: 0.6174
2024-06-03 10:13:56 [INFO]: Epoch 018 - training loss: 0.3822, validation loss: 0.6178
2024-06-03 10:14:09 [INFO]: Epoch 019 - training loss: 0.3785, validation loss: 0.6089
2024-06-03 10:14:22 [INFO]: Epoch 020 - training loss: 0.3733, validation loss: 0.6025
2024-06-03 10:14:35 [INFO]: Epoch 021 - training loss: 0.3676, validation loss: 0.6105
2024-06-03 10:14:48 [INFO]: Epoch 022 - training loss: 0.3656, validation loss: 0.6076
2024-06-03 10:15:01 [INFO]: Epoch 023 - training loss: 0.3590, validation loss: 0.6005
2024-06-03 10:15:14 [INFO]: Epoch 024 - training loss: 0.3527, validation loss: 0.6015
2024-06-03 10:15:26 [INFO]: Epoch 025 - training loss: 0.3528, validation loss: 0.5960
2024-06-03 10:15:38 [INFO]: Epoch 026 - training loss: 0.3486, validation loss: 0.5931
2024-06-03 10:15:50 [INFO]: Epoch 027 - training loss: 0.3525, validation loss: 0.6091
2024-06-03 10:16:03 [INFO]: Epoch 028 - training loss: 0.3502, validation loss: 0.5887
2024-06-03 10:16:15 [INFO]: Epoch 029 - training loss: 0.3415, validation loss: 0.6039
2024-06-03 10:16:29 [INFO]: Epoch 030 - training loss: 0.3375, validation loss: 0.6007
2024-06-03 10:16:42 [INFO]: Epoch 031 - training loss: 0.3378, validation loss: 0.5961
2024-06-03 10:16:55 [INFO]: Epoch 032 - training loss: 0.3342, validation loss: 0.5960
2024-06-03 10:17:07 [INFO]: Epoch 033 - training loss: 0.3295, validation loss: 0.5919
2024-06-03 10:17:19 [INFO]: Epoch 034 - training loss: 0.3304, validation loss: 0.5896
2024-06-03 10:17:32 [INFO]: Epoch 035 - training loss: 0.3253, validation loss: 0.5914
2024-06-03 10:17:45 [INFO]: Epoch 036 - training loss: 0.3200, validation loss: 0.5819
2024-06-03 10:17:59 [INFO]: Epoch 037 - training loss: 0.3194, validation loss: 0.5849
2024-06-03 10:18:12 [INFO]: Epoch 038 - training loss: 0.3183, validation loss: 0.5848
2024-06-03 10:18:25 [INFO]: Epoch 039 - training loss: 0.3168, validation loss: 0.5805
2024-06-03 10:18:37 [INFO]: Epoch 040 - training loss: 0.3167, validation loss: 0.5819
2024-06-03 10:18:49 [INFO]: Epoch 041 - training loss: 0.3132, validation loss: 0.5788
2024-06-03 10:19:01 [INFO]: Epoch 042 - training loss: 0.3117, validation loss: 0.5882
2024-06-03 10:19:15 [INFO]: Epoch 043 - training loss: 0.3073, validation loss: 0.5826
2024-06-03 10:19:28 [INFO]: Epoch 044 - training loss: 0.3067, validation loss: 0.5747
2024-06-03 10:19:41 [INFO]: Epoch 045 - training loss: 0.3073, validation loss: 0.5739
2024-06-03 10:19:54 [INFO]: Epoch 046 - training loss: 0.3020, validation loss: 0.5779
2024-06-03 10:20:07 [INFO]: Epoch 047 - training loss: 0.3013, validation loss: 0.5785
2024-06-03 10:20:20 [INFO]: Epoch 048 - training loss: 0.3025, validation loss: 0.5761
2024-06-03 10:20:32 [INFO]: Epoch 049 - training loss: 0.2986, validation loss: 0.5782
2024-06-03 10:20:46 [INFO]: Epoch 050 - training loss: 0.2982, validation loss: 0.5763
2024-06-03 10:20:59 [INFO]: Epoch 051 - training loss: 0.2957, validation loss: 0.5745
2024-06-03 10:21:13 [INFO]: Epoch 052 - training loss: 0.2980, validation loss: 0.5681
2024-06-03 10:21:26 [INFO]: Epoch 053 - training loss: 0.3024, validation loss: 0.5749
2024-06-03 10:21:39 [INFO]: Epoch 054 - training loss: 0.2946, validation loss: 0.5801
2024-06-03 10:21:51 [INFO]: Epoch 055 - training loss: 0.2915, validation loss: 0.5751
2024-06-03 10:22:03 [INFO]: Epoch 056 - training loss: 0.2905, validation loss: 0.5704
2024-06-03 10:22:16 [INFO]: Epoch 057 - training loss: 0.2870, validation loss: 0.5732
2024-06-03 10:22:29 [INFO]: Epoch 058 - training loss: 0.2881, validation loss: 0.5642
2024-06-03 10:22:42 [INFO]: Epoch 059 - training loss: 0.2837, validation loss: 0.5738
2024-06-03 10:22:56 [INFO]: Epoch 060 - training loss: 0.2859, validation loss: 0.5684
2024-06-03 10:23:09 [INFO]: Epoch 061 - training loss: 0.2851, validation loss: 0.5728
2024-06-03 10:23:22 [INFO]: Epoch 062 - training loss: 0.2836, validation loss: 0.5645
2024-06-03 10:23:34 [INFO]: Epoch 063 - training loss: 0.2825, validation loss: 0.5675
2024-06-03 10:23:46 [INFO]: Epoch 064 - training loss: 0.2805, validation loss: 0.5650
2024-06-03 10:23:59 [INFO]: Epoch 065 - training loss: 0.2796, validation loss: 0.5684
2024-06-03 10:24:12 [INFO]: Epoch 066 - training loss: 0.2789, validation loss: 0.5641
2024-06-03 10:24:26 [INFO]: Epoch 067 - training loss: 0.2788, validation loss: 0.5604
2024-06-03 10:24:38 [INFO]: Epoch 068 - training loss: 0.2811, validation loss: 0.5661
2024-06-03 10:24:51 [INFO]: Epoch 069 - training loss: 0.2797, validation loss: 0.5645
2024-06-03 10:25:04 [INFO]: Epoch 070 - training loss: 0.2750, validation loss: 0.5604
2024-06-03 10:25:15 [INFO]: Epoch 071 - training loss: 0.2811, validation loss: 0.5619
2024-06-03 10:25:26 [INFO]: Epoch 072 - training loss: 0.2813, validation loss: 0.5751
2024-06-03 10:25:38 [INFO]: Epoch 073 - training loss: 0.2814, validation loss: 0.5718
2024-06-03 10:25:51 [INFO]: Epoch 074 - training loss: 0.2757, validation loss: 0.5623
2024-06-03 10:26:04 [INFO]: Epoch 075 - training loss: 0.2756, validation loss: 0.5637
2024-06-03 10:26:17 [INFO]: Epoch 076 - training loss: 0.2708, validation loss: 0.5653
2024-06-03 10:26:29 [INFO]: Epoch 077 - training loss: 0.2712, validation loss: 0.5606
2024-06-03 10:26:29 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:26:29 [INFO]: Finished training. The best model is from epoch#67.
2024-06-03 10:26:30 [INFO]: Saved the model to results_block_rate05/PeMS/PatchTST_PeMS/round_0/20240603_T100954/PatchTST.pypots
2024-06-03 10:26:34 [INFO]: Successfully saved to results_block_rate05/PeMS/PatchTST_PeMS/round_0/imputation.pkl
2024-06-03 10:26:34 [INFO]: Round0 - PatchTST on PeMS: MAE=0.4229, MSE=0.8767, MRE=0.5064
2024-06-03 10:26:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:26:34 [INFO]: Using the given device: cuda:0
2024-06-03 10:26:34 [INFO]: Model files will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_1/20240603_T102634
2024-06-03 10:26:34 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_1/20240603_T102634/tensorboard
2024-06-03 10:26:34 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 10:26:34 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 10:26:35 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 10:26:47 [INFO]: Epoch 001 - training loss: 1.0623, validation loss: 0.9533
2024-06-03 10:26:58 [INFO]: Epoch 002 - training loss: 0.6900, validation loss: 0.7900
2024-06-03 10:27:10 [INFO]: Epoch 003 - training loss: 0.5920, validation loss: 0.8134
2024-06-03 10:27:22 [INFO]: Epoch 004 - training loss: 0.5454, validation loss: 0.7927
2024-06-03 10:27:35 [INFO]: Epoch 005 - training loss: 0.5175, validation loss: 0.7385
2024-06-03 10:27:47 [INFO]: Epoch 006 - training loss: 0.4950, validation loss: 0.6985
2024-06-03 10:28:01 [INFO]: Epoch 007 - training loss: 0.4839, validation loss: 0.7063
2024-06-03 10:28:13 [INFO]: Epoch 008 - training loss: 0.4687, validation loss: 0.6821
2024-06-03 10:28:25 [INFO]: Epoch 009 - training loss: 0.4583, validation loss: 0.6717
2024-06-03 10:28:37 [INFO]: Epoch 010 - training loss: 0.4470, validation loss: 0.6767
2024-06-03 10:28:48 [INFO]: Epoch 011 - training loss: 0.4333, validation loss: 0.6510
2024-06-03 10:29:01 [INFO]: Epoch 012 - training loss: 0.4307, validation loss: 0.6609
2024-06-03 10:29:13 [INFO]: Epoch 013 - training loss: 0.4168, validation loss: 0.6592
2024-06-03 10:29:25 [INFO]: Epoch 014 - training loss: 0.4114, validation loss: 0.6501
2024-06-03 10:29:38 [INFO]: Epoch 015 - training loss: 0.4078, validation loss: 0.6460
2024-06-03 10:29:50 [INFO]: Epoch 016 - training loss: 0.3966, validation loss: 0.6496
2024-06-03 10:30:03 [INFO]: Epoch 017 - training loss: 0.3883, validation loss: 0.6575
2024-06-03 10:30:15 [INFO]: Epoch 018 - training loss: 0.3869, validation loss: 0.6529
2024-06-03 10:30:26 [INFO]: Epoch 019 - training loss: 0.3812, validation loss: 0.6470
2024-06-03 10:30:39 [INFO]: Epoch 020 - training loss: 0.3811, validation loss: 0.6344
2024-06-03 10:30:52 [INFO]: Epoch 021 - training loss: 0.3722, validation loss: 0.6415
2024-06-03 10:31:04 [INFO]: Epoch 022 - training loss: 0.3724, validation loss: 0.6280
2024-06-03 10:31:17 [INFO]: Epoch 023 - training loss: 0.3696, validation loss: 0.6340
2024-06-03 10:31:30 [INFO]: Epoch 024 - training loss: 0.3666, validation loss: 0.6290
2024-06-03 10:31:42 [INFO]: Epoch 025 - training loss: 0.3637, validation loss: 0.6219
2024-06-03 10:31:53 [INFO]: Epoch 026 - training loss: 0.3677, validation loss: 0.6305
2024-06-03 10:32:05 [INFO]: Epoch 027 - training loss: 0.3540, validation loss: 0.6182
2024-06-03 10:32:18 [INFO]: Epoch 028 - training loss: 0.3547, validation loss: 0.6243
2024-06-03 10:32:30 [INFO]: Epoch 029 - training loss: 0.3504, validation loss: 0.6103
2024-06-03 10:32:43 [INFO]: Epoch 030 - training loss: 0.3524, validation loss: 0.6131
2024-06-03 10:32:55 [INFO]: Epoch 031 - training loss: 0.3489, validation loss: 0.6099
2024-06-03 10:33:07 [INFO]: Epoch 032 - training loss: 0.3388, validation loss: 0.6028
2024-06-03 10:33:19 [INFO]: Epoch 033 - training loss: 0.3377, validation loss: 0.6116
2024-06-03 10:33:30 [INFO]: Epoch 034 - training loss: 0.3429, validation loss: 0.6038
2024-06-03 10:33:43 [INFO]: Epoch 035 - training loss: 0.3354, validation loss: 0.6073
2024-06-03 10:33:55 [INFO]: Epoch 036 - training loss: 0.3437, validation loss: 0.5997
2024-06-03 10:34:07 [INFO]: Epoch 037 - training loss: 0.3324, validation loss: 0.5976
2024-06-03 10:34:19 [INFO]: Epoch 038 - training loss: 0.3293, validation loss: 0.5930
2024-06-03 10:34:32 [INFO]: Epoch 039 - training loss: 0.3253, validation loss: 0.6065
2024-06-03 10:34:44 [INFO]: Epoch 040 - training loss: 0.3212, validation loss: 0.5953
2024-06-03 10:34:55 [INFO]: Epoch 041 - training loss: 0.3259, validation loss: 0.5903
2024-06-03 10:35:06 [INFO]: Epoch 042 - training loss: 0.3227, validation loss: 0.5898
2024-06-03 10:35:19 [INFO]: Epoch 043 - training loss: 0.3200, validation loss: 0.5889
2024-06-03 10:35:31 [INFO]: Epoch 044 - training loss: 0.3182, validation loss: 0.5957
2024-06-03 10:35:43 [INFO]: Epoch 045 - training loss: 0.3181, validation loss: 0.5910
2024-06-03 10:35:54 [INFO]: Epoch 046 - training loss: 0.3252, validation loss: 0.5861
2024-06-03 10:36:07 [INFO]: Epoch 047 - training loss: 0.3147, validation loss: 0.5852
2024-06-03 10:36:19 [INFO]: Epoch 048 - training loss: 0.3121, validation loss: 0.5896
2024-06-03 10:36:31 [INFO]: Epoch 049 - training loss: 0.3132, validation loss: 0.5845
2024-06-03 10:36:42 [INFO]: Epoch 050 - training loss: 0.3136, validation loss: 0.5848
2024-06-03 10:36:54 [INFO]: Epoch 051 - training loss: 0.3098, validation loss: 0.5828
2024-06-03 10:37:06 [INFO]: Epoch 052 - training loss: 0.3061, validation loss: 0.5860
2024-06-03 10:37:18 [INFO]: Epoch 053 - training loss: 0.3048, validation loss: 0.5799
2024-06-03 10:37:30 [INFO]: Epoch 054 - training loss: 0.3064, validation loss: 0.5805
2024-06-03 10:37:42 [INFO]: Epoch 055 - training loss: 0.3048, validation loss: 0.5810
2024-06-03 10:37:54 [INFO]: Epoch 056 - training loss: 0.3004, validation loss: 0.5746
2024-06-03 10:38:06 [INFO]: Epoch 057 - training loss: 0.3022, validation loss: 0.5786
2024-06-03 10:38:17 [INFO]: Epoch 058 - training loss: 0.2994, validation loss: 0.5773
2024-06-03 10:38:28 [INFO]: Epoch 059 - training loss: 0.3060, validation loss: 0.5691
2024-06-03 10:38:39 [INFO]: Epoch 060 - training loss: 0.2978, validation loss: 0.5714
2024-06-03 10:38:52 [INFO]: Epoch 061 - training loss: 0.2991, validation loss: 0.5740
2024-06-03 10:39:05 [INFO]: Epoch 062 - training loss: 0.3008, validation loss: 0.5768
2024-06-03 10:39:17 [INFO]: Epoch 063 - training loss: 0.2986, validation loss: 0.5793
2024-06-03 10:39:29 [INFO]: Epoch 064 - training loss: 0.2979, validation loss: 0.5708
2024-06-03 10:39:41 [INFO]: Epoch 065 - training loss: 0.2980, validation loss: 0.5696
2024-06-03 10:39:52 [INFO]: Epoch 066 - training loss: 0.2945, validation loss: 0.5764
2024-06-03 10:40:04 [INFO]: Epoch 067 - training loss: 0.2891, validation loss: 0.5726
2024-06-03 10:40:15 [INFO]: Epoch 068 - training loss: 0.2926, validation loss: 0.5665
2024-06-03 10:40:27 [INFO]: Epoch 069 - training loss: 0.2929, validation loss: 0.5680
2024-06-03 10:40:39 [INFO]: Epoch 070 - training loss: 0.2922, validation loss: 0.5653
2024-06-03 10:40:51 [INFO]: Epoch 071 - training loss: 0.2912, validation loss: 0.5698
2024-06-03 10:41:03 [INFO]: Epoch 072 - training loss: 0.2878, validation loss: 0.5707
2024-06-03 10:41:15 [INFO]: Epoch 073 - training loss: 0.2871, validation loss: 0.5692
2024-06-03 10:41:27 [INFO]: Epoch 074 - training loss: 0.2854, validation loss: 0.5665
2024-06-03 10:41:38 [INFO]: Epoch 075 - training loss: 0.2842, validation loss: 0.5681
2024-06-03 10:41:49 [INFO]: Epoch 076 - training loss: 0.2867, validation loss: 0.5726
2024-06-03 10:42:01 [INFO]: Epoch 077 - training loss: 0.2830, validation loss: 0.5728
2024-06-03 10:42:14 [INFO]: Epoch 078 - training loss: 0.2880, validation loss: 0.5677
2024-06-03 10:42:26 [INFO]: Epoch 079 - training loss: 0.2808, validation loss: 0.5659
2024-06-03 10:42:37 [INFO]: Epoch 080 - training loss: 0.2813, validation loss: 0.5666
2024-06-03 10:42:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:42:37 [INFO]: Finished training. The best model is from epoch#70.
2024-06-03 10:42:37 [INFO]: Saved the model to results_block_rate05/PeMS/PatchTST_PeMS/round_1/20240603_T102634/PatchTST.pypots
2024-06-03 10:42:41 [INFO]: Successfully saved to results_block_rate05/PeMS/PatchTST_PeMS/round_1/imputation.pkl
2024-06-03 10:42:41 [INFO]: Round1 - PatchTST on PeMS: MAE=0.4131, MSE=0.8439, MRE=0.4946
2024-06-03 10:42:41 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:42:41 [INFO]: Using the given device: cuda:0
2024-06-03 10:42:41 [INFO]: Model files will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_2/20240603_T104241
2024-06-03 10:42:41 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_2/20240603_T104241/tensorboard
2024-06-03 10:42:41 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 10:42:41 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 10:42:42 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 10:42:54 [INFO]: Epoch 001 - training loss: 1.1566, validation loss: 0.9198
2024-06-03 10:43:06 [INFO]: Epoch 002 - training loss: 0.6942, validation loss: 0.8693
2024-06-03 10:43:17 [INFO]: Epoch 003 - training loss: 0.5907, validation loss: 0.8093
2024-06-03 10:43:28 [INFO]: Epoch 004 - training loss: 0.5541, validation loss: 0.7893
2024-06-03 10:43:40 [INFO]: Epoch 005 - training loss: 0.5236, validation loss: 0.7142
2024-06-03 10:43:52 [INFO]: Epoch 006 - training loss: 0.5024, validation loss: 0.6827
2024-06-03 10:44:04 [INFO]: Epoch 007 - training loss: 0.4856, validation loss: 0.6346
2024-06-03 10:44:15 [INFO]: Epoch 008 - training loss: 0.4686, validation loss: 0.6130
2024-06-03 10:44:28 [INFO]: Epoch 009 - training loss: 0.4511, validation loss: 0.5962
2024-06-03 10:44:40 [INFO]: Epoch 010 - training loss: 0.4377, validation loss: 0.5934
2024-06-03 10:44:51 [INFO]: Epoch 011 - training loss: 0.4330, validation loss: 0.5818
2024-06-03 10:45:02 [INFO]: Epoch 012 - training loss: 0.4198, validation loss: 0.5771
2024-06-03 10:45:12 [INFO]: Epoch 013 - training loss: 0.4110, validation loss: 0.5696
2024-06-03 10:45:24 [INFO]: Epoch 014 - training loss: 0.3983, validation loss: 0.5653
2024-06-03 10:45:35 [INFO]: Epoch 015 - training loss: 0.3944, validation loss: 0.5616
2024-06-03 10:45:46 [INFO]: Epoch 016 - training loss: 0.3886, validation loss: 0.5575
2024-06-03 10:45:58 [INFO]: Epoch 017 - training loss: 0.3824, validation loss: 0.5555
2024-06-03 10:46:09 [INFO]: Epoch 018 - training loss: 0.3785, validation loss: 0.5555
2024-06-03 10:46:20 [INFO]: Epoch 019 - training loss: 0.3774, validation loss: 0.5500
2024-06-03 10:46:31 [INFO]: Epoch 020 - training loss: 0.3690, validation loss: 0.5494
2024-06-03 10:46:41 [INFO]: Epoch 021 - training loss: 0.3616, validation loss: 0.5461
2024-06-03 10:46:52 [INFO]: Epoch 022 - training loss: 0.3574, validation loss: 0.5427
2024-06-03 10:47:04 [INFO]: Epoch 023 - training loss: 0.3521, validation loss: 0.5404
2024-06-03 10:47:15 [INFO]: Epoch 024 - training loss: 0.3553, validation loss: 0.5397
2024-06-03 10:47:26 [INFO]: Epoch 025 - training loss: 0.3462, validation loss: 0.5409
2024-06-03 10:47:37 [INFO]: Epoch 026 - training loss: 0.3463, validation loss: 0.5368
2024-06-03 10:47:49 [INFO]: Epoch 027 - training loss: 0.3416, validation loss: 0.5359
2024-06-03 10:48:00 [INFO]: Epoch 028 - training loss: 0.3370, validation loss: 0.5330
2024-06-03 10:48:10 [INFO]: Epoch 029 - training loss: 0.3337, validation loss: 0.5327
2024-06-03 10:48:20 [INFO]: Epoch 030 - training loss: 0.3334, validation loss: 0.5311
2024-06-03 10:48:31 [INFO]: Epoch 031 - training loss: 0.3369, validation loss: 0.5317
2024-06-03 10:48:42 [INFO]: Epoch 032 - training loss: 0.3372, validation loss: 0.5325
2024-06-03 10:48:52 [INFO]: Epoch 033 - training loss: 0.3296, validation loss: 0.5289
2024-06-03 10:49:03 [INFO]: Epoch 034 - training loss: 0.3265, validation loss: 0.5288
2024-06-03 10:49:14 [INFO]: Epoch 035 - training loss: 0.3222, validation loss: 0.5274
2024-06-03 10:49:25 [INFO]: Epoch 036 - training loss: 0.3223, validation loss: 0.5258
2024-06-03 10:49:36 [INFO]: Epoch 037 - training loss: 0.3189, validation loss: 0.5262
2024-06-03 10:49:47 [INFO]: Epoch 038 - training loss: 0.3131, validation loss: 0.5243
2024-06-03 10:49:58 [INFO]: Epoch 039 - training loss: 0.3097, validation loss: 0.5245
2024-06-03 10:50:10 [INFO]: Epoch 040 - training loss: 0.3083, validation loss: 0.5263
2024-06-03 10:50:21 [INFO]: Epoch 041 - training loss: 0.3112, validation loss: 0.5249
2024-06-03 10:50:32 [INFO]: Epoch 042 - training loss: 0.3120, validation loss: 0.5257
2024-06-03 10:50:43 [INFO]: Epoch 043 - training loss: 0.3053, validation loss: 0.5226
2024-06-03 10:50:54 [INFO]: Epoch 044 - training loss: 0.3053, validation loss: 0.5215
2024-06-03 10:51:06 [INFO]: Epoch 045 - training loss: 0.3058, validation loss: 0.5198
2024-06-03 10:51:16 [INFO]: Epoch 046 - training loss: 0.3033, validation loss: 0.5220
2024-06-03 10:51:26 [INFO]: Epoch 047 - training loss: 0.3017, validation loss: 0.5231
2024-06-03 10:51:37 [INFO]: Epoch 048 - training loss: 0.3008, validation loss: 0.5219
2024-06-03 10:51:47 [INFO]: Epoch 049 - training loss: 0.3056, validation loss: 0.5226
2024-06-03 10:51:58 [INFO]: Epoch 050 - training loss: 0.3040, validation loss: 0.5176
2024-06-03 10:52:09 [INFO]: Epoch 051 - training loss: 0.3016, validation loss: 0.5179
2024-06-03 10:52:20 [INFO]: Epoch 052 - training loss: 0.2958, validation loss: 0.5174
2024-06-03 10:52:31 [INFO]: Epoch 053 - training loss: 0.2996, validation loss: 0.5215
2024-06-03 10:52:42 [INFO]: Epoch 054 - training loss: 0.2946, validation loss: 0.5209
2024-06-03 10:52:53 [INFO]: Epoch 055 - training loss: 0.2920, validation loss: 0.5192
2024-06-03 10:53:04 [INFO]: Epoch 056 - training loss: 0.2897, validation loss: 0.5178
2024-06-03 10:53:14 [INFO]: Epoch 057 - training loss: 0.2903, validation loss: 0.5186
2024-06-03 10:53:25 [INFO]: Epoch 058 - training loss: 0.2874, validation loss: 0.5188
2024-06-03 10:53:36 [INFO]: Epoch 059 - training loss: 0.2890, validation loss: 0.5208
2024-06-03 10:53:47 [INFO]: Epoch 060 - training loss: 0.2905, validation loss: 0.5181
2024-06-03 10:53:58 [INFO]: Epoch 061 - training loss: 0.2874, validation loss: 0.5162
2024-06-03 10:54:09 [INFO]: Epoch 062 - training loss: 0.2850, validation loss: 0.5182
2024-06-03 10:54:20 [INFO]: Epoch 063 - training loss: 0.2814, validation loss: 0.5181
2024-06-03 10:54:30 [INFO]: Epoch 064 - training loss: 0.2831, validation loss: 0.5192
2024-06-03 10:54:39 [INFO]: Epoch 065 - training loss: 0.2828, validation loss: 0.5175
2024-06-03 10:54:49 [INFO]: Epoch 066 - training loss: 0.2817, validation loss: 0.5171
2024-06-03 10:55:00 [INFO]: Epoch 067 - training loss: 0.2805, validation loss: 0.5157
2024-06-03 10:55:10 [INFO]: Epoch 068 - training loss: 0.2812, validation loss: 0.5156
2024-06-03 10:55:21 [INFO]: Epoch 069 - training loss: 0.2785, validation loss: 0.5197
2024-06-03 10:55:31 [INFO]: Epoch 070 - training loss: 0.2795, validation loss: 0.5141
2024-06-03 10:55:42 [INFO]: Epoch 071 - training loss: 0.2773, validation loss: 0.5156
2024-06-03 10:55:52 [INFO]: Epoch 072 - training loss: 0.2752, validation loss: 0.5194
2024-06-03 10:56:03 [INFO]: Epoch 073 - training loss: 0.2790, validation loss: 0.5165
2024-06-03 10:56:13 [INFO]: Epoch 074 - training loss: 0.2754, validation loss: 0.5169
2024-06-03 10:56:23 [INFO]: Epoch 075 - training loss: 0.2736, validation loss: 0.5155
2024-06-03 10:56:33 [INFO]: Epoch 076 - training loss: 0.2709, validation loss: 0.5149
2024-06-03 10:56:44 [INFO]: Epoch 077 - training loss: 0.2709, validation loss: 0.5151
2024-06-03 10:56:54 [INFO]: Epoch 078 - training loss: 0.2745, validation loss: 0.5128
2024-06-03 10:57:05 [INFO]: Epoch 079 - training loss: 0.2721, validation loss: 0.5135
2024-06-03 10:57:15 [INFO]: Epoch 080 - training loss: 0.2701, validation loss: 0.5155
2024-06-03 10:57:25 [INFO]: Epoch 081 - training loss: 0.2676, validation loss: 0.5135
2024-06-03 10:57:36 [INFO]: Epoch 082 - training loss: 0.2693, validation loss: 0.5150
2024-06-03 10:57:45 [INFO]: Epoch 083 - training loss: 0.2690, validation loss: 0.5141
2024-06-03 10:57:55 [INFO]: Epoch 084 - training loss: 0.2682, validation loss: 0.5124
2024-06-03 10:58:05 [INFO]: Epoch 085 - training loss: 0.2648, validation loss: 0.5142
2024-06-03 10:58:15 [INFO]: Epoch 086 - training loss: 0.2656, validation loss: 0.5135
2024-06-03 10:58:25 [INFO]: Epoch 087 - training loss: 0.2669, validation loss: 0.5148
2024-06-03 10:58:35 [INFO]: Epoch 088 - training loss: 0.2697, validation loss: 0.5163
2024-06-03 10:58:46 [INFO]: Epoch 089 - training loss: 0.2708, validation loss: 0.5152
2024-06-03 10:58:56 [INFO]: Epoch 090 - training loss: 0.2664, validation loss: 0.5166
2024-06-03 10:59:06 [INFO]: Epoch 091 - training loss: 0.2656, validation loss: 0.5141
2024-06-03 10:59:16 [INFO]: Epoch 092 - training loss: 0.2624, validation loss: 0.5163
2024-06-03 10:59:26 [INFO]: Epoch 093 - training loss: 0.2620, validation loss: 0.5147
2024-06-03 10:59:37 [INFO]: Epoch 094 - training loss: 0.2630, validation loss: 0.5148
2024-06-03 10:59:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:59:37 [INFO]: Finished training. The best model is from epoch#84.
2024-06-03 10:59:37 [INFO]: Saved the model to results_block_rate05/PeMS/PatchTST_PeMS/round_2/20240603_T104241/PatchTST.pypots
2024-06-03 10:59:40 [INFO]: Successfully saved to results_block_rate05/PeMS/PatchTST_PeMS/round_2/imputation.pkl
2024-06-03 10:59:40 [INFO]: Round2 - PatchTST on PeMS: MAE=0.3750, MSE=0.7593, MRE=0.4490
2024-06-03 10:59:40 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:59:40 [INFO]: Using the given device: cuda:0
2024-06-03 10:59:41 [INFO]: Model files will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_3/20240603_T105940
2024-06-03 10:59:41 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_3/20240603_T105940/tensorboard
2024-06-03 10:59:41 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 10:59:41 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 10:59:41 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 10:59:51 [INFO]: Epoch 001 - training loss: 1.1418, validation loss: 1.0160
2024-06-03 11:00:00 [INFO]: Epoch 002 - training loss: 0.7029, validation loss: 0.8930
2024-06-03 11:00:09 [INFO]: Epoch 003 - training loss: 0.5800, validation loss: 0.7806
2024-06-03 11:00:18 [INFO]: Epoch 004 - training loss: 0.5297, validation loss: 0.7213
2024-06-03 11:00:26 [INFO]: Epoch 005 - training loss: 0.4951, validation loss: 0.6823
2024-06-03 11:00:35 [INFO]: Epoch 006 - training loss: 0.4736, validation loss: 0.6653
2024-06-03 11:00:43 [INFO]: Epoch 007 - training loss: 0.4654, validation loss: 0.6466
2024-06-03 11:00:51 [INFO]: Epoch 008 - training loss: 0.4593, validation loss: 0.6298
2024-06-03 11:01:00 [INFO]: Epoch 009 - training loss: 0.4435, validation loss: 0.6282
2024-06-03 11:01:09 [INFO]: Epoch 010 - training loss: 0.4310, validation loss: 0.6170
2024-06-03 11:01:18 [INFO]: Epoch 011 - training loss: 0.4215, validation loss: 0.6084
2024-06-03 11:01:26 [INFO]: Epoch 012 - training loss: 0.4174, validation loss: 0.6149
2024-06-03 11:01:35 [INFO]: Epoch 013 - training loss: 0.4052, validation loss: 0.6124
2024-06-03 11:01:44 [INFO]: Epoch 014 - training loss: 0.3997, validation loss: 0.6006
2024-06-03 11:01:53 [INFO]: Epoch 015 - training loss: 0.3993, validation loss: 0.6114
2024-06-03 11:02:00 [INFO]: Epoch 016 - training loss: 0.3945, validation loss: 0.6027
2024-06-03 11:02:08 [INFO]: Epoch 017 - training loss: 0.3924, validation loss: 0.5920
2024-06-03 11:02:17 [INFO]: Epoch 018 - training loss: 0.3832, validation loss: 0.5949
2024-06-03 11:02:26 [INFO]: Epoch 019 - training loss: 0.3743, validation loss: 0.5878
2024-06-03 11:02:35 [INFO]: Epoch 020 - training loss: 0.3724, validation loss: 0.5863
2024-06-03 11:02:43 [INFO]: Epoch 021 - training loss: 0.3668, validation loss: 0.5816
2024-06-03 11:02:52 [INFO]: Epoch 022 - training loss: 0.3647, validation loss: 0.5781
2024-06-03 11:03:01 [INFO]: Epoch 023 - training loss: 0.3627, validation loss: 0.5943
2024-06-03 11:03:09 [INFO]: Epoch 024 - training loss: 0.3549, validation loss: 0.5816
2024-06-03 11:03:17 [INFO]: Epoch 025 - training loss: 0.3494, validation loss: 0.5878
2024-06-03 11:03:24 [INFO]: Epoch 026 - training loss: 0.3485, validation loss: 0.5735
2024-06-03 11:03:33 [INFO]: Epoch 027 - training loss: 0.3444, validation loss: 0.5693
2024-06-03 11:03:41 [INFO]: Epoch 028 - training loss: 0.3410, validation loss: 0.5740
2024-06-03 11:03:50 [INFO]: Epoch 029 - training loss: 0.3461, validation loss: 0.5676
2024-06-03 11:03:58 [INFO]: Epoch 030 - training loss: 0.3341, validation loss: 0.5645
2024-06-03 11:04:07 [INFO]: Epoch 031 - training loss: 0.3288, validation loss: 0.5696
2024-06-03 11:04:15 [INFO]: Epoch 032 - training loss: 0.3293, validation loss: 0.5596
2024-06-03 11:04:24 [INFO]: Epoch 033 - training loss: 0.3265, validation loss: 0.5528
2024-06-03 11:04:32 [INFO]: Epoch 034 - training loss: 0.3239, validation loss: 0.5590
2024-06-03 11:04:39 [INFO]: Epoch 035 - training loss: 0.3197, validation loss: 0.5507
2024-06-03 11:04:47 [INFO]: Epoch 036 - training loss: 0.3215, validation loss: 0.5633
2024-06-03 11:04:56 [INFO]: Epoch 037 - training loss: 0.3217, validation loss: 0.5517
2024-06-03 11:05:04 [INFO]: Epoch 038 - training loss: 0.3183, validation loss: 0.5523
2024-06-03 11:05:13 [INFO]: Epoch 039 - training loss: 0.3181, validation loss: 0.5530
2024-06-03 11:05:22 [INFO]: Epoch 040 - training loss: 0.3112, validation loss: 0.5540
2024-06-03 11:05:30 [INFO]: Epoch 041 - training loss: 0.3085, validation loss: 0.5471
2024-06-03 11:05:39 [INFO]: Epoch 042 - training loss: 0.3062, validation loss: 0.5490
2024-06-03 11:05:47 [INFO]: Epoch 043 - training loss: 0.3055, validation loss: 0.5442
2024-06-03 11:05:55 [INFO]: Epoch 044 - training loss: 0.3066, validation loss: 0.5502
2024-06-03 11:06:03 [INFO]: Epoch 045 - training loss: 0.3098, validation loss: 0.5351
2024-06-03 11:06:11 [INFO]: Epoch 046 - training loss: 0.3052, validation loss: 0.5465
2024-06-03 11:06:20 [INFO]: Epoch 047 - training loss: 0.3006, validation loss: 0.5432
2024-06-03 11:06:29 [INFO]: Epoch 048 - training loss: 0.2989, validation loss: 0.5409
2024-06-03 11:06:37 [INFO]: Epoch 049 - training loss: 0.2985, validation loss: 0.5330
2024-06-03 11:06:45 [INFO]: Epoch 050 - training loss: 0.2953, validation loss: 0.5375
2024-06-03 11:06:54 [INFO]: Epoch 051 - training loss: 0.2966, validation loss: 0.5408
2024-06-03 11:07:02 [INFO]: Epoch 052 - training loss: 0.2964, validation loss: 0.5400
2024-06-03 11:07:10 [INFO]: Epoch 053 - training loss: 0.2914, validation loss: 0.5336
2024-06-03 11:07:18 [INFO]: Epoch 054 - training loss: 0.2905, validation loss: 0.5402
2024-06-03 11:07:26 [INFO]: Epoch 055 - training loss: 0.2945, validation loss: 0.5311
2024-06-03 11:07:35 [INFO]: Epoch 056 - training loss: 0.2941, validation loss: 0.5283
2024-06-03 11:07:44 [INFO]: Epoch 057 - training loss: 0.2913, validation loss: 0.5330
2024-06-03 11:07:52 [INFO]: Epoch 058 - training loss: 0.2901, validation loss: 0.5384
2024-06-03 11:08:00 [INFO]: Epoch 059 - training loss: 0.2869, validation loss: 0.5300
2024-06-03 11:08:09 [INFO]: Epoch 060 - training loss: 0.2847, validation loss: 0.5325
2024-06-03 11:08:17 [INFO]: Epoch 061 - training loss: 0.2834, validation loss: 0.5306
2024-06-03 11:08:25 [INFO]: Epoch 062 - training loss: 0.2854, validation loss: 0.5382
2024-06-03 11:08:33 [INFO]: Epoch 063 - training loss: 0.2815, validation loss: 0.5350
2024-06-03 11:08:41 [INFO]: Epoch 064 - training loss: 0.2831, validation loss: 0.5320
2024-06-03 11:08:50 [INFO]: Epoch 065 - training loss: 0.2780, validation loss: 0.5290
2024-06-03 11:08:58 [INFO]: Epoch 066 - training loss: 0.2763, validation loss: 0.5327
2024-06-03 11:08:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:08:58 [INFO]: Finished training. The best model is from epoch#56.
2024-06-03 11:08:58 [INFO]: Saved the model to results_block_rate05/PeMS/PatchTST_PeMS/round_3/20240603_T105940/PatchTST.pypots
2024-06-03 11:09:01 [INFO]: Successfully saved to results_block_rate05/PeMS/PatchTST_PeMS/round_3/imputation.pkl
2024-06-03 11:09:01 [INFO]: Round3 - PatchTST on PeMS: MAE=0.3823, MSE=0.7856, MRE=0.4578
2024-06-03 11:09:01 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 11:09:01 [INFO]: Using the given device: cuda:0
2024-06-03 11:09:02 [INFO]: Model files will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_4/20240603_T110901
2024-06-03 11:09:02 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/PatchTST_PeMS/round_4/20240603_T110901/tensorboard
2024-06-03 11:09:02 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 11:09:02 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 11:09:02 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 11:09:10 [INFO]: Epoch 001 - training loss: 1.1788, validation loss: 1.0845
2024-06-03 11:09:19 [INFO]: Epoch 002 - training loss: 0.7519, validation loss: 0.8993
2024-06-03 11:09:27 [INFO]: Epoch 003 - training loss: 0.6350, validation loss: 0.7945
2024-06-03 11:09:35 [INFO]: Epoch 004 - training loss: 0.5734, validation loss: 0.7199
2024-06-03 11:09:43 [INFO]: Epoch 005 - training loss: 0.5356, validation loss: 0.7102
2024-06-03 11:09:51 [INFO]: Epoch 006 - training loss: 0.4977, validation loss: 0.6725
2024-06-03 11:09:59 [INFO]: Epoch 007 - training loss: 0.4749, validation loss: 0.6439
2024-06-03 11:10:07 [INFO]: Epoch 008 - training loss: 0.4597, validation loss: 0.6249
2024-06-03 11:10:16 [INFO]: Epoch 009 - training loss: 0.4492, validation loss: 0.6200
2024-06-03 11:10:24 [INFO]: Epoch 010 - training loss: 0.4437, validation loss: 0.6013
2024-06-03 11:10:33 [INFO]: Epoch 011 - training loss: 0.4228, validation loss: 0.5940
2024-06-03 11:10:41 [INFO]: Epoch 012 - training loss: 0.4100, validation loss: 0.5809
2024-06-03 11:10:50 [INFO]: Epoch 013 - training loss: 0.4105, validation loss: 0.5836
2024-06-03 11:10:57 [INFO]: Epoch 014 - training loss: 0.3989, validation loss: 0.5758
2024-06-03 11:11:05 [INFO]: Epoch 015 - training loss: 0.3889, validation loss: 0.5699
2024-06-03 11:11:13 [INFO]: Epoch 016 - training loss: 0.3832, validation loss: 0.5658
2024-06-03 11:11:22 [INFO]: Epoch 017 - training loss: 0.3799, validation loss: 0.5647
2024-06-03 11:11:30 [INFO]: Epoch 018 - training loss: 0.3762, validation loss: 0.5651
2024-06-03 11:11:38 [INFO]: Epoch 019 - training loss: 0.3709, validation loss: 0.5570
2024-06-03 11:11:47 [INFO]: Epoch 020 - training loss: 0.3631, validation loss: 0.5565
2024-06-03 11:11:55 [INFO]: Epoch 021 - training loss: 0.3610, validation loss: 0.5543
2024-06-03 11:12:03 [INFO]: Epoch 022 - training loss: 0.3573, validation loss: 0.5513
2024-06-03 11:12:12 [INFO]: Epoch 023 - training loss: 0.3545, validation loss: 0.5511
2024-06-03 11:12:20 [INFO]: Epoch 024 - training loss: 0.3519, validation loss: 0.5469
2024-06-03 11:12:28 [INFO]: Epoch 025 - training loss: 0.3462, validation loss: 0.5469
2024-06-03 11:12:36 [INFO]: Epoch 026 - training loss: 0.3426, validation loss: 0.5440
2024-06-03 11:12:44 [INFO]: Epoch 027 - training loss: 0.3426, validation loss: 0.5455
2024-06-03 11:12:52 [INFO]: Epoch 028 - training loss: 0.3391, validation loss: 0.5421
2024-06-03 11:13:00 [INFO]: Epoch 029 - training loss: 0.3337, validation loss: 0.5420
2024-06-03 11:13:09 [INFO]: Epoch 030 - training loss: 0.3321, validation loss: 0.5437
2024-06-03 11:13:17 [INFO]: Epoch 031 - training loss: 0.3344, validation loss: 0.5381
2024-06-03 11:13:25 [INFO]: Epoch 032 - training loss: 0.3292, validation loss: 0.5383
2024-06-03 11:13:33 [INFO]: Epoch 033 - training loss: 0.3227, validation loss: 0.5389
2024-06-03 11:13:41 [INFO]: Epoch 034 - training loss: 0.3244, validation loss: 0.5403
2024-06-03 11:13:49 [INFO]: Epoch 035 - training loss: 0.3221, validation loss: 0.5329
2024-06-03 11:13:57 [INFO]: Epoch 036 - training loss: 0.3219, validation loss: 0.5342
2024-06-03 11:14:05 [INFO]: Epoch 037 - training loss: 0.3170, validation loss: 0.5344
2024-06-03 11:14:14 [INFO]: Epoch 038 - training loss: 0.3180, validation loss: 0.5325
2024-06-03 11:14:22 [INFO]: Epoch 039 - training loss: 0.3139, validation loss: 0.5355
2024-06-03 11:14:30 [INFO]: Epoch 040 - training loss: 0.3225, validation loss: 0.5316
2024-06-03 11:14:39 [INFO]: Epoch 041 - training loss: 0.3133, validation loss: 0.5342
2024-06-03 11:14:47 [INFO]: Epoch 042 - training loss: 0.3089, validation loss: 0.5302
2024-06-03 11:14:55 [INFO]: Epoch 043 - training loss: 0.3098, validation loss: 0.5293
2024-06-03 11:15:03 [INFO]: Epoch 044 - training loss: 0.3080, validation loss: 0.5280
2024-06-03 11:15:11 [INFO]: Epoch 045 - training loss: 0.3071, validation loss: 0.5291
2024-06-03 11:15:20 [INFO]: Epoch 046 - training loss: 0.3052, validation loss: 0.5277
2024-06-03 11:15:28 [INFO]: Epoch 047 - training loss: 0.3031, validation loss: 0.5248
2024-06-03 11:15:37 [INFO]: Epoch 048 - training loss: 0.3012, validation loss: 0.5233
2024-06-03 11:15:46 [INFO]: Epoch 049 - training loss: 0.3053, validation loss: 0.5240
2024-06-03 11:15:54 [INFO]: Epoch 050 - training loss: 0.3064, validation loss: 0.5272
2024-06-03 11:16:02 [INFO]: Epoch 051 - training loss: 0.3024, validation loss: 0.5256
2024-06-03 11:16:11 [INFO]: Epoch 052 - training loss: 0.2995, validation loss: 0.5258
2024-06-03 11:16:19 [INFO]: Epoch 053 - training loss: 0.3048, validation loss: 0.5223
2024-06-03 11:16:26 [INFO]: Epoch 054 - training loss: 0.2987, validation loss: 0.5232
2024-06-03 11:16:35 [INFO]: Epoch 055 - training loss: 0.2962, validation loss: 0.5222
2024-06-03 11:16:43 [INFO]: Epoch 056 - training loss: 0.3001, validation loss: 0.5224
2024-06-03 11:16:51 [INFO]: Epoch 057 - training loss: 0.2946, validation loss: 0.5236
2024-06-03 11:16:59 [INFO]: Epoch 058 - training loss: 0.2932, validation loss: 0.5222
2024-06-03 11:17:08 [INFO]: Epoch 059 - training loss: 0.2935, validation loss: 0.5211
2024-06-03 11:17:16 [INFO]: Epoch 060 - training loss: 0.2993, validation loss: 0.5239
2024-06-03 11:17:24 [INFO]: Epoch 061 - training loss: 0.2956, validation loss: 0.5212
2024-06-03 11:17:32 [INFO]: Epoch 062 - training loss: 0.2923, validation loss: 0.5214
2024-06-03 11:17:39 [INFO]: Epoch 063 - training loss: 0.2868, validation loss: 0.5211
2024-06-03 11:17:47 [INFO]: Epoch 064 - training loss: 0.2893, validation loss: 0.5197
2024-06-03 11:17:56 [INFO]: Epoch 065 - training loss: 0.2875, validation loss: 0.5177
2024-06-03 11:18:04 [INFO]: Epoch 066 - training loss: 0.2846, validation loss: 0.5186
2024-06-03 11:18:12 [INFO]: Epoch 067 - training loss: 0.2836, validation loss: 0.5188
2024-06-03 11:18:20 [INFO]: Epoch 068 - training loss: 0.2823, validation loss: 0.5177
2024-06-03 11:18:29 [INFO]: Epoch 069 - training loss: 0.2822, validation loss: 0.5183
2024-06-03 11:18:37 [INFO]: Epoch 070 - training loss: 0.2824, validation loss: 0.5185
2024-06-03 11:18:45 [INFO]: Epoch 071 - training loss: 0.2811, validation loss: 0.5182
2024-06-03 11:18:52 [INFO]: Epoch 072 - training loss: 0.2846, validation loss: 0.5194
2024-06-03 11:19:00 [INFO]: Epoch 073 - training loss: 0.2822, validation loss: 0.5213
2024-06-03 11:19:09 [INFO]: Epoch 074 - training loss: 0.2787, validation loss: 0.5169
2024-06-03 11:19:17 [INFO]: Epoch 075 - training loss: 0.2815, validation loss: 0.5185
2024-06-03 11:19:25 [INFO]: Epoch 076 - training loss: 0.2765, validation loss: 0.5159
2024-06-03 11:19:33 [INFO]: Epoch 077 - training loss: 0.2789, validation loss: 0.5139
2024-06-03 11:19:42 [INFO]: Epoch 078 - training loss: 0.2774, validation loss: 0.5189
2024-06-03 11:19:51 [INFO]: Epoch 079 - training loss: 0.2791, validation loss: 0.5200
2024-06-03 11:19:59 [INFO]: Epoch 080 - training loss: 0.2759, validation loss: 0.5163
2024-06-03 11:20:07 [INFO]: Epoch 081 - training loss: 0.2776, validation loss: 0.5164
2024-06-03 11:20:15 [INFO]: Epoch 082 - training loss: 0.2778, validation loss: 0.5136
2024-06-03 11:20:23 [INFO]: Epoch 083 - training loss: 0.2777, validation loss: 0.5158
2024-06-03 11:20:31 [INFO]: Epoch 084 - training loss: 0.2810, validation loss: 0.5187
2024-06-03 11:20:40 [INFO]: Epoch 085 - training loss: 0.2803, validation loss: 0.5181
2024-06-03 11:20:49 [INFO]: Epoch 086 - training loss: 0.2717, validation loss: 0.5145
2024-06-03 11:20:57 [INFO]: Epoch 087 - training loss: 0.2719, validation loss: 0.5142
2024-06-03 11:21:05 [INFO]: Epoch 088 - training loss: 0.2714, validation loss: 0.5164
2024-06-03 11:21:14 [INFO]: Epoch 089 - training loss: 0.2713, validation loss: 0.5136
2024-06-03 11:21:22 [INFO]: Epoch 090 - training loss: 0.2707, validation loss: 0.5158
2024-06-03 11:21:30 [INFO]: Epoch 091 - training loss: 0.2684, validation loss: 0.5154
2024-06-03 11:21:38 [INFO]: Epoch 092 - training loss: 0.2700, validation loss: 0.5159
2024-06-03 11:21:46 [INFO]: Epoch 093 - training loss: 0.2680, validation loss: 0.5149
2024-06-03 11:21:54 [INFO]: Epoch 094 - training loss: 0.2666, validation loss: 0.5157
2024-06-03 11:22:02 [INFO]: Epoch 095 - training loss: 0.2689, validation loss: 0.5104
2024-06-03 11:22:10 [INFO]: Epoch 096 - training loss: 0.2679, validation loss: 0.5178
2024-06-03 11:22:18 [INFO]: Epoch 097 - training loss: 0.2644, validation loss: 0.5153
2024-06-03 11:22:26 [INFO]: Epoch 098 - training loss: 0.2631, validation loss: 0.5150
2024-06-03 11:22:35 [INFO]: Epoch 099 - training loss: 0.2696, validation loss: 0.5183
2024-06-03 11:22:43 [INFO]: Epoch 100 - training loss: 0.2647, validation loss: 0.5149
2024-06-03 11:22:43 [INFO]: Finished training. The best model is from epoch#95.
2024-06-03 11:22:43 [INFO]: Saved the model to results_block_rate05/PeMS/PatchTST_PeMS/round_4/20240603_T110901/PatchTST.pypots
2024-06-03 11:22:46 [INFO]: Successfully saved to results_block_rate05/PeMS/PatchTST_PeMS/round_4/imputation.pkl
2024-06-03 11:22:46 [INFO]: Round4 - PatchTST on PeMS: MAE=0.3783, MSE=0.7596, MRE=0.4529
2024-06-03 11:22:46 [INFO]: Done! Final results:
Averaged PatchTST (3,045,238 params) on PeMS: MAE=0.3943 ± 0.019717396437065264, MSE=0.8050 ± 0.04729942451159092, MRE=0.4721 ± 0.023608862551799444, average inference time=0.63
