2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_0/20240603_T100955
2024-06-03 10:09:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_0/20240603_T100955/tensorboard
2024-06-03 10:09:55 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=8, d_k=256
2024-06-03 10:09:55 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-03 10:09:57 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 78,229,072
2024-06-03 10:10:25 [INFO]: Epoch 001 - training loss: 1.0041, validation loss: 0.8289
2024-06-03 10:10:39 [INFO]: Epoch 002 - training loss: 0.6066, validation loss: 0.6947
2024-06-03 10:10:54 [INFO]: Epoch 003 - training loss: 0.5098, validation loss: 0.6262
2024-06-03 10:11:09 [INFO]: Epoch 004 - training loss: 0.4683, validation loss: 0.5892
2024-06-03 10:11:24 [INFO]: Epoch 005 - training loss: 0.4457, validation loss: 0.5660
2024-06-03 10:11:39 [INFO]: Epoch 006 - training loss: 0.4302, validation loss: 0.5695
2024-06-03 10:11:53 [INFO]: Epoch 007 - training loss: 0.4185, validation loss: 0.5577
2024-06-03 10:12:05 [INFO]: Epoch 008 - training loss: 0.4063, validation loss: 0.5577
2024-06-03 10:12:18 [INFO]: Epoch 009 - training loss: 0.4037, validation loss: 0.5547
2024-06-03 10:12:34 [INFO]: Epoch 010 - training loss: 0.3989, validation loss: 0.5476
2024-06-03 10:12:48 [INFO]: Epoch 011 - training loss: 0.3955, validation loss: 0.5442
2024-06-03 10:13:02 [INFO]: Epoch 012 - training loss: 0.3868, validation loss: 0.5392
2024-06-03 10:13:16 [INFO]: Epoch 013 - training loss: 0.3841, validation loss: 0.5384
2024-06-03 10:13:31 [INFO]: Epoch 014 - training loss: 0.3800, validation loss: 0.5370
2024-06-03 10:13:45 [INFO]: Epoch 015 - training loss: 0.3740, validation loss: 0.5333
2024-06-03 10:13:58 [INFO]: Epoch 016 - training loss: 0.3687, validation loss: 0.5322
2024-06-03 10:14:13 [INFO]: Epoch 017 - training loss: 0.3641, validation loss: 0.5331
2024-06-03 10:14:27 [INFO]: Epoch 018 - training loss: 0.3646, validation loss: 0.5344
2024-06-03 10:14:42 [INFO]: Epoch 019 - training loss: 0.3590, validation loss: 0.5316
2024-06-03 10:14:57 [INFO]: Epoch 020 - training loss: 0.3535, validation loss: 0.5287
2024-06-03 10:15:12 [INFO]: Epoch 021 - training loss: 0.3562, validation loss: 0.5299
2024-06-03 10:15:26 [INFO]: Epoch 022 - training loss: 0.3479, validation loss: 0.5251
2024-06-03 10:15:39 [INFO]: Epoch 023 - training loss: 0.3509, validation loss: 0.5240
2024-06-03 10:15:52 [INFO]: Epoch 024 - training loss: 0.3465, validation loss: 0.5229
2024-06-03 10:16:06 [INFO]: Epoch 025 - training loss: 0.3428, validation loss: 0.5265
2024-06-03 10:16:20 [INFO]: Epoch 026 - training loss: 0.3411, validation loss: 0.5246
2024-06-03 10:16:35 [INFO]: Epoch 027 - training loss: 0.3406, validation loss: 0.5204
2024-06-03 10:16:49 [INFO]: Epoch 028 - training loss: 0.3364, validation loss: 0.5218
2024-06-03 10:17:03 [INFO]: Epoch 029 - training loss: 0.3341, validation loss: 0.5202
2024-06-03 10:17:17 [INFO]: Epoch 030 - training loss: 0.3357, validation loss: 0.5216
2024-06-03 10:17:31 [INFO]: Epoch 031 - training loss: 0.3315, validation loss: 0.5217
2024-06-03 10:17:45 [INFO]: Epoch 032 - training loss: 0.3304, validation loss: 0.5190
2024-06-03 10:18:00 [INFO]: Epoch 033 - training loss: 0.3269, validation loss: 0.5196
2024-06-03 10:18:14 [INFO]: Epoch 034 - training loss: 0.3286, validation loss: 0.5206
2024-06-03 10:18:29 [INFO]: Epoch 035 - training loss: 0.3306, validation loss: 0.5159
2024-06-03 10:18:41 [INFO]: Epoch 036 - training loss: 0.3234, validation loss: 0.5169
2024-06-03 10:18:54 [INFO]: Epoch 037 - training loss: 0.3201, validation loss: 0.5171
2024-06-03 10:19:09 [INFO]: Epoch 038 - training loss: 0.3187, validation loss: 0.5136
2024-06-03 10:19:22 [INFO]: Epoch 039 - training loss: 0.3170, validation loss: 0.5147
2024-06-03 10:19:37 [INFO]: Epoch 040 - training loss: 0.3140, validation loss: 0.5141
2024-06-03 10:19:51 [INFO]: Epoch 041 - training loss: 0.3117, validation loss: 0.5126
2024-06-03 10:20:05 [INFO]: Epoch 042 - training loss: 0.3097, validation loss: 0.5144
2024-06-03 10:20:17 [INFO]: Epoch 043 - training loss: 0.3108, validation loss: 0.5107
2024-06-03 10:20:30 [INFO]: Epoch 044 - training loss: 0.3071, validation loss: 0.5114
2024-06-03 10:20:44 [INFO]: Epoch 045 - training loss: 0.3077, validation loss: 0.5125
2024-06-03 10:20:59 [INFO]: Epoch 046 - training loss: 0.3054, validation loss: 0.5140
2024-06-03 10:21:13 [INFO]: Epoch 047 - training loss: 0.3027, validation loss: 0.5113
2024-06-03 10:21:27 [INFO]: Epoch 048 - training loss: 0.3049, validation loss: 0.5092
2024-06-03 10:21:42 [INFO]: Epoch 049 - training loss: 0.2999, validation loss: 0.5088
2024-06-03 10:21:54 [INFO]: Epoch 050 - training loss: 0.3022, validation loss: 0.5091
2024-06-03 10:22:08 [INFO]: Epoch 051 - training loss: 0.2971, validation loss: 0.5106
2024-06-03 10:22:22 [INFO]: Epoch 052 - training loss: 0.2976, validation loss: 0.5058
2024-06-03 10:22:37 [INFO]: Epoch 053 - training loss: 0.2995, validation loss: 0.5098
2024-06-03 10:22:52 [INFO]: Epoch 054 - training loss: 0.2968, validation loss: 0.5074
2024-06-03 10:23:06 [INFO]: Epoch 055 - training loss: 0.2926, validation loss: 0.5124
2024-06-03 10:23:21 [INFO]: Epoch 056 - training loss: 0.2917, validation loss: 0.5076
2024-06-03 10:23:35 [INFO]: Epoch 057 - training loss: 0.2911, validation loss: 0.5060
2024-06-03 10:23:48 [INFO]: Epoch 058 - training loss: 0.2907, validation loss: 0.5053
2024-06-03 10:24:02 [INFO]: Epoch 059 - training loss: 0.2875, validation loss: 0.5044
2024-06-03 10:24:17 [INFO]: Epoch 060 - training loss: 0.2880, validation loss: 0.5040
2024-06-03 10:24:32 [INFO]: Epoch 061 - training loss: 0.2882, validation loss: 0.5059
2024-06-03 10:24:46 [INFO]: Epoch 062 - training loss: 0.2869, validation loss: 0.5030
2024-06-03 10:25:00 [INFO]: Epoch 063 - training loss: 0.2884, validation loss: 0.5041
2024-06-03 10:25:13 [INFO]: Epoch 064 - training loss: 0.2878, validation loss: 0.5015
2024-06-03 10:25:26 [INFO]: Epoch 065 - training loss: 0.2840, validation loss: 0.5024
2024-06-03 10:25:40 [INFO]: Epoch 066 - training loss: 0.2805, validation loss: 0.5025
2024-06-03 10:25:53 [INFO]: Epoch 067 - training loss: 0.2812, validation loss: 0.5017
2024-06-03 10:26:07 [INFO]: Epoch 068 - training loss: 0.2793, validation loss: 0.5003
2024-06-03 10:26:21 [INFO]: Epoch 069 - training loss: 0.2805, validation loss: 0.5017
2024-06-03 10:26:34 [INFO]: Epoch 070 - training loss: 0.2801, validation loss: 0.4984
2024-06-03 10:26:48 [INFO]: Epoch 071 - training loss: 0.2797, validation loss: 0.5000
2024-06-03 10:27:00 [INFO]: Epoch 072 - training loss: 0.2757, validation loss: 0.4995
2024-06-03 10:27:15 [INFO]: Epoch 073 - training loss: 0.2769, validation loss: 0.4993
2024-06-03 10:27:28 [INFO]: Epoch 074 - training loss: 0.2746, validation loss: 0.5003
2024-06-03 10:27:42 [INFO]: Epoch 075 - training loss: 0.2743, validation loss: 0.4988
2024-06-03 10:27:55 [INFO]: Epoch 076 - training loss: 0.2778, validation loss: 0.5004
2024-06-03 10:28:09 [INFO]: Epoch 077 - training loss: 0.2719, validation loss: 0.4975
2024-06-03 10:28:23 [INFO]: Epoch 078 - training loss: 0.2718, validation loss: 0.4986
2024-06-03 10:28:35 [INFO]: Epoch 079 - training loss: 0.2674, validation loss: 0.4972
2024-06-03 10:28:48 [INFO]: Epoch 080 - training loss: 0.2687, validation loss: 0.4971
2024-06-03 10:29:02 [INFO]: Epoch 081 - training loss: 0.2683, validation loss: 0.4986
2024-06-03 10:29:16 [INFO]: Epoch 082 - training loss: 0.2679, validation loss: 0.4978
2024-06-03 10:29:30 [INFO]: Epoch 083 - training loss: 0.2664, validation loss: 0.4976
2024-06-03 10:29:44 [INFO]: Epoch 084 - training loss: 0.2663, validation loss: 0.4983
2024-06-03 10:29:58 [INFO]: Epoch 085 - training loss: 0.2657, validation loss: 0.4967
2024-06-03 10:30:11 [INFO]: Epoch 086 - training loss: 0.2632, validation loss: 0.4963
2024-06-03 10:30:25 [INFO]: Epoch 087 - training loss: 0.2617, validation loss: 0.4952
2024-06-03 10:30:38 [INFO]: Epoch 088 - training loss: 0.2617, validation loss: 0.4946
2024-06-03 10:30:52 [INFO]: Epoch 089 - training loss: 0.2644, validation loss: 0.4945
2024-06-03 10:31:06 [INFO]: Epoch 090 - training loss: 0.2626, validation loss: 0.4971
2024-06-03 10:31:20 [INFO]: Epoch 091 - training loss: 0.2626, validation loss: 0.4967
2024-06-03 10:31:33 [INFO]: Epoch 092 - training loss: 0.2585, validation loss: 0.4942
2024-06-03 10:31:46 [INFO]: Epoch 093 - training loss: 0.2574, validation loss: 0.4947
2024-06-03 10:31:59 [INFO]: Epoch 094 - training loss: 0.2578, validation loss: 0.4954
2024-06-03 10:32:12 [INFO]: Epoch 095 - training loss: 0.2567, validation loss: 0.4935
2024-06-03 10:32:26 [INFO]: Epoch 096 - training loss: 0.2557, validation loss: 0.4931
2024-06-03 10:32:38 [INFO]: Epoch 097 - training loss: 0.2559, validation loss: 0.4925
2024-06-03 10:32:52 [INFO]: Epoch 098 - training loss: 0.2537, validation loss: 0.4932
2024-06-03 10:33:06 [INFO]: Epoch 099 - training loss: 0.2543, validation loss: 0.4920
2024-06-03 10:33:18 [INFO]: Epoch 100 - training loss: 0.2516, validation loss: 0.4926
2024-06-03 10:33:18 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 10:33:22 [INFO]: Saved the model to results_block_rate05/PeMS/SAITS_PeMS/round_0/20240603_T100955/SAITS.pypots
2024-06-03 10:33:26 [INFO]: Successfully saved to results_block_rate05/PeMS/SAITS_PeMS/round_0/imputation.pkl
2024-06-03 10:33:26 [INFO]: Round0 - SAITS on PeMS: MAE=0.3302, MSE=0.7259, MRE=0.3954
2024-06-03 10:33:26 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:33:26 [INFO]: Using the given device: cuda:0
2024-06-03 10:33:26 [INFO]: Model files will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_1/20240603_T103326
2024-06-03 10:33:26 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_1/20240603_T103326/tensorboard
2024-06-03 10:33:26 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=8, d_k=256
2024-06-03 10:33:26 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-03 10:33:33 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 78,229,072
2024-06-03 10:33:48 [INFO]: Epoch 001 - training loss: 1.0181, validation loss: 0.8237
2024-06-03 10:34:01 [INFO]: Epoch 002 - training loss: 0.5954, validation loss: 0.6816
2024-06-03 10:34:14 [INFO]: Epoch 003 - training loss: 0.5044, validation loss: 0.6061
2024-06-03 10:34:27 [INFO]: Epoch 004 - training loss: 0.4644, validation loss: 0.5915
2024-06-03 10:34:40 [INFO]: Epoch 005 - training loss: 0.4426, validation loss: 0.5677
2024-06-03 10:34:52 [INFO]: Epoch 006 - training loss: 0.4261, validation loss: 0.5678
2024-06-03 10:35:04 [INFO]: Epoch 007 - training loss: 0.4147, validation loss: 0.5625
2024-06-03 10:35:17 [INFO]: Epoch 008 - training loss: 0.4097, validation loss: 0.5622
2024-06-03 10:35:30 [INFO]: Epoch 009 - training loss: 0.3991, validation loss: 0.5561
2024-06-03 10:35:42 [INFO]: Epoch 010 - training loss: 0.3972, validation loss: 0.5523
2024-06-03 10:35:56 [INFO]: Epoch 011 - training loss: 0.3930, validation loss: 0.5459
2024-06-03 10:36:09 [INFO]: Epoch 012 - training loss: 0.3844, validation loss: 0.5485
2024-06-03 10:36:22 [INFO]: Epoch 013 - training loss: 0.3832, validation loss: 0.5437
2024-06-03 10:36:34 [INFO]: Epoch 014 - training loss: 0.3793, validation loss: 0.5390
2024-06-03 10:36:46 [INFO]: Epoch 015 - training loss: 0.3738, validation loss: 0.5424
2024-06-03 10:36:58 [INFO]: Epoch 016 - training loss: 0.3691, validation loss: 0.5362
2024-06-03 10:37:11 [INFO]: Epoch 017 - training loss: 0.3633, validation loss: 0.5355
2024-06-03 10:37:24 [INFO]: Epoch 018 - training loss: 0.3612, validation loss: 0.5349
2024-06-03 10:37:36 [INFO]: Epoch 019 - training loss: 0.3560, validation loss: 0.5299
2024-06-03 10:37:49 [INFO]: Epoch 020 - training loss: 0.3561, validation loss: 0.5301
2024-06-03 10:38:02 [INFO]: Epoch 021 - training loss: 0.3534, validation loss: 0.5284
2024-06-03 10:38:15 [INFO]: Epoch 022 - training loss: 0.3518, validation loss: 0.5300
2024-06-03 10:38:26 [INFO]: Epoch 023 - training loss: 0.3482, validation loss: 0.5241
2024-06-03 10:38:40 [INFO]: Epoch 024 - training loss: 0.3458, validation loss: 0.5263
2024-06-03 10:38:53 [INFO]: Epoch 025 - training loss: 0.3414, validation loss: 0.5282
2024-06-03 10:39:06 [INFO]: Epoch 026 - training loss: 0.3417, validation loss: 0.5232
2024-06-03 10:39:19 [INFO]: Epoch 027 - training loss: 0.3403, validation loss: 0.5254
2024-06-03 10:39:31 [INFO]: Epoch 028 - training loss: 0.3377, validation loss: 0.5240
2024-06-03 10:39:45 [INFO]: Epoch 029 - training loss: 0.3333, validation loss: 0.5212
2024-06-03 10:39:56 [INFO]: Epoch 030 - training loss: 0.3346, validation loss: 0.5166
2024-06-03 10:40:08 [INFO]: Epoch 031 - training loss: 0.3276, validation loss: 0.5201
2024-06-03 10:40:21 [INFO]: Epoch 032 - training loss: 0.3290, validation loss: 0.5203
2024-06-03 10:40:33 [INFO]: Epoch 033 - training loss: 0.3280, validation loss: 0.5179
2024-06-03 10:40:46 [INFO]: Epoch 034 - training loss: 0.3272, validation loss: 0.5190
2024-06-03 10:40:59 [INFO]: Epoch 035 - training loss: 0.3260, validation loss: 0.5161
2024-06-03 10:41:12 [INFO]: Epoch 036 - training loss: 0.3207, validation loss: 0.5189
2024-06-03 10:41:25 [INFO]: Epoch 037 - training loss: 0.3201, validation loss: 0.5164
2024-06-03 10:41:37 [INFO]: Epoch 038 - training loss: 0.3158, validation loss: 0.5133
2024-06-03 10:41:49 [INFO]: Epoch 039 - training loss: 0.3126, validation loss: 0.5136
2024-06-03 10:42:03 [INFO]: Epoch 040 - training loss: 0.3152, validation loss: 0.5107
2024-06-03 10:42:15 [INFO]: Epoch 041 - training loss: 0.3120, validation loss: 0.5123
2024-06-03 10:42:28 [INFO]: Epoch 042 - training loss: 0.3124, validation loss: 0.5134
2024-06-03 10:42:40 [INFO]: Epoch 043 - training loss: 0.3106, validation loss: 0.5070
2024-06-03 10:42:53 [INFO]: Epoch 044 - training loss: 0.3105, validation loss: 0.5088
2024-06-03 10:43:05 [INFO]: Epoch 045 - training loss: 0.3074, validation loss: 0.5075
2024-06-03 10:43:17 [INFO]: Epoch 046 - training loss: 0.3044, validation loss: 0.5082
2024-06-03 10:43:28 [INFO]: Epoch 047 - training loss: 0.3023, validation loss: 0.5091
2024-06-03 10:43:41 [INFO]: Epoch 048 - training loss: 0.3031, validation loss: 0.5083
2024-06-03 10:43:53 [INFO]: Epoch 049 - training loss: 0.3048, validation loss: 0.5072
2024-06-03 10:44:06 [INFO]: Epoch 050 - training loss: 0.3013, validation loss: 0.5085
2024-06-03 10:44:18 [INFO]: Epoch 051 - training loss: 0.3000, validation loss: 0.5074
2024-06-03 10:44:31 [INFO]: Epoch 052 - training loss: 0.2984, validation loss: 0.5029
2024-06-03 10:44:43 [INFO]: Epoch 053 - training loss: 0.2974, validation loss: 0.5034
2024-06-03 10:44:54 [INFO]: Epoch 054 - training loss: 0.2962, validation loss: 0.5031
2024-06-03 10:45:05 [INFO]: Epoch 055 - training loss: 0.2920, validation loss: 0.5038
2024-06-03 10:45:17 [INFO]: Epoch 056 - training loss: 0.2942, validation loss: 0.5025
2024-06-03 10:45:29 [INFO]: Epoch 057 - training loss: 0.2893, validation loss: 0.5053
2024-06-03 10:45:40 [INFO]: Epoch 058 - training loss: 0.2900, validation loss: 0.5026
2024-06-03 10:45:52 [INFO]: Epoch 059 - training loss: 0.2881, validation loss: 0.5024
2024-06-03 10:46:04 [INFO]: Epoch 060 - training loss: 0.2861, validation loss: 0.5014
2024-06-03 10:46:16 [INFO]: Epoch 061 - training loss: 0.2888, validation loss: 0.5010
2024-06-03 10:46:27 [INFO]: Epoch 062 - training loss: 0.2860, validation loss: 0.5026
2024-06-03 10:46:38 [INFO]: Epoch 063 - training loss: 0.2863, validation loss: 0.5012
2024-06-03 10:46:49 [INFO]: Epoch 064 - training loss: 0.2840, validation loss: 0.5034
2024-06-03 10:47:01 [INFO]: Epoch 065 - training loss: 0.2840, validation loss: 0.5006
2024-06-03 10:47:13 [INFO]: Epoch 066 - training loss: 0.2816, validation loss: 0.4992
2024-06-03 10:47:24 [INFO]: Epoch 067 - training loss: 0.2825, validation loss: 0.5030
2024-06-03 10:47:35 [INFO]: Epoch 068 - training loss: 0.2793, validation loss: 0.4995
2024-06-03 10:47:46 [INFO]: Epoch 069 - training loss: 0.2780, validation loss: 0.5003
2024-06-03 10:47:58 [INFO]: Epoch 070 - training loss: 0.2778, validation loss: 0.4981
2024-06-03 10:48:08 [INFO]: Epoch 071 - training loss: 0.2773, validation loss: 0.4968
2024-06-03 10:48:18 [INFO]: Epoch 072 - training loss: 0.2761, validation loss: 0.4983
2024-06-03 10:48:29 [INFO]: Epoch 073 - training loss: 0.2753, validation loss: 0.4983
2024-06-03 10:48:41 [INFO]: Epoch 074 - training loss: 0.2713, validation loss: 0.4966
2024-06-03 10:48:51 [INFO]: Epoch 075 - training loss: 0.2713, validation loss: 0.4981
2024-06-03 10:49:03 [INFO]: Epoch 076 - training loss: 0.2722, validation loss: 0.4959
2024-06-03 10:49:14 [INFO]: Epoch 077 - training loss: 0.2706, validation loss: 0.4950
2024-06-03 10:49:26 [INFO]: Epoch 078 - training loss: 0.2708, validation loss: 0.4953
2024-06-03 10:49:37 [INFO]: Epoch 079 - training loss: 0.2747, validation loss: 0.4971
2024-06-03 10:49:48 [INFO]: Epoch 080 - training loss: 0.2713, validation loss: 0.4946
2024-06-03 10:49:59 [INFO]: Epoch 081 - training loss: 0.2669, validation loss: 0.4979
2024-06-03 10:50:10 [INFO]: Epoch 082 - training loss: 0.2666, validation loss: 0.4965
2024-06-03 10:50:22 [INFO]: Epoch 083 - training loss: 0.2656, validation loss: 0.4950
2024-06-03 10:50:33 [INFO]: Epoch 084 - training loss: 0.2658, validation loss: 0.4948
2024-06-03 10:50:45 [INFO]: Epoch 085 - training loss: 0.2655, validation loss: 0.4958
2024-06-03 10:50:56 [INFO]: Epoch 086 - training loss: 0.2667, validation loss: 0.4953
2024-06-03 10:51:07 [INFO]: Epoch 087 - training loss: 0.2639, validation loss: 0.4965
2024-06-03 10:51:19 [INFO]: Epoch 088 - training loss: 0.2621, validation loss: 0.4948
2024-06-03 10:51:29 [INFO]: Epoch 089 - training loss: 0.2611, validation loss: 0.4924
2024-06-03 10:51:39 [INFO]: Epoch 090 - training loss: 0.2583, validation loss: 0.4919
2024-06-03 10:51:50 [INFO]: Epoch 091 - training loss: 0.2588, validation loss: 0.4947
2024-06-03 10:52:02 [INFO]: Epoch 092 - training loss: 0.2600, validation loss: 0.4917
2024-06-03 10:52:13 [INFO]: Epoch 093 - training loss: 0.2599, validation loss: 0.4916
2024-06-03 10:52:26 [INFO]: Epoch 094 - training loss: 0.2604, validation loss: 0.4918
2024-06-03 10:52:36 [INFO]: Epoch 095 - training loss: 0.2586, validation loss: 0.4933
2024-06-03 10:52:47 [INFO]: Epoch 096 - training loss: 0.2555, validation loss: 0.4916
2024-06-03 10:52:58 [INFO]: Epoch 097 - training loss: 0.2543, validation loss: 0.4904
2024-06-03 10:53:08 [INFO]: Epoch 098 - training loss: 0.2546, validation loss: 0.4932
2024-06-03 10:53:19 [INFO]: Epoch 099 - training loss: 0.2558, validation loss: 0.4931
2024-06-03 10:53:31 [INFO]: Epoch 100 - training loss: 0.2543, validation loss: 0.4923
2024-06-03 10:53:31 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 10:53:34 [INFO]: Saved the model to results_block_rate05/PeMS/SAITS_PeMS/round_1/20240603_T103326/SAITS.pypots
2024-06-03 10:53:37 [INFO]: Successfully saved to results_block_rate05/PeMS/SAITS_PeMS/round_1/imputation.pkl
2024-06-03 10:53:37 [INFO]: Round1 - SAITS on PeMS: MAE=0.3303, MSE=0.7238, MRE=0.3955
2024-06-03 10:53:37 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:53:37 [INFO]: Using the given device: cuda:0
2024-06-03 10:53:37 [INFO]: Model files will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_2/20240603_T105337
2024-06-03 10:53:37 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_2/20240603_T105337/tensorboard
2024-06-03 10:53:37 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=8, d_k=256
2024-06-03 10:53:37 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-03 10:53:42 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 78,229,072
2024-06-03 10:53:54 [INFO]: Epoch 001 - training loss: 1.0004, validation loss: 0.8295
2024-06-03 10:54:06 [INFO]: Epoch 002 - training loss: 0.5998, validation loss: 0.7005
2024-06-03 10:54:17 [INFO]: Epoch 003 - training loss: 0.5084, validation loss: 0.6168
2024-06-03 10:54:29 [INFO]: Epoch 004 - training loss: 0.4670, validation loss: 0.5877
2024-06-03 10:54:38 [INFO]: Epoch 005 - training loss: 0.4378, validation loss: 0.5816
2024-06-03 10:54:48 [INFO]: Epoch 006 - training loss: 0.4245, validation loss: 0.5625
2024-06-03 10:54:59 [INFO]: Epoch 007 - training loss: 0.4143, validation loss: 0.5596
2024-06-03 10:55:10 [INFO]: Epoch 008 - training loss: 0.4046, validation loss: 0.5557
2024-06-03 10:55:20 [INFO]: Epoch 009 - training loss: 0.4022, validation loss: 0.5540
2024-06-03 10:55:31 [INFO]: Epoch 010 - training loss: 0.3993, validation loss: 0.5435
2024-06-03 10:55:42 [INFO]: Epoch 011 - training loss: 0.3928, validation loss: 0.5425
2024-06-03 10:55:52 [INFO]: Epoch 012 - training loss: 0.3866, validation loss: 0.5381
2024-06-03 10:56:02 [INFO]: Epoch 013 - training loss: 0.3837, validation loss: 0.5421
2024-06-03 10:56:13 [INFO]: Epoch 014 - training loss: 0.3775, validation loss: 0.5376
2024-06-03 10:56:21 [INFO]: Epoch 015 - training loss: 0.3728, validation loss: 0.5340
2024-06-03 10:56:32 [INFO]: Epoch 016 - training loss: 0.3695, validation loss: 0.5338
2024-06-03 10:56:42 [INFO]: Epoch 017 - training loss: 0.3625, validation loss: 0.5359
2024-06-03 10:56:53 [INFO]: Epoch 018 - training loss: 0.3623, validation loss: 0.5333
2024-06-03 10:57:04 [INFO]: Epoch 019 - training loss: 0.3574, validation loss: 0.5378
2024-06-03 10:57:14 [INFO]: Epoch 020 - training loss: 0.3611, validation loss: 0.5297
2024-06-03 10:57:24 [INFO]: Epoch 021 - training loss: 0.3552, validation loss: 0.5303
2024-06-03 10:57:35 [INFO]: Epoch 022 - training loss: 0.3539, validation loss: 0.5269
2024-06-03 10:57:45 [INFO]: Epoch 023 - training loss: 0.3486, validation loss: 0.5311
2024-06-03 10:57:55 [INFO]: Epoch 024 - training loss: 0.3498, validation loss: 0.5260
2024-06-03 10:58:06 [INFO]: Epoch 025 - training loss: 0.3457, validation loss: 0.5242
2024-06-03 10:58:17 [INFO]: Epoch 026 - training loss: 0.3431, validation loss: 0.5249
2024-06-03 10:58:28 [INFO]: Epoch 027 - training loss: 0.3394, validation loss: 0.5235
2024-06-03 10:58:39 [INFO]: Epoch 028 - training loss: 0.3359, validation loss: 0.5207
2024-06-03 10:58:49 [INFO]: Epoch 029 - training loss: 0.3357, validation loss: 0.5197
2024-06-03 10:59:00 [INFO]: Epoch 030 - training loss: 0.3360, validation loss: 0.5220
2024-06-03 10:59:11 [INFO]: Epoch 031 - training loss: 0.3360, validation loss: 0.5191
2024-06-03 10:59:20 [INFO]: Epoch 032 - training loss: 0.3311, validation loss: 0.5219
2024-06-03 10:59:30 [INFO]: Epoch 033 - training loss: 0.3275, validation loss: 0.5193
2024-06-03 10:59:41 [INFO]: Epoch 034 - training loss: 0.3248, validation loss: 0.5189
2024-06-03 10:59:51 [INFO]: Epoch 035 - training loss: 0.3271, validation loss: 0.5148
2024-06-03 11:00:01 [INFO]: Epoch 036 - training loss: 0.3251, validation loss: 0.5218
2024-06-03 11:00:09 [INFO]: Epoch 037 - training loss: 0.3211, validation loss: 0.5184
2024-06-03 11:00:18 [INFO]: Epoch 038 - training loss: 0.3206, validation loss: 0.5154
2024-06-03 11:00:27 [INFO]: Epoch 039 - training loss: 0.3184, validation loss: 0.5157
2024-06-03 11:00:35 [INFO]: Epoch 040 - training loss: 0.3174, validation loss: 0.5137
2024-06-03 11:00:44 [INFO]: Epoch 041 - training loss: 0.3147, validation loss: 0.5170
2024-06-03 11:00:53 [INFO]: Epoch 042 - training loss: 0.3128, validation loss: 0.5135
2024-06-03 11:01:01 [INFO]: Epoch 043 - training loss: 0.3113, validation loss: 0.5138
2024-06-03 11:01:10 [INFO]: Epoch 044 - training loss: 0.3111, validation loss: 0.5140
2024-06-03 11:01:19 [INFO]: Epoch 045 - training loss: 0.3073, validation loss: 0.5166
2024-06-03 11:01:27 [INFO]: Epoch 046 - training loss: 0.3070, validation loss: 0.5108
2024-06-03 11:01:36 [INFO]: Epoch 047 - training loss: 0.3086, validation loss: 0.5093
2024-06-03 11:01:45 [INFO]: Epoch 048 - training loss: 0.3103, validation loss: 0.5114
2024-06-03 11:01:54 [INFO]: Epoch 049 - training loss: 0.3046, validation loss: 0.5126
2024-06-03 11:02:03 [INFO]: Epoch 050 - training loss: 0.3005, validation loss: 0.5094
2024-06-03 11:02:10 [INFO]: Epoch 051 - training loss: 0.2998, validation loss: 0.5086
2024-06-03 11:02:19 [INFO]: Epoch 052 - training loss: 0.2996, validation loss: 0.5090
2024-06-03 11:02:28 [INFO]: Epoch 053 - training loss: 0.2984, validation loss: 0.5072
2024-06-03 11:02:36 [INFO]: Epoch 054 - training loss: 0.2971, validation loss: 0.5070
2024-06-03 11:02:45 [INFO]: Epoch 055 - training loss: 0.2958, validation loss: 0.5085
2024-06-03 11:02:53 [INFO]: Epoch 056 - training loss: 0.2953, validation loss: 0.5061
2024-06-03 11:03:02 [INFO]: Epoch 057 - training loss: 0.2931, validation loss: 0.5056
2024-06-03 11:03:11 [INFO]: Epoch 058 - training loss: 0.2913, validation loss: 0.5077
2024-06-03 11:03:19 [INFO]: Epoch 059 - training loss: 0.2907, validation loss: 0.5053
2024-06-03 11:03:27 [INFO]: Epoch 060 - training loss: 0.2880, validation loss: 0.5078
2024-06-03 11:03:35 [INFO]: Epoch 061 - training loss: 0.2890, validation loss: 0.5042
2024-06-03 11:03:44 [INFO]: Epoch 062 - training loss: 0.2896, validation loss: 0.5018
2024-06-03 11:03:52 [INFO]: Epoch 063 - training loss: 0.2861, validation loss: 0.5067
2024-06-03 11:04:01 [INFO]: Epoch 064 - training loss: 0.2831, validation loss: 0.5035
2024-06-03 11:04:09 [INFO]: Epoch 065 - training loss: 0.2843, validation loss: 0.5019
2024-06-03 11:04:17 [INFO]: Epoch 066 - training loss: 0.2824, validation loss: 0.5033
2024-06-03 11:04:26 [INFO]: Epoch 067 - training loss: 0.2821, validation loss: 0.5028
2024-06-03 11:04:35 [INFO]: Epoch 068 - training loss: 0.2815, validation loss: 0.4996
2024-06-03 11:04:43 [INFO]: Epoch 069 - training loss: 0.2796, validation loss: 0.5017
2024-06-03 11:04:51 [INFO]: Epoch 070 - training loss: 0.2797, validation loss: 0.5032
2024-06-03 11:05:00 [INFO]: Epoch 071 - training loss: 0.2784, validation loss: 0.5000
2024-06-03 11:05:08 [INFO]: Epoch 072 - training loss: 0.2785, validation loss: 0.5015
2024-06-03 11:05:17 [INFO]: Epoch 073 - training loss: 0.2783, validation loss: 0.4979
2024-06-03 11:05:25 [INFO]: Epoch 074 - training loss: 0.2785, validation loss: 0.4985
2024-06-03 11:05:33 [INFO]: Epoch 075 - training loss: 0.2782, validation loss: 0.4986
2024-06-03 11:05:42 [INFO]: Epoch 076 - training loss: 0.2725, validation loss: 0.4994
2024-06-03 11:05:50 [INFO]: Epoch 077 - training loss: 0.2714, validation loss: 0.4965
2024-06-03 11:05:58 [INFO]: Epoch 078 - training loss: 0.2716, validation loss: 0.4999
2024-06-03 11:06:06 [INFO]: Epoch 079 - training loss: 0.2696, validation loss: 0.4971
2024-06-03 11:06:15 [INFO]: Epoch 080 - training loss: 0.2696, validation loss: 0.4980
2024-06-03 11:06:23 [INFO]: Epoch 081 - training loss: 0.2667, validation loss: 0.4986
2024-06-03 11:06:32 [INFO]: Epoch 082 - training loss: 0.2684, validation loss: 0.4992
2024-06-03 11:06:40 [INFO]: Epoch 083 - training loss: 0.2683, validation loss: 0.4969
2024-06-03 11:06:49 [INFO]: Epoch 084 - training loss: 0.2690, validation loss: 0.5006
2024-06-03 11:06:57 [INFO]: Epoch 085 - training loss: 0.2703, validation loss: 0.4992
2024-06-03 11:07:06 [INFO]: Epoch 086 - training loss: 0.2656, validation loss: 0.4965
2024-06-03 11:07:14 [INFO]: Epoch 087 - training loss: 0.2639, validation loss: 0.4955
2024-06-03 11:07:22 [INFO]: Epoch 088 - training loss: 0.2658, validation loss: 0.4943
2024-06-03 11:07:30 [INFO]: Epoch 089 - training loss: 0.2639, validation loss: 0.4941
2024-06-03 11:07:39 [INFO]: Epoch 090 - training loss: 0.2618, validation loss: 0.4938
2024-06-03 11:07:47 [INFO]: Epoch 091 - training loss: 0.2615, validation loss: 0.4965
2024-06-03 11:07:56 [INFO]: Epoch 092 - training loss: 0.2598, validation loss: 0.4943
2024-06-03 11:08:04 [INFO]: Epoch 093 - training loss: 0.2601, validation loss: 0.4946
2024-06-03 11:08:12 [INFO]: Epoch 094 - training loss: 0.2575, validation loss: 0.4931
2024-06-03 11:08:20 [INFO]: Epoch 095 - training loss: 0.2568, validation loss: 0.4953
2024-06-03 11:08:29 [INFO]: Epoch 096 - training loss: 0.2587, validation loss: 0.4954
2024-06-03 11:08:37 [INFO]: Epoch 097 - training loss: 0.2588, validation loss: 0.4947
2024-06-03 11:08:45 [INFO]: Epoch 098 - training loss: 0.2594, validation loss: 0.4932
2024-06-03 11:08:53 [INFO]: Epoch 099 - training loss: 0.2570, validation loss: 0.4945
2024-06-03 11:09:02 [INFO]: Epoch 100 - training loss: 0.2563, validation loss: 0.4936
2024-06-03 11:09:02 [INFO]: Finished training. The best model is from epoch#94.
2024-06-03 11:09:04 [INFO]: Saved the model to results_block_rate05/PeMS/SAITS_PeMS/round_2/20240603_T105337/SAITS.pypots
2024-06-03 11:09:07 [INFO]: Successfully saved to results_block_rate05/PeMS/SAITS_PeMS/round_2/imputation.pkl
2024-06-03 11:09:07 [INFO]: Round2 - SAITS on PeMS: MAE=0.3321, MSE=0.7287, MRE=0.3977
2024-06-03 11:09:07 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 11:09:07 [INFO]: Using the given device: cuda:0
2024-06-03 11:09:07 [INFO]: Model files will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_3/20240603_T110907
2024-06-03 11:09:07 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_3/20240603_T110907/tensorboard
2024-06-03 11:09:07 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=8, d_k=256
2024-06-03 11:09:07 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-03 11:09:10 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 78,229,072
2024-06-03 11:09:19 [INFO]: Epoch 001 - training loss: 1.0060, validation loss: 0.8286
2024-06-03 11:09:28 [INFO]: Epoch 002 - training loss: 0.6034, validation loss: 0.6915
2024-06-03 11:09:36 [INFO]: Epoch 003 - training loss: 0.5116, validation loss: 0.6216
2024-06-03 11:09:44 [INFO]: Epoch 004 - training loss: 0.4659, validation loss: 0.5934
2024-06-03 11:09:52 [INFO]: Epoch 005 - training loss: 0.4455, validation loss: 0.5691
2024-06-03 11:10:01 [INFO]: Epoch 006 - training loss: 0.4281, validation loss: 0.5645
2024-06-03 11:10:09 [INFO]: Epoch 007 - training loss: 0.4156, validation loss: 0.5589
2024-06-03 11:10:17 [INFO]: Epoch 008 - training loss: 0.4050, validation loss: 0.5576
2024-06-03 11:10:26 [INFO]: Epoch 009 - training loss: 0.4021, validation loss: 0.5533
2024-06-03 11:10:34 [INFO]: Epoch 010 - training loss: 0.3975, validation loss: 0.5490
2024-06-03 11:10:43 [INFO]: Epoch 011 - training loss: 0.3913, validation loss: 0.5487
2024-06-03 11:10:51 [INFO]: Epoch 012 - training loss: 0.3869, validation loss: 0.5433
2024-06-03 11:10:59 [INFO]: Epoch 013 - training loss: 0.3812, validation loss: 0.5444
2024-06-03 11:11:06 [INFO]: Epoch 014 - training loss: 0.3802, validation loss: 0.5391
2024-06-03 11:11:14 [INFO]: Epoch 015 - training loss: 0.3729, validation loss: 0.5363
2024-06-03 11:11:23 [INFO]: Epoch 016 - training loss: 0.3678, validation loss: 0.5358
2024-06-03 11:11:30 [INFO]: Epoch 017 - training loss: 0.3677, validation loss: 0.5327
2024-06-03 11:11:39 [INFO]: Epoch 018 - training loss: 0.3603, validation loss: 0.5336
2024-06-03 11:11:47 [INFO]: Epoch 019 - training loss: 0.3572, validation loss: 0.5276
2024-06-03 11:11:56 [INFO]: Epoch 020 - training loss: 0.3566, validation loss: 0.5269
2024-06-03 11:12:04 [INFO]: Epoch 021 - training loss: 0.3549, validation loss: 0.5287
2024-06-03 11:12:12 [INFO]: Epoch 022 - training loss: 0.3532, validation loss: 0.5274
2024-06-03 11:12:20 [INFO]: Epoch 023 - training loss: 0.3491, validation loss: 0.5281
2024-06-03 11:12:28 [INFO]: Epoch 024 - training loss: 0.3482, validation loss: 0.5246
2024-06-03 11:12:37 [INFO]: Epoch 025 - training loss: 0.3455, validation loss: 0.5268
2024-06-03 11:12:45 [INFO]: Epoch 026 - training loss: 0.3437, validation loss: 0.5276
2024-06-03 11:12:53 [INFO]: Epoch 027 - training loss: 0.3432, validation loss: 0.5227
2024-06-03 11:13:02 [INFO]: Epoch 028 - training loss: 0.3451, validation loss: 0.5199
2024-06-03 11:13:10 [INFO]: Epoch 029 - training loss: 0.3380, validation loss: 0.5226
2024-06-03 11:13:19 [INFO]: Epoch 030 - training loss: 0.3354, validation loss: 0.5214
2024-06-03 11:13:27 [INFO]: Epoch 031 - training loss: 0.3332, validation loss: 0.5210
2024-06-03 11:13:35 [INFO]: Epoch 032 - training loss: 0.3298, validation loss: 0.5170
2024-06-03 11:13:43 [INFO]: Epoch 033 - training loss: 0.3317, validation loss: 0.5180
2024-06-03 11:13:52 [INFO]: Epoch 034 - training loss: 0.3253, validation loss: 0.5222
2024-06-03 11:14:01 [INFO]: Epoch 035 - training loss: 0.3241, validation loss: 0.5212
2024-06-03 11:14:09 [INFO]: Epoch 036 - training loss: 0.3222, validation loss: 0.5138
2024-06-03 11:14:17 [INFO]: Epoch 037 - training loss: 0.3181, validation loss: 0.5140
2024-06-03 11:14:26 [INFO]: Epoch 038 - training loss: 0.3206, validation loss: 0.5169
2024-06-03 11:14:34 [INFO]: Epoch 039 - training loss: 0.3180, validation loss: 0.5133
2024-06-03 11:14:42 [INFO]: Epoch 040 - training loss: 0.3154, validation loss: 0.5126
2024-06-03 11:14:51 [INFO]: Epoch 041 - training loss: 0.3118, validation loss: 0.5110
2024-06-03 11:14:58 [INFO]: Epoch 042 - training loss: 0.3125, validation loss: 0.5129
2024-06-03 11:15:06 [INFO]: Epoch 043 - training loss: 0.3101, validation loss: 0.5112
2024-06-03 11:15:14 [INFO]: Epoch 044 - training loss: 0.3089, validation loss: 0.5142
2024-06-03 11:15:23 [INFO]: Epoch 045 - training loss: 0.3091, validation loss: 0.5100
2024-06-03 11:15:31 [INFO]: Epoch 046 - training loss: 0.3042, validation loss: 0.5094
2024-06-03 11:15:39 [INFO]: Epoch 047 - training loss: 0.3060, validation loss: 0.5079
2024-06-03 11:15:48 [INFO]: Epoch 048 - training loss: 0.3063, validation loss: 0.5087
2024-06-03 11:15:56 [INFO]: Epoch 049 - training loss: 0.3068, validation loss: 0.5071
2024-06-03 11:16:04 [INFO]: Epoch 050 - training loss: 0.3030, validation loss: 0.5101
2024-06-03 11:16:12 [INFO]: Epoch 051 - training loss: 0.3005, validation loss: 0.5064
2024-06-03 11:16:20 [INFO]: Epoch 052 - training loss: 0.2966, validation loss: 0.5081
2024-06-03 11:16:27 [INFO]: Epoch 053 - training loss: 0.2995, validation loss: 0.5055
2024-06-03 11:16:36 [INFO]: Epoch 054 - training loss: 0.2971, validation loss: 0.5050
2024-06-03 11:16:44 [INFO]: Epoch 055 - training loss: 0.2948, validation loss: 0.5045
2024-06-03 11:16:53 [INFO]: Epoch 056 - training loss: 0.2922, validation loss: 0.5051
2024-06-03 11:17:01 [INFO]: Epoch 057 - training loss: 0.2940, validation loss: 0.5048
2024-06-03 11:17:09 [INFO]: Epoch 058 - training loss: 0.2902, validation loss: 0.5031
2024-06-03 11:17:18 [INFO]: Epoch 059 - training loss: 0.2897, validation loss: 0.5060
2024-06-03 11:17:26 [INFO]: Epoch 060 - training loss: 0.2899, validation loss: 0.5046
2024-06-03 11:17:34 [INFO]: Epoch 061 - training loss: 0.2902, validation loss: 0.5035
2024-06-03 11:17:42 [INFO]: Epoch 062 - training loss: 0.2874, validation loss: 0.5053
2024-06-03 11:17:51 [INFO]: Epoch 063 - training loss: 0.2836, validation loss: 0.5048
2024-06-03 11:17:59 [INFO]: Epoch 064 - training loss: 0.2859, validation loss: 0.5022
2024-06-03 11:18:07 [INFO]: Epoch 065 - training loss: 0.2841, validation loss: 0.5019
2024-06-03 11:18:16 [INFO]: Epoch 066 - training loss: 0.2821, validation loss: 0.4993
2024-06-03 11:18:24 [INFO]: Epoch 067 - training loss: 0.2822, validation loss: 0.4998
2024-06-03 11:18:33 [INFO]: Epoch 068 - training loss: 0.2812, validation loss: 0.5000
2024-06-03 11:18:41 [INFO]: Epoch 069 - training loss: 0.2783, validation loss: 0.5000
2024-06-03 11:18:49 [INFO]: Epoch 070 - training loss: 0.2777, validation loss: 0.5004
2024-06-03 11:18:56 [INFO]: Epoch 071 - training loss: 0.2766, validation loss: 0.5001
2024-06-03 11:19:04 [INFO]: Epoch 072 - training loss: 0.2766, validation loss: 0.4997
2024-06-03 11:19:13 [INFO]: Epoch 073 - training loss: 0.2766, validation loss: 0.4978
2024-06-03 11:19:21 [INFO]: Epoch 074 - training loss: 0.2730, validation loss: 0.5010
2024-06-03 11:19:30 [INFO]: Epoch 075 - training loss: 0.2747, validation loss: 0.4988
2024-06-03 11:19:38 [INFO]: Epoch 076 - training loss: 0.2740, validation loss: 0.4972
2024-06-03 11:19:46 [INFO]: Epoch 077 - training loss: 0.2737, validation loss: 0.4982
2024-06-03 11:19:55 [INFO]: Epoch 078 - training loss: 0.2724, validation loss: 0.4966
2024-06-03 11:20:03 [INFO]: Epoch 079 - training loss: 0.2715, validation loss: 0.4987
2024-06-03 11:20:11 [INFO]: Epoch 080 - training loss: 0.2708, validation loss: 0.4965
2024-06-03 11:20:18 [INFO]: Epoch 081 - training loss: 0.2692, validation loss: 0.4965
2024-06-03 11:20:27 [INFO]: Epoch 082 - training loss: 0.2667, validation loss: 0.4968
2024-06-03 11:20:35 [INFO]: Epoch 083 - training loss: 0.2699, validation loss: 0.4968
2024-06-03 11:20:44 [INFO]: Epoch 084 - training loss: 0.2670, validation loss: 0.4964
2024-06-03 11:20:52 [INFO]: Epoch 085 - training loss: 0.2651, validation loss: 0.4953
2024-06-03 11:21:00 [INFO]: Epoch 086 - training loss: 0.2674, validation loss: 0.4949
2024-06-03 11:21:09 [INFO]: Epoch 087 - training loss: 0.2675, validation loss: 0.4937
2024-06-03 11:21:17 [INFO]: Epoch 088 - training loss: 0.2650, validation loss: 0.4942
2024-06-03 11:21:25 [INFO]: Epoch 089 - training loss: 0.2650, validation loss: 0.4967
2024-06-03 11:21:32 [INFO]: Epoch 090 - training loss: 0.2621, validation loss: 0.4926
2024-06-03 11:21:40 [INFO]: Epoch 091 - training loss: 0.2611, validation loss: 0.4920
2024-06-03 11:21:49 [INFO]: Epoch 092 - training loss: 0.2601, validation loss: 0.4907
2024-06-03 11:21:57 [INFO]: Epoch 093 - training loss: 0.2611, validation loss: 0.4945
2024-06-03 11:22:06 [INFO]: Epoch 094 - training loss: 0.2599, validation loss: 0.4943
2024-06-03 11:22:14 [INFO]: Epoch 095 - training loss: 0.2578, validation loss: 0.4905
2024-06-03 11:22:22 [INFO]: Epoch 096 - training loss: 0.2563, validation loss: 0.4924
2024-06-03 11:22:30 [INFO]: Epoch 097 - training loss: 0.2555, validation loss: 0.4930
2024-06-03 11:22:39 [INFO]: Epoch 098 - training loss: 0.2566, validation loss: 0.4945
2024-06-03 11:22:46 [INFO]: Epoch 099 - training loss: 0.2543, validation loss: 0.4916
2024-06-03 11:22:52 [INFO]: Epoch 100 - training loss: 0.2543, validation loss: 0.4924
2024-06-03 11:22:52 [INFO]: Finished training. The best model is from epoch#95.
2024-06-03 11:22:54 [INFO]: Saved the model to results_block_rate05/PeMS/SAITS_PeMS/round_3/20240603_T110907/SAITS.pypots
2024-06-03 11:22:56 [INFO]: Successfully saved to results_block_rate05/PeMS/SAITS_PeMS/round_3/imputation.pkl
2024-06-03 11:22:56 [INFO]: Round3 - SAITS on PeMS: MAE=0.3313, MSE=0.7252, MRE=0.3967
2024-06-03 11:22:56 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 11:22:56 [INFO]: Using the given device: cuda:0
2024-06-03 11:22:57 [INFO]: Model files will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_4/20240603_T112256
2024-06-03 11:22:57 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/SAITS_PeMS/round_4/20240603_T112256/tensorboard
2024-06-03 11:22:57 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=8, d_k=256
2024-06-03 11:22:57 [WARNING]: ⚠️ d_model is reset to 2048 = n_heads (8) * d_k (256)
2024-06-03 11:22:59 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 78,229,072
2024-06-03 11:23:07 [INFO]: Epoch 001 - training loss: 1.0098, validation loss: 0.8395
2024-06-03 11:23:14 [INFO]: Epoch 002 - training loss: 0.6021, validation loss: 0.6934
2024-06-03 11:23:20 [INFO]: Epoch 003 - training loss: 0.5035, validation loss: 0.6181
2024-06-03 11:23:28 [INFO]: Epoch 004 - training loss: 0.4658, validation loss: 0.5829
2024-06-03 11:23:35 [INFO]: Epoch 005 - training loss: 0.4385, validation loss: 0.5757
2024-06-03 11:23:42 [INFO]: Epoch 006 - training loss: 0.4249, validation loss: 0.5608
2024-06-03 11:23:49 [INFO]: Epoch 007 - training loss: 0.4212, validation loss: 0.5579
2024-06-03 11:23:55 [INFO]: Epoch 008 - training loss: 0.4062, validation loss: 0.5571
2024-06-03 11:24:02 [INFO]: Epoch 009 - training loss: 0.4004, validation loss: 0.5480
2024-06-03 11:24:09 [INFO]: Epoch 010 - training loss: 0.3977, validation loss: 0.5536
2024-06-03 11:24:16 [INFO]: Epoch 011 - training loss: 0.3912, validation loss: 0.5519
2024-06-03 11:24:23 [INFO]: Epoch 012 - training loss: 0.3907, validation loss: 0.5414
2024-06-03 11:24:30 [INFO]: Epoch 013 - training loss: 0.3814, validation loss: 0.5386
2024-06-03 11:24:37 [INFO]: Epoch 014 - training loss: 0.3727, validation loss: 0.5376
2024-06-03 11:24:44 [INFO]: Epoch 015 - training loss: 0.3734, validation loss: 0.5343
2024-06-03 11:24:51 [INFO]: Epoch 016 - training loss: 0.3671, validation loss: 0.5343
2024-06-03 11:24:57 [INFO]: Epoch 017 - training loss: 0.3631, validation loss: 0.5313
2024-06-03 11:25:04 [INFO]: Epoch 018 - training loss: 0.3635, validation loss: 0.5311
2024-06-03 11:25:11 [INFO]: Epoch 019 - training loss: 0.3611, validation loss: 0.5286
2024-06-03 11:25:18 [INFO]: Epoch 020 - training loss: 0.3563, validation loss: 0.5365
2024-06-03 11:25:25 [INFO]: Epoch 021 - training loss: 0.3533, validation loss: 0.5302
2024-06-03 11:25:32 [INFO]: Epoch 022 - training loss: 0.3525, validation loss: 0.5273
2024-06-03 11:25:39 [INFO]: Epoch 023 - training loss: 0.3512, validation loss: 0.5248
2024-06-03 11:25:46 [INFO]: Epoch 024 - training loss: 0.3484, validation loss: 0.5223
2024-06-03 11:25:53 [INFO]: Epoch 025 - training loss: 0.3475, validation loss: 0.5270
2024-06-03 11:25:59 [INFO]: Epoch 026 - training loss: 0.3453, validation loss: 0.5230
2024-06-03 11:26:06 [INFO]: Epoch 027 - training loss: 0.3439, validation loss: 0.5256
2024-06-03 11:26:13 [INFO]: Epoch 028 - training loss: 0.3379, validation loss: 0.5238
2024-06-03 11:26:20 [INFO]: Epoch 029 - training loss: 0.3387, validation loss: 0.5195
2024-06-03 11:26:27 [INFO]: Epoch 030 - training loss: 0.3338, validation loss: 0.5182
2024-06-03 11:26:34 [INFO]: Epoch 031 - training loss: 0.3311, validation loss: 0.5168
2024-06-03 11:26:42 [INFO]: Epoch 032 - training loss: 0.3284, validation loss: 0.5161
2024-06-03 11:26:48 [INFO]: Epoch 033 - training loss: 0.3257, validation loss: 0.5166
2024-06-03 11:26:56 [INFO]: Epoch 034 - training loss: 0.3259, validation loss: 0.5181
2024-06-03 11:27:02 [INFO]: Epoch 035 - training loss: 0.3261, validation loss: 0.5158
2024-06-03 11:27:08 [INFO]: Epoch 036 - training loss: 0.3222, validation loss: 0.5184
2024-06-03 11:27:15 [INFO]: Epoch 037 - training loss: 0.3194, validation loss: 0.5148
2024-06-03 11:27:22 [INFO]: Epoch 038 - training loss: 0.3181, validation loss: 0.5151
2024-06-03 11:27:29 [INFO]: Epoch 039 - training loss: 0.3166, validation loss: 0.5145
2024-06-03 11:27:37 [INFO]: Epoch 040 - training loss: 0.3140, validation loss: 0.5103
2024-06-03 11:27:44 [INFO]: Epoch 041 - training loss: 0.3144, validation loss: 0.5131
2024-06-03 11:27:51 [INFO]: Epoch 042 - training loss: 0.3115, validation loss: 0.5137
2024-06-03 11:27:57 [INFO]: Epoch 043 - training loss: 0.3107, validation loss: 0.5126
2024-06-03 11:28:04 [INFO]: Epoch 044 - training loss: 0.3085, validation loss: 0.5144
2024-06-03 11:28:11 [INFO]: Epoch 045 - training loss: 0.3080, validation loss: 0.5104
2024-06-03 11:28:17 [INFO]: Epoch 046 - training loss: 0.3078, validation loss: 0.5119
2024-06-03 11:28:25 [INFO]: Epoch 047 - training loss: 0.3063, validation loss: 0.5078
2024-06-03 11:28:32 [INFO]: Epoch 048 - training loss: 0.3052, validation loss: 0.5086
2024-06-03 11:28:39 [INFO]: Epoch 049 - training loss: 0.3005, validation loss: 0.5096
2024-06-03 11:28:46 [INFO]: Epoch 050 - training loss: 0.3000, validation loss: 0.5076
2024-06-03 11:28:53 [INFO]: Epoch 051 - training loss: 0.2991, validation loss: 0.5073
2024-06-03 11:29:00 [INFO]: Epoch 052 - training loss: 0.2984, validation loss: 0.5078
2024-06-03 11:29:07 [INFO]: Epoch 053 - training loss: 0.2993, validation loss: 0.5034
2024-06-03 11:29:13 [INFO]: Epoch 054 - training loss: 0.2979, validation loss: 0.5073
2024-06-03 11:29:19 [INFO]: Epoch 055 - training loss: 0.2957, validation loss: 0.5047
2024-06-03 11:29:26 [INFO]: Epoch 056 - training loss: 0.2963, validation loss: 0.5065
2024-06-03 11:29:33 [INFO]: Epoch 057 - training loss: 0.2937, validation loss: 0.5096
2024-06-03 11:29:40 [INFO]: Epoch 058 - training loss: 0.2914, validation loss: 0.5043
2024-06-03 11:29:46 [INFO]: Epoch 059 - training loss: 0.2910, validation loss: 0.5035
2024-06-03 11:29:53 [INFO]: Epoch 060 - training loss: 0.2867, validation loss: 0.5050
2024-06-03 11:30:00 [INFO]: Epoch 061 - training loss: 0.2869, validation loss: 0.5025
2024-06-03 11:30:06 [INFO]: Epoch 062 - training loss: 0.2868, validation loss: 0.5024
2024-06-03 11:30:13 [INFO]: Epoch 063 - training loss: 0.2865, validation loss: 0.5025
2024-06-03 11:30:19 [INFO]: Epoch 064 - training loss: 0.2839, validation loss: 0.4999
2024-06-03 11:30:25 [INFO]: Epoch 065 - training loss: 0.2854, validation loss: 0.5025
2024-06-03 11:30:31 [INFO]: Epoch 066 - training loss: 0.2833, validation loss: 0.5006
2024-06-03 11:30:38 [INFO]: Epoch 067 - training loss: 0.2820, validation loss: 0.5017
2024-06-03 11:30:44 [INFO]: Epoch 068 - training loss: 0.2820, validation loss: 0.5033
2024-06-03 11:30:50 [INFO]: Epoch 069 - training loss: 0.2805, validation loss: 0.4995
2024-06-03 11:30:57 [INFO]: Epoch 070 - training loss: 0.2785, validation loss: 0.5008
2024-06-03 11:31:03 [INFO]: Epoch 071 - training loss: 0.2789, validation loss: 0.4984
2024-06-03 11:31:09 [INFO]: Epoch 072 - training loss: 0.2796, validation loss: 0.5000
2024-06-03 11:31:15 [INFO]: Epoch 073 - training loss: 0.2786, validation loss: 0.4981
2024-06-03 11:31:21 [INFO]: Epoch 074 - training loss: 0.2747, validation loss: 0.5007
2024-06-03 11:31:27 [INFO]: Epoch 075 - training loss: 0.2761, validation loss: 0.4985
2024-06-03 11:31:33 [INFO]: Epoch 076 - training loss: 0.2760, validation loss: 0.4991
2024-06-03 11:31:39 [INFO]: Epoch 077 - training loss: 0.2708, validation loss: 0.4970
2024-06-03 11:31:45 [INFO]: Epoch 078 - training loss: 0.2713, validation loss: 0.4956
2024-06-03 11:31:52 [INFO]: Epoch 079 - training loss: 0.2736, validation loss: 0.4964
2024-06-03 11:31:58 [INFO]: Epoch 080 - training loss: 0.2708, validation loss: 0.4964
2024-06-03 11:32:05 [INFO]: Epoch 081 - training loss: 0.2669, validation loss: 0.4958
2024-06-03 11:32:11 [INFO]: Epoch 082 - training loss: 0.2662, validation loss: 0.4959
2024-06-03 11:32:17 [INFO]: Epoch 083 - training loss: 0.2695, validation loss: 0.4956
2024-06-03 11:32:23 [INFO]: Epoch 084 - training loss: 0.2677, validation loss: 0.4967
2024-06-03 11:32:29 [INFO]: Epoch 085 - training loss: 0.2653, validation loss: 0.4944
2024-06-03 11:32:35 [INFO]: Epoch 086 - training loss: 0.2648, validation loss: 0.4974
2024-06-03 11:32:41 [INFO]: Epoch 087 - training loss: 0.2643, validation loss: 0.4963
2024-06-03 11:32:47 [INFO]: Epoch 088 - training loss: 0.2645, validation loss: 0.4957
2024-06-03 11:32:54 [INFO]: Epoch 089 - training loss: 0.2632, validation loss: 0.4947
2024-06-03 11:33:00 [INFO]: Epoch 090 - training loss: 0.2627, validation loss: 0.4951
2024-06-03 11:33:07 [INFO]: Epoch 091 - training loss: 0.2586, validation loss: 0.4933
2024-06-03 11:33:13 [INFO]: Epoch 092 - training loss: 0.2598, validation loss: 0.4886
2024-06-03 11:33:19 [INFO]: Epoch 093 - training loss: 0.2592, validation loss: 0.4920
2024-06-03 11:33:26 [INFO]: Epoch 094 - training loss: 0.2585, validation loss: 0.4953
2024-06-03 11:33:32 [INFO]: Epoch 095 - training loss: 0.2565, validation loss: 0.4931
2024-06-03 11:33:38 [INFO]: Epoch 096 - training loss: 0.2576, validation loss: 0.4931
2024-06-03 11:33:43 [INFO]: Epoch 097 - training loss: 0.2576, validation loss: 0.4907
2024-06-03 11:33:50 [INFO]: Epoch 098 - training loss: 0.2570, validation loss: 0.4926
2024-06-03 11:33:56 [INFO]: Epoch 099 - training loss: 0.2552, validation loss: 0.4950
2024-06-03 11:34:02 [INFO]: Epoch 100 - training loss: 0.2538, validation loss: 0.4917
2024-06-03 11:34:02 [INFO]: Finished training. The best model is from epoch#92.
2024-06-03 11:34:04 [INFO]: Saved the model to results_block_rate05/PeMS/SAITS_PeMS/round_4/20240603_T112256/SAITS.pypots
2024-06-03 11:34:06 [INFO]: Successfully saved to results_block_rate05/PeMS/SAITS_PeMS/round_4/imputation.pkl
2024-06-03 11:34:06 [INFO]: Round4 - SAITS on PeMS: MAE=0.3327, MSE=0.7256, MRE=0.3984
2024-06-03 11:34:06 [INFO]: Done! Final results:
Averaged SAITS (78,229,072 params) on PeMS: MAE=0.3313 ± 0.0009729311138210237, MSE=0.7259 ± 0.001581146817767596, MRE=0.3967 ± 0.00116495080939746, average inference time=0.44
