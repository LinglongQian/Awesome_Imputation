2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_0/20240603_T100955
2024-06-03 10:09:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_0/20240603_T100955/tensorboard
2024-06-03 10:10:00 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 10:10:14 [INFO]: Epoch 001 - training loss: 2.9208, validation loss: 1.6239
2024-06-03 10:10:21 [INFO]: Epoch 002 - training loss: 0.9984, validation loss: 1.0054
2024-06-03 10:10:30 [INFO]: Epoch 003 - training loss: 0.6690, validation loss: 0.8362
2024-06-03 10:10:37 [INFO]: Epoch 004 - training loss: 0.5571, validation loss: 0.7688
2024-06-03 10:10:46 [INFO]: Epoch 005 - training loss: 0.5071, validation loss: 0.7050
2024-06-03 10:10:56 [INFO]: Epoch 006 - training loss: 0.4747, validation loss: 0.6663
2024-06-03 10:11:05 [INFO]: Epoch 007 - training loss: 0.4498, validation loss: 0.6400
2024-06-03 10:11:14 [INFO]: Epoch 008 - training loss: 0.4417, validation loss: 0.6178
2024-06-03 10:11:23 [INFO]: Epoch 009 - training loss: 0.4242, validation loss: 0.6017
2024-06-03 10:11:33 [INFO]: Epoch 010 - training loss: 0.4072, validation loss: 0.5974
2024-06-03 10:11:42 [INFO]: Epoch 011 - training loss: 0.3989, validation loss: 0.5946
2024-06-03 10:11:51 [INFO]: Epoch 012 - training loss: 0.3844, validation loss: 0.5791
2024-06-03 10:12:00 [INFO]: Epoch 013 - training loss: 0.3780, validation loss: 0.5714
2024-06-03 10:12:08 [INFO]: Epoch 014 - training loss: 0.3731, validation loss: 0.5667
2024-06-03 10:12:17 [INFO]: Epoch 015 - training loss: 0.3688, validation loss: 0.5778
2024-06-03 10:12:27 [INFO]: Epoch 016 - training loss: 0.3641, validation loss: 0.5618
2024-06-03 10:12:35 [INFO]: Epoch 017 - training loss: 0.3527, validation loss: 0.5603
2024-06-03 10:12:44 [INFO]: Epoch 018 - training loss: 0.3438, validation loss: 0.5522
2024-06-03 10:12:53 [INFO]: Epoch 019 - training loss: 0.3386, validation loss: 0.5476
2024-06-03 10:13:02 [INFO]: Epoch 020 - training loss: 0.3344, validation loss: 0.5519
2024-06-03 10:13:11 [INFO]: Epoch 021 - training loss: 0.3314, validation loss: 0.5486
2024-06-03 10:13:20 [INFO]: Epoch 022 - training loss: 0.3275, validation loss: 0.5442
2024-06-03 10:13:30 [INFO]: Epoch 023 - training loss: 0.3256, validation loss: 0.5444
2024-06-03 10:13:39 [INFO]: Epoch 024 - training loss: 0.3236, validation loss: 0.5475
2024-06-03 10:13:47 [INFO]: Epoch 025 - training loss: 0.3180, validation loss: 0.5403
2024-06-03 10:13:55 [INFO]: Epoch 026 - training loss: 0.3168, validation loss: 0.5426
2024-06-03 10:14:04 [INFO]: Epoch 027 - training loss: 0.3156, validation loss: 0.5435
2024-06-03 10:14:13 [INFO]: Epoch 028 - training loss: 0.3092, validation loss: 0.5440
2024-06-03 10:14:22 [INFO]: Epoch 029 - training loss: 0.3146, validation loss: 0.5452
2024-06-03 10:14:32 [INFO]: Epoch 030 - training loss: 0.3137, validation loss: 0.5438
2024-06-03 10:14:41 [INFO]: Epoch 031 - training loss: 0.3013, validation loss: 0.5401
2024-06-03 10:14:50 [INFO]: Epoch 032 - training loss: 0.3006, validation loss: 0.5392
2024-06-03 10:15:00 [INFO]: Epoch 033 - training loss: 0.3001, validation loss: 0.5425
2024-06-03 10:15:08 [INFO]: Epoch 034 - training loss: 0.3021, validation loss: 0.5398
2024-06-03 10:15:17 [INFO]: Epoch 035 - training loss: 0.2959, validation loss: 0.5385
2024-06-03 10:15:26 [INFO]: Epoch 036 - training loss: 0.2948, validation loss: 0.5374
2024-06-03 10:15:34 [INFO]: Epoch 037 - training loss: 0.2941, validation loss: 0.5384
2024-06-03 10:15:43 [INFO]: Epoch 038 - training loss: 0.2938, validation loss: 0.5363
2024-06-03 10:15:52 [INFO]: Epoch 039 - training loss: 0.2898, validation loss: 0.5375
2024-06-03 10:16:01 [INFO]: Epoch 040 - training loss: 0.2863, validation loss: 0.5407
2024-06-03 10:16:10 [INFO]: Epoch 041 - training loss: 0.2848, validation loss: 0.5358
2024-06-03 10:16:19 [INFO]: Epoch 042 - training loss: 0.2857, validation loss: 0.5356
2024-06-03 10:16:28 [INFO]: Epoch 043 - training loss: 0.2857, validation loss: 0.5327
2024-06-03 10:16:37 [INFO]: Epoch 044 - training loss: 0.2856, validation loss: 0.5379
2024-06-03 10:16:46 [INFO]: Epoch 045 - training loss: 0.2858, validation loss: 0.5431
2024-06-03 10:16:54 [INFO]: Epoch 046 - training loss: 0.2863, validation loss: 0.5360
2024-06-03 10:17:03 [INFO]: Epoch 047 - training loss: 0.2806, validation loss: 0.5384
2024-06-03 10:17:11 [INFO]: Epoch 048 - training loss: 0.2797, validation loss: 0.5350
2024-06-03 10:17:20 [INFO]: Epoch 049 - training loss: 0.2802, validation loss: 0.5421
2024-06-03 10:17:29 [INFO]: Epoch 050 - training loss: 0.2804, validation loss: 0.5332
2024-06-03 10:17:38 [INFO]: Epoch 051 - training loss: 0.2776, validation loss: 0.5384
2024-06-03 10:17:48 [INFO]: Epoch 052 - training loss: 0.2760, validation loss: 0.5350
2024-06-03 10:17:57 [INFO]: Epoch 053 - training loss: 0.2767, validation loss: 0.5430
2024-06-03 10:17:57 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:17:57 [INFO]: Finished training. The best model is from epoch#43.
2024-06-03 10:17:57 [INFO]: Saved the model to results_block_rate05/PeMS/DLinear_PeMS/round_0/20240603_T100955/DLinear.pypots
2024-06-03 10:18:00 [INFO]: Successfully saved to results_block_rate05/PeMS/DLinear_PeMS/round_0/imputation.pkl
2024-06-03 10:18:00 [INFO]: Round0 - DLinear on PeMS: MAE=0.4321, MSE=0.7961, MRE=0.5174
2024-06-03 10:18:00 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:18:00 [INFO]: Using the given device: cuda:0
2024-06-03 10:18:00 [INFO]: Model files will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_1/20240603_T101800
2024-06-03 10:18:00 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_1/20240603_T101800/tensorboard
2024-06-03 10:18:00 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 10:18:10 [INFO]: Epoch 001 - training loss: 2.9193, validation loss: 1.3035
2024-06-03 10:18:19 [INFO]: Epoch 002 - training loss: 0.9278, validation loss: 0.9278
2024-06-03 10:18:28 [INFO]: Epoch 003 - training loss: 0.6737, validation loss: 0.9556
2024-06-03 10:18:36 [INFO]: Epoch 004 - training loss: 0.5721, validation loss: 0.8623
2024-06-03 10:18:44 [INFO]: Epoch 005 - training loss: 0.5167, validation loss: 0.8168
2024-06-03 10:18:52 [INFO]: Epoch 006 - training loss: 0.4938, validation loss: 0.7643
2024-06-03 10:19:01 [INFO]: Epoch 007 - training loss: 0.4867, validation loss: 0.7543
2024-06-03 10:19:10 [INFO]: Epoch 008 - training loss: 0.4681, validation loss: 0.7564
2024-06-03 10:19:19 [INFO]: Epoch 009 - training loss: 0.4566, validation loss: 0.7226
2024-06-03 10:19:28 [INFO]: Epoch 010 - training loss: 0.4446, validation loss: 0.7279
2024-06-03 10:19:38 [INFO]: Epoch 011 - training loss: 0.4297, validation loss: 0.7296
2024-06-03 10:19:47 [INFO]: Epoch 012 - training loss: 0.4231, validation loss: 0.7304
2024-06-03 10:19:57 [INFO]: Epoch 013 - training loss: 0.4165, validation loss: 0.7239
2024-06-03 10:20:06 [INFO]: Epoch 014 - training loss: 0.4094, validation loss: 0.7171
2024-06-03 10:20:15 [INFO]: Epoch 015 - training loss: 0.4100, validation loss: 0.7137
2024-06-03 10:20:23 [INFO]: Epoch 016 - training loss: 0.4047, validation loss: 0.6956
2024-06-03 10:20:32 [INFO]: Epoch 017 - training loss: 0.3961, validation loss: 0.6930
2024-06-03 10:20:41 [INFO]: Epoch 018 - training loss: 0.3884, validation loss: 0.7226
2024-06-03 10:20:50 [INFO]: Epoch 019 - training loss: 0.3885, validation loss: 0.7064
2024-06-03 10:21:00 [INFO]: Epoch 020 - training loss: 0.3771, validation loss: 0.6885
2024-06-03 10:21:09 [INFO]: Epoch 021 - training loss: 0.3731, validation loss: 0.6931
2024-06-03 10:21:18 [INFO]: Epoch 022 - training loss: 0.3697, validation loss: 0.6868
2024-06-03 10:21:26 [INFO]: Epoch 023 - training loss: 0.3643, validation loss: 0.6474
2024-06-03 10:21:35 [INFO]: Epoch 024 - training loss: 0.3683, validation loss: 0.6782
2024-06-03 10:21:44 [INFO]: Epoch 025 - training loss: 0.3634, validation loss: 0.6604
2024-06-03 10:21:52 [INFO]: Epoch 026 - training loss: 0.3639, validation loss: 0.6860
2024-06-03 10:22:01 [INFO]: Epoch 027 - training loss: 0.3596, validation loss: 0.6538
2024-06-03 10:22:09 [INFO]: Epoch 028 - training loss: 0.3524, validation loss: 0.6556
2024-06-03 10:22:19 [INFO]: Epoch 029 - training loss: 0.3438, validation loss: 0.6564
2024-06-03 10:22:28 [INFO]: Epoch 030 - training loss: 0.3435, validation loss: 0.6681
2024-06-03 10:22:37 [INFO]: Epoch 031 - training loss: 0.3371, validation loss: 0.6527
2024-06-03 10:22:46 [INFO]: Epoch 032 - training loss: 0.3384, validation loss: 0.6512
2024-06-03 10:22:55 [INFO]: Epoch 033 - training loss: 0.3301, validation loss: 0.6509
2024-06-03 10:22:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:22:55 [INFO]: Finished training. The best model is from epoch#23.
2024-06-03 10:22:56 [INFO]: Saved the model to results_block_rate05/PeMS/DLinear_PeMS/round_1/20240603_T101800/DLinear.pypots
2024-06-03 10:22:59 [INFO]: Successfully saved to results_block_rate05/PeMS/DLinear_PeMS/round_1/imputation.pkl
2024-06-03 10:22:59 [INFO]: Round1 - DLinear on PeMS: MAE=0.5647, MSE=1.0841, MRE=0.6762
2024-06-03 10:22:59 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:22:59 [INFO]: Using the given device: cuda:0
2024-06-03 10:22:59 [INFO]: Model files will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_2/20240603_T102259
2024-06-03 10:22:59 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_2/20240603_T102259/tensorboard
2024-06-03 10:22:59 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 10:23:09 [INFO]: Epoch 001 - training loss: 3.1340, validation loss: 1.5956
2024-06-03 10:23:18 [INFO]: Epoch 002 - training loss: 0.9673, validation loss: 1.0329
2024-06-03 10:23:28 [INFO]: Epoch 003 - training loss: 0.6602, validation loss: 0.8651
2024-06-03 10:23:36 [INFO]: Epoch 004 - training loss: 0.5534, validation loss: 0.7843
2024-06-03 10:23:44 [INFO]: Epoch 005 - training loss: 0.5052, validation loss: 0.7089
2024-06-03 10:23:52 [INFO]: Epoch 006 - training loss: 0.4702, validation loss: 0.6616
2024-06-03 10:24:01 [INFO]: Epoch 007 - training loss: 0.4434, validation loss: 0.6225
2024-06-03 10:24:10 [INFO]: Epoch 008 - training loss: 0.4294, validation loss: 0.6036
2024-06-03 10:24:19 [INFO]: Epoch 009 - training loss: 0.4132, validation loss: 0.5872
2024-06-03 10:24:29 [INFO]: Epoch 010 - training loss: 0.3955, validation loss: 0.5819
2024-06-03 10:24:37 [INFO]: Epoch 011 - training loss: 0.3876, validation loss: 0.5781
2024-06-03 10:24:46 [INFO]: Epoch 012 - training loss: 0.3826, validation loss: 0.5716
2024-06-03 10:24:55 [INFO]: Epoch 013 - training loss: 0.3755, validation loss: 0.5619
2024-06-03 10:25:04 [INFO]: Epoch 014 - training loss: 0.3686, validation loss: 0.5610
2024-06-03 10:25:12 [INFO]: Epoch 015 - training loss: 0.3637, validation loss: 0.5549
2024-06-03 10:25:20 [INFO]: Epoch 016 - training loss: 0.3530, validation loss: 0.5592
2024-06-03 10:25:28 [INFO]: Epoch 017 - training loss: 0.3484, validation loss: 0.5475
2024-06-03 10:25:37 [INFO]: Epoch 018 - training loss: 0.3434, validation loss: 0.5430
2024-06-03 10:25:45 [INFO]: Epoch 019 - training loss: 0.3387, validation loss: 0.5423
2024-06-03 10:25:54 [INFO]: Epoch 020 - training loss: 0.3329, validation loss: 0.5415
2024-06-03 10:26:03 [INFO]: Epoch 021 - training loss: 0.3290, validation loss: 0.5469
2024-06-03 10:26:12 [INFO]: Epoch 022 - training loss: 0.3277, validation loss: 0.5414
2024-06-03 10:26:19 [INFO]: Epoch 023 - training loss: 0.3212, validation loss: 0.5364
2024-06-03 10:26:27 [INFO]: Epoch 024 - training loss: 0.3209, validation loss: 0.5384
2024-06-03 10:26:36 [INFO]: Epoch 025 - training loss: 0.3148, validation loss: 0.5374
2024-06-03 10:26:44 [INFO]: Epoch 026 - training loss: 0.3102, validation loss: 0.5403
2024-06-03 10:26:52 [INFO]: Epoch 027 - training loss: 0.3099, validation loss: 0.5379
2024-06-03 10:27:00 [INFO]: Epoch 028 - training loss: 0.3077, validation loss: 0.5368
2024-06-03 10:27:09 [INFO]: Epoch 029 - training loss: 0.3072, validation loss: 0.5341
2024-06-03 10:27:17 [INFO]: Epoch 030 - training loss: 0.3032, validation loss: 0.5361
2024-06-03 10:27:25 [INFO]: Epoch 031 - training loss: 0.3001, validation loss: 0.5339
2024-06-03 10:27:34 [INFO]: Epoch 032 - training loss: 0.3015, validation loss: 0.5367
2024-06-03 10:27:42 [INFO]: Epoch 033 - training loss: 0.2961, validation loss: 0.5363
2024-06-03 10:27:51 [INFO]: Epoch 034 - training loss: 0.2991, validation loss: 0.5319
2024-06-03 10:28:00 [INFO]: Epoch 035 - training loss: 0.3017, validation loss: 0.5371
2024-06-03 10:28:08 [INFO]: Epoch 036 - training loss: 0.2964, validation loss: 0.5353
2024-06-03 10:28:16 [INFO]: Epoch 037 - training loss: 0.2938, validation loss: 0.5366
2024-06-03 10:28:24 [INFO]: Epoch 038 - training loss: 0.2898, validation loss: 0.5346
2024-06-03 10:28:32 [INFO]: Epoch 039 - training loss: 0.2896, validation loss: 0.5352
2024-06-03 10:28:39 [INFO]: Epoch 040 - training loss: 0.2889, validation loss: 0.5349
2024-06-03 10:28:47 [INFO]: Epoch 041 - training loss: 0.2872, validation loss: 0.5321
2024-06-03 10:28:55 [INFO]: Epoch 042 - training loss: 0.2848, validation loss: 0.5300
2024-06-03 10:29:04 [INFO]: Epoch 043 - training loss: 0.2858, validation loss: 0.5301
2024-06-03 10:29:12 [INFO]: Epoch 044 - training loss: 0.2850, validation loss: 0.5340
2024-06-03 10:29:21 [INFO]: Epoch 045 - training loss: 0.2825, validation loss: 0.5317
2024-06-03 10:29:29 [INFO]: Epoch 046 - training loss: 0.2820, validation loss: 0.5320
2024-06-03 10:29:38 [INFO]: Epoch 047 - training loss: 0.2817, validation loss: 0.5357
2024-06-03 10:29:46 [INFO]: Epoch 048 - training loss: 0.2809, validation loss: 0.5324
2024-06-03 10:29:55 [INFO]: Epoch 049 - training loss: 0.2802, validation loss: 0.5347
2024-06-03 10:30:03 [INFO]: Epoch 050 - training loss: 0.2787, validation loss: 0.5319
2024-06-03 10:30:11 [INFO]: Epoch 051 - training loss: 0.2799, validation loss: 0.5291
2024-06-03 10:30:19 [INFO]: Epoch 052 - training loss: 0.2804, validation loss: 0.5343
2024-06-03 10:30:28 [INFO]: Epoch 053 - training loss: 0.2867, validation loss: 0.5301
2024-06-03 10:30:36 [INFO]: Epoch 054 - training loss: 0.2782, validation loss: 0.5377
2024-06-03 10:30:46 [INFO]: Epoch 055 - training loss: 0.2790, validation loss: 0.5316
2024-06-03 10:30:55 [INFO]: Epoch 056 - training loss: 0.2771, validation loss: 0.5297
2024-06-03 10:31:03 [INFO]: Epoch 057 - training loss: 0.2783, validation loss: 0.5376
2024-06-03 10:31:12 [INFO]: Epoch 058 - training loss: 0.2799, validation loss: 0.5310
2024-06-03 10:31:21 [INFO]: Epoch 059 - training loss: 0.2762, validation loss: 0.5308
2024-06-03 10:31:29 [INFO]: Epoch 060 - training loss: 0.2743, validation loss: 0.5311
2024-06-03 10:31:37 [INFO]: Epoch 061 - training loss: 0.2747, validation loss: 0.5269
2024-06-03 10:31:46 [INFO]: Epoch 062 - training loss: 0.2729, validation loss: 0.5327
2024-06-03 10:31:54 [INFO]: Epoch 063 - training loss: 0.2751, validation loss: 0.5298
2024-06-03 10:32:02 [INFO]: Epoch 064 - training loss: 0.2708, validation loss: 0.5303
2024-06-03 10:32:10 [INFO]: Epoch 065 - training loss: 0.2732, validation loss: 0.5289
2024-06-03 10:32:18 [INFO]: Epoch 066 - training loss: 0.2709, validation loss: 0.5334
2024-06-03 10:32:27 [INFO]: Epoch 067 - training loss: 0.2719, validation loss: 0.5331
2024-06-03 10:32:35 [INFO]: Epoch 068 - training loss: 0.2707, validation loss: 0.5318
2024-06-03 10:32:43 [INFO]: Epoch 069 - training loss: 0.2704, validation loss: 0.5304
2024-06-03 10:32:52 [INFO]: Epoch 070 - training loss: 0.2689, validation loss: 0.5328
2024-06-03 10:33:00 [INFO]: Epoch 071 - training loss: 0.2708, validation loss: 0.5280
2024-06-03 10:33:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:33:00 [INFO]: Finished training. The best model is from epoch#61.
2024-06-03 10:33:00 [INFO]: Saved the model to results_block_rate05/PeMS/DLinear_PeMS/round_2/20240603_T102259/DLinear.pypots
2024-06-03 10:33:03 [INFO]: Successfully saved to results_block_rate05/PeMS/DLinear_PeMS/round_2/imputation.pkl
2024-06-03 10:33:03 [INFO]: Round2 - DLinear on PeMS: MAE=0.4154, MSE=0.7785, MRE=0.4974
2024-06-03 10:33:03 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:33:03 [INFO]: Using the given device: cuda:0
2024-06-03 10:33:03 [INFO]: Model files will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_3/20240603_T103303
2024-06-03 10:33:03 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_3/20240603_T103303/tensorboard
2024-06-03 10:33:03 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 10:33:12 [INFO]: Epoch 001 - training loss: 3.0644, validation loss: 1.5931
2024-06-03 10:33:19 [INFO]: Epoch 002 - training loss: 1.0338, validation loss: 1.0334
2024-06-03 10:33:26 [INFO]: Epoch 003 - training loss: 0.6830, validation loss: 0.8412
2024-06-03 10:33:33 [INFO]: Epoch 004 - training loss: 0.5700, validation loss: 0.7610
2024-06-03 10:33:41 [INFO]: Epoch 005 - training loss: 0.5105, validation loss: 0.7005
2024-06-03 10:33:50 [INFO]: Epoch 006 - training loss: 0.4741, validation loss: 0.6737
2024-06-03 10:33:58 [INFO]: Epoch 007 - training loss: 0.4534, validation loss: 0.6538
2024-06-03 10:34:06 [INFO]: Epoch 008 - training loss: 0.4350, validation loss: 0.6365
2024-06-03 10:34:15 [INFO]: Epoch 009 - training loss: 0.4184, validation loss: 0.6144
2024-06-03 10:34:23 [INFO]: Epoch 010 - training loss: 0.4136, validation loss: 0.6122
2024-06-03 10:34:31 [INFO]: Epoch 011 - training loss: 0.3964, validation loss: 0.5953
2024-06-03 10:34:39 [INFO]: Epoch 012 - training loss: 0.3883, validation loss: 0.5796
2024-06-03 10:34:48 [INFO]: Epoch 013 - training loss: 0.3793, validation loss: 0.5740
2024-06-03 10:34:55 [INFO]: Epoch 014 - training loss: 0.3717, validation loss: 0.5682
2024-06-03 10:35:03 [INFO]: Epoch 015 - training loss: 0.3748, validation loss: 0.5682
2024-06-03 10:35:11 [INFO]: Epoch 016 - training loss: 0.3622, validation loss: 0.5619
2024-06-03 10:35:19 [INFO]: Epoch 017 - training loss: 0.3565, validation loss: 0.5656
2024-06-03 10:35:27 [INFO]: Epoch 018 - training loss: 0.3493, validation loss: 0.5593
2024-06-03 10:35:35 [INFO]: Epoch 019 - training loss: 0.3412, validation loss: 0.5582
2024-06-03 10:35:43 [INFO]: Epoch 020 - training loss: 0.3408, validation loss: 0.5470
2024-06-03 10:35:51 [INFO]: Epoch 021 - training loss: 0.3433, validation loss: 0.5489
2024-06-03 10:35:59 [INFO]: Epoch 022 - training loss: 0.3305, validation loss: 0.5533
2024-06-03 10:36:07 [INFO]: Epoch 023 - training loss: 0.3252, validation loss: 0.5457
2024-06-03 10:36:15 [INFO]: Epoch 024 - training loss: 0.3233, validation loss: 0.5416
2024-06-03 10:36:23 [INFO]: Epoch 025 - training loss: 0.3167, validation loss: 0.5405
2024-06-03 10:36:31 [INFO]: Epoch 026 - training loss: 0.3143, validation loss: 0.5402
2024-06-03 10:36:39 [INFO]: Epoch 027 - training loss: 0.3154, validation loss: 0.5423
2024-06-03 10:36:45 [INFO]: Epoch 028 - training loss: 0.3118, validation loss: 0.5437
2024-06-03 10:36:53 [INFO]: Epoch 029 - training loss: 0.3054, validation loss: 0.5437
2024-06-03 10:37:00 [INFO]: Epoch 030 - training loss: 0.3161, validation loss: 0.5403
2024-06-03 10:37:09 [INFO]: Epoch 031 - training loss: 0.3098, validation loss: 0.5446
2024-06-03 10:37:17 [INFO]: Epoch 032 - training loss: 0.3038, validation loss: 0.5413
2024-06-03 10:37:24 [INFO]: Epoch 033 - training loss: 0.2970, validation loss: 0.5410
2024-06-03 10:37:32 [INFO]: Epoch 034 - training loss: 0.2967, validation loss: 0.5373
2024-06-03 10:37:40 [INFO]: Epoch 035 - training loss: 0.2936, validation loss: 0.5351
2024-06-03 10:37:49 [INFO]: Epoch 036 - training loss: 0.2941, validation loss: 0.5343
2024-06-03 10:37:56 [INFO]: Epoch 037 - training loss: 0.2942, validation loss: 0.5342
2024-06-03 10:38:04 [INFO]: Epoch 038 - training loss: 0.2951, validation loss: 0.5405
2024-06-03 10:38:11 [INFO]: Epoch 039 - training loss: 0.2921, validation loss: 0.5373
2024-06-03 10:38:19 [INFO]: Epoch 040 - training loss: 0.2914, validation loss: 0.5364
2024-06-03 10:38:27 [INFO]: Epoch 041 - training loss: 0.2891, validation loss: 0.5371
2024-06-03 10:38:34 [INFO]: Epoch 042 - training loss: 0.2855, validation loss: 0.5388
2024-06-03 10:38:43 [INFO]: Epoch 043 - training loss: 0.2871, validation loss: 0.5372
2024-06-03 10:38:51 [INFO]: Epoch 044 - training loss: 0.2849, validation loss: 0.5401
2024-06-03 10:38:59 [INFO]: Epoch 045 - training loss: 0.2866, validation loss: 0.5391
2024-06-03 10:39:07 [INFO]: Epoch 046 - training loss: 0.2823, validation loss: 0.5375
2024-06-03 10:39:15 [INFO]: Epoch 047 - training loss: 0.2811, validation loss: 0.5403
2024-06-03 10:39:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:39:15 [INFO]: Finished training. The best model is from epoch#37.
2024-06-03 10:39:15 [INFO]: Saved the model to results_block_rate05/PeMS/DLinear_PeMS/round_3/20240603_T103303/DLinear.pypots
2024-06-03 10:39:17 [INFO]: Successfully saved to results_block_rate05/PeMS/DLinear_PeMS/round_3/imputation.pkl
2024-06-03 10:39:17 [INFO]: Round3 - DLinear on PeMS: MAE=0.4239, MSE=0.7947, MRE=0.5076
2024-06-03 10:39:17 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:39:17 [INFO]: Using the given device: cuda:0
2024-06-03 10:39:17 [INFO]: Model files will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_4/20240603_T103917
2024-06-03 10:39:17 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/DLinear_PeMS/round_4/20240603_T103917/tensorboard
2024-06-03 10:39:18 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 10:39:26 [INFO]: Epoch 001 - training loss: 3.4453, validation loss: 1.3483
2024-06-03 10:39:34 [INFO]: Epoch 002 - training loss: 0.9282, validation loss: 0.9094
2024-06-03 10:39:42 [INFO]: Epoch 003 - training loss: 0.6714, validation loss: 0.8218
2024-06-03 10:39:49 [INFO]: Epoch 004 - training loss: 0.5679, validation loss: 0.7599
2024-06-03 10:39:56 [INFO]: Epoch 005 - training loss: 0.5180, validation loss: 0.7095
2024-06-03 10:40:04 [INFO]: Epoch 006 - training loss: 0.4777, validation loss: 0.6736
2024-06-03 10:40:11 [INFO]: Epoch 007 - training loss: 0.4528, validation loss: 0.6546
2024-06-03 10:40:19 [INFO]: Epoch 008 - training loss: 0.4409, validation loss: 0.6249
2024-06-03 10:40:27 [INFO]: Epoch 009 - training loss: 0.4236, validation loss: 0.6076
2024-06-03 10:40:35 [INFO]: Epoch 010 - training loss: 0.4141, validation loss: 0.5976
2024-06-03 10:40:43 [INFO]: Epoch 011 - training loss: 0.4089, validation loss: 0.5899
2024-06-03 10:40:51 [INFO]: Epoch 012 - training loss: 0.3942, validation loss: 0.5834
2024-06-03 10:40:58 [INFO]: Epoch 013 - training loss: 0.3899, validation loss: 0.5981
2024-06-03 10:41:06 [INFO]: Epoch 014 - training loss: 0.3867, validation loss: 0.5789
2024-06-03 10:41:14 [INFO]: Epoch 015 - training loss: 0.3750, validation loss: 0.5777
2024-06-03 10:41:22 [INFO]: Epoch 016 - training loss: 0.3700, validation loss: 0.5759
2024-06-03 10:41:30 [INFO]: Epoch 017 - training loss: 0.3650, validation loss: 0.5734
2024-06-03 10:41:38 [INFO]: Epoch 018 - training loss: 0.3575, validation loss: 0.5840
2024-06-03 10:41:44 [INFO]: Epoch 019 - training loss: 0.3528, validation loss: 0.5655
2024-06-03 10:41:52 [INFO]: Epoch 020 - training loss: 0.3456, validation loss: 0.5652
2024-06-03 10:41:59 [INFO]: Epoch 021 - training loss: 0.3447, validation loss: 0.5681
2024-06-03 10:42:08 [INFO]: Epoch 022 - training loss: 0.3420, validation loss: 0.5672
2024-06-03 10:42:15 [INFO]: Epoch 023 - training loss: 0.3416, validation loss: 0.5654
2024-06-03 10:42:23 [INFO]: Epoch 024 - training loss: 0.3395, validation loss: 0.5602
2024-06-03 10:42:30 [INFO]: Epoch 025 - training loss: 0.3301, validation loss: 0.5586
2024-06-03 10:42:38 [INFO]: Epoch 026 - training loss: 0.3264, validation loss: 0.5604
2024-06-03 10:42:45 [INFO]: Epoch 027 - training loss: 0.3227, validation loss: 0.5629
2024-06-03 10:42:52 [INFO]: Epoch 028 - training loss: 0.3264, validation loss: 0.5699
2024-06-03 10:43:00 [INFO]: Epoch 029 - training loss: 0.3222, validation loss: 0.5571
2024-06-03 10:43:07 [INFO]: Epoch 030 - training loss: 0.3162, validation loss: 0.5568
2024-06-03 10:43:14 [INFO]: Epoch 031 - training loss: 0.3085, validation loss: 0.5517
2024-06-03 10:43:21 [INFO]: Epoch 032 - training loss: 0.3141, validation loss: 0.5537
2024-06-03 10:43:28 [INFO]: Epoch 033 - training loss: 0.3121, validation loss: 0.5557
2024-06-03 10:43:35 [INFO]: Epoch 034 - training loss: 0.3045, validation loss: 0.5554
2024-06-03 10:43:43 [INFO]: Epoch 035 - training loss: 0.3045, validation loss: 0.5620
2024-06-03 10:43:51 [INFO]: Epoch 036 - training loss: 0.3035, validation loss: 0.5620
2024-06-03 10:43:59 [INFO]: Epoch 037 - training loss: 0.3046, validation loss: 0.5516
2024-06-03 10:44:06 [INFO]: Epoch 038 - training loss: 0.2980, validation loss: 0.5576
2024-06-03 10:44:14 [INFO]: Epoch 039 - training loss: 0.2954, validation loss: 0.5584
2024-06-03 10:44:21 [INFO]: Epoch 040 - training loss: 0.2953, validation loss: 0.5516
2024-06-03 10:44:29 [INFO]: Epoch 041 - training loss: 0.2942, validation loss: 0.5497
2024-06-03 10:44:36 [INFO]: Epoch 042 - training loss: 0.2918, validation loss: 0.5511
2024-06-03 10:44:43 [INFO]: Epoch 043 - training loss: 0.2908, validation loss: 0.5495
2024-06-03 10:44:50 [INFO]: Epoch 044 - training loss: 0.2890, validation loss: 0.5505
2024-06-03 10:44:57 [INFO]: Epoch 045 - training loss: 0.2880, validation loss: 0.5493
2024-06-03 10:45:04 [INFO]: Epoch 046 - training loss: 0.2873, validation loss: 0.5527
2024-06-03 10:45:11 [INFO]: Epoch 047 - training loss: 0.2867, validation loss: 0.5493
2024-06-03 10:45:18 [INFO]: Epoch 048 - training loss: 0.2914, validation loss: 0.5538
2024-06-03 10:45:25 [INFO]: Epoch 049 - training loss: 0.2909, validation loss: 0.5549
2024-06-03 10:45:32 [INFO]: Epoch 050 - training loss: 0.2869, validation loss: 0.5482
2024-06-03 10:45:39 [INFO]: Epoch 051 - training loss: 0.2875, validation loss: 0.5488
2024-06-03 10:45:46 [INFO]: Epoch 052 - training loss: 0.2838, validation loss: 0.5497
2024-06-03 10:45:52 [INFO]: Epoch 053 - training loss: 0.2842, validation loss: 0.5473
2024-06-03 10:45:59 [INFO]: Epoch 054 - training loss: 0.2823, validation loss: 0.5443
2024-06-03 10:46:07 [INFO]: Epoch 055 - training loss: 0.2813, validation loss: 0.5489
2024-06-03 10:46:14 [INFO]: Epoch 056 - training loss: 0.2804, validation loss: 0.5477
2024-06-03 10:46:21 [INFO]: Epoch 057 - training loss: 0.2802, validation loss: 0.5452
2024-06-03 10:46:28 [INFO]: Epoch 058 - training loss: 0.2774, validation loss: 0.5471
2024-06-03 10:46:35 [INFO]: Epoch 059 - training loss: 0.2763, validation loss: 0.5461
2024-06-03 10:46:41 [INFO]: Epoch 060 - training loss: 0.2797, validation loss: 0.5475
2024-06-03 10:46:48 [INFO]: Epoch 061 - training loss: 0.2795, validation loss: 0.5514
2024-06-03 10:46:55 [INFO]: Epoch 062 - training loss: 0.2782, validation loss: 0.5457
2024-06-03 10:47:02 [INFO]: Epoch 063 - training loss: 0.2804, validation loss: 0.5590
2024-06-03 10:47:10 [INFO]: Epoch 064 - training loss: 0.2785, validation loss: 0.5506
2024-06-03 10:47:10 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:47:10 [INFO]: Finished training. The best model is from epoch#54.
2024-06-03 10:47:10 [INFO]: Saved the model to results_block_rate05/PeMS/DLinear_PeMS/round_4/20240603_T103917/DLinear.pypots
2024-06-03 10:47:12 [INFO]: Successfully saved to results_block_rate05/PeMS/DLinear_PeMS/round_4/imputation.pkl
2024-06-03 10:47:12 [INFO]: Round4 - DLinear on PeMS: MAE=0.4336, MSE=0.8088, MRE=0.5192
2024-06-03 10:47:12 [INFO]: Done! Final results:
Averaged DLinear (5,301,100 params) on PeMS: MAE=0.4540 ± 0.055754383598372634, MSE=0.8524 ± 0.11621474476161826, MRE=0.5436 ± 0.06675818398416296, average inference time=0.33
