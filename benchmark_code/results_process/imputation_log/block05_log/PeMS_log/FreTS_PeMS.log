2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:54 [INFO]: Model files will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_0/20240603_T100954
2024-06-03 10:09:54 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_0/20240603_T100954/tensorboard
2024-06-03 10:09:59 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-03 10:10:16 [INFO]: Epoch 001 - training loss: 1.1332, validation loss: 0.9459
2024-06-03 10:10:25 [INFO]: Epoch 002 - training loss: 0.7810, validation loss: 0.8459
2024-06-03 10:10:35 [INFO]: Epoch 003 - training loss: 0.6711, validation loss: 0.7459
2024-06-03 10:10:44 [INFO]: Epoch 004 - training loss: 0.5781, validation loss: 0.6927
2024-06-03 10:10:55 [INFO]: Epoch 005 - training loss: 0.5539, validation loss: 0.6892
2024-06-03 10:11:05 [INFO]: Epoch 006 - training loss: 0.5261, validation loss: 0.6684
2024-06-03 10:11:16 [INFO]: Epoch 007 - training loss: 0.5019, validation loss: 0.6448
2024-06-03 10:11:26 [INFO]: Epoch 008 - training loss: 0.4830, validation loss: 0.6295
2024-06-03 10:11:36 [INFO]: Epoch 009 - training loss: 0.4800, validation loss: 0.6282
2024-06-03 10:11:47 [INFO]: Epoch 010 - training loss: 0.4778, validation loss: 0.6249
2024-06-03 10:11:57 [INFO]: Epoch 011 - training loss: 0.4982, validation loss: 0.6238
2024-06-03 10:12:06 [INFO]: Epoch 012 - training loss: 0.4449, validation loss: 0.6142
2024-06-03 10:12:16 [INFO]: Epoch 013 - training loss: 0.4259, validation loss: 0.6110
2024-06-03 10:12:26 [INFO]: Epoch 014 - training loss: 0.4347, validation loss: 0.6093
2024-06-03 10:12:36 [INFO]: Epoch 015 - training loss: 0.4295, validation loss: 0.6199
2024-06-03 10:12:46 [INFO]: Epoch 016 - training loss: 0.4209, validation loss: 0.5960
2024-06-03 10:12:56 [INFO]: Epoch 017 - training loss: 0.4169, validation loss: 0.6029
2024-06-03 10:13:06 [INFO]: Epoch 018 - training loss: 0.4042, validation loss: 0.5942
2024-06-03 10:13:16 [INFO]: Epoch 019 - training loss: 0.3962, validation loss: 0.5857
2024-06-03 10:13:26 [INFO]: Epoch 020 - training loss: 0.4134, validation loss: 0.6023
2024-06-03 10:13:36 [INFO]: Epoch 021 - training loss: 0.3993, validation loss: 0.5973
2024-06-03 10:13:45 [INFO]: Epoch 022 - training loss: 0.3909, validation loss: 0.5891
2024-06-03 10:13:55 [INFO]: Epoch 023 - training loss: 0.3855, validation loss: 0.5898
2024-06-03 10:14:04 [INFO]: Epoch 024 - training loss: 0.3801, validation loss: 0.5871
2024-06-03 10:14:14 [INFO]: Epoch 025 - training loss: 0.3803, validation loss: 0.5788
2024-06-03 10:14:24 [INFO]: Epoch 026 - training loss: 0.3844, validation loss: 0.5957
2024-06-03 10:14:34 [INFO]: Epoch 027 - training loss: 0.3795, validation loss: 0.5791
2024-06-03 10:14:45 [INFO]: Epoch 028 - training loss: 0.3803, validation loss: 0.5876
2024-06-03 10:14:55 [INFO]: Epoch 029 - training loss: 0.3772, validation loss: 0.5762
2024-06-03 10:15:05 [INFO]: Epoch 030 - training loss: 0.3621, validation loss: 0.5741
2024-06-03 10:15:14 [INFO]: Epoch 031 - training loss: 0.3635, validation loss: 0.5804
2024-06-03 10:15:24 [INFO]: Epoch 032 - training loss: 0.3630, validation loss: 0.5754
2024-06-03 10:15:33 [INFO]: Epoch 033 - training loss: 0.3607, validation loss: 0.5765
2024-06-03 10:15:43 [INFO]: Epoch 034 - training loss: 0.3502, validation loss: 0.5791
2024-06-03 10:15:52 [INFO]: Epoch 035 - training loss: 0.3574, validation loss: 0.5945
2024-06-03 10:16:02 [INFO]: Epoch 036 - training loss: 0.3591, validation loss: 0.5686
2024-06-03 10:16:11 [INFO]: Epoch 037 - training loss: 0.3507, validation loss: 0.5739
2024-06-03 10:16:21 [INFO]: Epoch 038 - training loss: 0.3558, validation loss: 0.5801
2024-06-03 10:16:31 [INFO]: Epoch 039 - training loss: 0.3576, validation loss: 0.5732
2024-06-03 10:16:41 [INFO]: Epoch 040 - training loss: 0.3392, validation loss: 0.5766
2024-06-03 10:16:51 [INFO]: Epoch 041 - training loss: 0.3430, validation loss: 0.5674
2024-06-03 10:17:00 [INFO]: Epoch 042 - training loss: 0.3412, validation loss: 0.5699
2024-06-03 10:17:10 [INFO]: Epoch 043 - training loss: 0.3391, validation loss: 0.5687
2024-06-03 10:17:20 [INFO]: Epoch 044 - training loss: 0.3357, validation loss: 0.5722
2024-06-03 10:17:30 [INFO]: Epoch 045 - training loss: 0.3374, validation loss: 0.5657
2024-06-03 10:17:39 [INFO]: Epoch 046 - training loss: 0.3300, validation loss: 0.5737
2024-06-03 10:17:49 [INFO]: Epoch 047 - training loss: 0.3363, validation loss: 0.5736
2024-06-03 10:17:59 [INFO]: Epoch 048 - training loss: 0.3465, validation loss: 0.5753
2024-06-03 10:18:09 [INFO]: Epoch 049 - training loss: 0.3404, validation loss: 0.5778
2024-06-03 10:18:19 [INFO]: Epoch 050 - training loss: 0.3413, validation loss: 0.5685
2024-06-03 10:18:29 [INFO]: Epoch 051 - training loss: 0.3284, validation loss: 0.5663
2024-06-03 10:18:38 [INFO]: Epoch 052 - training loss: 0.3260, validation loss: 0.5666
2024-06-03 10:18:47 [INFO]: Epoch 053 - training loss: 0.3291, validation loss: 0.5698
2024-06-03 10:18:57 [INFO]: Epoch 054 - training loss: 0.3385, validation loss: 0.5672
2024-06-03 10:19:07 [INFO]: Epoch 055 - training loss: 0.3351, validation loss: 0.5689
2024-06-03 10:19:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:19:07 [INFO]: Finished training. The best model is from epoch#45.
2024-06-03 10:19:07 [INFO]: Saved the model to results_block_rate05/PeMS/FreTS_PeMS/round_0/20240603_T100954/FreTS.pypots
2024-06-03 10:19:11 [INFO]: Successfully saved to results_block_rate05/PeMS/FreTS_PeMS/round_0/imputation.pkl
2024-06-03 10:19:11 [INFO]: Round0 - FreTS on PeMS: MAE=0.4329, MSE=0.8246, MRE=0.5183
2024-06-03 10:19:11 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:19:11 [INFO]: Using the given device: cuda:0
2024-06-03 10:19:11 [INFO]: Model files will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_1/20240603_T101911
2024-06-03 10:19:11 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_1/20240603_T101911/tensorboard
2024-06-03 10:19:11 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-03 10:19:21 [INFO]: Epoch 001 - training loss: 1.1975, validation loss: 0.9893
2024-06-03 10:19:32 [INFO]: Epoch 002 - training loss: 0.7288, validation loss: 0.8175
2024-06-03 10:19:42 [INFO]: Epoch 003 - training loss: 0.6321, validation loss: 0.7348
2024-06-03 10:19:52 [INFO]: Epoch 004 - training loss: 0.5715, validation loss: 0.7160
2024-06-03 10:20:02 [INFO]: Epoch 005 - training loss: 0.5372, validation loss: 0.6828
2024-06-03 10:20:11 [INFO]: Epoch 006 - training loss: 0.5434, validation loss: 0.6705
2024-06-03 10:20:21 [INFO]: Epoch 007 - training loss: 0.5074, validation loss: 0.6648
2024-06-03 10:20:31 [INFO]: Epoch 008 - training loss: 0.5085, validation loss: 0.6563
2024-06-03 10:20:41 [INFO]: Epoch 009 - training loss: 0.5022, validation loss: 0.6434
2024-06-03 10:20:51 [INFO]: Epoch 010 - training loss: 0.4761, validation loss: 0.6361
2024-06-03 10:21:00 [INFO]: Epoch 011 - training loss: 0.4716, validation loss: 0.6579
2024-06-03 10:21:10 [INFO]: Epoch 012 - training loss: 0.4596, validation loss: 0.6431
2024-06-03 10:21:20 [INFO]: Epoch 013 - training loss: 0.4457, validation loss: 0.6286
2024-06-03 10:21:30 [INFO]: Epoch 014 - training loss: 0.4308, validation loss: 0.6197
2024-06-03 10:21:39 [INFO]: Epoch 015 - training loss: 0.4305, validation loss: 0.6349
2024-06-03 10:21:49 [INFO]: Epoch 016 - training loss: 0.4379, validation loss: 0.6224
2024-06-03 10:21:58 [INFO]: Epoch 017 - training loss: 0.4209, validation loss: 0.6125
2024-06-03 10:22:08 [INFO]: Epoch 018 - training loss: 0.4104, validation loss: 0.6162
2024-06-03 10:22:17 [INFO]: Epoch 019 - training loss: 0.4089, validation loss: 0.6204
2024-06-03 10:22:27 [INFO]: Epoch 020 - training loss: 0.3985, validation loss: 0.6097
2024-06-03 10:22:37 [INFO]: Epoch 021 - training loss: 0.3943, validation loss: 0.6153
2024-06-03 10:22:47 [INFO]: Epoch 022 - training loss: 0.3912, validation loss: 0.6083
2024-06-03 10:22:57 [INFO]: Epoch 023 - training loss: 0.3881, validation loss: 0.6173
2024-06-03 10:23:07 [INFO]: Epoch 024 - training loss: 0.3895, validation loss: 0.6098
2024-06-03 10:23:17 [INFO]: Epoch 025 - training loss: 0.3926, validation loss: 0.6105
2024-06-03 10:23:27 [INFO]: Epoch 026 - training loss: 0.3795, validation loss: 0.5968
2024-06-03 10:23:36 [INFO]: Epoch 027 - training loss: 0.3869, validation loss: 0.6221
2024-06-03 10:23:45 [INFO]: Epoch 028 - training loss: 0.3955, validation loss: 0.6073
2024-06-03 10:23:55 [INFO]: Epoch 029 - training loss: 0.3748, validation loss: 0.6073
2024-06-03 10:24:05 [INFO]: Epoch 030 - training loss: 0.3669, validation loss: 0.5984
2024-06-03 10:24:15 [INFO]: Epoch 031 - training loss: 0.3659, validation loss: 0.6007
2024-06-03 10:24:26 [INFO]: Epoch 032 - training loss: 0.3667, validation loss: 0.5937
2024-06-03 10:24:36 [INFO]: Epoch 033 - training loss: 0.3621, validation loss: 0.6056
2024-06-03 10:24:46 [INFO]: Epoch 034 - training loss: 0.3536, validation loss: 0.6014
2024-06-03 10:24:55 [INFO]: Epoch 035 - training loss: 0.3564, validation loss: 0.6023
2024-06-03 10:25:05 [INFO]: Epoch 036 - training loss: 0.3538, validation loss: 0.6028
2024-06-03 10:25:14 [INFO]: Epoch 037 - training loss: 0.3497, validation loss: 0.6065
2024-06-03 10:25:23 [INFO]: Epoch 038 - training loss: 0.3498, validation loss: 0.6015
2024-06-03 10:25:32 [INFO]: Epoch 039 - training loss: 0.3569, validation loss: 0.5964
2024-06-03 10:25:42 [INFO]: Epoch 040 - training loss: 0.3439, validation loss: 0.6016
2024-06-03 10:25:51 [INFO]: Epoch 041 - training loss: 0.3418, validation loss: 0.5915
2024-06-03 10:26:01 [INFO]: Epoch 042 - training loss: 0.3437, validation loss: 0.5922
2024-06-03 10:26:11 [INFO]: Epoch 043 - training loss: 0.3441, validation loss: 0.6036
2024-06-03 10:26:20 [INFO]: Epoch 044 - training loss: 0.3491, validation loss: 0.5940
2024-06-03 10:26:29 [INFO]: Epoch 045 - training loss: 0.3468, validation loss: 0.6060
2024-06-03 10:26:38 [INFO]: Epoch 046 - training loss: 0.3384, validation loss: 0.5890
2024-06-03 10:26:47 [INFO]: Epoch 047 - training loss: 0.3367, validation loss: 0.5973
2024-06-03 10:26:56 [INFO]: Epoch 048 - training loss: 0.3345, validation loss: 0.6021
2024-06-03 10:27:06 [INFO]: Epoch 049 - training loss: 0.3348, validation loss: 0.6015
2024-06-03 10:27:15 [INFO]: Epoch 050 - training loss: 0.3412, validation loss: 0.5929
2024-06-03 10:27:25 [INFO]: Epoch 051 - training loss: 0.3443, validation loss: 0.6085
2024-06-03 10:27:34 [INFO]: Epoch 052 - training loss: 0.3324, validation loss: 0.5971
2024-06-03 10:27:44 [INFO]: Epoch 053 - training loss: 0.3306, validation loss: 0.5834
2024-06-03 10:27:54 [INFO]: Epoch 054 - training loss: 0.3259, validation loss: 0.5916
2024-06-03 10:28:04 [INFO]: Epoch 055 - training loss: 0.3321, validation loss: 0.5874
2024-06-03 10:28:13 [INFO]: Epoch 056 - training loss: 0.3318, validation loss: 0.5974
2024-06-03 10:28:23 [INFO]: Epoch 057 - training loss: 0.3301, validation loss: 0.5943
2024-06-03 10:28:32 [INFO]: Epoch 058 - training loss: 0.3238, validation loss: 0.5941
2024-06-03 10:28:41 [INFO]: Epoch 059 - training loss: 0.3184, validation loss: 0.5817
2024-06-03 10:28:50 [INFO]: Epoch 060 - training loss: 0.3159, validation loss: 0.5958
2024-06-03 10:28:59 [INFO]: Epoch 061 - training loss: 0.3191, validation loss: 0.6023
2024-06-03 10:29:09 [INFO]: Epoch 062 - training loss: 0.3266, validation loss: 0.5942
2024-06-03 10:29:19 [INFO]: Epoch 063 - training loss: 0.3191, validation loss: 0.5971
2024-06-03 10:29:28 [INFO]: Epoch 064 - training loss: 0.3197, validation loss: 0.5931
2024-06-03 10:29:37 [INFO]: Epoch 065 - training loss: 0.3188, validation loss: 0.5805
2024-06-03 10:29:47 [INFO]: Epoch 066 - training loss: 0.3126, validation loss: 0.5950
2024-06-03 10:29:57 [INFO]: Epoch 067 - training loss: 0.3155, validation loss: 0.5918
2024-06-03 10:30:06 [INFO]: Epoch 068 - training loss: 0.3155, validation loss: 0.5974
2024-06-03 10:30:14 [INFO]: Epoch 069 - training loss: 0.3199, validation loss: 0.6058
2024-06-03 10:30:24 [INFO]: Epoch 070 - training loss: 0.3249, validation loss: 0.6177
2024-06-03 10:30:33 [INFO]: Epoch 071 - training loss: 0.3216, validation loss: 0.5898
2024-06-03 10:30:43 [INFO]: Epoch 072 - training loss: 0.3142, validation loss: 0.5839
2024-06-03 10:30:53 [INFO]: Epoch 073 - training loss: 0.3164, validation loss: 0.5937
2024-06-03 10:31:03 [INFO]: Epoch 074 - training loss: 0.3123, validation loss: 0.5817
2024-06-03 10:31:12 [INFO]: Epoch 075 - training loss: 0.3094, validation loss: 0.5905
2024-06-03 10:31:12 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:31:12 [INFO]: Finished training. The best model is from epoch#65.
2024-06-03 10:31:13 [INFO]: Saved the model to results_block_rate05/PeMS/FreTS_PeMS/round_1/20240603_T101911/FreTS.pypots
2024-06-03 10:31:16 [INFO]: Successfully saved to results_block_rate05/PeMS/FreTS_PeMS/round_1/imputation.pkl
2024-06-03 10:31:16 [INFO]: Round1 - FreTS on PeMS: MAE=0.4728, MSE=0.8715, MRE=0.5661
2024-06-03 10:31:16 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:31:16 [INFO]: Using the given device: cuda:0
2024-06-03 10:31:16 [INFO]: Model files will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_2/20240603_T103116
2024-06-03 10:31:16 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_2/20240603_T103116/tensorboard
2024-06-03 10:31:16 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-03 10:31:26 [INFO]: Epoch 001 - training loss: 1.1343, validation loss: 1.0736
2024-06-03 10:31:36 [INFO]: Epoch 002 - training loss: 0.7619, validation loss: 0.8314
2024-06-03 10:31:45 [INFO]: Epoch 003 - training loss: 0.6614, validation loss: 0.7911
2024-06-03 10:31:54 [INFO]: Epoch 004 - training loss: 0.5988, validation loss: 0.7345
2024-06-03 10:32:03 [INFO]: Epoch 005 - training loss: 0.5432, validation loss: 0.6690
2024-06-03 10:32:13 [INFO]: Epoch 006 - training loss: 0.5747, validation loss: 0.6986
2024-06-03 10:32:22 [INFO]: Epoch 007 - training loss: 0.5184, validation loss: 0.6675
2024-06-03 10:32:31 [INFO]: Epoch 008 - training loss: 0.4960, validation loss: 0.6359
2024-06-03 10:32:40 [INFO]: Epoch 009 - training loss: 0.4915, validation loss: 0.6574
2024-06-03 10:32:50 [INFO]: Epoch 010 - training loss: 0.4639, validation loss: 0.6296
2024-06-03 10:32:59 [INFO]: Epoch 011 - training loss: 0.4730, validation loss: 0.6355
2024-06-03 10:33:08 [INFO]: Epoch 012 - training loss: 0.4418, validation loss: 0.6164
2024-06-03 10:33:17 [INFO]: Epoch 013 - training loss: 0.4371, validation loss: 0.6105
2024-06-03 10:33:25 [INFO]: Epoch 014 - training loss: 0.4497, validation loss: 0.6126
2024-06-03 10:33:34 [INFO]: Epoch 015 - training loss: 0.4300, validation loss: 0.6138
2024-06-03 10:33:44 [INFO]: Epoch 016 - training loss: 0.4166, validation loss: 0.6254
2024-06-03 10:33:52 [INFO]: Epoch 017 - training loss: 0.4180, validation loss: 0.5978
2024-06-03 10:34:01 [INFO]: Epoch 018 - training loss: 0.4104, validation loss: 0.6049
2024-06-03 10:34:10 [INFO]: Epoch 019 - training loss: 0.4064, validation loss: 0.6087
2024-06-03 10:34:19 [INFO]: Epoch 020 - training loss: 0.3983, validation loss: 0.6017
2024-06-03 10:34:29 [INFO]: Epoch 021 - training loss: 0.3931, validation loss: 0.5834
2024-06-03 10:34:38 [INFO]: Epoch 022 - training loss: 0.3857, validation loss: 0.5874
2024-06-03 10:34:47 [INFO]: Epoch 023 - training loss: 0.3833, validation loss: 0.5860
2024-06-03 10:34:56 [INFO]: Epoch 024 - training loss: 0.3834, validation loss: 0.5937
2024-06-03 10:35:04 [INFO]: Epoch 025 - training loss: 0.3878, validation loss: 0.5944
2024-06-03 10:35:13 [INFO]: Epoch 026 - training loss: 0.3691, validation loss: 0.5876
2024-06-03 10:35:22 [INFO]: Epoch 027 - training loss: 0.3705, validation loss: 0.5786
2024-06-03 10:35:31 [INFO]: Epoch 028 - training loss: 0.3774, validation loss: 0.5769
2024-06-03 10:35:40 [INFO]: Epoch 029 - training loss: 0.3800, validation loss: 0.5819
2024-06-03 10:35:49 [INFO]: Epoch 030 - training loss: 0.3710, validation loss: 0.5800
2024-06-03 10:35:57 [INFO]: Epoch 031 - training loss: 0.3602, validation loss: 0.5874
2024-06-03 10:36:06 [INFO]: Epoch 032 - training loss: 0.3628, validation loss: 0.5761
2024-06-03 10:36:14 [INFO]: Epoch 033 - training loss: 0.3526, validation loss: 0.5772
2024-06-03 10:36:23 [INFO]: Epoch 034 - training loss: 0.3473, validation loss: 0.5688
2024-06-03 10:36:32 [INFO]: Epoch 035 - training loss: 0.3458, validation loss: 0.5721
2024-06-03 10:36:40 [INFO]: Epoch 036 - training loss: 0.3514, validation loss: 0.5650
2024-06-03 10:36:49 [INFO]: Epoch 037 - training loss: 0.3562, validation loss: 0.5704
2024-06-03 10:36:58 [INFO]: Epoch 038 - training loss: 0.3429, validation loss: 0.5659
2024-06-03 10:37:06 [INFO]: Epoch 039 - training loss: 0.3473, validation loss: 0.5754
2024-06-03 10:37:14 [INFO]: Epoch 040 - training loss: 0.3476, validation loss: 0.5656
2024-06-03 10:37:23 [INFO]: Epoch 041 - training loss: 0.3377, validation loss: 0.5649
2024-06-03 10:37:32 [INFO]: Epoch 042 - training loss: 0.3363, validation loss: 0.5615
2024-06-03 10:37:41 [INFO]: Epoch 043 - training loss: 0.3373, validation loss: 0.5725
2024-06-03 10:37:49 [INFO]: Epoch 044 - training loss: 0.3373, validation loss: 0.5642
2024-06-03 10:37:58 [INFO]: Epoch 045 - training loss: 0.3392, validation loss: 0.5635
2024-06-03 10:38:06 [INFO]: Epoch 046 - training loss: 0.3422, validation loss: 0.5781
2024-06-03 10:38:15 [INFO]: Epoch 047 - training loss: 0.3338, validation loss: 0.5570
2024-06-03 10:38:23 [INFO]: Epoch 048 - training loss: 0.3255, validation loss: 0.5686
2024-06-03 10:38:32 [INFO]: Epoch 049 - training loss: 0.3361, validation loss: 0.5693
2024-06-03 10:38:41 [INFO]: Epoch 050 - training loss: 0.3339, validation loss: 0.5636
2024-06-03 10:38:50 [INFO]: Epoch 051 - training loss: 0.3306, validation loss: 0.5625
2024-06-03 10:38:59 [INFO]: Epoch 052 - training loss: 0.3223, validation loss: 0.5624
2024-06-03 10:39:08 [INFO]: Epoch 053 - training loss: 0.3208, validation loss: 0.5564
2024-06-03 10:39:16 [INFO]: Epoch 054 - training loss: 0.3208, validation loss: 0.5675
2024-06-03 10:39:25 [INFO]: Epoch 055 - training loss: 0.3243, validation loss: 0.5678
2024-06-03 10:39:34 [INFO]: Epoch 056 - training loss: 0.3253, validation loss: 0.5628
2024-06-03 10:39:42 [INFO]: Epoch 057 - training loss: 0.3241, validation loss: 0.5621
2024-06-03 10:39:51 [INFO]: Epoch 058 - training loss: 0.3168, validation loss: 0.5553
2024-06-03 10:39:59 [INFO]: Epoch 059 - training loss: 0.3150, validation loss: 0.5589
2024-06-03 10:40:07 [INFO]: Epoch 060 - training loss: 0.3140, validation loss: 0.5566
2024-06-03 10:40:16 [INFO]: Epoch 061 - training loss: 0.3309, validation loss: 0.5578
2024-06-03 10:40:25 [INFO]: Epoch 062 - training loss: 0.3201, validation loss: 0.5641
2024-06-03 10:40:34 [INFO]: Epoch 063 - training loss: 0.3140, validation loss: 0.5519
2024-06-03 10:40:43 [INFO]: Epoch 064 - training loss: 0.3166, validation loss: 0.5578
2024-06-03 10:40:51 [INFO]: Epoch 065 - training loss: 0.3175, validation loss: 0.5535
2024-06-03 10:41:00 [INFO]: Epoch 066 - training loss: 0.3115, validation loss: 0.5594
2024-06-03 10:41:09 [INFO]: Epoch 067 - training loss: 0.3155, validation loss: 0.5600
2024-06-03 10:41:18 [INFO]: Epoch 068 - training loss: 0.3138, validation loss: 0.5563
2024-06-03 10:41:26 [INFO]: Epoch 069 - training loss: 0.3087, validation loss: 0.5612
2024-06-03 10:41:35 [INFO]: Epoch 070 - training loss: 0.3062, validation loss: 0.5493
2024-06-03 10:41:43 [INFO]: Epoch 071 - training loss: 0.3208, validation loss: 0.5553
2024-06-03 10:41:51 [INFO]: Epoch 072 - training loss: 0.3270, validation loss: 0.5563
2024-06-03 10:42:00 [INFO]: Epoch 073 - training loss: 0.3092, validation loss: 0.5529
2024-06-03 10:42:09 [INFO]: Epoch 074 - training loss: 0.3034, validation loss: 0.5511
2024-06-03 10:42:17 [INFO]: Epoch 075 - training loss: 0.3063, validation loss: 0.5532
2024-06-03 10:42:25 [INFO]: Epoch 076 - training loss: 0.3072, validation loss: 0.5628
2024-06-03 10:42:34 [INFO]: Epoch 077 - training loss: 0.3053, validation loss: 0.5547
2024-06-03 10:42:42 [INFO]: Epoch 078 - training loss: 0.3004, validation loss: 0.5510
2024-06-03 10:42:51 [INFO]: Epoch 079 - training loss: 0.3010, validation loss: 0.5522
2024-06-03 10:42:59 [INFO]: Epoch 080 - training loss: 0.3001, validation loss: 0.5546
2024-06-03 10:42:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:42:59 [INFO]: Finished training. The best model is from epoch#70.
2024-06-03 10:42:59 [INFO]: Saved the model to results_block_rate05/PeMS/FreTS_PeMS/round_2/20240603_T103116/FreTS.pypots
2024-06-03 10:43:02 [INFO]: Successfully saved to results_block_rate05/PeMS/FreTS_PeMS/round_2/imputation.pkl
2024-06-03 10:43:02 [INFO]: Round2 - FreTS on PeMS: MAE=0.4429, MSE=0.8258, MRE=0.5303
2024-06-03 10:43:02 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:43:02 [INFO]: Using the given device: cuda:0
2024-06-03 10:43:02 [INFO]: Model files will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_3/20240603_T104302
2024-06-03 10:43:02 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_3/20240603_T104302/tensorboard
2024-06-03 10:43:02 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-03 10:43:10 [INFO]: Epoch 001 - training loss: 1.1105, validation loss: 0.9657
2024-06-03 10:43:18 [INFO]: Epoch 002 - training loss: 0.7370, validation loss: 0.8770
2024-06-03 10:43:26 [INFO]: Epoch 003 - training loss: 0.6265, validation loss: 0.7493
2024-06-03 10:43:34 [INFO]: Epoch 004 - training loss: 0.5786, validation loss: 0.7553
2024-06-03 10:43:43 [INFO]: Epoch 005 - training loss: 0.5378, validation loss: 0.6976
2024-06-03 10:43:51 [INFO]: Epoch 006 - training loss: 0.5405, validation loss: 0.7034
2024-06-03 10:43:59 [INFO]: Epoch 007 - training loss: 0.5196, validation loss: 0.6876
2024-06-03 10:44:07 [INFO]: Epoch 008 - training loss: 0.4959, validation loss: 0.6615
2024-06-03 10:44:15 [INFO]: Epoch 009 - training loss: 0.4902, validation loss: 0.6641
2024-06-03 10:44:24 [INFO]: Epoch 010 - training loss: 0.4907, validation loss: 0.6510
2024-06-03 10:44:32 [INFO]: Epoch 011 - training loss: 0.4789, validation loss: 0.6833
2024-06-03 10:44:40 [INFO]: Epoch 012 - training loss: 0.4626, validation loss: 0.6437
2024-06-03 10:44:48 [INFO]: Epoch 013 - training loss: 0.4447, validation loss: 0.6426
2024-06-03 10:44:56 [INFO]: Epoch 014 - training loss: 0.4355, validation loss: 0.6258
2024-06-03 10:45:03 [INFO]: Epoch 015 - training loss: 0.4424, validation loss: 0.6291
2024-06-03 10:45:11 [INFO]: Epoch 016 - training loss: 0.4195, validation loss: 0.6218
2024-06-03 10:45:19 [INFO]: Epoch 017 - training loss: 0.4319, validation loss: 0.6230
2024-06-03 10:45:27 [INFO]: Epoch 018 - training loss: 0.4137, validation loss: 0.6220
2024-06-03 10:45:35 [INFO]: Epoch 019 - training loss: 0.4069, validation loss: 0.6151
2024-06-03 10:45:43 [INFO]: Epoch 020 - training loss: 0.4161, validation loss: 0.6226
2024-06-03 10:45:51 [INFO]: Epoch 021 - training loss: 0.3997, validation loss: 0.6104
2024-06-03 10:45:59 [INFO]: Epoch 022 - training loss: 0.3887, validation loss: 0.6048
2024-06-03 10:46:07 [INFO]: Epoch 023 - training loss: 0.3973, validation loss: 0.6034
2024-06-03 10:46:14 [INFO]: Epoch 024 - training loss: 0.4010, validation loss: 0.5981
2024-06-03 10:46:22 [INFO]: Epoch 025 - training loss: 0.3827, validation loss: 0.5911
2024-06-03 10:46:30 [INFO]: Epoch 026 - training loss: 0.3898, validation loss: 0.6113
2024-06-03 10:46:37 [INFO]: Epoch 027 - training loss: 0.3998, validation loss: 0.6068
2024-06-03 10:46:45 [INFO]: Epoch 028 - training loss: 0.3775, validation loss: 0.5879
2024-06-03 10:46:53 [INFO]: Epoch 029 - training loss: 0.3669, validation loss: 0.5819
2024-06-03 10:47:01 [INFO]: Epoch 030 - training loss: 0.3654, validation loss: 0.5837
2024-06-03 10:47:08 [INFO]: Epoch 031 - training loss: 0.3610, validation loss: 0.5905
2024-06-03 10:47:16 [INFO]: Epoch 032 - training loss: 0.3615, validation loss: 0.5906
2024-06-03 10:47:24 [INFO]: Epoch 033 - training loss: 0.3537, validation loss: 0.5824
2024-06-03 10:47:31 [INFO]: Epoch 034 - training loss: 0.3503, validation loss: 0.5792
2024-06-03 10:47:39 [INFO]: Epoch 035 - training loss: 0.3496, validation loss: 0.5846
2024-06-03 10:47:46 [INFO]: Epoch 036 - training loss: 0.3493, validation loss: 0.5809
2024-06-03 10:47:53 [INFO]: Epoch 037 - training loss: 0.3563, validation loss: 0.5785
2024-06-03 10:48:00 [INFO]: Epoch 038 - training loss: 0.3427, validation loss: 0.5712
2024-06-03 10:48:08 [INFO]: Epoch 039 - training loss: 0.3451, validation loss: 0.5791
2024-06-03 10:48:15 [INFO]: Epoch 040 - training loss: 0.3426, validation loss: 0.5847
2024-06-03 10:48:22 [INFO]: Epoch 041 - training loss: 0.3480, validation loss: 0.5717
2024-06-03 10:48:30 [INFO]: Epoch 042 - training loss: 0.3467, validation loss: 0.5916
2024-06-03 10:48:37 [INFO]: Epoch 043 - training loss: 0.3389, validation loss: 0.5692
2024-06-03 10:48:45 [INFO]: Epoch 044 - training loss: 0.3436, validation loss: 0.5852
2024-06-03 10:48:52 [INFO]: Epoch 045 - training loss: 0.3444, validation loss: 0.5845
2024-06-03 10:49:00 [INFO]: Epoch 046 - training loss: 0.3437, validation loss: 0.5708
2024-06-03 10:49:07 [INFO]: Epoch 047 - training loss: 0.3345, validation loss: 0.5729
2024-06-03 10:49:15 [INFO]: Epoch 048 - training loss: 0.3302, validation loss: 0.5749
2024-06-03 10:49:23 [INFO]: Epoch 049 - training loss: 0.3315, validation loss: 0.5635
2024-06-03 10:49:30 [INFO]: Epoch 050 - training loss: 0.3257, validation loss: 0.5732
2024-06-03 10:49:37 [INFO]: Epoch 051 - training loss: 0.3311, validation loss: 0.5708
2024-06-03 10:49:44 [INFO]: Epoch 052 - training loss: 0.3222, validation loss: 0.5598
2024-06-03 10:49:52 [INFO]: Epoch 053 - training loss: 0.3304, validation loss: 0.5663
2024-06-03 10:49:59 [INFO]: Epoch 054 - training loss: 0.3278, validation loss: 0.5648
2024-06-03 10:50:06 [INFO]: Epoch 055 - training loss: 0.3285, validation loss: 0.5688
2024-06-03 10:50:14 [INFO]: Epoch 056 - training loss: 0.3188, validation loss: 0.5628
2024-06-03 10:50:22 [INFO]: Epoch 057 - training loss: 0.3250, validation loss: 0.5688
2024-06-03 10:50:29 [INFO]: Epoch 058 - training loss: 0.3229, validation loss: 0.5680
2024-06-03 10:50:37 [INFO]: Epoch 059 - training loss: 0.3315, validation loss: 0.5758
2024-06-03 10:50:44 [INFO]: Epoch 060 - training loss: 0.3252, validation loss: 0.5823
2024-06-03 10:50:52 [INFO]: Epoch 061 - training loss: 0.3243, validation loss: 0.5713
2024-06-03 10:50:59 [INFO]: Epoch 062 - training loss: 0.3268, validation loss: 0.5740
2024-06-03 10:50:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:50:59 [INFO]: Finished training. The best model is from epoch#52.
2024-06-03 10:51:00 [INFO]: Saved the model to results_block_rate05/PeMS/FreTS_PeMS/round_3/20240603_T104302/FreTS.pypots
2024-06-03 10:51:02 [INFO]: Successfully saved to results_block_rate05/PeMS/FreTS_PeMS/round_3/imputation.pkl
2024-06-03 10:51:02 [INFO]: Round3 - FreTS on PeMS: MAE=0.4566, MSE=0.8463, MRE=0.5467
2024-06-03 10:51:02 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:51:02 [INFO]: Using the given device: cuda:0
2024-06-03 10:51:02 [INFO]: Model files will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_4/20240603_T105102
2024-06-03 10:51:02 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/FreTS_PeMS/round_4/20240603_T105102/tensorboard
2024-06-03 10:51:02 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 1,715,958
2024-06-03 10:51:10 [INFO]: Epoch 001 - training loss: 1.0976, validation loss: 0.9490
2024-06-03 10:51:17 [INFO]: Epoch 002 - training loss: 0.7161, validation loss: 0.7963
2024-06-03 10:51:25 [INFO]: Epoch 003 - training loss: 0.6269, validation loss: 0.7109
2024-06-03 10:51:33 [INFO]: Epoch 004 - training loss: 0.6067, validation loss: 0.7145
2024-06-03 10:51:41 [INFO]: Epoch 005 - training loss: 0.5556, validation loss: 0.7061
2024-06-03 10:51:49 [INFO]: Epoch 006 - training loss: 0.5336, validation loss: 0.6758
2024-06-03 10:51:56 [INFO]: Epoch 007 - training loss: 0.5106, validation loss: 0.6678
2024-06-03 10:52:04 [INFO]: Epoch 008 - training loss: 0.5019, validation loss: 0.6778
2024-06-03 10:52:12 [INFO]: Epoch 009 - training loss: 0.5027, validation loss: 0.6888
2024-06-03 10:52:19 [INFO]: Epoch 010 - training loss: 0.4798, validation loss: 0.6644
2024-06-03 10:52:27 [INFO]: Epoch 011 - training loss: 0.4680, validation loss: 0.6651
2024-06-03 10:52:35 [INFO]: Epoch 012 - training loss: 0.4643, validation loss: 0.6178
2024-06-03 10:52:42 [INFO]: Epoch 013 - training loss: 0.4654, validation loss: 0.6192
2024-06-03 10:52:49 [INFO]: Epoch 014 - training loss: 0.4315, validation loss: 0.6169
2024-06-03 10:52:56 [INFO]: Epoch 015 - training loss: 0.4248, validation loss: 0.6108
2024-06-03 10:53:04 [INFO]: Epoch 016 - training loss: 0.4149, validation loss: 0.6265
2024-06-03 10:53:11 [INFO]: Epoch 017 - training loss: 0.4212, validation loss: 0.6053
2024-06-03 10:53:18 [INFO]: Epoch 018 - training loss: 0.4057, validation loss: 0.6087
2024-06-03 10:53:26 [INFO]: Epoch 019 - training loss: 0.3995, validation loss: 0.6104
2024-06-03 10:53:34 [INFO]: Epoch 020 - training loss: 0.3998, validation loss: 0.6052
2024-06-03 10:53:41 [INFO]: Epoch 021 - training loss: 0.3966, validation loss: 0.5950
2024-06-03 10:53:49 [INFO]: Epoch 022 - training loss: 0.3896, validation loss: 0.5962
2024-06-03 10:53:56 [INFO]: Epoch 023 - training loss: 0.4074, validation loss: 0.5969
2024-06-03 10:54:04 [INFO]: Epoch 024 - training loss: 0.3957, validation loss: 0.5897
2024-06-03 10:54:12 [INFO]: Epoch 025 - training loss: 0.3830, validation loss: 0.5871
2024-06-03 10:54:19 [INFO]: Epoch 026 - training loss: 0.3780, validation loss: 0.5871
2024-06-03 10:54:27 [INFO]: Epoch 027 - training loss: 0.3678, validation loss: 0.5924
2024-06-03 10:54:34 [INFO]: Epoch 028 - training loss: 0.3671, validation loss: 0.5837
2024-06-03 10:54:41 [INFO]: Epoch 029 - training loss: 0.3618, validation loss: 0.5891
2024-06-03 10:54:48 [INFO]: Epoch 030 - training loss: 0.3740, validation loss: 0.5772
2024-06-03 10:54:55 [INFO]: Epoch 031 - training loss: 0.3736, validation loss: 0.6078
2024-06-03 10:55:02 [INFO]: Epoch 032 - training loss: 0.3801, validation loss: 0.5892
2024-06-03 10:55:10 [INFO]: Epoch 033 - training loss: 0.3745, validation loss: 0.5739
2024-06-03 10:55:17 [INFO]: Epoch 034 - training loss: 0.3828, validation loss: 0.5950
2024-06-03 10:55:24 [INFO]: Epoch 035 - training loss: 0.3557, validation loss: 0.5793
2024-06-03 10:55:30 [INFO]: Epoch 036 - training loss: 0.3471, validation loss: 0.5701
2024-06-03 10:55:37 [INFO]: Epoch 037 - training loss: 0.3452, validation loss: 0.5815
2024-06-03 10:55:44 [INFO]: Epoch 038 - training loss: 0.3544, validation loss: 0.5895
2024-06-03 10:55:51 [INFO]: Epoch 039 - training loss: 0.3658, validation loss: 0.5804
2024-06-03 10:55:58 [INFO]: Epoch 040 - training loss: 0.3488, validation loss: 0.5809
2024-06-03 10:56:05 [INFO]: Epoch 041 - training loss: 0.3504, validation loss: 0.5758
2024-06-03 10:56:12 [INFO]: Epoch 042 - training loss: 0.3397, validation loss: 0.5666
2024-06-03 10:56:18 [INFO]: Epoch 043 - training loss: 0.3368, validation loss: 0.5764
2024-06-03 10:56:25 [INFO]: Epoch 044 - training loss: 0.3421, validation loss: 0.5769
2024-06-03 10:56:32 [INFO]: Epoch 045 - training loss: 0.3352, validation loss: 0.5697
2024-06-03 10:56:39 [INFO]: Epoch 046 - training loss: 0.3322, validation loss: 0.5692
2024-06-03 10:56:46 [INFO]: Epoch 047 - training loss: 0.3354, validation loss: 0.5854
2024-06-03 10:56:53 [INFO]: Epoch 048 - training loss: 0.3344, validation loss: 0.5750
2024-06-03 10:57:00 [INFO]: Epoch 049 - training loss: 0.3277, validation loss: 0.5725
2024-06-03 10:57:07 [INFO]: Epoch 050 - training loss: 0.3267, validation loss: 0.5707
2024-06-03 10:57:14 [INFO]: Epoch 051 - training loss: 0.3209, validation loss: 0.5731
2024-06-03 10:57:21 [INFO]: Epoch 052 - training loss: 0.3235, validation loss: 0.5652
2024-06-03 10:57:27 [INFO]: Epoch 053 - training loss: 0.3222, validation loss: 0.5743
2024-06-03 10:57:35 [INFO]: Epoch 054 - training loss: 0.3287, validation loss: 0.5790
2024-06-03 10:57:41 [INFO]: Epoch 055 - training loss: 0.3259, validation loss: 0.5617
2024-06-03 10:57:48 [INFO]: Epoch 056 - training loss: 0.3210, validation loss: 0.5625
2024-06-03 10:57:55 [INFO]: Epoch 057 - training loss: 0.3162, validation loss: 0.5607
2024-06-03 10:58:01 [INFO]: Epoch 058 - training loss: 0.3183, validation loss: 0.5611
2024-06-03 10:58:09 [INFO]: Epoch 059 - training loss: 0.3204, validation loss: 0.5647
2024-06-03 10:58:16 [INFO]: Epoch 060 - training loss: 0.3202, validation loss: 0.5676
2024-06-03 10:58:22 [INFO]: Epoch 061 - training loss: 0.3226, validation loss: 0.5732
2024-06-03 10:58:29 [INFO]: Epoch 062 - training loss: 0.3226, validation loss: 0.5632
2024-06-03 10:58:36 [INFO]: Epoch 063 - training loss: 0.3169, validation loss: 0.5716
2024-06-03 10:58:43 [INFO]: Epoch 064 - training loss: 0.3149, validation loss: 0.5497
2024-06-03 10:58:50 [INFO]: Epoch 065 - training loss: 0.3111, validation loss: 0.5545
2024-06-03 10:58:57 [INFO]: Epoch 066 - training loss: 0.3085, validation loss: 0.5617
2024-06-03 10:59:04 [INFO]: Epoch 067 - training loss: 0.3107, validation loss: 0.5600
2024-06-03 10:59:11 [INFO]: Epoch 068 - training loss: 0.3247, validation loss: 0.5683
2024-06-03 10:59:18 [INFO]: Epoch 069 - training loss: 0.3210, validation loss: 0.5597
2024-06-03 10:59:25 [INFO]: Epoch 070 - training loss: 0.3178, validation loss: 0.5663
2024-06-03 10:59:31 [INFO]: Epoch 071 - training loss: 0.3283, validation loss: 0.5661
2024-06-03 10:59:38 [INFO]: Epoch 072 - training loss: 0.3295, validation loss: 0.5542
2024-06-03 10:59:45 [INFO]: Epoch 073 - training loss: 0.3139, validation loss: 0.5616
2024-06-03 10:59:52 [INFO]: Epoch 074 - training loss: 0.3040, validation loss: 0.5599
2024-06-03 10:59:52 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:59:52 [INFO]: Finished training. The best model is from epoch#64.
2024-06-03 10:59:52 [INFO]: Saved the model to results_block_rate05/PeMS/FreTS_PeMS/round_4/20240603_T105102/FreTS.pypots
2024-06-03 10:59:54 [INFO]: Successfully saved to results_block_rate05/PeMS/FreTS_PeMS/round_4/imputation.pkl
2024-06-03 10:59:54 [INFO]: Round4 - FreTS on PeMS: MAE=0.4544, MSE=0.8370, MRE=0.5441
2024-06-03 10:59:54 [INFO]: Done! Final results:
Averaged FreTS (1,715,958 params) on PeMS: MAE=0.4519 ± 0.013477411033914878, MSE=0.8410 ± 0.01719836010201503, MRE=0.5411 ± 0.01613734073204852, average inference time=0.39
