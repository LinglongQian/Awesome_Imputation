2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_0/20240603_T100955
2024-06-03 10:09:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_0/20240603_T100955/tensorboard
2024-06-03 10:10:00 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 10:10:45 [INFO]: Epoch 001 - training loss: 1.1133, validation loss: 1.0397
2024-06-03 10:11:40 [INFO]: Epoch 002 - training loss: 0.7510, validation loss: 1.0410
2024-06-03 10:12:34 [INFO]: Epoch 003 - training loss: 0.6889, validation loss: 1.0291
2024-06-03 10:13:35 [INFO]: Epoch 004 - training loss: 0.6270, validation loss: 1.0452
2024-06-03 10:14:28 [INFO]: Epoch 005 - training loss: 0.5856, validation loss: 0.8516
2024-06-03 10:15:25 [INFO]: Epoch 006 - training loss: 0.5422, validation loss: 0.8476
2024-06-03 10:16:19 [INFO]: Epoch 007 - training loss: 0.5096, validation loss: 0.7776
2024-06-03 10:17:14 [INFO]: Epoch 008 - training loss: 0.4852, validation loss: 0.7956
2024-06-03 10:18:15 [INFO]: Epoch 009 - training loss: 0.4808, validation loss: 0.7518
2024-06-03 10:19:03 [INFO]: Epoch 010 - training loss: 0.4582, validation loss: 0.7315
2024-06-03 10:20:10 [INFO]: Epoch 011 - training loss: 0.4517, validation loss: 0.7026
2024-06-03 10:21:00 [INFO]: Epoch 012 - training loss: 0.4280, validation loss: 0.7082
2024-06-03 10:21:58 [INFO]: Epoch 013 - training loss: 0.4168, validation loss: 0.6905
2024-06-03 10:22:52 [INFO]: Epoch 014 - training loss: 0.4075, validation loss: 0.6899
2024-06-03 10:23:45 [INFO]: Epoch 015 - training loss: 0.3990, validation loss: 0.6830
2024-06-03 10:24:50 [INFO]: Epoch 016 - training loss: 0.3931, validation loss: 0.6708
2024-06-03 10:25:35 [INFO]: Epoch 017 - training loss: 0.3851, validation loss: 0.6710
2024-06-03 10:26:35 [INFO]: Epoch 018 - training loss: 0.3794, validation loss: 0.6767
2024-06-03 10:27:28 [INFO]: Epoch 019 - training loss: 0.3774, validation loss: 0.6643
2024-06-03 10:28:26 [INFO]: Epoch 020 - training loss: 0.3691, validation loss: 0.6583
2024-06-03 10:29:13 [INFO]: Epoch 021 - training loss: 0.3651, validation loss: 0.6579
2024-06-03 10:30:14 [INFO]: Epoch 022 - training loss: 0.3648, validation loss: 0.6491
2024-06-03 10:31:11 [INFO]: Epoch 023 - training loss: 0.3581, validation loss: 0.6398
2024-06-03 10:32:10 [INFO]: Epoch 024 - training loss: 0.3554, validation loss: 0.6467
2024-06-03 10:33:05 [INFO]: Epoch 025 - training loss: 0.3537, validation loss: 0.6356
2024-06-03 10:33:55 [INFO]: Epoch 026 - training loss: 0.3569, validation loss: 0.6385
2024-06-03 10:34:51 [INFO]: Epoch 027 - training loss: 0.3501, validation loss: 0.6328
2024-06-03 10:35:37 [INFO]: Epoch 028 - training loss: 0.3513, validation loss: 0.6308
2024-06-03 10:36:35 [INFO]: Epoch 029 - training loss: 0.3493, validation loss: 0.6390
2024-06-03 10:37:26 [INFO]: Epoch 030 - training loss: 0.3416, validation loss: 0.6350
2024-06-03 10:38:22 [INFO]: Epoch 031 - training loss: 0.3419, validation loss: 0.6286
2024-06-03 10:39:22 [INFO]: Epoch 032 - training loss: 0.3390, validation loss: 0.6356
2024-06-03 10:40:10 [INFO]: Epoch 033 - training loss: 0.3340, validation loss: 0.6280
2024-06-03 10:41:04 [INFO]: Epoch 034 - training loss: 0.3280, validation loss: 0.6288
2024-06-03 10:41:57 [INFO]: Epoch 035 - training loss: 0.3312, validation loss: 0.6302
2024-06-03 10:42:53 [INFO]: Epoch 036 - training loss: 0.3233, validation loss: 0.6257
2024-06-03 10:43:43 [INFO]: Epoch 037 - training loss: 0.3249, validation loss: 0.6280
2024-06-03 10:44:38 [INFO]: Epoch 038 - training loss: 0.3254, validation loss: 0.6292
2024-06-03 10:45:20 [INFO]: Epoch 039 - training loss: 0.3248, validation loss: 0.6235
2024-06-03 10:46:09 [INFO]: Epoch 040 - training loss: 0.3259, validation loss: 0.6400
2024-06-03 10:46:53 [INFO]: Epoch 041 - training loss: 0.3212, validation loss: 0.6282
2024-06-03 10:47:43 [INFO]: Epoch 042 - training loss: 0.3182, validation loss: 0.6360
2024-06-03 10:48:31 [INFO]: Epoch 043 - training loss: 0.3157, validation loss: 0.6293
2024-06-03 10:49:22 [INFO]: Epoch 044 - training loss: 0.3180, validation loss: 0.6246
2024-06-03 10:50:04 [INFO]: Epoch 045 - training loss: 0.3243, validation loss: 0.6234
2024-06-03 10:50:54 [INFO]: Epoch 046 - training loss: 0.3170, validation loss: 0.6291
2024-06-03 10:51:41 [INFO]: Epoch 047 - training loss: 0.3176, validation loss: 0.6147
2024-06-03 10:52:30 [INFO]: Epoch 048 - training loss: 0.3137, validation loss: 0.6294
2024-06-03 10:53:15 [INFO]: Epoch 049 - training loss: 0.3113, validation loss: 0.6304
2024-06-03 10:54:07 [INFO]: Epoch 050 - training loss: 0.3143, validation loss: 0.6169
2024-06-03 10:54:52 [INFO]: Epoch 051 - training loss: 0.3119, validation loss: 0.6183
2024-06-03 10:55:40 [INFO]: Epoch 052 - training loss: 0.3085, validation loss: 0.6182
2024-06-03 10:56:21 [INFO]: Epoch 053 - training loss: 0.3111, validation loss: 0.6150
2024-06-03 10:57:04 [INFO]: Epoch 054 - training loss: 0.3135, validation loss: 0.6182
2024-06-03 10:57:46 [INFO]: Epoch 055 - training loss: 0.3117, validation loss: 0.6135
2024-06-03 10:58:27 [INFO]: Epoch 056 - training loss: 0.3040, validation loss: 0.6301
2024-06-03 10:59:12 [INFO]: Epoch 057 - training loss: 0.3079, validation loss: 0.6263
2024-06-03 10:59:55 [INFO]: Epoch 058 - training loss: 0.3027, validation loss: 0.6209
2024-06-03 11:00:32 [INFO]: Epoch 059 - training loss: 0.2988, validation loss: 0.6144
2024-06-03 11:01:07 [INFO]: Epoch 060 - training loss: 0.3052, validation loss: 0.6146
2024-06-03 11:01:46 [INFO]: Epoch 061 - training loss: 0.2998, validation loss: 0.6254
2024-06-03 11:02:20 [INFO]: Epoch 062 - training loss: 0.3036, validation loss: 0.6093
2024-06-03 11:02:55 [INFO]: Epoch 063 - training loss: 0.2984, validation loss: 0.6191
2024-06-03 11:03:28 [INFO]: Epoch 064 - training loss: 0.2983, validation loss: 0.6198
2024-06-03 11:04:03 [INFO]: Epoch 065 - training loss: 0.3007, validation loss: 0.6161
2024-06-03 11:04:39 [INFO]: Epoch 066 - training loss: 0.2990, validation loss: 0.6195
2024-06-03 11:05:14 [INFO]: Epoch 067 - training loss: 0.2994, validation loss: 0.6203
2024-06-03 11:05:50 [INFO]: Epoch 068 - training loss: 0.3006, validation loss: 0.6191
2024-06-03 11:06:22 [INFO]: Epoch 069 - training loss: 0.2972, validation loss: 0.6194
2024-06-03 11:06:58 [INFO]: Epoch 070 - training loss: 0.2980, validation loss: 0.6132
2024-06-03 11:07:30 [INFO]: Epoch 071 - training loss: 0.2987, validation loss: 0.6228
2024-06-03 11:08:06 [INFO]: Epoch 072 - training loss: 0.3013, validation loss: 0.6225
2024-06-03 11:08:06 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:08:06 [INFO]: Finished training. The best model is from epoch#62.
2024-06-03 11:08:06 [INFO]: Saved the model to results_block_rate05/PeMS/Koopa_PeMS/round_0/20240603_T100955/Koopa.pypots
2024-06-03 11:08:11 [INFO]: Successfully saved to results_block_rate05/PeMS/Koopa_PeMS/round_0/imputation.pkl
2024-06-03 11:08:11 [INFO]: Round0 - Koopa on PeMS: MAE=0.5186, MSE=0.9692, MRE=0.6209
2024-06-03 11:08:11 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 11:08:11 [INFO]: Using the given device: cuda:0
2024-06-03 11:08:11 [INFO]: Model files will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_1/20240603_T110811
2024-06-03 11:08:11 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_1/20240603_T110811/tensorboard
2024-06-03 11:08:12 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 11:08:43 [INFO]: Epoch 001 - training loss: 1.1946, validation loss: 1.1130
2024-06-03 11:09:17 [INFO]: Epoch 002 - training loss: 0.7518, validation loss: 1.0266
2024-06-03 11:09:47 [INFO]: Epoch 003 - training loss: 0.6699, validation loss: 0.9557
2024-06-03 11:10:21 [INFO]: Epoch 004 - training loss: 0.6143, validation loss: 0.9126
2024-06-03 11:10:57 [INFO]: Epoch 005 - training loss: 0.5675, validation loss: 0.8845
2024-06-03 11:11:28 [INFO]: Epoch 006 - training loss: 0.5383, validation loss: 0.8645
2024-06-03 11:12:03 [INFO]: Epoch 007 - training loss: 0.5209, validation loss: 0.7934
2024-06-03 11:12:34 [INFO]: Epoch 008 - training loss: 0.4979, validation loss: 0.7607
2024-06-03 11:13:12 [INFO]: Epoch 009 - training loss: 0.4842, validation loss: 0.7984
2024-06-03 11:13:45 [INFO]: Epoch 010 - training loss: 0.4648, validation loss: 0.7303
2024-06-03 11:14:20 [INFO]: Epoch 011 - training loss: 0.4529, validation loss: 0.7585
2024-06-03 11:14:52 [INFO]: Epoch 012 - training loss: 0.4346, validation loss: 0.6948
2024-06-03 11:15:23 [INFO]: Epoch 013 - training loss: 0.4252, validation loss: 0.6837
2024-06-03 11:16:01 [INFO]: Epoch 014 - training loss: 0.4176, validation loss: 0.6676
2024-06-03 11:16:35 [INFO]: Epoch 015 - training loss: 0.4010, validation loss: 0.6475
2024-06-03 11:17:12 [INFO]: Epoch 016 - training loss: 0.3864, validation loss: 0.6348
2024-06-03 11:17:45 [INFO]: Epoch 017 - training loss: 0.3779, validation loss: 0.6428
2024-06-03 11:18:23 [INFO]: Epoch 018 - training loss: 0.3743, validation loss: 0.6346
2024-06-03 11:18:57 [INFO]: Epoch 019 - training loss: 0.3683, validation loss: 0.6325
2024-06-03 11:19:35 [INFO]: Epoch 020 - training loss: 0.3660, validation loss: 0.6260
2024-06-03 11:20:09 [INFO]: Epoch 021 - training loss: 0.3588, validation loss: 0.6280
2024-06-03 11:20:46 [INFO]: Epoch 022 - training loss: 0.3528, validation loss: 0.6250
2024-06-03 11:21:22 [INFO]: Epoch 023 - training loss: 0.3482, validation loss: 0.6154
2024-06-03 11:21:55 [INFO]: Epoch 024 - training loss: 0.3443, validation loss: 0.6144
2024-06-03 11:22:28 [INFO]: Epoch 025 - training loss: 0.3442, validation loss: 0.6097
2024-06-03 11:22:55 [INFO]: Epoch 026 - training loss: 0.3425, validation loss: 0.6059
2024-06-03 11:23:20 [INFO]: Epoch 027 - training loss: 0.3346, validation loss: 0.6050
2024-06-03 11:23:46 [INFO]: Epoch 028 - training loss: 0.3341, validation loss: 0.6063
2024-06-03 11:24:13 [INFO]: Epoch 029 - training loss: 0.3293, validation loss: 0.6042
2024-06-03 11:24:42 [INFO]: Epoch 030 - training loss: 0.3286, validation loss: 0.6066
2024-06-03 11:25:04 [INFO]: Epoch 031 - training loss: 0.3351, validation loss: 0.6093
2024-06-03 11:25:33 [INFO]: Epoch 032 - training loss: 0.3278, validation loss: 0.6049
2024-06-03 11:25:57 [INFO]: Epoch 033 - training loss: 0.3262, validation loss: 0.6102
2024-06-03 11:26:20 [INFO]: Epoch 034 - training loss: 0.3253, validation loss: 0.6076
2024-06-03 11:26:50 [INFO]: Epoch 035 - training loss: 0.3195, validation loss: 0.6088
2024-06-03 11:27:13 [INFO]: Epoch 036 - training loss: 0.3174, validation loss: 0.5981
2024-06-03 11:27:42 [INFO]: Epoch 037 - training loss: 0.3196, validation loss: 0.5953
2024-06-03 11:28:08 [INFO]: Epoch 038 - training loss: 0.3132, validation loss: 0.6070
2024-06-03 11:28:33 [INFO]: Epoch 039 - training loss: 0.3263, validation loss: 0.6576
2024-06-03 11:29:01 [INFO]: Epoch 040 - training loss: 5.4256, validation loss: 11.0564
2024-06-03 11:29:26 [INFO]: Epoch 041 - training loss: 4.0490, validation loss: 4.8405
2024-06-03 11:29:56 [INFO]: Epoch 042 - training loss: 2.5874, validation loss: 2.5288
2024-06-03 11:30:20 [INFO]: Epoch 043 - training loss: 1.9452, validation loss: 1.7202
2024-06-03 11:30:45 [INFO]: Epoch 044 - training loss: 1.6210, validation loss: 1.4303
2024-06-03 11:31:07 [INFO]: Epoch 045 - training loss: 1.4433, validation loss: 1.3318
2024-06-03 11:31:31 [INFO]: Epoch 046 - training loss: 1.3356, validation loss: 1.1371
2024-06-03 11:31:58 [INFO]: Epoch 047 - training loss: 1.2444, validation loss: 1.0961
2024-06-03 11:31:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:31:58 [INFO]: Finished training. The best model is from epoch#37.
2024-06-03 11:31:58 [INFO]: Saved the model to results_block_rate05/PeMS/Koopa_PeMS/round_1/20240603_T110811/Koopa.pypots
2024-06-03 11:32:01 [INFO]: Successfully saved to results_block_rate05/PeMS/Koopa_PeMS/round_1/imputation.pkl
2024-06-03 11:32:01 [INFO]: Round1 - Koopa on PeMS: MAE=0.8064, MSE=1.4471, MRE=0.9656
2024-06-03 11:32:01 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 11:32:01 [INFO]: Using the given device: cuda:0
2024-06-03 11:32:01 [INFO]: Model files will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_2/20240603_T113201
2024-06-03 11:32:01 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_2/20240603_T113201/tensorboard
2024-06-03 11:32:02 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 11:32:29 [INFO]: Epoch 001 - training loss: 1.0901, validation loss: 1.0766
2024-06-03 11:32:54 [INFO]: Epoch 002 - training loss: 0.7639, validation loss: 1.0632
2024-06-03 11:33:21 [INFO]: Epoch 003 - training loss: 0.6970, validation loss: 1.0067
2024-06-03 11:33:45 [INFO]: Epoch 004 - training loss: 0.6583, validation loss: 0.9894
2024-06-03 11:34:11 [INFO]: Epoch 005 - training loss: 0.6212, validation loss: 0.9477
2024-06-03 11:34:34 [INFO]: Epoch 006 - training loss: 0.5855, validation loss: 0.9151
2024-06-03 11:34:52 [INFO]: Epoch 007 - training loss: 0.5594, validation loss: 0.9248
2024-06-03 11:35:16 [INFO]: Epoch 008 - training loss: 0.5364, validation loss: 0.8520
2024-06-03 11:35:38 [INFO]: Epoch 009 - training loss: 0.5189, validation loss: 0.8541
2024-06-03 11:35:58 [INFO]: Epoch 010 - training loss: 0.4975, validation loss: 0.8321
2024-06-03 11:36:22 [INFO]: Epoch 011 - training loss: 0.4893, validation loss: 0.8086
2024-06-03 11:36:43 [INFO]: Epoch 012 - training loss: 0.4772, validation loss: 0.7222
2024-06-03 11:37:03 [INFO]: Epoch 013 - training loss: 0.4591, validation loss: 0.7476
2024-06-03 11:37:24 [INFO]: Epoch 014 - training loss: 0.4424, validation loss: 0.7266
2024-06-03 11:37:47 [INFO]: Epoch 015 - training loss: 0.4278, validation loss: 0.7098
2024-06-03 11:38:10 [INFO]: Epoch 016 - training loss: 0.4172, validation loss: 0.6938
2024-06-03 11:38:31 [INFO]: Epoch 017 - training loss: 0.4111, validation loss: 0.7131
2024-06-03 11:38:51 [INFO]: Epoch 018 - training loss: 0.4016, validation loss: 0.6782
2024-06-03 11:39:12 [INFO]: Epoch 019 - training loss: 0.3972, validation loss: 0.6806
2024-06-03 11:39:35 [INFO]: Epoch 020 - training loss: 0.3983, validation loss: 0.6879
2024-06-03 11:39:57 [INFO]: Epoch 021 - training loss: 0.3858, validation loss: 0.6725
2024-06-03 11:40:17 [INFO]: Epoch 022 - training loss: 0.3832, validation loss: 0.6666
2024-06-03 11:40:40 [INFO]: Epoch 023 - training loss: 0.3765, validation loss: 0.6669
2024-06-03 11:40:59 [INFO]: Epoch 024 - training loss: 0.3745, validation loss: 0.6695
2024-06-03 11:41:19 [INFO]: Epoch 025 - training loss: 0.3700, validation loss: 0.6554
2024-06-03 11:41:41 [INFO]: Epoch 026 - training loss: 0.3660, validation loss: 0.6464
2024-06-03 11:42:01 [INFO]: Epoch 027 - training loss: 0.3682, validation loss: 0.6601
2024-06-03 11:42:23 [INFO]: Epoch 028 - training loss: 0.3612, validation loss: 0.6452
2024-06-03 11:42:44 [INFO]: Epoch 029 - training loss: 0.3585, validation loss: 0.6468
2024-06-03 11:43:02 [INFO]: Epoch 030 - training loss: 0.3604, validation loss: 0.6625
2024-06-03 11:43:21 [INFO]: Epoch 031 - training loss: 0.3541, validation loss: 0.6447
2024-06-03 11:43:42 [INFO]: Epoch 032 - training loss: 0.3769, validation loss: 0.7044
2024-06-03 11:44:03 [INFO]: Epoch 033 - training loss: 3.5293, validation loss: 17.8601
2024-06-03 11:44:22 [INFO]: Epoch 034 - training loss: 4.3590, validation loss: 5.0523
2024-06-03 11:44:43 [INFO]: Epoch 035 - training loss: 2.7928, validation loss: 2.8378
2024-06-03 11:45:05 [INFO]: Epoch 036 - training loss: 2.0211, validation loss: 1.8978
2024-06-03 11:45:24 [INFO]: Epoch 037 - training loss: 1.6563, validation loss: 1.5495
2024-06-03 11:45:45 [INFO]: Epoch 038 - training loss: 1.4781, validation loss: 1.3449
2024-06-03 11:46:05 [INFO]: Epoch 039 - training loss: 1.3684, validation loss: 1.2439
2024-06-03 11:46:25 [INFO]: Epoch 040 - training loss: 1.2882, validation loss: 1.1940
2024-06-03 11:46:47 [INFO]: Epoch 041 - training loss: 1.2438, validation loss: 1.1299
2024-06-03 11:46:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:46:47 [INFO]: Finished training. The best model is from epoch#31.
2024-06-03 11:46:47 [INFO]: Saved the model to results_block_rate05/PeMS/Koopa_PeMS/round_2/20240603_T113201/Koopa.pypots
2024-06-03 11:46:49 [INFO]: Successfully saved to results_block_rate05/PeMS/Koopa_PeMS/round_2/imputation.pkl
2024-06-03 11:46:49 [INFO]: Round2 - Koopa on PeMS: MAE=0.8081, MSE=1.4891, MRE=0.9676
2024-06-03 11:46:49 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 11:46:49 [INFO]: Using the given device: cuda:0
2024-06-03 11:46:49 [INFO]: Model files will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_3/20240603_T114649
2024-06-03 11:46:49 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_3/20240603_T114649/tensorboard
2024-06-03 11:46:50 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 11:47:11 [INFO]: Epoch 001 - training loss: 1.1718, validation loss: 1.0905
2024-06-03 11:47:30 [INFO]: Epoch 002 - training loss: 0.7881, validation loss: 1.0984
2024-06-03 11:47:51 [INFO]: Epoch 003 - training loss: 0.7157, validation loss: 1.0319
2024-06-03 11:48:02 [INFO]: Epoch 004 - training loss: 0.6665, validation loss: 0.9947
2024-06-03 11:48:10 [INFO]: Epoch 005 - training loss: 0.6032, validation loss: 0.9090
2024-06-03 11:48:20 [INFO]: Epoch 006 - training loss: 0.5618, validation loss: 0.8735
2024-06-03 11:48:34 [INFO]: Epoch 007 - training loss: 0.5325, validation loss: 0.8780
2024-06-03 11:48:46 [INFO]: Epoch 008 - training loss: 0.5145, validation loss: 0.8017
2024-06-03 11:48:58 [INFO]: Epoch 009 - training loss: 0.4975, validation loss: 0.7647
2024-06-03 11:49:12 [INFO]: Epoch 010 - training loss: 0.4822, validation loss: 0.7521
2024-06-03 11:49:25 [INFO]: Epoch 011 - training loss: 0.4653, validation loss: 0.7462
2024-06-03 11:49:38 [INFO]: Epoch 012 - training loss: 0.4510, validation loss: 0.7471
2024-06-03 11:49:51 [INFO]: Epoch 013 - training loss: 0.4332, validation loss: 0.7099
2024-06-03 11:50:04 [INFO]: Epoch 014 - training loss: 0.4187, validation loss: 0.6943
2024-06-03 11:50:17 [INFO]: Epoch 015 - training loss: 0.4087, validation loss: 0.6956
2024-06-03 11:50:30 [INFO]: Epoch 016 - training loss: 0.4008, validation loss: 0.6853
2024-06-03 11:50:43 [INFO]: Epoch 017 - training loss: 0.3967, validation loss: 0.6764
2024-06-03 11:50:56 [INFO]: Epoch 018 - training loss: 0.3901, validation loss: 0.6621
2024-06-03 11:51:09 [INFO]: Epoch 019 - training loss: 0.3841, validation loss: 0.6667
2024-06-03 11:51:22 [INFO]: Epoch 020 - training loss: 0.3745, validation loss: 0.6559
2024-06-03 11:51:35 [INFO]: Epoch 021 - training loss: 0.3784, validation loss: 0.6505
2024-06-03 11:51:47 [INFO]: Epoch 022 - training loss: 0.3722, validation loss: 0.6444
2024-06-03 11:51:59 [INFO]: Epoch 023 - training loss: 0.3693, validation loss: 0.6430
2024-06-03 11:52:12 [INFO]: Epoch 024 - training loss: 0.3610, validation loss: 0.6427
2024-06-03 11:52:24 [INFO]: Epoch 025 - training loss: 0.3599, validation loss: 0.6357
2024-06-03 11:52:36 [INFO]: Epoch 026 - training loss: 0.3523, validation loss: 0.6340
2024-06-03 11:52:48 [INFO]: Epoch 027 - training loss: 0.3508, validation loss: 0.6297
2024-06-03 11:53:02 [INFO]: Epoch 028 - training loss: 0.3541, validation loss: 0.6390
2024-06-03 11:53:15 [INFO]: Epoch 029 - training loss: 0.3475, validation loss: 0.6334
2024-06-03 11:53:27 [INFO]: Epoch 030 - training loss: 0.3457, validation loss: 0.6377
2024-06-03 11:53:40 [INFO]: Epoch 031 - training loss: 0.3426, validation loss: 0.6111
2024-06-03 11:53:53 [INFO]: Epoch 032 - training loss: 0.3358, validation loss: 0.6227
2024-06-03 11:54:06 [INFO]: Epoch 033 - training loss: 0.3406, validation loss: 0.6205
2024-06-03 11:54:19 [INFO]: Epoch 034 - training loss: 0.3386, validation loss: 0.6205
2024-06-03 11:54:31 [INFO]: Epoch 035 - training loss: 0.3339, validation loss: 0.6203
2024-06-03 11:54:44 [INFO]: Epoch 036 - training loss: 0.3320, validation loss: 0.6174
2024-06-03 11:54:55 [INFO]: Epoch 037 - training loss: 0.3279, validation loss: 0.6100
2024-06-03 11:55:07 [INFO]: Epoch 038 - training loss: 0.3288, validation loss: 0.6105
2024-06-03 11:55:20 [INFO]: Epoch 039 - training loss: 0.3316, validation loss: 0.6106
2024-06-03 11:55:31 [INFO]: Epoch 040 - training loss: 0.3283, validation loss: 0.6093
2024-06-03 11:55:44 [INFO]: Epoch 041 - training loss: 0.3293, validation loss: 0.6096
2024-06-03 11:55:53 [INFO]: Epoch 042 - training loss: 0.3235, validation loss: 0.6099
2024-06-03 11:56:06 [INFO]: Epoch 043 - training loss: 0.3220, validation loss: 0.6088
2024-06-03 11:56:20 [INFO]: Epoch 044 - training loss: 0.3214, validation loss: 0.6177
2024-06-03 11:56:33 [INFO]: Epoch 045 - training loss: 0.3202, validation loss: 0.6114
2024-06-03 11:56:46 [INFO]: Epoch 046 - training loss: 0.3191, validation loss: 0.6189
2024-06-03 11:56:58 [INFO]: Epoch 047 - training loss: 0.3166, validation loss: 0.6190
2024-06-03 11:57:11 [INFO]: Epoch 048 - training loss: 0.3160, validation loss: 0.6051
2024-06-03 11:57:23 [INFO]: Epoch 049 - training loss: 0.3172, validation loss: 0.6139
2024-06-03 11:57:36 [INFO]: Epoch 050 - training loss: 0.3190, validation loss: 0.6161
2024-06-03 11:57:49 [INFO]: Epoch 051 - training loss: 0.3218, validation loss: 0.6075
2024-06-03 11:58:02 [INFO]: Epoch 052 - training loss: 0.3183, validation loss: 0.6176
2024-06-03 11:58:14 [INFO]: Epoch 053 - training loss: 0.3156, validation loss: 0.6088
2024-06-03 11:58:27 [INFO]: Epoch 054 - training loss: 0.3113, validation loss: 0.6281
2024-06-03 11:58:40 [INFO]: Epoch 055 - training loss: 0.3087, validation loss: 0.6084
2024-06-03 11:58:52 [INFO]: Epoch 056 - training loss: 0.3174, validation loss: 0.6214
2024-06-03 11:59:06 [INFO]: Epoch 057 - training loss: 0.3101, validation loss: 0.6116
2024-06-03 11:59:18 [INFO]: Epoch 058 - training loss: 0.3073, validation loss: 0.6237
2024-06-03 11:59:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:59:18 [INFO]: Finished training. The best model is from epoch#48.
2024-06-03 11:59:18 [INFO]: Saved the model to results_block_rate05/PeMS/Koopa_PeMS/round_3/20240603_T114649/Koopa.pypots
2024-06-03 11:59:20 [INFO]: Successfully saved to results_block_rate05/PeMS/Koopa_PeMS/round_3/imputation.pkl
2024-06-03 11:59:20 [INFO]: Round3 - Koopa on PeMS: MAE=0.4991, MSE=0.9481, MRE=0.5975
2024-06-03 11:59:20 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 11:59:20 [INFO]: Using the given device: cuda:0
2024-06-03 11:59:20 [INFO]: Model files will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_4/20240603_T115920
2024-06-03 11:59:20 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Koopa_PeMS/round_4/20240603_T115920/tensorboard
2024-06-03 11:59:20 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 11:59:34 [INFO]: Epoch 001 - training loss: 1.1114, validation loss: 1.0676
2024-06-03 11:59:46 [INFO]: Epoch 002 - training loss: 0.7646, validation loss: 1.0554
2024-06-03 12:00:00 [INFO]: Epoch 003 - training loss: 0.6785, validation loss: 1.0480
2024-06-03 12:00:13 [INFO]: Epoch 004 - training loss: 0.6257, validation loss: 0.9615
2024-06-03 12:00:25 [INFO]: Epoch 005 - training loss: 0.5904, validation loss: 0.8947
2024-06-03 12:00:38 [INFO]: Epoch 006 - training loss: 0.5533, validation loss: 0.8571
2024-06-03 12:00:50 [INFO]: Epoch 007 - training loss: 0.5286, validation loss: 0.8488
2024-06-03 12:01:02 [INFO]: Epoch 008 - training loss: 0.5040, validation loss: 0.8259
2024-06-03 12:01:15 [INFO]: Epoch 009 - training loss: 0.4861, validation loss: 0.7852
2024-06-03 12:01:29 [INFO]: Epoch 010 - training loss: 0.4807, validation loss: 0.8161
2024-06-03 12:01:42 [INFO]: Epoch 011 - training loss: 0.4730, validation loss: 0.7887
2024-06-03 12:01:56 [INFO]: Epoch 012 - training loss: 0.4557, validation loss: 0.7541
2024-06-03 12:02:09 [INFO]: Epoch 013 - training loss: 0.4430, validation loss: 0.7645
2024-06-03 12:02:21 [INFO]: Epoch 014 - training loss: 0.4315, validation loss: 0.7772
2024-06-03 12:02:35 [INFO]: Epoch 015 - training loss: 0.4232, validation loss: 0.7436
2024-06-03 12:02:47 [INFO]: Epoch 016 - training loss: 0.4209, validation loss: 0.7454
2024-06-03 12:03:00 [INFO]: Epoch 017 - training loss: 0.4197, validation loss: 0.7039
2024-06-03 12:03:13 [INFO]: Epoch 018 - training loss: 0.4038, validation loss: 0.6985
2024-06-03 12:03:25 [INFO]: Epoch 019 - training loss: 0.3989, validation loss: 0.7032
2024-06-03 12:03:37 [INFO]: Epoch 020 - training loss: 0.3928, validation loss: 0.7078
2024-06-03 12:03:51 [INFO]: Epoch 021 - training loss: 0.3873, validation loss: 0.6754
2024-06-03 12:04:03 [INFO]: Epoch 022 - training loss: 0.3802, validation loss: 0.6682
2024-06-03 12:04:16 [INFO]: Epoch 023 - training loss: 0.3809, validation loss: 0.6671
2024-06-03 12:04:29 [INFO]: Epoch 024 - training loss: 0.3796, validation loss: 0.6779
2024-06-03 12:04:43 [INFO]: Epoch 025 - training loss: 0.3755, validation loss: 0.6611
2024-06-03 12:04:56 [INFO]: Epoch 026 - training loss: 0.3733, validation loss: 0.6516
2024-06-03 12:05:10 [INFO]: Epoch 027 - training loss: 0.3669, validation loss: 0.6510
2024-06-03 12:05:23 [INFO]: Epoch 028 - training loss: 0.3664, validation loss: 0.6586
2024-06-03 12:05:36 [INFO]: Epoch 029 - training loss: 0.3645, validation loss: 0.6430
2024-06-03 12:05:50 [INFO]: Epoch 030 - training loss: 0.3607, validation loss: 0.6527
2024-06-03 12:06:03 [INFO]: Epoch 031 - training loss: 0.3584, validation loss: 0.6424
2024-06-03 12:06:15 [INFO]: Epoch 032 - training loss: 0.3526, validation loss: 0.6436
2024-06-03 12:06:28 [INFO]: Epoch 033 - training loss: 0.3490, validation loss: 0.6316
2024-06-03 12:06:41 [INFO]: Epoch 034 - training loss: 0.3507, validation loss: 0.6380
2024-06-03 12:06:54 [INFO]: Epoch 035 - training loss: 0.3509, validation loss: 0.6350
2024-06-03 12:07:07 [INFO]: Epoch 036 - training loss: 0.3479, validation loss: 0.6402
2024-06-03 12:07:20 [INFO]: Epoch 037 - training loss: 0.3441, validation loss: 0.6382
2024-06-03 12:07:32 [INFO]: Epoch 038 - training loss: 0.3394, validation loss: 0.6356
2024-06-03 12:07:45 [INFO]: Epoch 039 - training loss: 0.3447, validation loss: 0.6305
2024-06-03 12:07:57 [INFO]: Epoch 040 - training loss: 0.3386, validation loss: 0.6342
2024-06-03 12:08:09 [INFO]: Epoch 041 - training loss: 0.3372, validation loss: 0.6326
2024-06-03 12:08:22 [INFO]: Epoch 042 - training loss: 0.3364, validation loss: 0.6227
2024-06-03 12:08:35 [INFO]: Epoch 043 - training loss: 0.3359, validation loss: 0.6207
2024-06-03 12:08:48 [INFO]: Epoch 044 - training loss: 0.3322, validation loss: 0.6164
2024-06-03 12:09:00 [INFO]: Epoch 045 - training loss: 0.3356, validation loss: 0.6297
2024-06-03 12:09:14 [INFO]: Epoch 046 - training loss: 0.3390, validation loss: 0.6236
2024-06-03 12:09:26 [INFO]: Epoch 047 - training loss: 0.3287, validation loss: 0.6245
2024-06-03 12:09:39 [INFO]: Epoch 048 - training loss: 0.3324, validation loss: 0.6091
2024-06-03 12:09:52 [INFO]: Epoch 049 - training loss: 0.3306, validation loss: 0.6187
2024-06-03 12:10:05 [INFO]: Epoch 050 - training loss: 0.3308, validation loss: 0.6257
2024-06-03 12:10:18 [INFO]: Epoch 051 - training loss: 0.3267, validation loss: 0.6169
2024-06-03 12:10:31 [INFO]: Epoch 052 - training loss: 0.3261, validation loss: 0.6133
2024-06-03 12:10:43 [INFO]: Epoch 053 - training loss: 0.3289, validation loss: 0.6233
2024-06-03 12:10:57 [INFO]: Epoch 054 - training loss: 0.3351, validation loss: 0.6153
2024-06-03 12:11:09 [INFO]: Epoch 055 - training loss: 0.3294, validation loss: 0.6151
2024-06-03 12:11:23 [INFO]: Epoch 056 - training loss: 0.3253, validation loss: 0.6123
2024-06-03 12:11:35 [INFO]: Epoch 057 - training loss: 0.3190, validation loss: 0.6055
2024-06-03 12:11:48 [INFO]: Epoch 058 - training loss: 0.3189, validation loss: 0.6156
2024-06-03 12:12:01 [INFO]: Epoch 059 - training loss: 0.3168, validation loss: 0.6084
2024-06-03 12:12:13 [INFO]: Epoch 060 - training loss: 0.3175, validation loss: 0.6116
2024-06-03 12:12:26 [INFO]: Epoch 061 - training loss: 0.3203, validation loss: 0.6016
2024-06-03 12:12:39 [INFO]: Epoch 062 - training loss: 0.3221, validation loss: 0.6145
2024-06-03 12:12:52 [INFO]: Epoch 063 - training loss: 0.3203, validation loss: 0.6263
2024-06-03 12:13:04 [INFO]: Epoch 064 - training loss: 0.3165, validation loss: 0.6077
2024-06-03 12:13:18 [INFO]: Epoch 065 - training loss: 0.3134, validation loss: 0.6118
2024-06-03 12:13:31 [INFO]: Epoch 066 - training loss: 0.3154, validation loss: 0.6143
2024-06-03 12:13:44 [INFO]: Epoch 067 - training loss: 0.3113, validation loss: 0.6089
2024-06-03 12:13:57 [INFO]: Epoch 068 - training loss: 0.3134, validation loss: 0.6176
2024-06-03 12:14:10 [INFO]: Epoch 069 - training loss: 0.3256, validation loss: 0.6198
2024-06-03 12:14:22 [INFO]: Epoch 070 - training loss: 0.3196, validation loss: 0.6151
2024-06-03 12:14:36 [INFO]: Epoch 071 - training loss: 0.3155, validation loss: 0.6100
2024-06-03 12:14:36 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:14:36 [INFO]: Finished training. The best model is from epoch#61.
2024-06-03 12:14:36 [INFO]: Saved the model to results_block_rate05/PeMS/Koopa_PeMS/round_4/20240603_T115920/Koopa.pypots
2024-06-03 12:14:38 [INFO]: Successfully saved to results_block_rate05/PeMS/Koopa_PeMS/round_4/imputation.pkl
2024-06-03 12:14:38 [INFO]: Round4 - Koopa on PeMS: MAE=0.4915, MSE=0.9385, MRE=0.5885
2024-06-03 12:14:38 [INFO]: Done! Final results:
Averaged Koopa (13,306,214 params) on PeMS: MAE=0.6247 ± 0.1493010656941482, MSE=1.1584 ± 0.2534114584539585, MRE=0.7480 ± 0.17876743261013223, average inference time=0.47
