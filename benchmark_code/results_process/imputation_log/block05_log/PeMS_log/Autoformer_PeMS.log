2024-06-03 10:09:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:09:54 [INFO]: Using the given device: cuda:0
2024-06-03 10:09:55 [INFO]: Model files will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_0/20240603_T100955
2024-06-03 10:09:55 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_0/20240603_T100955/tensorboard
2024-06-03 10:10:00 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 10:10:28 [INFO]: Epoch 001 - training loss: 1.7241, validation loss: 1.5929
2024-06-03 10:10:49 [INFO]: Epoch 002 - training loss: 1.6190, validation loss: 1.5112
2024-06-03 10:11:15 [INFO]: Epoch 003 - training loss: 1.5534, validation loss: 1.4583
2024-06-03 10:11:40 [INFO]: Epoch 004 - training loss: 1.5064, validation loss: 1.4274
2024-06-03 10:12:04 [INFO]: Epoch 005 - training loss: 1.4526, validation loss: 1.4019
2024-06-03 10:12:28 [INFO]: Epoch 006 - training loss: 1.3727, validation loss: 1.3743
2024-06-03 10:12:52 [INFO]: Epoch 007 - training loss: 1.2542, validation loss: 1.3490
2024-06-03 10:13:17 [INFO]: Epoch 008 - training loss: 1.1003, validation loss: 1.3070
2024-06-03 10:13:40 [INFO]: Epoch 009 - training loss: 0.9778, validation loss: 1.2746
2024-06-03 10:14:02 [INFO]: Epoch 010 - training loss: 0.8950, validation loss: 1.1503
2024-06-03 10:14:26 [INFO]: Epoch 011 - training loss: 0.8352, validation loss: 1.0827
2024-06-03 10:14:51 [INFO]: Epoch 012 - training loss: 0.7990, validation loss: 1.0573
2024-06-03 10:15:15 [INFO]: Epoch 013 - training loss: 0.7878, validation loss: 1.1232
2024-06-03 10:15:37 [INFO]: Epoch 014 - training loss: 0.7852, validation loss: 1.0844
2024-06-03 10:15:59 [INFO]: Epoch 015 - training loss: 0.7679, validation loss: 1.0646
2024-06-03 10:16:24 [INFO]: Epoch 016 - training loss: 0.7596, validation loss: 1.0402
2024-06-03 10:16:47 [INFO]: Epoch 017 - training loss: 0.7517, validation loss: 1.0207
2024-06-03 10:17:08 [INFO]: Epoch 018 - training loss: 0.7478, validation loss: 1.0175
2024-06-03 10:17:33 [INFO]: Epoch 019 - training loss: 0.7438, validation loss: 0.9966
2024-06-03 10:17:58 [INFO]: Epoch 020 - training loss: 0.7397, validation loss: 0.9923
2024-06-03 10:18:21 [INFO]: Epoch 021 - training loss: 0.7362, validation loss: 0.9842
2024-06-03 10:18:42 [INFO]: Epoch 022 - training loss: 0.7260, validation loss: 0.9764
2024-06-03 10:19:04 [INFO]: Epoch 023 - training loss: 0.7221, validation loss: 0.9695
2024-06-03 10:19:29 [INFO]: Epoch 024 - training loss: 0.7243, validation loss: 0.9675
2024-06-03 10:19:54 [INFO]: Epoch 025 - training loss: 0.7215, validation loss: 0.9640
2024-06-03 10:20:17 [INFO]: Epoch 026 - training loss: 0.7188, validation loss: 0.9601
2024-06-03 10:20:39 [INFO]: Epoch 027 - training loss: 0.7179, validation loss: 0.9582
2024-06-03 10:21:04 [INFO]: Epoch 028 - training loss: 0.7112, validation loss: 0.9541
2024-06-03 10:21:27 [INFO]: Epoch 029 - training loss: 0.7117, validation loss: 0.9506
2024-06-03 10:21:51 [INFO]: Epoch 030 - training loss: 0.7093, validation loss: 0.9579
2024-06-03 10:22:12 [INFO]: Epoch 031 - training loss: 0.7115, validation loss: 0.9569
2024-06-03 10:22:37 [INFO]: Epoch 032 - training loss: 0.7026, validation loss: 0.9506
2024-06-03 10:23:03 [INFO]: Epoch 033 - training loss: 0.7025, validation loss: 0.9462
2024-06-03 10:23:27 [INFO]: Epoch 034 - training loss: 0.7058, validation loss: 0.9425
2024-06-03 10:23:49 [INFO]: Epoch 035 - training loss: 0.7045, validation loss: 0.9470
2024-06-03 10:24:14 [INFO]: Epoch 036 - training loss: 0.7024, validation loss: 0.9422
2024-06-03 10:24:38 [INFO]: Epoch 037 - training loss: 0.6987, validation loss: 0.9394
2024-06-03 10:25:00 [INFO]: Epoch 038 - training loss: 0.6992, validation loss: 0.9380
2024-06-03 10:25:22 [INFO]: Epoch 039 - training loss: 0.6952, validation loss: 0.9451
2024-06-03 10:25:44 [INFO]: Epoch 040 - training loss: 0.6946, validation loss: 0.9460
2024-06-03 10:26:07 [INFO]: Epoch 041 - training loss: 0.6975, validation loss: 0.9279
2024-06-03 10:26:31 [INFO]: Epoch 042 - training loss: 0.6936, validation loss: 0.9374
2024-06-03 10:26:53 [INFO]: Epoch 043 - training loss: 0.6983, validation loss: 0.9315
2024-06-03 10:27:15 [INFO]: Epoch 044 - training loss: 0.6944, validation loss: 0.9319
2024-06-03 10:27:37 [INFO]: Epoch 045 - training loss: 0.6884, validation loss: 0.9360
2024-06-03 10:28:00 [INFO]: Epoch 046 - training loss: 0.6897, validation loss: 0.9301
2024-06-03 10:28:23 [INFO]: Epoch 047 - training loss: 0.6916, validation loss: 0.9353
2024-06-03 10:28:42 [INFO]: Epoch 048 - training loss: 0.6873, validation loss: 0.9312
2024-06-03 10:29:04 [INFO]: Epoch 049 - training loss: 0.6901, validation loss: 0.9286
2024-06-03 10:29:28 [INFO]: Epoch 050 - training loss: 0.6837, validation loss: 0.9300
2024-06-03 10:29:51 [INFO]: Epoch 051 - training loss: 0.6892, validation loss: 0.9287
2024-06-03 10:29:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:29:51 [INFO]: Finished training. The best model is from epoch#41.
2024-06-03 10:29:51 [INFO]: Saved the model to results_block_rate05/PeMS/Autoformer_PeMS/round_0/20240603_T100955/Autoformer.pypots
2024-06-03 10:29:58 [INFO]: Successfully saved to results_block_rate05/PeMS/Autoformer_PeMS/round_0/imputation.pkl
2024-06-03 10:29:58 [INFO]: Round0 - Autoformer on PeMS: MAE=0.5835, MSE=1.2264, MRE=0.6987
2024-06-03 10:29:58 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:29:58 [INFO]: Using the given device: cuda:0
2024-06-03 10:29:58 [INFO]: Model files will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_1/20240603_T102958
2024-06-03 10:29:58 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_1/20240603_T102958/tensorboard
2024-06-03 10:29:58 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 10:30:18 [INFO]: Epoch 001 - training loss: 1.7393, validation loss: 1.6569
2024-06-03 10:30:40 [INFO]: Epoch 002 - training loss: 1.6437, validation loss: 1.5407
2024-06-03 10:31:03 [INFO]: Epoch 003 - training loss: 1.5740, validation loss: 1.4759
2024-06-03 10:31:26 [INFO]: Epoch 004 - training loss: 1.5217, validation loss: 1.4401
2024-06-03 10:31:47 [INFO]: Epoch 005 - training loss: 1.4947, validation loss: 1.4207
2024-06-03 10:32:10 [INFO]: Epoch 006 - training loss: 1.4326, validation loss: 1.3989
2024-06-03 10:32:33 [INFO]: Epoch 007 - training loss: 1.3400, validation loss: 1.3824
2024-06-03 10:32:55 [INFO]: Epoch 008 - training loss: 1.2269, validation loss: 1.3619
2024-06-03 10:33:16 [INFO]: Epoch 009 - training loss: 1.0753, validation loss: 1.3032
2024-06-03 10:33:35 [INFO]: Epoch 010 - training loss: 0.9508, validation loss: 1.2714
2024-06-03 10:33:58 [INFO]: Epoch 011 - training loss: 0.8794, validation loss: 1.2105
2024-06-03 10:34:19 [INFO]: Epoch 012 - training loss: 0.8293, validation loss: 1.1571
2024-06-03 10:34:42 [INFO]: Epoch 013 - training loss: 0.7993, validation loss: 1.1264
2024-06-03 10:35:02 [INFO]: Epoch 014 - training loss: 0.7835, validation loss: 1.0912
2024-06-03 10:35:23 [INFO]: Epoch 015 - training loss: 0.7676, validation loss: 1.0758
2024-06-03 10:35:44 [INFO]: Epoch 016 - training loss: 0.7718, validation loss: 1.0443
2024-06-03 10:36:07 [INFO]: Epoch 017 - training loss: 0.7539, validation loss: 1.0520
2024-06-03 10:36:27 [INFO]: Epoch 018 - training loss: 0.7500, validation loss: 1.0311
2024-06-03 10:36:47 [INFO]: Epoch 019 - training loss: 0.7468, validation loss: 1.0301
2024-06-03 10:37:08 [INFO]: Epoch 020 - training loss: 0.7397, validation loss: 1.0099
2024-06-03 10:37:30 [INFO]: Epoch 021 - training loss: 0.7374, validation loss: 1.0042
2024-06-03 10:37:51 [INFO]: Epoch 022 - training loss: 0.7383, validation loss: 1.0060
2024-06-03 10:38:12 [INFO]: Epoch 023 - training loss: 0.7266, validation loss: 0.9870
2024-06-03 10:38:32 [INFO]: Epoch 024 - training loss: 0.7218, validation loss: 0.9840
2024-06-03 10:38:54 [INFO]: Epoch 025 - training loss: 0.7193, validation loss: 0.9766
2024-06-03 10:39:16 [INFO]: Epoch 026 - training loss: 0.7142, validation loss: 0.9814
2024-06-03 10:39:37 [INFO]: Epoch 027 - training loss: 0.7121, validation loss: 0.9739
2024-06-03 10:39:58 [INFO]: Epoch 028 - training loss: 0.7072, validation loss: 0.9724
2024-06-03 10:40:18 [INFO]: Epoch 029 - training loss: 0.7075, validation loss: 0.9605
2024-06-03 10:40:39 [INFO]: Epoch 030 - training loss: 0.7084, validation loss: 0.9798
2024-06-03 10:41:01 [INFO]: Epoch 031 - training loss: 0.7085, validation loss: 0.9678
2024-06-03 10:41:23 [INFO]: Epoch 032 - training loss: 0.7071, validation loss: 0.9633
2024-06-03 10:41:42 [INFO]: Epoch 033 - training loss: 0.7025, validation loss: 0.9581
2024-06-03 10:42:04 [INFO]: Epoch 034 - training loss: 0.7004, validation loss: 0.9663
2024-06-03 10:42:25 [INFO]: Epoch 035 - training loss: 0.7012, validation loss: 0.9605
2024-06-03 10:42:46 [INFO]: Epoch 036 - training loss: 0.6993, validation loss: 0.9638
2024-06-03 10:43:07 [INFO]: Epoch 037 - training loss: 0.7044, validation loss: 0.9587
2024-06-03 10:43:25 [INFO]: Epoch 038 - training loss: 0.6923, validation loss: 0.9668
2024-06-03 10:43:47 [INFO]: Epoch 039 - training loss: 0.6914, validation loss: 0.9598
2024-06-03 10:44:07 [INFO]: Epoch 040 - training loss: 0.6941, validation loss: 0.9592
2024-06-03 10:44:28 [INFO]: Epoch 041 - training loss: 0.6894, validation loss: 0.9521
2024-06-03 10:44:48 [INFO]: Epoch 042 - training loss: 0.6892, validation loss: 0.9531
2024-06-03 10:45:05 [INFO]: Epoch 043 - training loss: 0.6933, validation loss: 0.9587
2024-06-03 10:45:24 [INFO]: Epoch 044 - training loss: 0.6895, validation loss: 0.9581
2024-06-03 10:45:44 [INFO]: Epoch 045 - training loss: 0.6917, validation loss: 0.9581
2024-06-03 10:46:03 [INFO]: Epoch 046 - training loss: 0.6877, validation loss: 0.9616
2024-06-03 10:46:22 [INFO]: Epoch 047 - training loss: 0.6878, validation loss: 0.9647
2024-06-03 10:46:39 [INFO]: Epoch 048 - training loss: 0.6884, validation loss: 0.9547
2024-06-03 10:46:58 [INFO]: Epoch 049 - training loss: 0.6833, validation loss: 0.9591
2024-06-03 10:47:17 [INFO]: Epoch 050 - training loss: 0.6840, validation loss: 0.9551
2024-06-03 10:47:36 [INFO]: Epoch 051 - training loss: 0.6921, validation loss: 0.9501
2024-06-03 10:47:55 [INFO]: Epoch 052 - training loss: 0.6854, validation loss: 0.9666
2024-06-03 10:48:12 [INFO]: Epoch 053 - training loss: 0.6828, validation loss: 0.9557
2024-06-03 10:48:28 [INFO]: Epoch 054 - training loss: 0.6751, validation loss: 0.9519
2024-06-03 10:48:47 [INFO]: Epoch 055 - training loss: 0.6802, validation loss: 0.9634
2024-06-03 10:49:05 [INFO]: Epoch 056 - training loss: 0.6830, validation loss: 0.9665
2024-06-03 10:49:25 [INFO]: Epoch 057 - training loss: 0.6813, validation loss: 0.9619
2024-06-03 10:49:43 [INFO]: Epoch 058 - training loss: 0.6785, validation loss: 0.9648
2024-06-03 10:50:00 [INFO]: Epoch 059 - training loss: 0.6759, validation loss: 0.9644
2024-06-03 10:50:18 [INFO]: Epoch 060 - training loss: 0.6748, validation loss: 0.9598
2024-06-03 10:50:37 [INFO]: Epoch 061 - training loss: 0.6807, validation loss: 0.9703
2024-06-03 10:50:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:50:37 [INFO]: Finished training. The best model is from epoch#51.
2024-06-03 10:50:37 [INFO]: Saved the model to results_block_rate05/PeMS/Autoformer_PeMS/round_1/20240603_T102958/Autoformer.pypots
2024-06-03 10:50:42 [INFO]: Successfully saved to results_block_rate05/PeMS/Autoformer_PeMS/round_1/imputation.pkl
2024-06-03 10:50:42 [INFO]: Round1 - Autoformer on PeMS: MAE=0.6235, MSE=1.3012, MRE=0.7465
2024-06-03 10:50:42 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:50:42 [INFO]: Using the given device: cuda:0
2024-06-03 10:50:42 [INFO]: Model files will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_2/20240603_T105042
2024-06-03 10:50:42 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_2/20240603_T105042/tensorboard
2024-06-03 10:50:42 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 10:51:01 [INFO]: Epoch 001 - training loss: 1.7337, validation loss: 1.6294
2024-06-03 10:51:18 [INFO]: Epoch 002 - training loss: 1.6335, validation loss: 1.5234
2024-06-03 10:51:35 [INFO]: Epoch 003 - training loss: 1.5530, validation loss: 1.4619
2024-06-03 10:51:54 [INFO]: Epoch 004 - training loss: 1.5054, validation loss: 1.4258
2024-06-03 10:52:13 [INFO]: Epoch 005 - training loss: 1.4523, validation loss: 1.4121
2024-06-03 10:52:31 [INFO]: Epoch 006 - training loss: 1.3796, validation loss: 1.3841
2024-06-03 10:52:49 [INFO]: Epoch 007 - training loss: 1.2759, validation loss: 1.3490
2024-06-03 10:53:05 [INFO]: Epoch 008 - training loss: 1.1209, validation loss: 1.3054
2024-06-03 10:53:24 [INFO]: Epoch 009 - training loss: 0.9920, validation loss: 1.2651
2024-06-03 10:53:42 [INFO]: Epoch 010 - training loss: 0.8941, validation loss: 1.1069
2024-06-03 10:54:01 [INFO]: Epoch 011 - training loss: 0.8563, validation loss: 1.0941
2024-06-03 10:54:19 [INFO]: Epoch 012 - training loss: 0.8136, validation loss: 1.0857
2024-06-03 10:54:36 [INFO]: Epoch 013 - training loss: 0.7826, validation loss: 1.0475
2024-06-03 10:54:52 [INFO]: Epoch 014 - training loss: 0.7706, validation loss: 1.0472
2024-06-03 10:55:10 [INFO]: Epoch 015 - training loss: 0.7607, validation loss: 0.9987
2024-06-03 10:55:27 [INFO]: Epoch 016 - training loss: 0.7566, validation loss: 1.0078
2024-06-03 10:55:45 [INFO]: Epoch 017 - training loss: 0.7468, validation loss: 0.9885
2024-06-03 10:56:02 [INFO]: Epoch 018 - training loss: 0.7397, validation loss: 1.0016
2024-06-03 10:56:18 [INFO]: Epoch 019 - training loss: 0.7422, validation loss: 0.9863
2024-06-03 10:56:35 [INFO]: Epoch 020 - training loss: 0.7351, validation loss: 0.9802
2024-06-03 10:56:53 [INFO]: Epoch 021 - training loss: 0.7347, validation loss: 0.9698
2024-06-03 10:57:10 [INFO]: Epoch 022 - training loss: 0.7293, validation loss: 0.9780
2024-06-03 10:57:26 [INFO]: Epoch 023 - training loss: 0.7266, validation loss: 0.9850
2024-06-03 10:57:43 [INFO]: Epoch 024 - training loss: 0.7197, validation loss: 0.9850
2024-06-03 10:57:58 [INFO]: Epoch 025 - training loss: 0.7231, validation loss: 0.9653
2024-06-03 10:58:16 [INFO]: Epoch 026 - training loss: 0.7155, validation loss: 0.9767
2024-06-03 10:58:33 [INFO]: Epoch 027 - training loss: 0.7164, validation loss: 0.9629
2024-06-03 10:58:51 [INFO]: Epoch 028 - training loss: 0.7155, validation loss: 0.9700
2024-06-03 10:59:08 [INFO]: Epoch 029 - training loss: 0.7090, validation loss: 0.9629
2024-06-03 10:59:23 [INFO]: Epoch 030 - training loss: 0.7089, validation loss: 0.9711
2024-06-03 10:59:40 [INFO]: Epoch 031 - training loss: 0.7063, validation loss: 0.9602
2024-06-03 10:59:55 [INFO]: Epoch 032 - training loss: 0.7096, validation loss: 0.9663
2024-06-03 11:00:10 [INFO]: Epoch 033 - training loss: 0.7044, validation loss: 0.9709
2024-06-03 11:00:25 [INFO]: Epoch 034 - training loss: 0.7055, validation loss: 0.9569
2024-06-03 11:00:39 [INFO]: Epoch 035 - training loss: 0.7073, validation loss: 0.9609
2024-06-03 11:00:52 [INFO]: Epoch 036 - training loss: 0.7011, validation loss: 0.9602
2024-06-03 11:01:06 [INFO]: Epoch 037 - training loss: 0.7014, validation loss: 0.9593
2024-06-03 11:01:21 [INFO]: Epoch 038 - training loss: 0.7005, validation loss: 0.9722
2024-06-03 11:01:35 [INFO]: Epoch 039 - training loss: 0.6951, validation loss: 0.9746
2024-06-03 11:01:50 [INFO]: Epoch 040 - training loss: 0.6967, validation loss: 0.9526
2024-06-03 11:02:02 [INFO]: Epoch 041 - training loss: 0.6947, validation loss: 0.9620
2024-06-03 11:02:15 [INFO]: Epoch 042 - training loss: 0.6954, validation loss: 0.9564
2024-06-03 11:02:29 [INFO]: Epoch 043 - training loss: 0.6958, validation loss: 0.9707
2024-06-03 11:02:43 [INFO]: Epoch 044 - training loss: 0.6938, validation loss: 0.9663
2024-06-03 11:02:56 [INFO]: Epoch 045 - training loss: 0.6860, validation loss: 0.9748
2024-06-03 11:03:10 [INFO]: Epoch 046 - training loss: 0.6936, validation loss: 0.9778
2024-06-03 11:03:22 [INFO]: Epoch 047 - training loss: 0.6856, validation loss: 0.9763
2024-06-03 11:03:36 [INFO]: Epoch 048 - training loss: 0.6919, validation loss: 0.9630
2024-06-03 11:03:50 [INFO]: Epoch 049 - training loss: 0.6881, validation loss: 0.9560
2024-06-03 11:04:04 [INFO]: Epoch 050 - training loss: 0.6870, validation loss: 0.9649
2024-06-03 11:04:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:04:04 [INFO]: Finished training. The best model is from epoch#40.
2024-06-03 11:04:04 [INFO]: Saved the model to results_block_rate05/PeMS/Autoformer_PeMS/round_2/20240603_T105042/Autoformer.pypots
2024-06-03 11:04:07 [INFO]: Successfully saved to results_block_rate05/PeMS/Autoformer_PeMS/round_2/imputation.pkl
2024-06-03 11:04:07 [INFO]: Round2 - Autoformer on PeMS: MAE=0.6167, MSE=1.2814, MRE=0.7385
2024-06-03 11:04:07 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 11:04:07 [INFO]: Using the given device: cuda:0
2024-06-03 11:04:07 [INFO]: Model files will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_3/20240603_T110407
2024-06-03 11:04:07 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_3/20240603_T110407/tensorboard
2024-06-03 11:04:08 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 11:04:22 [INFO]: Epoch 001 - training loss: 1.7249, validation loss: 1.6028
2024-06-03 11:04:35 [INFO]: Epoch 002 - training loss: 1.6196, validation loss: 1.5352
2024-06-03 11:04:48 [INFO]: Epoch 003 - training loss: 1.5603, validation loss: 1.4757
2024-06-03 11:05:01 [INFO]: Epoch 004 - training loss: 1.5163, validation loss: 1.4442
2024-06-03 11:05:15 [INFO]: Epoch 005 - training loss: 1.4819, validation loss: 1.4234
2024-06-03 11:05:30 [INFO]: Epoch 006 - training loss: 1.4506, validation loss: 1.4073
2024-06-03 11:05:43 [INFO]: Epoch 007 - training loss: 1.3864, validation loss: 1.3999
2024-06-03 11:05:55 [INFO]: Epoch 008 - training loss: 1.3307, validation loss: 1.3915
2024-06-03 11:06:09 [INFO]: Epoch 009 - training loss: 1.2895, validation loss: 1.3801
2024-06-03 11:06:23 [INFO]: Epoch 010 - training loss: 1.1706, validation loss: 1.3442
2024-06-03 11:06:38 [INFO]: Epoch 011 - training loss: 1.0388, validation loss: 1.2370
2024-06-03 11:06:52 [INFO]: Epoch 012 - training loss: 0.9258, validation loss: 1.1907
2024-06-03 11:07:05 [INFO]: Epoch 013 - training loss: 0.8600, validation loss: 1.1506
2024-06-03 11:07:17 [INFO]: Epoch 014 - training loss: 0.8315, validation loss: 1.1617
2024-06-03 11:07:31 [INFO]: Epoch 015 - training loss: 0.8048, validation loss: 1.1192
2024-06-03 11:07:45 [INFO]: Epoch 016 - training loss: 0.7915, validation loss: 1.1149
2024-06-03 11:07:57 [INFO]: Epoch 017 - training loss: 0.7791, validation loss: 1.0949
2024-06-03 11:08:11 [INFO]: Epoch 018 - training loss: 0.7740, validation loss: 1.1014
2024-06-03 11:08:25 [INFO]: Epoch 019 - training loss: 0.7602, validation loss: 1.0912
2024-06-03 11:08:37 [INFO]: Epoch 020 - training loss: 0.7583, validation loss: 1.0972
2024-06-03 11:08:50 [INFO]: Epoch 021 - training loss: 0.7554, validation loss: 1.0898
2024-06-03 11:09:03 [INFO]: Epoch 022 - training loss: 0.7582, validation loss: 1.0855
2024-06-03 11:09:15 [INFO]: Epoch 023 - training loss: 0.7495, validation loss: 1.0828
2024-06-03 11:09:29 [INFO]: Epoch 024 - training loss: 0.7530, validation loss: 1.0827
2024-06-03 11:09:42 [INFO]: Epoch 025 - training loss: 0.7521, validation loss: 1.0842
2024-06-03 11:09:53 [INFO]: Epoch 026 - training loss: 0.7495, validation loss: 1.0775
2024-06-03 11:10:06 [INFO]: Epoch 027 - training loss: 0.7390, validation loss: 1.0707
2024-06-03 11:10:20 [INFO]: Epoch 028 - training loss: 0.7392, validation loss: 1.0696
2024-06-03 11:10:34 [INFO]: Epoch 029 - training loss: 0.7436, validation loss: 1.0725
2024-06-03 11:10:47 [INFO]: Epoch 030 - training loss: 0.7427, validation loss: 1.0689
2024-06-03 11:10:59 [INFO]: Epoch 031 - training loss: 0.7332, validation loss: 1.0707
2024-06-03 11:11:12 [INFO]: Epoch 032 - training loss: 0.7362, validation loss: 1.0716
2024-06-03 11:11:25 [INFO]: Epoch 033 - training loss: 0.7307, validation loss: 1.0745
2024-06-03 11:11:37 [INFO]: Epoch 034 - training loss: 0.7277, validation loss: 1.0658
2024-06-03 11:11:51 [INFO]: Epoch 035 - training loss: 0.7265, validation loss: 1.0643
2024-06-03 11:12:04 [INFO]: Epoch 036 - training loss: 0.7292, validation loss: 1.0608
2024-06-03 11:12:18 [INFO]: Epoch 037 - training loss: 0.7231, validation loss: 1.0590
2024-06-03 11:12:30 [INFO]: Epoch 038 - training loss: 0.7255, validation loss: 1.0589
2024-06-03 11:12:43 [INFO]: Epoch 039 - training loss: 0.7305, validation loss: 1.0735
2024-06-03 11:12:56 [INFO]: Epoch 040 - training loss: 0.7237, validation loss: 1.0578
2024-06-03 11:13:09 [INFO]: Epoch 041 - training loss: 0.7199, validation loss: 1.0561
2024-06-03 11:13:22 [INFO]: Epoch 042 - training loss: 0.7199, validation loss: 1.0563
2024-06-03 11:13:34 [INFO]: Epoch 043 - training loss: 0.7184, validation loss: 1.0541
2024-06-03 11:13:46 [INFO]: Epoch 044 - training loss: 0.7214, validation loss: 1.0545
2024-06-03 11:13:59 [INFO]: Epoch 045 - training loss: 0.7160, validation loss: 1.0520
2024-06-03 11:14:13 [INFO]: Epoch 046 - training loss: 0.7167, validation loss: 1.0578
2024-06-03 11:14:27 [INFO]: Epoch 047 - training loss: 0.7103, validation loss: 1.0547
2024-06-03 11:14:40 [INFO]: Epoch 048 - training loss: 0.7130, validation loss: 1.0546
2024-06-03 11:14:54 [INFO]: Epoch 049 - training loss: 0.7060, validation loss: 1.0486
2024-06-03 11:15:06 [INFO]: Epoch 050 - training loss: 0.7173, validation loss: 1.0567
2024-06-03 11:15:20 [INFO]: Epoch 051 - training loss: 0.7079, validation loss: 1.0478
2024-06-03 11:15:33 [INFO]: Epoch 052 - training loss: 0.7046, validation loss: 1.0521
2024-06-03 11:15:46 [INFO]: Epoch 053 - training loss: 0.7084, validation loss: 1.0583
2024-06-03 11:16:00 [INFO]: Epoch 054 - training loss: 0.7054, validation loss: 1.0431
2024-06-03 11:16:12 [INFO]: Epoch 055 - training loss: 0.7020, validation loss: 1.0510
2024-06-03 11:16:25 [INFO]: Epoch 056 - training loss: 0.7015, validation loss: 1.0522
2024-06-03 11:16:39 [INFO]: Epoch 057 - training loss: 0.6990, validation loss: 1.0531
2024-06-03 11:16:52 [INFO]: Epoch 058 - training loss: 0.6905, validation loss: 1.0465
2024-06-03 11:17:06 [INFO]: Epoch 059 - training loss: 0.6932, validation loss: 1.0495
2024-06-03 11:17:20 [INFO]: Epoch 060 - training loss: 0.7025, validation loss: 1.0425
2024-06-03 11:17:31 [INFO]: Epoch 061 - training loss: 0.6951, validation loss: 1.0436
2024-06-03 11:17:43 [INFO]: Epoch 062 - training loss: 0.6937, validation loss: 1.0501
2024-06-03 11:17:57 [INFO]: Epoch 063 - training loss: 0.7017, validation loss: 1.0449
2024-06-03 11:18:11 [INFO]: Epoch 064 - training loss: 0.6915, validation loss: 1.0441
2024-06-03 11:18:24 [INFO]: Epoch 065 - training loss: 0.6929, validation loss: 1.0426
2024-06-03 11:18:38 [INFO]: Epoch 066 - training loss: 0.6915, validation loss: 1.0449
2024-06-03 11:18:50 [INFO]: Epoch 067 - training loss: 0.6891, validation loss: 1.0413
2024-06-03 11:19:03 [INFO]: Epoch 068 - training loss: 0.6849, validation loss: 1.0446
2024-06-03 11:19:16 [INFO]: Epoch 069 - training loss: 0.6831, validation loss: 1.0379
2024-06-03 11:19:31 [INFO]: Epoch 070 - training loss: 0.6822, validation loss: 1.0437
2024-06-03 11:19:44 [INFO]: Epoch 071 - training loss: 0.6829, validation loss: 1.0477
2024-06-03 11:19:57 [INFO]: Epoch 072 - training loss: 0.6792, validation loss: 1.0514
2024-06-03 11:20:10 [INFO]: Epoch 073 - training loss: 0.6870, validation loss: 1.0641
2024-06-03 11:20:23 [INFO]: Epoch 074 - training loss: 0.6832, validation loss: 1.0496
2024-06-03 11:20:35 [INFO]: Epoch 075 - training loss: 0.6758, validation loss: 1.0539
2024-06-03 11:20:48 [INFO]: Epoch 076 - training loss: 0.6734, validation loss: 1.0614
2024-06-03 11:21:02 [INFO]: Epoch 077 - training loss: 0.6752, validation loss: 1.0403
2024-06-03 11:21:15 [INFO]: Epoch 078 - training loss: 0.6752, validation loss: 1.0417
2024-06-03 11:21:28 [INFO]: Epoch 079 - training loss: 0.6693, validation loss: 1.0681
2024-06-03 11:21:28 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:21:28 [INFO]: Finished training. The best model is from epoch#69.
2024-06-03 11:21:28 [INFO]: Saved the model to results_block_rate05/PeMS/Autoformer_PeMS/round_3/20240603_T110407/Autoformer.pypots
2024-06-03 11:21:32 [INFO]: Successfully saved to results_block_rate05/PeMS/Autoformer_PeMS/round_3/imputation.pkl
2024-06-03 11:21:32 [INFO]: Round3 - Autoformer on PeMS: MAE=0.6607, MSE=1.4100, MRE=0.7911
2024-06-03 11:21:32 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 11:21:32 [INFO]: Using the given device: cuda:0
2024-06-03 11:21:32 [INFO]: Model files will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_4/20240603_T112132
2024-06-03 11:21:32 [INFO]: Tensorboard file will be saved to results_block_rate05/PeMS/Autoformer_PeMS/round_4/20240603_T112132/tensorboard
2024-06-03 11:21:32 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 11:21:46 [INFO]: Epoch 001 - training loss: 1.7464, validation loss: 1.6817
2024-06-03 11:22:00 [INFO]: Epoch 002 - training loss: 1.6678, validation loss: 1.5717
2024-06-03 11:22:13 [INFO]: Epoch 003 - training loss: 1.5846, validation loss: 1.4830
2024-06-03 11:22:27 [INFO]: Epoch 004 - training loss: 1.5229, validation loss: 1.4465
2024-06-03 11:22:39 [INFO]: Epoch 005 - training loss: 1.4923, validation loss: 1.4319
2024-06-03 11:22:50 [INFO]: Epoch 006 - training loss: 1.4729, validation loss: 1.4228
2024-06-03 11:23:00 [INFO]: Epoch 007 - training loss: 1.4457, validation loss: 1.4074
2024-06-03 11:23:10 [INFO]: Epoch 008 - training loss: 1.3907, validation loss: 1.3959
2024-06-03 11:23:21 [INFO]: Epoch 009 - training loss: 1.3129, validation loss: 1.3800
2024-06-03 11:23:32 [INFO]: Epoch 010 - training loss: 1.2138, validation loss: 1.3521
2024-06-03 11:23:43 [INFO]: Epoch 011 - training loss: 1.1069, validation loss: 1.3293
2024-06-03 11:23:53 [INFO]: Epoch 012 - training loss: 1.0312, validation loss: 1.3074
2024-06-03 11:24:04 [INFO]: Epoch 013 - training loss: 0.9714, validation loss: 1.2715
2024-06-03 11:24:17 [INFO]: Epoch 014 - training loss: 0.9064, validation loss: 1.2549
2024-06-03 11:24:28 [INFO]: Epoch 015 - training loss: 0.8601, validation loss: 1.2400
2024-06-03 11:24:38 [INFO]: Epoch 016 - training loss: 0.8275, validation loss: 1.2213
2024-06-03 11:24:49 [INFO]: Epoch 017 - training loss: 0.8072, validation loss: 1.1785
2024-06-03 11:24:58 [INFO]: Epoch 018 - training loss: 0.7881, validation loss: 1.1264
2024-06-03 11:25:08 [INFO]: Epoch 019 - training loss: 0.7717, validation loss: 1.1071
2024-06-03 11:25:19 [INFO]: Epoch 020 - training loss: 0.7657, validation loss: 1.0902
2024-06-03 11:25:31 [INFO]: Epoch 021 - training loss: 0.7580, validation loss: 1.0723
2024-06-03 11:25:42 [INFO]: Epoch 022 - training loss: 0.7506, validation loss: 1.0607
2024-06-03 11:25:53 [INFO]: Epoch 023 - training loss: 0.7487, validation loss: 1.0659
2024-06-03 11:26:02 [INFO]: Epoch 024 - training loss: 0.7411, validation loss: 1.0503
2024-06-03 11:26:12 [INFO]: Epoch 025 - training loss: 0.7393, validation loss: 1.0458
2024-06-03 11:26:24 [INFO]: Epoch 026 - training loss: 0.7388, validation loss: 1.0463
2024-06-03 11:26:35 [INFO]: Epoch 027 - training loss: 0.7316, validation loss: 1.0642
2024-06-03 11:26:46 [INFO]: Epoch 028 - training loss: 0.7272, validation loss: 1.0459
2024-06-03 11:26:56 [INFO]: Epoch 029 - training loss: 0.7236, validation loss: 1.0420
2024-06-03 11:27:06 [INFO]: Epoch 030 - training loss: 0.7194, validation loss: 1.0356
2024-06-03 11:27:17 [INFO]: Epoch 031 - training loss: 0.7219, validation loss: 1.0409
2024-06-03 11:27:27 [INFO]: Epoch 032 - training loss: 0.7205, validation loss: 1.0454
2024-06-03 11:27:39 [INFO]: Epoch 033 - training loss: 0.7190, validation loss: 1.0443
2024-06-03 11:27:50 [INFO]: Epoch 034 - training loss: 0.7139, validation loss: 1.0459
2024-06-03 11:28:01 [INFO]: Epoch 035 - training loss: 0.7092, validation loss: 1.0513
2024-06-03 11:28:11 [INFO]: Epoch 036 - training loss: 0.7075, validation loss: 1.0497
2024-06-03 11:28:21 [INFO]: Epoch 037 - training loss: 0.7018, validation loss: 1.0599
2024-06-03 11:28:32 [INFO]: Epoch 038 - training loss: 0.7075, validation loss: 1.0597
2024-06-03 11:28:44 [INFO]: Epoch 039 - training loss: 0.6994, validation loss: 1.0486
2024-06-03 11:28:55 [INFO]: Epoch 040 - training loss: 0.7045, validation loss: 1.0418
2024-06-03 11:28:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:28:55 [INFO]: Finished training. The best model is from epoch#30.
2024-06-03 11:28:55 [INFO]: Saved the model to results_block_rate05/PeMS/Autoformer_PeMS/round_4/20240603_T112132/Autoformer.pypots
2024-06-03 11:28:58 [INFO]: Successfully saved to results_block_rate05/PeMS/Autoformer_PeMS/round_4/imputation.pkl
2024-06-03 11:28:58 [INFO]: Round4 - Autoformer on PeMS: MAE=0.6506, MSE=1.3816, MRE=0.7790
2024-06-03 11:28:58 [INFO]: Done! Final results:
Averaged Autoformer (608,926 params) on PeMS: MAE=0.6270 ± 0.027208178601132817, MSE=1.3201 ± 0.0670769451671636, MRE=0.7508 ± 0.032578040966475846, average inference time=0.77
