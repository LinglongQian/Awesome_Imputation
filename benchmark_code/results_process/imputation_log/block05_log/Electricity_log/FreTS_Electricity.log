2024-06-03 12:16:17 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 12:16:17 [INFO]: Using the given device: cuda:0
2024-06-03 12:16:17 [INFO]: Model files will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_0/20240603_T121617
2024-06-03 12:16:17 [INFO]: Tensorboard file will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_0/20240603_T121617/tensorboard
2024-06-03 12:16:18 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-03 12:16:27 [INFO]: Epoch 001 - training loss: 1.2321, validation loss: 2.8393
2024-06-03 12:16:34 [INFO]: Epoch 002 - training loss: 0.8261, validation loss: 2.4331
2024-06-03 12:16:42 [INFO]: Epoch 003 - training loss: 0.7259, validation loss: 2.2570
2024-06-03 12:16:49 [INFO]: Epoch 004 - training loss: 0.6773, validation loss: 2.2053
2024-06-03 12:16:57 [INFO]: Epoch 005 - training loss: 0.6458, validation loss: 2.2278
2024-06-03 12:17:04 [INFO]: Epoch 006 - training loss: 0.6241, validation loss: 2.1273
2024-06-03 12:17:12 [INFO]: Epoch 007 - training loss: 0.6037, validation loss: 2.0462
2024-06-03 12:17:19 [INFO]: Epoch 008 - training loss: 0.5939, validation loss: 1.9669
2024-06-03 12:17:27 [INFO]: Epoch 009 - training loss: 0.5791, validation loss: 1.9624
2024-06-03 12:17:34 [INFO]: Epoch 010 - training loss: 0.5717, validation loss: 1.9743
2024-06-03 12:17:42 [INFO]: Epoch 011 - training loss: 0.5595, validation loss: 1.9088
2024-06-03 12:17:50 [INFO]: Epoch 012 - training loss: 0.5510, validation loss: 1.8612
2024-06-03 12:17:57 [INFO]: Epoch 013 - training loss: 0.5482, validation loss: 1.8265
2024-06-03 12:18:06 [INFO]: Epoch 014 - training loss: 0.5392, validation loss: 1.8576
2024-06-03 12:18:14 [INFO]: Epoch 015 - training loss: 0.5353, validation loss: 1.8563
2024-06-03 12:18:21 [INFO]: Epoch 016 - training loss: 0.5341, validation loss: 1.8480
2024-06-03 12:18:28 [INFO]: Epoch 017 - training loss: 0.5244, validation loss: 1.8224
2024-06-03 12:18:36 [INFO]: Epoch 018 - training loss: 0.5208, validation loss: 1.8084
2024-06-03 12:18:43 [INFO]: Epoch 019 - training loss: 0.5176, validation loss: 1.7765
2024-06-03 12:18:51 [INFO]: Epoch 020 - training loss: 0.5128, validation loss: 1.7049
2024-06-03 12:18:59 [INFO]: Epoch 021 - training loss: 0.5097, validation loss: 1.7611
2024-06-03 12:19:07 [INFO]: Epoch 022 - training loss: 0.5089, validation loss: 1.7629
2024-06-03 12:19:15 [INFO]: Epoch 023 - training loss: 0.5028, validation loss: 1.7222
2024-06-03 12:19:22 [INFO]: Epoch 024 - training loss: 0.5016, validation loss: 1.7147
2024-06-03 12:19:30 [INFO]: Epoch 025 - training loss: 0.4985, validation loss: 1.7253
2024-06-03 12:19:38 [INFO]: Epoch 026 - training loss: 0.4967, validation loss: 1.7480
2024-06-03 12:19:46 [INFO]: Epoch 027 - training loss: 0.4943, validation loss: 1.7001
2024-06-03 12:19:54 [INFO]: Epoch 028 - training loss: 0.4903, validation loss: 1.6879
2024-06-03 12:20:02 [INFO]: Epoch 029 - training loss: 0.4896, validation loss: 1.6894
2024-06-03 12:20:09 [INFO]: Epoch 030 - training loss: 0.4890, validation loss: 1.6854
2024-06-03 12:20:17 [INFO]: Epoch 031 - training loss: 0.4871, validation loss: 1.6686
2024-06-03 12:20:25 [INFO]: Epoch 032 - training loss: 0.4828, validation loss: 1.6622
2024-06-03 12:20:33 [INFO]: Epoch 033 - training loss: 0.4807, validation loss: 1.6621
2024-06-03 12:20:40 [INFO]: Epoch 034 - training loss: 0.4794, validation loss: 1.6694
2024-06-03 12:20:47 [INFO]: Epoch 035 - training loss: 0.4778, validation loss: 1.6771
2024-06-03 12:20:55 [INFO]: Epoch 036 - training loss: 0.4775, validation loss: 1.6423
2024-06-03 12:21:03 [INFO]: Epoch 037 - training loss: 0.4755, validation loss: 1.6193
2024-06-03 12:21:10 [INFO]: Epoch 038 - training loss: 0.4779, validation loss: 1.6436
2024-06-03 12:21:18 [INFO]: Epoch 039 - training loss: 0.4729, validation loss: 1.6334
2024-06-03 12:21:25 [INFO]: Epoch 040 - training loss: 0.4716, validation loss: 1.6739
2024-06-03 12:21:32 [INFO]: Epoch 041 - training loss: 0.4706, validation loss: 1.6248
2024-06-03 12:21:39 [INFO]: Epoch 042 - training loss: 0.4700, validation loss: 1.6153
2024-06-03 12:21:45 [INFO]: Epoch 043 - training loss: 0.4681, validation loss: 1.5868
2024-06-03 12:21:52 [INFO]: Epoch 044 - training loss: 0.4681, validation loss: 1.6062
2024-06-03 12:22:00 [INFO]: Epoch 045 - training loss: 0.4641, validation loss: 1.5858
2024-06-03 12:22:08 [INFO]: Epoch 046 - training loss: 0.4631, validation loss: 1.5975
2024-06-03 12:22:16 [INFO]: Epoch 047 - training loss: 0.4626, validation loss: 1.5959
2024-06-03 12:22:23 [INFO]: Epoch 048 - training loss: 0.4599, validation loss: 1.5886
2024-06-03 12:22:31 [INFO]: Epoch 049 - training loss: 0.4598, validation loss: 1.5887
2024-06-03 12:22:39 [INFO]: Epoch 050 - training loss: 0.4586, validation loss: 1.5792
2024-06-03 12:22:47 [INFO]: Epoch 051 - training loss: 0.4584, validation loss: 1.5821
2024-06-03 12:22:55 [INFO]: Epoch 052 - training loss: 0.4568, validation loss: 1.5998
2024-06-03 12:23:02 [INFO]: Epoch 053 - training loss: 0.4556, validation loss: 1.6065
2024-06-03 12:23:10 [INFO]: Epoch 054 - training loss: 0.4550, validation loss: 1.5900
2024-06-03 12:23:18 [INFO]: Epoch 055 - training loss: 0.4559, validation loss: 1.5824
2024-06-03 12:23:26 [INFO]: Epoch 056 - training loss: 0.4568, validation loss: 1.6068
2024-06-03 12:23:33 [INFO]: Epoch 057 - training loss: 0.4532, validation loss: 1.5942
2024-06-03 12:23:41 [INFO]: Epoch 058 - training loss: 0.4497, validation loss: 1.5724
2024-06-03 12:23:49 [INFO]: Epoch 059 - training loss: 0.4495, validation loss: 1.5992
2024-06-03 12:23:57 [INFO]: Epoch 060 - training loss: 0.4512, validation loss: 1.5965
2024-06-03 12:24:05 [INFO]: Epoch 061 - training loss: 0.4490, validation loss: 1.5855
2024-06-03 12:24:13 [INFO]: Epoch 062 - training loss: 0.4491, validation loss: 1.5992
2024-06-03 12:24:20 [INFO]: Epoch 063 - training loss: 0.4488, validation loss: 1.5664
2024-06-03 12:24:27 [INFO]: Epoch 064 - training loss: 0.4491, validation loss: 1.5636
2024-06-03 12:24:35 [INFO]: Epoch 065 - training loss: 0.4496, validation loss: 1.5536
2024-06-03 12:24:43 [INFO]: Epoch 066 - training loss: 0.4459, validation loss: 1.6108
2024-06-03 12:24:50 [INFO]: Epoch 067 - training loss: 0.4457, validation loss: 1.5482
2024-06-03 12:24:58 [INFO]: Epoch 068 - training loss: 0.4457, validation loss: 1.5674
2024-06-03 12:25:06 [INFO]: Epoch 069 - training loss: 0.4442, validation loss: 1.5649
2024-06-03 12:25:13 [INFO]: Epoch 070 - training loss: 0.4445, validation loss: 1.5820
2024-06-03 12:25:21 [INFO]: Epoch 071 - training loss: 0.4418, validation loss: 1.5597
2024-06-03 12:25:29 [INFO]: Epoch 072 - training loss: 0.4423, validation loss: 1.5876
2024-06-03 12:25:37 [INFO]: Epoch 073 - training loss: 0.4420, validation loss: 1.5813
2024-06-03 12:25:44 [INFO]: Epoch 074 - training loss: 0.4434, validation loss: 1.5469
2024-06-03 12:25:52 [INFO]: Epoch 075 - training loss: 0.4442, validation loss: 1.5316
2024-06-03 12:26:00 [INFO]: Epoch 076 - training loss: 0.4425, validation loss: 1.5602
2024-06-03 12:26:07 [INFO]: Epoch 077 - training loss: 0.4387, validation loss: 1.5517
2024-06-03 12:26:13 [INFO]: Epoch 078 - training loss: 0.4378, validation loss: 1.5609
2024-06-03 12:26:20 [INFO]: Epoch 079 - training loss: 0.4387, validation loss: 1.5632
2024-06-03 12:26:27 [INFO]: Epoch 080 - training loss: 0.4378, validation loss: 1.5906
2024-06-03 12:26:34 [INFO]: Epoch 081 - training loss: 0.4380, validation loss: 1.5932
2024-06-03 12:26:41 [INFO]: Epoch 082 - training loss: 0.4382, validation loss: 1.5759
2024-06-03 12:26:49 [INFO]: Epoch 083 - training loss: 0.4384, validation loss: 1.5859
2024-06-03 12:26:57 [INFO]: Epoch 084 - training loss: 0.4370, validation loss: 1.5779
2024-06-03 12:27:04 [INFO]: Epoch 085 - training loss: 0.4362, validation loss: 1.5651
2024-06-03 12:27:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:27:04 [INFO]: Finished training. The best model is from epoch#75.
2024-06-03 12:27:04 [INFO]: Saved the model to results_block_rate05/Electricity/FreTS_Electricity/round_0/20240603_T121617/FreTS.pypots
2024-06-03 12:27:09 [INFO]: Successfully saved to results_block_rate05/Electricity/FreTS_Electricity/round_0/imputation.pkl
2024-06-03 12:27:09 [INFO]: Round0 - FreTS on Electricity: MAE=1.0016, MSE=2.0446, MRE=0.5376
2024-06-03 12:27:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 12:27:09 [INFO]: Using the given device: cuda:0
2024-06-03 12:27:09 [INFO]: Model files will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_1/20240603_T122709
2024-06-03 12:27:09 [INFO]: Tensorboard file will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_1/20240603_T122709/tensorboard
2024-06-03 12:27:09 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-03 12:27:17 [INFO]: Epoch 001 - training loss: 1.3326, validation loss: 3.3772
2024-06-03 12:27:25 [INFO]: Epoch 002 - training loss: 0.9057, validation loss: 2.6074
2024-06-03 12:27:33 [INFO]: Epoch 003 - training loss: 0.7458, validation loss: 2.3245
2024-06-03 12:27:40 [INFO]: Epoch 004 - training loss: 0.6889, validation loss: 2.1804
2024-06-03 12:27:48 [INFO]: Epoch 005 - training loss: 0.6520, validation loss: 2.1627
2024-06-03 12:27:56 [INFO]: Epoch 006 - training loss: 0.6278, validation loss: 2.0154
2024-06-03 12:28:03 [INFO]: Epoch 007 - training loss: 0.6080, validation loss: 1.9907
2024-06-03 12:28:11 [INFO]: Epoch 008 - training loss: 0.6006, validation loss: 1.9253
2024-06-03 12:28:19 [INFO]: Epoch 009 - training loss: 0.5830, validation loss: 1.8787
2024-06-03 12:28:27 [INFO]: Epoch 010 - training loss: 0.5712, validation loss: 1.8857
2024-06-03 12:28:34 [INFO]: Epoch 011 - training loss: 0.5637, validation loss: 1.8645
2024-06-03 12:28:42 [INFO]: Epoch 012 - training loss: 0.5539, validation loss: 1.8209
2024-06-03 12:28:50 [INFO]: Epoch 013 - training loss: 0.5523, validation loss: 1.8391
2024-06-03 12:28:57 [INFO]: Epoch 014 - training loss: 0.5432, validation loss: 1.8090
2024-06-03 12:29:04 [INFO]: Epoch 015 - training loss: 0.5390, validation loss: 1.8009
2024-06-03 12:29:12 [INFO]: Epoch 016 - training loss: 0.5331, validation loss: 1.8152
2024-06-03 12:29:19 [INFO]: Epoch 017 - training loss: 0.5262, validation loss: 1.7618
2024-06-03 12:29:27 [INFO]: Epoch 018 - training loss: 0.5235, validation loss: 1.7498
2024-06-03 12:29:35 [INFO]: Epoch 019 - training loss: 0.5189, validation loss: 1.7306
2024-06-03 12:29:42 [INFO]: Epoch 020 - training loss: 0.5172, validation loss: 1.7344
2024-06-03 12:29:50 [INFO]: Epoch 021 - training loss: 0.5146, validation loss: 1.7248
2024-06-03 12:29:57 [INFO]: Epoch 022 - training loss: 0.5086, validation loss: 1.7178
2024-06-03 12:30:03 [INFO]: Epoch 023 - training loss: 0.5089, validation loss: 1.7145
2024-06-03 12:30:10 [INFO]: Epoch 024 - training loss: 0.5043, validation loss: 1.6983
2024-06-03 12:30:17 [INFO]: Epoch 025 - training loss: 0.5008, validation loss: 1.6814
2024-06-03 12:30:23 [INFO]: Epoch 026 - training loss: 0.5008, validation loss: 1.6838
2024-06-03 12:30:31 [INFO]: Epoch 027 - training loss: 0.4976, validation loss: 1.6977
2024-06-03 12:30:39 [INFO]: Epoch 028 - training loss: 0.4956, validation loss: 1.6605
2024-06-03 12:30:46 [INFO]: Epoch 029 - training loss: 0.4934, validation loss: 1.6638
2024-06-03 12:30:54 [INFO]: Epoch 030 - training loss: 0.4891, validation loss: 1.6676
2024-06-03 12:31:02 [INFO]: Epoch 031 - training loss: 0.4890, validation loss: 1.6521
2024-06-03 12:31:09 [INFO]: Epoch 032 - training loss: 0.4848, validation loss: 1.6520
2024-06-03 12:31:17 [INFO]: Epoch 033 - training loss: 0.4842, validation loss: 1.6245
2024-06-03 12:31:25 [INFO]: Epoch 034 - training loss: 0.4832, validation loss: 1.6471
2024-06-03 12:31:32 [INFO]: Epoch 035 - training loss: 0.4830, validation loss: 1.6342
2024-06-03 12:31:40 [INFO]: Epoch 036 - training loss: 0.4807, validation loss: 1.6226
2024-06-03 12:31:48 [INFO]: Epoch 037 - training loss: 0.4784, validation loss: 1.6150
2024-06-03 12:31:55 [INFO]: Epoch 038 - training loss: 0.4765, validation loss: 1.6144
2024-06-03 12:32:03 [INFO]: Epoch 039 - training loss: 0.4753, validation loss: 1.6079
2024-06-03 12:32:11 [INFO]: Epoch 040 - training loss: 0.4743, validation loss: 1.6151
2024-06-03 12:32:18 [INFO]: Epoch 041 - training loss: 0.4731, validation loss: 1.6095
2024-06-03 12:32:26 [INFO]: Epoch 042 - training loss: 0.4720, validation loss: 1.5912
2024-06-03 12:32:34 [INFO]: Epoch 043 - training loss: 0.4708, validation loss: 1.5737
2024-06-03 12:32:41 [INFO]: Epoch 044 - training loss: 0.4693, validation loss: 1.5860
2024-06-03 12:32:49 [INFO]: Epoch 045 - training loss: 0.4670, validation loss: 1.5882
2024-06-03 12:32:56 [INFO]: Epoch 046 - training loss: 0.4672, validation loss: 1.5780
2024-06-03 12:33:03 [INFO]: Epoch 047 - training loss: 0.4650, validation loss: 1.5538
2024-06-03 12:33:11 [INFO]: Epoch 048 - training loss: 0.4619, validation loss: 1.5489
2024-06-03 12:33:19 [INFO]: Epoch 049 - training loss: 0.4627, validation loss: 1.5406
2024-06-03 12:33:26 [INFO]: Epoch 050 - training loss: 0.4622, validation loss: 1.5488
2024-06-03 12:33:34 [INFO]: Epoch 051 - training loss: 0.4638, validation loss: 1.5692
2024-06-03 12:33:41 [INFO]: Epoch 052 - training loss: 0.4607, validation loss: 1.5310
2024-06-03 12:33:49 [INFO]: Epoch 053 - training loss: 0.4589, validation loss: 1.5650
2024-06-03 12:33:57 [INFO]: Epoch 054 - training loss: 0.4605, validation loss: 1.5182
2024-06-03 12:34:05 [INFO]: Epoch 055 - training loss: 0.4584, validation loss: 1.5307
2024-06-03 12:34:12 [INFO]: Epoch 056 - training loss: 0.4573, validation loss: 1.5226
2024-06-03 12:34:20 [INFO]: Epoch 057 - training loss: 0.4577, validation loss: 1.5512
2024-06-03 12:34:27 [INFO]: Epoch 058 - training loss: 0.4556, validation loss: 1.5126
2024-06-03 12:34:34 [INFO]: Epoch 059 - training loss: 0.4537, validation loss: 1.5276
2024-06-03 12:34:40 [INFO]: Epoch 060 - training loss: 0.4534, validation loss: 1.5141
2024-06-03 12:34:46 [INFO]: Epoch 061 - training loss: 0.4531, validation loss: 1.5255
2024-06-03 12:34:53 [INFO]: Epoch 062 - training loss: 0.4537, validation loss: 1.5046
2024-06-03 12:35:00 [INFO]: Epoch 063 - training loss: 0.4526, validation loss: 1.5285
2024-06-03 12:35:08 [INFO]: Epoch 064 - training loss: 0.4511, validation loss: 1.5018
2024-06-03 12:35:15 [INFO]: Epoch 065 - training loss: 0.4518, validation loss: 1.5179
2024-06-03 12:35:23 [INFO]: Epoch 066 - training loss: 0.4542, validation loss: 1.5158
2024-06-03 12:35:30 [INFO]: Epoch 067 - training loss: 0.4514, validation loss: 1.5255
2024-06-03 12:35:38 [INFO]: Epoch 068 - training loss: 0.4479, validation loss: 1.4847
2024-06-03 12:35:46 [INFO]: Epoch 069 - training loss: 0.4465, validation loss: 1.5055
2024-06-03 12:35:53 [INFO]: Epoch 070 - training loss: 0.4475, validation loss: 1.4979
2024-06-03 12:36:01 [INFO]: Epoch 071 - training loss: 0.4465, validation loss: 1.4783
2024-06-03 12:36:08 [INFO]: Epoch 072 - training loss: 0.4450, validation loss: 1.4813
2024-06-03 12:36:16 [INFO]: Epoch 073 - training loss: 0.4462, validation loss: 1.4928
2024-06-03 12:36:23 [INFO]: Epoch 074 - training loss: 0.4455, validation loss: 1.4870
2024-06-03 12:36:31 [INFO]: Epoch 075 - training loss: 0.4438, validation loss: 1.4875
2024-06-03 12:36:39 [INFO]: Epoch 076 - training loss: 0.4431, validation loss: 1.4674
2024-06-03 12:36:47 [INFO]: Epoch 077 - training loss: 0.4440, validation loss: 1.4609
2024-06-03 12:36:54 [INFO]: Epoch 078 - training loss: 0.4429, validation loss: 1.4908
2024-06-03 12:37:02 [INFO]: Epoch 079 - training loss: 0.4439, validation loss: 1.4819
2024-06-03 12:37:09 [INFO]: Epoch 080 - training loss: 0.4412, validation loss: 1.4695
2024-06-03 12:37:17 [INFO]: Epoch 081 - training loss: 0.4410, validation loss: 1.4569
2024-06-03 12:37:25 [INFO]: Epoch 082 - training loss: 0.4399, validation loss: 1.4846
2024-06-03 12:37:33 [INFO]: Epoch 083 - training loss: 0.4396, validation loss: 1.4522
2024-06-03 12:37:41 [INFO]: Epoch 084 - training loss: 0.4408, validation loss: 1.4391
2024-06-03 12:37:48 [INFO]: Epoch 085 - training loss: 0.4400, validation loss: 1.4520
2024-06-03 12:37:56 [INFO]: Epoch 086 - training loss: 0.4403, validation loss: 1.4606
2024-06-03 12:38:04 [INFO]: Epoch 087 - training loss: 0.4400, validation loss: 1.4618
2024-06-03 12:38:11 [INFO]: Epoch 088 - training loss: 0.4396, validation loss: 1.4272
2024-06-03 12:38:19 [INFO]: Epoch 089 - training loss: 0.4376, validation loss: 1.4436
2024-06-03 12:38:27 [INFO]: Epoch 090 - training loss: 0.4381, validation loss: 1.4369
2024-06-03 12:38:35 [INFO]: Epoch 091 - training loss: 0.4362, validation loss: 1.4691
2024-06-03 12:38:42 [INFO]: Epoch 092 - training loss: 0.4368, validation loss: 1.4573
2024-06-03 12:38:49 [INFO]: Epoch 093 - training loss: 0.4369, validation loss: 1.4464
2024-06-03 12:38:56 [INFO]: Epoch 094 - training loss: 0.4356, validation loss: 1.4424
2024-06-03 12:39:03 [INFO]: Epoch 095 - training loss: 0.4362, validation loss: 1.4251
2024-06-03 12:39:09 [INFO]: Epoch 096 - training loss: 0.4344, validation loss: 1.4306
2024-06-03 12:39:15 [INFO]: Epoch 097 - training loss: 0.4339, validation loss: 1.4318
2024-06-03 12:39:22 [INFO]: Epoch 098 - training loss: 0.4333, validation loss: 1.4311
2024-06-03 12:39:28 [INFO]: Epoch 099 - training loss: 0.4361, validation loss: 1.4219
2024-06-03 12:39:34 [INFO]: Epoch 100 - training loss: 0.4354, validation loss: 1.4518
2024-06-03 12:39:34 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 12:39:34 [INFO]: Saved the model to results_block_rate05/Electricity/FreTS_Electricity/round_1/20240603_T122709/FreTS.pypots
2024-06-03 12:39:38 [INFO]: Successfully saved to results_block_rate05/Electricity/FreTS_Electricity/round_1/imputation.pkl
2024-06-03 12:39:38 [INFO]: Round1 - FreTS on Electricity: MAE=0.9823, MSE=2.0609, MRE=0.5273
2024-06-03 12:39:38 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 12:39:38 [INFO]: Using the given device: cuda:0
2024-06-03 12:39:38 [INFO]: Model files will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_2/20240603_T123938
2024-06-03 12:39:38 [INFO]: Tensorboard file will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_2/20240603_T123938/tensorboard
2024-06-03 12:39:38 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-03 12:39:45 [INFO]: Epoch 001 - training loss: 1.2939, validation loss: 3.0063
2024-06-03 12:39:52 [INFO]: Epoch 002 - training loss: 0.8415, validation loss: 2.4307
2024-06-03 12:39:58 [INFO]: Epoch 003 - training loss: 0.7293, validation loss: 2.3084
2024-06-03 12:40:04 [INFO]: Epoch 004 - training loss: 0.6858, validation loss: 2.2098
2024-06-03 12:40:10 [INFO]: Epoch 005 - training loss: 0.6535, validation loss: 2.1211
2024-06-03 12:40:16 [INFO]: Epoch 006 - training loss: 0.6283, validation loss: 2.0784
2024-06-03 12:40:22 [INFO]: Epoch 007 - training loss: 0.6145, validation loss: 2.0100
2024-06-03 12:40:28 [INFO]: Epoch 008 - training loss: 0.5961, validation loss: 1.9449
2024-06-03 12:40:35 [INFO]: Epoch 009 - training loss: 0.5857, validation loss: 2.0016
2024-06-03 12:40:41 [INFO]: Epoch 010 - training loss: 0.5770, validation loss: 1.9364
2024-06-03 12:40:48 [INFO]: Epoch 011 - training loss: 0.5686, validation loss: 1.9310
2024-06-03 12:40:52 [INFO]: Epoch 012 - training loss: 0.5618, validation loss: 1.8924
2024-06-03 12:40:56 [INFO]: Epoch 013 - training loss: 0.5546, validation loss: 1.8868
2024-06-03 12:41:00 [INFO]: Epoch 014 - training loss: 0.5468, validation loss: 1.9006
2024-06-03 12:41:04 [INFO]: Epoch 015 - training loss: 0.5421, validation loss: 1.8464
2024-06-03 12:41:08 [INFO]: Epoch 016 - training loss: 0.5367, validation loss: 1.9077
2024-06-03 12:41:12 [INFO]: Epoch 017 - training loss: 0.5298, validation loss: 1.8734
2024-06-03 12:41:16 [INFO]: Epoch 018 - training loss: 0.5270, validation loss: 1.8105
2024-06-03 12:41:20 [INFO]: Epoch 019 - training loss: 0.5212, validation loss: 1.8075
2024-06-03 12:41:23 [INFO]: Epoch 020 - training loss: 0.5184, validation loss: 1.7949
2024-06-03 12:41:27 [INFO]: Epoch 021 - training loss: 0.5159, validation loss: 1.7902
2024-06-03 12:41:32 [INFO]: Epoch 022 - training loss: 0.5131, validation loss: 1.7715
2024-06-03 12:41:36 [INFO]: Epoch 023 - training loss: 0.5129, validation loss: 1.7975
2024-06-03 12:41:40 [INFO]: Epoch 024 - training loss: 0.5084, validation loss: 1.7795
2024-06-03 12:41:44 [INFO]: Epoch 025 - training loss: 0.5052, validation loss: 1.7703
2024-06-03 12:41:48 [INFO]: Epoch 026 - training loss: 0.5036, validation loss: 1.7359
2024-06-03 12:41:52 [INFO]: Epoch 027 - training loss: 0.4991, validation loss: 1.7054
2024-06-03 12:41:56 [INFO]: Epoch 028 - training loss: 0.5026, validation loss: 1.7596
2024-06-03 12:42:00 [INFO]: Epoch 029 - training loss: 0.4984, validation loss: 1.7600
2024-06-03 12:42:04 [INFO]: Epoch 030 - training loss: 0.4955, validation loss: 1.7111
2024-06-03 12:42:08 [INFO]: Epoch 031 - training loss: 0.4936, validation loss: 1.7166
2024-06-03 12:42:12 [INFO]: Epoch 032 - training loss: 0.4890, validation loss: 1.7078
2024-06-03 12:42:16 [INFO]: Epoch 033 - training loss: 0.4891, validation loss: 1.7017
2024-06-03 12:42:20 [INFO]: Epoch 034 - training loss: 0.4867, validation loss: 1.6881
2024-06-03 12:42:25 [INFO]: Epoch 035 - training loss: 0.4859, validation loss: 1.6685
2024-06-03 12:42:28 [INFO]: Epoch 036 - training loss: 0.4827, validation loss: 1.6827
2024-06-03 12:42:32 [INFO]: Epoch 037 - training loss: 0.4824, validation loss: 1.6698
2024-06-03 12:42:36 [INFO]: Epoch 038 - training loss: 0.4794, validation loss: 1.6645
2024-06-03 12:42:40 [INFO]: Epoch 039 - training loss: 0.4791, validation loss: 1.6656
2024-06-03 12:42:44 [INFO]: Epoch 040 - training loss: 0.4774, validation loss: 1.6765
2024-06-03 12:42:48 [INFO]: Epoch 041 - training loss: 0.4758, validation loss: 1.6421
2024-06-03 12:42:52 [INFO]: Epoch 042 - training loss: 0.4729, validation loss: 1.6353
2024-06-03 12:42:56 [INFO]: Epoch 043 - training loss: 0.4733, validation loss: 1.6314
2024-06-03 12:43:00 [INFO]: Epoch 044 - training loss: 0.4738, validation loss: 1.6041
2024-06-03 12:43:04 [INFO]: Epoch 045 - training loss: 0.4758, validation loss: 1.6182
2024-06-03 12:43:08 [INFO]: Epoch 046 - training loss: 0.4699, validation loss: 1.6095
2024-06-03 12:43:12 [INFO]: Epoch 047 - training loss: 0.4692, validation loss: 1.5888
2024-06-03 12:43:16 [INFO]: Epoch 048 - training loss: 0.4662, validation loss: 1.6089
2024-06-03 12:43:19 [INFO]: Epoch 049 - training loss: 0.4671, validation loss: 1.5850
2024-06-03 12:43:23 [INFO]: Epoch 050 - training loss: 0.4656, validation loss: 1.5658
2024-06-03 12:43:27 [INFO]: Epoch 051 - training loss: 0.4633, validation loss: 1.5914
2024-06-03 12:43:31 [INFO]: Epoch 052 - training loss: 0.4631, validation loss: 1.5826
2024-06-03 12:43:35 [INFO]: Epoch 053 - training loss: 0.4629, validation loss: 1.5805
2024-06-03 12:43:39 [INFO]: Epoch 054 - training loss: 0.4619, validation loss: 1.5637
2024-06-03 12:43:43 [INFO]: Epoch 055 - training loss: 0.4604, validation loss: 1.5597
2024-06-03 12:43:47 [INFO]: Epoch 056 - training loss: 0.4588, validation loss: 1.5425
2024-06-03 12:43:51 [INFO]: Epoch 057 - training loss: 0.4583, validation loss: 1.5411
2024-06-03 12:43:54 [INFO]: Epoch 058 - training loss: 0.4560, validation loss: 1.5305
2024-06-03 12:43:59 [INFO]: Epoch 059 - training loss: 0.4564, validation loss: 1.5512
2024-06-03 12:44:02 [INFO]: Epoch 060 - training loss: 0.4553, validation loss: 1.5384
2024-06-03 12:44:06 [INFO]: Epoch 061 - training loss: 0.4546, validation loss: 1.5513
2024-06-03 12:44:10 [INFO]: Epoch 062 - training loss: 0.4541, validation loss: 1.5512
2024-06-03 12:44:14 [INFO]: Epoch 063 - training loss: 0.4552, validation loss: 1.5244
2024-06-03 12:44:18 [INFO]: Epoch 064 - training loss: 0.4539, validation loss: 1.5439
2024-06-03 12:44:22 [INFO]: Epoch 065 - training loss: 0.4532, validation loss: 1.5172
2024-06-03 12:44:26 [INFO]: Epoch 066 - training loss: 0.4536, validation loss: 1.5200
2024-06-03 12:44:30 [INFO]: Epoch 067 - training loss: 0.4526, validation loss: 1.5284
2024-06-03 12:44:34 [INFO]: Epoch 068 - training loss: 0.4512, validation loss: 1.5357
2024-06-03 12:44:38 [INFO]: Epoch 069 - training loss: 0.4503, validation loss: 1.5481
2024-06-03 12:44:42 [INFO]: Epoch 070 - training loss: 0.4502, validation loss: 1.5219
2024-06-03 12:44:47 [INFO]: Epoch 071 - training loss: 0.4499, validation loss: 1.5043
2024-06-03 12:44:51 [INFO]: Epoch 072 - training loss: 0.4489, validation loss: 1.4982
2024-06-03 12:44:55 [INFO]: Epoch 073 - training loss: 0.4484, validation loss: 1.5006
2024-06-03 12:44:59 [INFO]: Epoch 074 - training loss: 0.4486, validation loss: 1.5167
2024-06-03 12:45:03 [INFO]: Epoch 075 - training loss: 0.4501, validation loss: 1.5153
2024-06-03 12:45:07 [INFO]: Epoch 076 - training loss: 0.4448, validation loss: 1.4881
2024-06-03 12:45:11 [INFO]: Epoch 077 - training loss: 0.4467, validation loss: 1.4908
2024-06-03 12:45:15 [INFO]: Epoch 078 - training loss: 0.4462, validation loss: 1.5049
2024-06-03 12:45:19 [INFO]: Epoch 079 - training loss: 0.4446, validation loss: 1.4862
2024-06-03 12:45:23 [INFO]: Epoch 080 - training loss: 0.4451, validation loss: 1.4960
2024-06-03 12:45:27 [INFO]: Epoch 081 - training loss: 0.4443, validation loss: 1.4774
2024-06-03 12:45:31 [INFO]: Epoch 082 - training loss: 0.4441, validation loss: 1.4911
2024-06-03 12:45:35 [INFO]: Epoch 083 - training loss: 0.4438, validation loss: 1.4875
2024-06-03 12:45:39 [INFO]: Epoch 084 - training loss: 0.4455, validation loss: 1.5040
2024-06-03 12:45:44 [INFO]: Epoch 085 - training loss: 0.4426, validation loss: 1.4857
2024-06-03 12:45:48 [INFO]: Epoch 086 - training loss: 0.4413, validation loss: 1.4641
2024-06-03 12:45:52 [INFO]: Epoch 087 - training loss: 0.4408, validation loss: 1.4630
2024-06-03 12:45:56 [INFO]: Epoch 088 - training loss: 0.4411, validation loss: 1.4691
2024-06-03 12:46:00 [INFO]: Epoch 089 - training loss: 0.4399, validation loss: 1.4781
2024-06-03 12:46:04 [INFO]: Epoch 090 - training loss: 0.4393, validation loss: 1.4775
2024-06-03 12:46:08 [INFO]: Epoch 091 - training loss: 0.4409, validation loss: 1.4611
2024-06-03 12:46:12 [INFO]: Epoch 092 - training loss: 0.4398, validation loss: 1.4744
2024-06-03 12:46:16 [INFO]: Epoch 093 - training loss: 0.4391, validation loss: 1.4750
2024-06-03 12:46:20 [INFO]: Epoch 094 - training loss: 0.4396, validation loss: 1.4823
2024-06-03 12:46:24 [INFO]: Epoch 095 - training loss: 0.4391, validation loss: 1.4786
2024-06-03 12:46:28 [INFO]: Epoch 096 - training loss: 0.4388, validation loss: 1.4802
2024-06-03 12:46:32 [INFO]: Epoch 097 - training loss: 0.4393, validation loss: 1.4895
2024-06-03 12:46:36 [INFO]: Epoch 098 - training loss: 0.4385, validation loss: 1.4722
2024-06-03 12:46:40 [INFO]: Epoch 099 - training loss: 0.4369, validation loss: 1.4731
2024-06-03 12:46:44 [INFO]: Epoch 100 - training loss: 0.4372, validation loss: 1.4705
2024-06-03 12:46:44 [INFO]: Finished training. The best model is from epoch#91.
2024-06-03 12:46:44 [INFO]: Saved the model to results_block_rate05/Electricity/FreTS_Electricity/round_2/20240603_T123938/FreTS.pypots
2024-06-03 12:46:46 [INFO]: Successfully saved to results_block_rate05/Electricity/FreTS_Electricity/round_2/imputation.pkl
2024-06-03 12:46:46 [INFO]: Round2 - FreTS on Electricity: MAE=0.9978, MSE=2.0884, MRE=0.5356
2024-06-03 12:46:46 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 12:46:46 [INFO]: Using the given device: cuda:0
2024-06-03 12:46:46 [INFO]: Model files will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_3/20240603_T124646
2024-06-03 12:46:46 [INFO]: Tensorboard file will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_3/20240603_T124646/tensorboard
2024-06-03 12:46:47 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-03 12:46:51 [INFO]: Epoch 001 - training loss: 1.3080, validation loss: 2.8879
2024-06-03 12:46:55 [INFO]: Epoch 002 - training loss: 0.8524, validation loss: 2.4211
2024-06-03 12:46:59 [INFO]: Epoch 003 - training loss: 0.7265, validation loss: 2.2846
2024-06-03 12:47:03 [INFO]: Epoch 004 - training loss: 0.6808, validation loss: 2.2595
2024-06-03 12:47:07 [INFO]: Epoch 005 - training loss: 0.6467, validation loss: 2.1374
2024-06-03 12:47:11 [INFO]: Epoch 006 - training loss: 0.6277, validation loss: 2.0988
2024-06-03 12:47:15 [INFO]: Epoch 007 - training loss: 0.6078, validation loss: 2.0721
2024-06-03 12:47:19 [INFO]: Epoch 008 - training loss: 0.5922, validation loss: 2.0099
2024-06-03 12:47:23 [INFO]: Epoch 009 - training loss: 0.5872, validation loss: 1.9877
2024-06-03 12:47:27 [INFO]: Epoch 010 - training loss: 0.5735, validation loss: 1.9865
2024-06-03 12:47:31 [INFO]: Epoch 011 - training loss: 0.5628, validation loss: 1.8878
2024-06-03 12:47:35 [INFO]: Epoch 012 - training loss: 0.5544, validation loss: 1.8658
2024-06-03 12:47:39 [INFO]: Epoch 013 - training loss: 0.5527, validation loss: 1.8632
2024-06-03 12:47:43 [INFO]: Epoch 014 - training loss: 0.5429, validation loss: 1.8481
2024-06-03 12:47:47 [INFO]: Epoch 015 - training loss: 0.5387, validation loss: 1.8386
2024-06-03 12:47:51 [INFO]: Epoch 016 - training loss: 0.5344, validation loss: 1.8573
2024-06-03 12:47:55 [INFO]: Epoch 017 - training loss: 0.5317, validation loss: 1.7859
2024-06-03 12:47:59 [INFO]: Epoch 018 - training loss: 0.5270, validation loss: 1.7767
2024-06-03 12:48:03 [INFO]: Epoch 019 - training loss: 0.5225, validation loss: 1.8092
2024-06-03 12:48:07 [INFO]: Epoch 020 - training loss: 0.5174, validation loss: 1.7483
2024-06-03 12:48:12 [INFO]: Epoch 021 - training loss: 0.5146, validation loss: 1.7262
2024-06-03 12:48:16 [INFO]: Epoch 022 - training loss: 0.5132, validation loss: 1.7402
2024-06-03 12:48:19 [INFO]: Epoch 023 - training loss: 0.5079, validation loss: 1.7343
2024-06-03 12:48:24 [INFO]: Epoch 024 - training loss: 0.5081, validation loss: 1.7090
2024-06-03 12:48:27 [INFO]: Epoch 025 - training loss: 0.5061, validation loss: 1.6903
2024-06-03 12:48:32 [INFO]: Epoch 026 - training loss: 0.5010, validation loss: 1.6942
2024-06-03 12:48:36 [INFO]: Epoch 027 - training loss: 0.4988, validation loss: 1.6814
2024-06-03 12:48:40 [INFO]: Epoch 028 - training loss: 0.4982, validation loss: 1.6633
2024-06-03 12:48:44 [INFO]: Epoch 029 - training loss: 0.4945, validation loss: 1.6507
2024-06-03 12:48:48 [INFO]: Epoch 030 - training loss: 0.4917, validation loss: 1.6548
2024-06-03 12:48:52 [INFO]: Epoch 031 - training loss: 0.4900, validation loss: 1.6239
2024-06-03 12:48:56 [INFO]: Epoch 032 - training loss: 0.4890, validation loss: 1.6327
2024-06-03 12:49:00 [INFO]: Epoch 033 - training loss: 0.4888, validation loss: 1.6367
2024-06-03 12:49:04 [INFO]: Epoch 034 - training loss: 0.4892, validation loss: 1.6149
2024-06-03 12:49:08 [INFO]: Epoch 035 - training loss: 0.4861, validation loss: 1.6000
2024-06-03 12:49:12 [INFO]: Epoch 036 - training loss: 0.4829, validation loss: 1.5696
2024-06-03 12:49:16 [INFO]: Epoch 037 - training loss: 0.4807, validation loss: 1.5814
2024-06-03 12:49:20 [INFO]: Epoch 038 - training loss: 0.4805, validation loss: 1.5770
2024-06-03 12:49:24 [INFO]: Epoch 039 - training loss: 0.4771, validation loss: 1.5660
2024-06-03 12:49:28 [INFO]: Epoch 040 - training loss: 0.4755, validation loss: 1.5521
2024-06-03 12:49:32 [INFO]: Epoch 041 - training loss: 0.4758, validation loss: 1.5561
2024-06-03 12:49:36 [INFO]: Epoch 042 - training loss: 0.4764, validation loss: 1.5637
2024-06-03 12:49:40 [INFO]: Epoch 043 - training loss: 0.4754, validation loss: 1.5313
2024-06-03 12:49:44 [INFO]: Epoch 044 - training loss: 0.4710, validation loss: 1.5234
2024-06-03 12:49:48 [INFO]: Epoch 045 - training loss: 0.4709, validation loss: 1.5381
2024-06-03 12:49:52 [INFO]: Epoch 046 - training loss: 0.4696, validation loss: 1.5149
2024-06-03 12:49:56 [INFO]: Epoch 047 - training loss: 0.4696, validation loss: 1.5098
2024-06-03 12:50:00 [INFO]: Epoch 048 - training loss: 0.4710, validation loss: 1.5074
2024-06-03 12:50:05 [INFO]: Epoch 049 - training loss: 0.4679, validation loss: 1.5192
2024-06-03 12:50:09 [INFO]: Epoch 050 - training loss: 0.4655, validation loss: 1.5182
2024-06-03 12:50:13 [INFO]: Epoch 051 - training loss: 0.4646, validation loss: 1.4926
2024-06-03 12:50:17 [INFO]: Epoch 052 - training loss: 0.4645, validation loss: 1.4961
2024-06-03 12:50:21 [INFO]: Epoch 053 - training loss: 0.4636, validation loss: 1.4927
2024-06-03 12:50:25 [INFO]: Epoch 054 - training loss: 0.4612, validation loss: 1.4833
2024-06-03 12:50:29 [INFO]: Epoch 055 - training loss: 0.4613, validation loss: 1.4827
2024-06-03 12:50:33 [INFO]: Epoch 056 - training loss: 0.4609, validation loss: 1.4871
2024-06-03 12:50:37 [INFO]: Epoch 057 - training loss: 0.4598, validation loss: 1.4798
2024-06-03 12:50:41 [INFO]: Epoch 058 - training loss: 0.4584, validation loss: 1.4761
2024-06-03 12:50:45 [INFO]: Epoch 059 - training loss: 0.4570, validation loss: 1.4701
2024-06-03 12:50:49 [INFO]: Epoch 060 - training loss: 0.4566, validation loss: 1.4694
2024-06-03 12:50:53 [INFO]: Epoch 061 - training loss: 0.4571, validation loss: 1.4560
2024-06-03 12:50:57 [INFO]: Epoch 062 - training loss: 0.4563, validation loss: 1.4600
2024-06-03 12:51:01 [INFO]: Epoch 063 - training loss: 0.4558, validation loss: 1.4486
2024-06-03 12:51:05 [INFO]: Epoch 064 - training loss: 0.4539, validation loss: 1.4299
2024-06-03 12:51:09 [INFO]: Epoch 065 - training loss: 0.4557, validation loss: 1.4578
2024-06-03 12:51:13 [INFO]: Epoch 066 - training loss: 0.4532, validation loss: 1.4493
2024-06-03 12:51:17 [INFO]: Epoch 067 - training loss: 0.4533, validation loss: 1.4641
2024-06-03 12:51:21 [INFO]: Epoch 068 - training loss: 0.4532, validation loss: 1.4309
2024-06-03 12:51:25 [INFO]: Epoch 069 - training loss: 0.4513, validation loss: 1.4332
2024-06-03 12:51:29 [INFO]: Epoch 070 - training loss: 0.4508, validation loss: 1.4381
2024-06-03 12:51:33 [INFO]: Epoch 071 - training loss: 0.4502, validation loss: 1.4338
2024-06-03 12:51:37 [INFO]: Epoch 072 - training loss: 0.4498, validation loss: 1.4470
2024-06-03 12:51:41 [INFO]: Epoch 073 - training loss: 0.4496, validation loss: 1.4204
2024-06-03 12:51:45 [INFO]: Epoch 074 - training loss: 0.4493, validation loss: 1.4258
2024-06-03 12:51:49 [INFO]: Epoch 075 - training loss: 0.4481, validation loss: 1.4046
2024-06-03 12:51:53 [INFO]: Epoch 076 - training loss: 0.4465, validation loss: 1.4191
2024-06-03 12:51:57 [INFO]: Epoch 077 - training loss: 0.4465, validation loss: 1.4573
2024-06-03 12:52:02 [INFO]: Epoch 078 - training loss: 0.4467, validation loss: 1.4311
2024-06-03 12:52:06 [INFO]: Epoch 079 - training loss: 0.4456, validation loss: 1.4330
2024-06-03 12:52:10 [INFO]: Epoch 080 - training loss: 0.4469, validation loss: 1.4253
2024-06-03 12:52:14 [INFO]: Epoch 081 - training loss: 0.4441, validation loss: 1.4047
2024-06-03 12:52:18 [INFO]: Epoch 082 - training loss: 0.4464, validation loss: 1.4315
2024-06-03 12:52:22 [INFO]: Epoch 083 - training loss: 0.4453, validation loss: 1.4205
2024-06-03 12:52:26 [INFO]: Epoch 084 - training loss: 0.4454, validation loss: 1.4266
2024-06-03 12:52:30 [INFO]: Epoch 085 - training loss: 0.4445, validation loss: 1.4150
2024-06-03 12:52:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:52:30 [INFO]: Finished training. The best model is from epoch#75.
2024-06-03 12:52:30 [INFO]: Saved the model to results_block_rate05/Electricity/FreTS_Electricity/round_3/20240603_T124646/FreTS.pypots
2024-06-03 12:52:32 [INFO]: Successfully saved to results_block_rate05/Electricity/FreTS_Electricity/round_3/imputation.pkl
2024-06-03 12:52:32 [INFO]: Round3 - FreTS on Electricity: MAE=1.0203, MSE=2.1960, MRE=0.5477
2024-06-03 12:52:32 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 12:52:32 [INFO]: Using the given device: cuda:0
2024-06-03 12:52:33 [INFO]: Model files will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_4/20240603_T125232
2024-06-03 12:52:33 [INFO]: Tensorboard file will be saved to results_block_rate05/Electricity/FreTS_Electricity/round_4/20240603_T125232/tensorboard
2024-06-03 12:52:33 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-03 12:52:37 [INFO]: Epoch 001 - training loss: 1.2853, validation loss: 2.7429
2024-06-03 12:52:41 [INFO]: Epoch 002 - training loss: 0.8357, validation loss: 2.3926
2024-06-03 12:52:45 [INFO]: Epoch 003 - training loss: 0.7279, validation loss: 2.3902
2024-06-03 12:52:49 [INFO]: Epoch 004 - training loss: 0.6749, validation loss: 2.2984
2024-06-03 12:52:53 [INFO]: Epoch 005 - training loss: 0.6466, validation loss: 2.2512
2024-06-03 12:52:58 [INFO]: Epoch 006 - training loss: 0.6229, validation loss: 2.1164
2024-06-03 12:53:02 [INFO]: Epoch 007 - training loss: 0.6059, validation loss: 2.0123
2024-06-03 12:53:06 [INFO]: Epoch 008 - training loss: 0.5934, validation loss: 1.9976
2024-06-03 12:53:10 [INFO]: Epoch 009 - training loss: 0.5777, validation loss: 2.0085
2024-06-03 12:53:14 [INFO]: Epoch 010 - training loss: 0.5743, validation loss: 1.9414
2024-06-03 12:53:18 [INFO]: Epoch 011 - training loss: 0.5627, validation loss: 1.9259
2024-06-03 12:53:22 [INFO]: Epoch 012 - training loss: 0.5570, validation loss: 1.8709
2024-06-03 12:53:25 [INFO]: Epoch 013 - training loss: 0.5470, validation loss: 1.8577
2024-06-03 12:53:29 [INFO]: Epoch 014 - training loss: 0.5383, validation loss: 1.8462
2024-06-03 12:53:33 [INFO]: Epoch 015 - training loss: 0.5370, validation loss: 1.8137
2024-06-03 12:53:38 [INFO]: Epoch 016 - training loss: 0.5342, validation loss: 1.7650
2024-06-03 12:53:41 [INFO]: Epoch 017 - training loss: 0.5276, validation loss: 1.7713
2024-06-03 12:53:45 [INFO]: Epoch 018 - training loss: 0.5230, validation loss: 1.7517
2024-06-03 12:53:49 [INFO]: Epoch 019 - training loss: 0.5174, validation loss: 1.7626
2024-06-03 12:53:53 [INFO]: Epoch 020 - training loss: 0.5183, validation loss: 1.7474
2024-06-03 12:53:57 [INFO]: Epoch 021 - training loss: 0.5145, validation loss: 1.7110
2024-06-03 12:54:01 [INFO]: Epoch 022 - training loss: 0.5080, validation loss: 1.7215
2024-06-03 12:54:05 [INFO]: Epoch 023 - training loss: 0.5060, validation loss: 1.7125
2024-06-03 12:54:09 [INFO]: Epoch 024 - training loss: 0.5014, validation loss: 1.7016
2024-06-03 12:54:12 [INFO]: Epoch 025 - training loss: 0.5001, validation loss: 1.6822
2024-06-03 12:54:15 [INFO]: Epoch 026 - training loss: 0.4967, validation loss: 1.6686
2024-06-03 12:54:19 [INFO]: Epoch 027 - training loss: 0.4957, validation loss: 1.6678
2024-06-03 12:54:22 [INFO]: Epoch 028 - training loss: 0.4931, validation loss: 1.6492
2024-06-03 12:54:25 [INFO]: Epoch 029 - training loss: 0.4918, validation loss: 1.6323
2024-06-03 12:54:28 [INFO]: Epoch 030 - training loss: 0.4893, validation loss: 1.6219
2024-06-03 12:54:32 [INFO]: Epoch 031 - training loss: 0.4868, validation loss: 1.6301
2024-06-03 12:54:35 [INFO]: Epoch 032 - training loss: 0.4843, validation loss: 1.6119
2024-06-03 12:54:38 [INFO]: Epoch 033 - training loss: 0.4817, validation loss: 1.5911
2024-06-03 12:54:41 [INFO]: Epoch 034 - training loss: 0.4813, validation loss: 1.5851
2024-06-03 12:54:44 [INFO]: Epoch 035 - training loss: 0.4797, validation loss: 1.5860
2024-06-03 12:54:47 [INFO]: Epoch 036 - training loss: 0.4802, validation loss: 1.5779
2024-06-03 12:54:50 [INFO]: Epoch 037 - training loss: 0.4780, validation loss: 1.5714
2024-06-03 12:54:53 [INFO]: Epoch 038 - training loss: 0.4760, validation loss: 1.5570
2024-06-03 12:54:57 [INFO]: Epoch 039 - training loss: 0.4778, validation loss: 1.5706
2024-06-03 12:55:00 [INFO]: Epoch 040 - training loss: 0.4718, validation loss: 1.5599
2024-06-03 12:55:03 [INFO]: Epoch 041 - training loss: 0.4714, validation loss: 1.5490
2024-06-03 12:55:06 [INFO]: Epoch 042 - training loss: 0.4689, validation loss: 1.5356
2024-06-03 12:55:10 [INFO]: Epoch 043 - training loss: 0.4677, validation loss: 1.5225
2024-06-03 12:55:13 [INFO]: Epoch 044 - training loss: 0.4664, validation loss: 1.5201
2024-06-03 12:55:16 [INFO]: Epoch 045 - training loss: 0.4664, validation loss: 1.5262
2024-06-03 12:55:20 [INFO]: Epoch 046 - training loss: 0.4662, validation loss: 1.4981
2024-06-03 12:55:23 [INFO]: Epoch 047 - training loss: 0.4638, validation loss: 1.5173
2024-06-03 12:55:26 [INFO]: Epoch 048 - training loss: 0.4627, validation loss: 1.4855
2024-06-03 12:55:30 [INFO]: Epoch 049 - training loss: 0.4602, validation loss: 1.5088
2024-06-03 12:55:33 [INFO]: Epoch 050 - training loss: 0.4599, validation loss: 1.4940
2024-06-03 12:55:37 [INFO]: Epoch 051 - training loss: 0.4595, validation loss: 1.4627
2024-06-03 12:55:40 [INFO]: Epoch 052 - training loss: 0.4585, validation loss: 1.5036
2024-06-03 12:55:43 [INFO]: Epoch 053 - training loss: 0.4577, validation loss: 1.4895
2024-06-03 12:55:46 [INFO]: Epoch 054 - training loss: 0.4561, validation loss: 1.5037
2024-06-03 12:55:50 [INFO]: Epoch 055 - training loss: 0.4546, validation loss: 1.4807
2024-06-03 12:55:53 [INFO]: Epoch 056 - training loss: 0.4541, validation loss: 1.4620
2024-06-03 12:55:56 [INFO]: Epoch 057 - training loss: 0.4548, validation loss: 1.4637
2024-06-03 12:55:59 [INFO]: Epoch 058 - training loss: 0.4541, validation loss: 1.4590
2024-06-03 12:56:03 [INFO]: Epoch 059 - training loss: 0.4517, validation loss: 1.4597
2024-06-03 12:56:06 [INFO]: Epoch 060 - training loss: 0.4507, validation loss: 1.4791
2024-06-03 12:56:09 [INFO]: Epoch 061 - training loss: 0.4517, validation loss: 1.4359
2024-06-03 12:56:13 [INFO]: Epoch 062 - training loss: 0.4497, validation loss: 1.4398
2024-06-03 12:56:16 [INFO]: Epoch 063 - training loss: 0.4499, validation loss: 1.4635
2024-06-03 12:56:19 [INFO]: Epoch 064 - training loss: 0.4513, validation loss: 1.4487
2024-06-03 12:56:22 [INFO]: Epoch 065 - training loss: 0.4488, validation loss: 1.4410
2024-06-03 12:56:25 [INFO]: Epoch 066 - training loss: 0.4497, validation loss: 1.4268
2024-06-03 12:56:28 [INFO]: Epoch 067 - training loss: 0.4481, validation loss: 1.4301
2024-06-03 12:56:32 [INFO]: Epoch 068 - training loss: 0.4469, validation loss: 1.4474
2024-06-03 12:56:35 [INFO]: Epoch 069 - training loss: 0.4465, validation loss: 1.4273
2024-06-03 12:56:38 [INFO]: Epoch 070 - training loss: 0.4451, validation loss: 1.4092
2024-06-03 12:56:41 [INFO]: Epoch 071 - training loss: 0.4456, validation loss: 1.4229
2024-06-03 12:56:45 [INFO]: Epoch 072 - training loss: 0.4441, validation loss: 1.4270
2024-06-03 12:56:48 [INFO]: Epoch 073 - training loss: 0.4436, validation loss: 1.4260
2024-06-03 12:56:52 [INFO]: Epoch 074 - training loss: 0.4427, validation loss: 1.4064
2024-06-03 12:56:55 [INFO]: Epoch 075 - training loss: 0.4420, validation loss: 1.4064
2024-06-03 12:56:58 [INFO]: Epoch 076 - training loss: 0.4426, validation loss: 1.4042
2024-06-03 12:57:01 [INFO]: Epoch 077 - training loss: 0.4408, validation loss: 1.4191
2024-06-03 12:57:05 [INFO]: Epoch 078 - training loss: 0.4415, validation loss: 1.4195
2024-06-03 12:57:08 [INFO]: Epoch 079 - training loss: 0.4408, validation loss: 1.4013
2024-06-03 12:57:12 [INFO]: Epoch 080 - training loss: 0.4398, validation loss: 1.3991
2024-06-03 12:57:15 [INFO]: Epoch 081 - training loss: 0.4401, validation loss: 1.4178
2024-06-03 12:57:18 [INFO]: Epoch 082 - training loss: 0.4408, validation loss: 1.4183
2024-06-03 12:57:22 [INFO]: Epoch 083 - training loss: 0.4402, validation loss: 1.4045
2024-06-03 12:57:25 [INFO]: Epoch 084 - training loss: 0.4396, validation loss: 1.4383
2024-06-03 12:57:28 [INFO]: Epoch 085 - training loss: 0.4415, validation loss: 1.3837
2024-06-03 12:57:32 [INFO]: Epoch 086 - training loss: 0.4392, validation loss: 1.3918
2024-06-03 12:57:35 [INFO]: Epoch 087 - training loss: 0.4371, validation loss: 1.3806
2024-06-03 12:57:38 [INFO]: Epoch 088 - training loss: 0.4368, validation loss: 1.3983
2024-06-03 12:57:42 [INFO]: Epoch 089 - training loss: 0.4381, validation loss: 1.3811
2024-06-03 12:57:45 [INFO]: Epoch 090 - training loss: 0.4405, validation loss: 1.3856
2024-06-03 12:57:48 [INFO]: Epoch 091 - training loss: 0.4368, validation loss: 1.3989
2024-06-03 12:57:51 [INFO]: Epoch 092 - training loss: 0.4353, validation loss: 1.3946
2024-06-03 12:57:55 [INFO]: Epoch 093 - training loss: 0.4361, validation loss: 1.3881
2024-06-03 12:57:58 [INFO]: Epoch 094 - training loss: 0.4358, validation loss: 1.4010
2024-06-03 12:58:01 [INFO]: Epoch 095 - training loss: 0.4348, validation loss: 1.3927
2024-06-03 12:58:04 [INFO]: Epoch 096 - training loss: 0.4342, validation loss: 1.3712
2024-06-03 12:58:08 [INFO]: Epoch 097 - training loss: 0.4336, validation loss: 1.3870
2024-06-03 12:58:11 [INFO]: Epoch 098 - training loss: 0.4339, validation loss: 1.4000
2024-06-03 12:58:14 [INFO]: Epoch 099 - training loss: 0.4333, validation loss: 1.3928
2024-06-03 12:58:18 [INFO]: Epoch 100 - training loss: 0.4328, validation loss: 1.3713
2024-06-03 12:58:18 [INFO]: Finished training. The best model is from epoch#96.
2024-06-03 12:58:18 [INFO]: Saved the model to results_block_rate05/Electricity/FreTS_Electricity/round_4/20240603_T125232/FreTS.pypots
2024-06-03 12:58:20 [INFO]: Successfully saved to results_block_rate05/Electricity/FreTS_Electricity/round_4/imputation.pkl
2024-06-03 12:58:20 [INFO]: Round4 - FreTS on Electricity: MAE=0.9976, MSE=2.0629, MRE=0.5355
2024-06-03 12:58:20 [INFO]: Done! Final results:
Averaged FreTS (3,706,194 params) on Electricity: MAE=0.9999 ± 0.012148259265365316, MSE=2.0905 ± 0.05456392253217591, MRE=0.5367 ± 0.006520576710586986, average inference time=0.54
