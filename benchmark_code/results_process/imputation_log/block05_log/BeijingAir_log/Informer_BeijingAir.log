2024-06-03 15:13:43 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 15:13:43 [INFO]: Using the given device: cuda:0
2024-06-03 15:13:43 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_0/20240603_T151343
2024-06-03 15:13:43 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_0/20240603_T151343/tensorboard
2024-06-03 15:13:44 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 15:14:04 [INFO]: Epoch 001 - training loss: 0.9968, validation loss: 0.4159
2024-06-03 15:14:12 [INFO]: Epoch 002 - training loss: 0.6511, validation loss: 0.3421
2024-06-03 15:14:21 [INFO]: Epoch 003 - training loss: 0.5528, validation loss: 0.3158
2024-06-03 15:14:30 [INFO]: Epoch 004 - training loss: 0.5083, validation loss: 0.2955
2024-06-03 15:14:38 [INFO]: Epoch 005 - training loss: 0.4771, validation loss: 0.2845
2024-06-03 15:14:47 [INFO]: Epoch 006 - training loss: 0.4647, validation loss: 0.2788
2024-06-03 15:14:55 [INFO]: Epoch 007 - training loss: 0.4481, validation loss: 0.2742
2024-06-03 15:15:04 [INFO]: Epoch 008 - training loss: 0.4342, validation loss: 0.2711
2024-06-03 15:15:12 [INFO]: Epoch 009 - training loss: 0.4248, validation loss: 0.2704
2024-06-03 15:15:20 [INFO]: Epoch 010 - training loss: 0.4147, validation loss: 0.2651
2024-06-03 15:15:28 [INFO]: Epoch 011 - training loss: 0.4108, validation loss: 0.2654
2024-06-03 15:15:37 [INFO]: Epoch 012 - training loss: 0.4028, validation loss: 0.2622
2024-06-03 15:15:45 [INFO]: Epoch 013 - training loss: 0.3977, validation loss: 0.2666
2024-06-03 15:15:53 [INFO]: Epoch 014 - training loss: 0.3966, validation loss: 0.2601
2024-06-03 15:16:02 [INFO]: Epoch 015 - training loss: 0.3910, validation loss: 0.2591
2024-06-03 15:16:11 [INFO]: Epoch 016 - training loss: 0.3899, validation loss: 0.2570
2024-06-03 15:16:19 [INFO]: Epoch 017 - training loss: 0.3798, validation loss: 0.2566
2024-06-03 15:16:28 [INFO]: Epoch 018 - training loss: 0.3744, validation loss: 0.2532
2024-06-03 15:16:36 [INFO]: Epoch 019 - training loss: 0.3730, validation loss: 0.2563
2024-06-03 15:16:45 [INFO]: Epoch 020 - training loss: 0.3683, validation loss: 0.2499
2024-06-03 15:16:54 [INFO]: Epoch 021 - training loss: 0.3629, validation loss: 0.2510
2024-06-03 15:17:02 [INFO]: Epoch 022 - training loss: 0.3644, validation loss: 0.2462
2024-06-03 15:17:11 [INFO]: Epoch 023 - training loss: 0.3599, validation loss: 0.2454
2024-06-03 15:17:19 [INFO]: Epoch 024 - training loss: 0.3540, validation loss: 0.2498
2024-06-03 15:17:28 [INFO]: Epoch 025 - training loss: 0.3562, validation loss: 0.2499
2024-06-03 15:17:36 [INFO]: Epoch 026 - training loss: 0.3557, validation loss: 0.2518
2024-06-03 15:17:45 [INFO]: Epoch 027 - training loss: 0.3530, validation loss: 0.2486
2024-06-03 15:17:53 [INFO]: Epoch 028 - training loss: 0.3447, validation loss: 0.2418
2024-06-03 15:18:02 [INFO]: Epoch 029 - training loss: 0.3450, validation loss: 0.2455
2024-06-03 15:18:10 [INFO]: Epoch 030 - training loss: 0.3436, validation loss: 0.2425
2024-06-03 15:18:19 [INFO]: Epoch 031 - training loss: 0.3440, validation loss: 0.2462
2024-06-03 15:18:27 [INFO]: Epoch 032 - training loss: 0.3390, validation loss: 0.2403
2024-06-03 15:18:35 [INFO]: Epoch 033 - training loss: 0.3376, validation loss: 0.2400
2024-06-03 15:18:44 [INFO]: Epoch 034 - training loss: 0.3318, validation loss: 0.2383
2024-06-03 15:18:52 [INFO]: Epoch 035 - training loss: 0.3339, validation loss: 0.2347
2024-06-03 15:19:01 [INFO]: Epoch 036 - training loss: 0.3309, validation loss: 0.2422
2024-06-03 15:19:10 [INFO]: Epoch 037 - training loss: 0.3310, validation loss: 0.2417
2024-06-03 15:19:18 [INFO]: Epoch 038 - training loss: 0.3294, validation loss: 0.2348
2024-06-03 15:19:27 [INFO]: Epoch 039 - training loss: 0.3278, validation loss: 0.2364
2024-06-03 15:19:36 [INFO]: Epoch 040 - training loss: 0.3272, validation loss: 0.2365
2024-06-03 15:19:44 [INFO]: Epoch 041 - training loss: 0.3215, validation loss: 0.2341
2024-06-03 15:19:53 [INFO]: Epoch 042 - training loss: 0.3161, validation loss: 0.2358
2024-06-03 15:20:01 [INFO]: Epoch 043 - training loss: 0.3179, validation loss: 0.2392
2024-06-03 15:20:10 [INFO]: Epoch 044 - training loss: 0.3174, validation loss: 0.2363
2024-06-03 15:20:18 [INFO]: Epoch 045 - training loss: 0.3172, validation loss: 0.2332
2024-06-03 15:20:27 [INFO]: Epoch 046 - training loss: 0.3175, validation loss: 0.2299
2024-06-03 15:20:35 [INFO]: Epoch 047 - training loss: 0.3194, validation loss: 0.2356
2024-06-03 15:20:44 [INFO]: Epoch 048 - training loss: 0.3133, validation loss: 0.2331
2024-06-03 15:20:53 [INFO]: Epoch 049 - training loss: 0.3108, validation loss: 0.2325
2024-06-03 15:21:01 [INFO]: Epoch 050 - training loss: 0.3079, validation loss: 0.2310
2024-06-03 15:21:10 [INFO]: Epoch 051 - training loss: 0.3070, validation loss: 0.2346
2024-06-03 15:21:19 [INFO]: Epoch 052 - training loss: 0.3027, validation loss: 0.2339
2024-06-03 15:21:27 [INFO]: Epoch 053 - training loss: 0.3033, validation loss: 0.2323
2024-06-03 15:21:35 [INFO]: Epoch 054 - training loss: 0.3005, validation loss: 0.2300
2024-06-03 15:21:44 [INFO]: Epoch 055 - training loss: 0.3056, validation loss: 0.2316
2024-06-03 15:21:53 [INFO]: Epoch 056 - training loss: 0.3020, validation loss: 0.2295
2024-06-03 15:22:02 [INFO]: Epoch 057 - training loss: 0.2966, validation loss: 0.2256
2024-06-03 15:22:10 [INFO]: Epoch 058 - training loss: 0.2957, validation loss: 0.2294
2024-06-03 15:22:19 [INFO]: Epoch 059 - training loss: 0.3002, validation loss: 0.2353
2024-06-03 15:22:28 [INFO]: Epoch 060 - training loss: 0.3056, validation loss: 0.2264
2024-06-03 15:22:36 [INFO]: Epoch 061 - training loss: 0.2992, validation loss: 0.2266
2024-06-03 15:22:44 [INFO]: Epoch 062 - training loss: 0.2960, validation loss: 0.2255
2024-06-03 15:22:51 [INFO]: Epoch 063 - training loss: 0.2922, validation loss: 0.2256
2024-06-03 15:22:58 [INFO]: Epoch 064 - training loss: 0.2886, validation loss: 0.2242
2024-06-03 15:23:05 [INFO]: Epoch 065 - training loss: 0.2875, validation loss: 0.2250
2024-06-03 15:23:13 [INFO]: Epoch 066 - training loss: 0.2897, validation loss: 0.2286
2024-06-03 15:23:20 [INFO]: Epoch 067 - training loss: 0.2884, validation loss: 0.2239
2024-06-03 15:23:27 [INFO]: Epoch 068 - training loss: 0.2861, validation loss: 0.2224
2024-06-03 15:23:35 [INFO]: Epoch 069 - training loss: 0.2869, validation loss: 0.2263
2024-06-03 15:23:42 [INFO]: Epoch 070 - training loss: 0.2839, validation loss: 0.2203
2024-06-03 15:23:50 [INFO]: Epoch 071 - training loss: 0.2857, validation loss: 0.2219
2024-06-03 15:23:58 [INFO]: Epoch 072 - training loss: 0.2840, validation loss: 0.2205
2024-06-03 15:24:05 [INFO]: Epoch 073 - training loss: 0.2835, validation loss: 0.2267
2024-06-03 15:24:13 [INFO]: Epoch 074 - training loss: 0.2833, validation loss: 0.2198
2024-06-03 15:24:21 [INFO]: Epoch 075 - training loss: 0.2826, validation loss: 0.2234
2024-06-03 15:24:28 [INFO]: Epoch 076 - training loss: 0.2814, validation loss: 0.2227
2024-06-03 15:24:36 [INFO]: Epoch 077 - training loss: 0.2786, validation loss: 0.2299
2024-06-03 15:24:44 [INFO]: Epoch 078 - training loss: 0.2780, validation loss: 0.2228
2024-06-03 15:24:51 [INFO]: Epoch 079 - training loss: 0.2756, validation loss: 0.2271
2024-06-03 15:24:59 [INFO]: Epoch 080 - training loss: 0.2780, validation loss: 0.2213
2024-06-03 15:25:07 [INFO]: Epoch 081 - training loss: 0.2768, validation loss: 0.2173
2024-06-03 15:25:15 [INFO]: Epoch 082 - training loss: 0.2746, validation loss: 0.2184
2024-06-03 15:25:22 [INFO]: Epoch 083 - training loss: 0.2784, validation loss: 0.2246
2024-06-03 15:25:30 [INFO]: Epoch 084 - training loss: 0.2795, validation loss: 0.2201
2024-06-03 15:25:38 [INFO]: Epoch 085 - training loss: 0.2750, validation loss: 0.2218
2024-06-03 15:25:45 [INFO]: Epoch 086 - training loss: 0.2745, validation loss: 0.2215
2024-06-03 15:25:53 [INFO]: Epoch 087 - training loss: 0.2700, validation loss: 0.2181
2024-06-03 15:26:00 [INFO]: Epoch 088 - training loss: 0.2683, validation loss: 0.2182
2024-06-03 15:26:08 [INFO]: Epoch 089 - training loss: 0.2684, validation loss: 0.2213
2024-06-03 15:26:15 [INFO]: Epoch 090 - training loss: 0.2725, validation loss: 0.2171
2024-06-03 15:26:23 [INFO]: Epoch 091 - training loss: 0.2732, validation loss: 0.2159
2024-06-03 15:26:31 [INFO]: Epoch 092 - training loss: 0.2703, validation loss: 0.2179
2024-06-03 15:26:38 [INFO]: Epoch 093 - training loss: 0.2681, validation loss: 0.2189
2024-06-03 15:26:46 [INFO]: Epoch 094 - training loss: 0.2677, validation loss: 0.2192
2024-06-03 15:26:53 [INFO]: Epoch 095 - training loss: 0.2690, validation loss: 0.2173
2024-06-03 15:27:01 [INFO]: Epoch 096 - training loss: 0.2731, validation loss: 0.2218
2024-06-03 15:27:07 [INFO]: Epoch 097 - training loss: 0.2665, validation loss: 0.2160
2024-06-03 15:27:15 [INFO]: Epoch 098 - training loss: 0.2658, validation loss: 0.2172
2024-06-03 15:27:22 [INFO]: Epoch 099 - training loss: 0.2627, validation loss: 0.2171
2024-06-03 15:27:30 [INFO]: Epoch 100 - training loss: 0.2662, validation loss: 0.2140
2024-06-03 15:27:30 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 15:27:30 [INFO]: Saved the model to results_block_rate05/BeijingAir/Informer_BeijingAir/round_0/20240603_T151343/Informer.pypots
2024-06-03 15:27:35 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_0/imputation.pkl
2024-06-03 15:27:35 [INFO]: Round0 - Informer on BeijingAir: MAE=0.2152, MSE=0.2617, MRE=0.2909
2024-06-03 15:27:35 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 15:27:35 [INFO]: Using the given device: cuda:0
2024-06-03 15:27:35 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_1/20240603_T152735
2024-06-03 15:27:35 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_1/20240603_T152735/tensorboard
2024-06-03 15:27:36 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 15:27:43 [INFO]: Epoch 001 - training loss: 1.0019, validation loss: 0.4576
2024-06-03 15:27:51 [INFO]: Epoch 002 - training loss: 0.6527, validation loss: 0.3556
2024-06-03 15:27:58 [INFO]: Epoch 003 - training loss: 0.5523, validation loss: 0.3146
2024-06-03 15:28:06 [INFO]: Epoch 004 - training loss: 0.5101, validation loss: 0.3027
2024-06-03 15:28:13 [INFO]: Epoch 005 - training loss: 0.4842, validation loss: 0.2938
2024-06-03 15:28:21 [INFO]: Epoch 006 - training loss: 0.4644, validation loss: 0.2864
2024-06-03 15:28:29 [INFO]: Epoch 007 - training loss: 0.4477, validation loss: 0.2751
2024-06-03 15:28:36 [INFO]: Epoch 008 - training loss: 0.4353, validation loss: 0.2687
2024-06-03 15:28:44 [INFO]: Epoch 009 - training loss: 0.4246, validation loss: 0.2683
2024-06-03 15:28:51 [INFO]: Epoch 010 - training loss: 0.4182, validation loss: 0.2673
2024-06-03 15:28:59 [INFO]: Epoch 011 - training loss: 0.4114, validation loss: 0.2664
2024-06-03 15:29:07 [INFO]: Epoch 012 - training loss: 0.4108, validation loss: 0.2642
2024-06-03 15:29:14 [INFO]: Epoch 013 - training loss: 0.3945, validation loss: 0.2594
2024-06-03 15:29:22 [INFO]: Epoch 014 - training loss: 0.3894, validation loss: 0.2559
2024-06-03 15:29:29 [INFO]: Epoch 015 - training loss: 0.3852, validation loss: 0.2600
2024-06-03 15:29:37 [INFO]: Epoch 016 - training loss: 0.3846, validation loss: 0.2626
2024-06-03 15:29:44 [INFO]: Epoch 017 - training loss: 0.3770, validation loss: 0.2613
2024-06-03 15:29:52 [INFO]: Epoch 018 - training loss: 0.3767, validation loss: 0.2590
2024-06-03 15:30:00 [INFO]: Epoch 019 - training loss: 0.3751, validation loss: 0.2495
2024-06-03 15:30:07 [INFO]: Epoch 020 - training loss: 0.3727, validation loss: 0.2462
2024-06-03 15:30:15 [INFO]: Epoch 021 - training loss: 0.3669, validation loss: 0.2530
2024-06-03 15:30:22 [INFO]: Epoch 022 - training loss: 0.3623, validation loss: 0.2588
2024-06-03 15:30:30 [INFO]: Epoch 023 - training loss: 0.3600, validation loss: 0.2515
2024-06-03 15:30:37 [INFO]: Epoch 024 - training loss: 0.3582, validation loss: 0.2480
2024-06-03 15:30:45 [INFO]: Epoch 025 - training loss: 0.3517, validation loss: 0.2461
2024-06-03 15:30:52 [INFO]: Epoch 026 - training loss: 0.3503, validation loss: 0.2437
2024-06-03 15:30:59 [INFO]: Epoch 027 - training loss: 0.3488, validation loss: 0.2501
2024-06-03 15:31:07 [INFO]: Epoch 028 - training loss: 0.3449, validation loss: 0.2417
2024-06-03 15:31:14 [INFO]: Epoch 029 - training loss: 0.3423, validation loss: 0.2482
2024-06-03 15:31:21 [INFO]: Epoch 030 - training loss: 0.3457, validation loss: 0.2477
2024-06-03 15:31:29 [INFO]: Epoch 031 - training loss: 0.3448, validation loss: 0.2428
2024-06-03 15:31:37 [INFO]: Epoch 032 - training loss: 0.3385, validation loss: 0.2480
2024-06-03 15:31:44 [INFO]: Epoch 033 - training loss: 0.3333, validation loss: 0.2450
2024-06-03 15:31:52 [INFO]: Epoch 034 - training loss: 0.3343, validation loss: 0.2389
2024-06-03 15:31:59 [INFO]: Epoch 035 - training loss: 0.3307, validation loss: 0.2387
2024-06-03 15:32:07 [INFO]: Epoch 036 - training loss: 0.3287, validation loss: 0.2400
2024-06-03 15:32:14 [INFO]: Epoch 037 - training loss: 0.3262, validation loss: 0.2379
2024-06-03 15:32:22 [INFO]: Epoch 038 - training loss: 0.3287, validation loss: 0.2412
2024-06-03 15:32:29 [INFO]: Epoch 039 - training loss: 0.3308, validation loss: 0.2370
2024-06-03 15:32:37 [INFO]: Epoch 040 - training loss: 0.3210, validation loss: 0.2358
2024-06-03 15:32:45 [INFO]: Epoch 041 - training loss: 0.3196, validation loss: 0.2333
2024-06-03 15:32:52 [INFO]: Epoch 042 - training loss: 0.3175, validation loss: 0.2325
2024-06-03 15:32:59 [INFO]: Epoch 043 - training loss: 0.3185, validation loss: 0.2394
2024-06-03 15:33:06 [INFO]: Epoch 044 - training loss: 0.3153, validation loss: 0.2378
2024-06-03 15:33:13 [INFO]: Epoch 045 - training loss: 0.3145, validation loss: 0.2353
2024-06-03 15:33:20 [INFO]: Epoch 046 - training loss: 0.3084, validation loss: 0.2321
2024-06-03 15:33:26 [INFO]: Epoch 047 - training loss: 0.3109, validation loss: 0.2324
2024-06-03 15:33:33 [INFO]: Epoch 048 - training loss: 0.3095, validation loss: 0.2331
2024-06-03 15:33:41 [INFO]: Epoch 049 - training loss: 0.3056, validation loss: 0.2307
2024-06-03 15:33:48 [INFO]: Epoch 050 - training loss: 0.3037, validation loss: 0.2270
2024-06-03 15:33:55 [INFO]: Epoch 051 - training loss: 0.3049, validation loss: 0.2305
2024-06-03 15:34:02 [INFO]: Epoch 052 - training loss: 0.3037, validation loss: 0.2286
2024-06-03 15:34:09 [INFO]: Epoch 053 - training loss: 0.3024, validation loss: 0.2293
2024-06-03 15:34:16 [INFO]: Epoch 054 - training loss: 0.2994, validation loss: 0.2286
2024-06-03 15:34:23 [INFO]: Epoch 055 - training loss: 0.3032, validation loss: 0.2299
2024-06-03 15:34:30 [INFO]: Epoch 056 - training loss: 0.2993, validation loss: 0.2309
2024-06-03 15:34:36 [INFO]: Epoch 057 - training loss: 0.3015, validation loss: 0.2362
2024-06-03 15:34:43 [INFO]: Epoch 058 - training loss: 0.2993, validation loss: 0.2279
2024-06-03 15:34:48 [INFO]: Epoch 059 - training loss: 0.3006, validation loss: 0.2236
2024-06-03 15:34:54 [INFO]: Epoch 060 - training loss: 0.2948, validation loss: 0.2236
2024-06-03 15:35:00 [INFO]: Epoch 061 - training loss: 0.2962, validation loss: 0.2264
2024-06-03 15:35:05 [INFO]: Epoch 062 - training loss: 0.2910, validation loss: 0.2285
2024-06-03 15:35:11 [INFO]: Epoch 063 - training loss: 0.2875, validation loss: 0.2233
2024-06-03 15:35:17 [INFO]: Epoch 064 - training loss: 0.2898, validation loss: 0.2248
2024-06-03 15:35:22 [INFO]: Epoch 065 - training loss: 0.2865, validation loss: 0.2265
2024-06-03 15:35:29 [INFO]: Epoch 066 - training loss: 0.2840, validation loss: 0.2266
2024-06-03 15:35:35 [INFO]: Epoch 067 - training loss: 0.2904, validation loss: 0.2292
2024-06-03 15:35:40 [INFO]: Epoch 068 - training loss: 0.2869, validation loss: 0.2246
2024-06-03 15:35:46 [INFO]: Epoch 069 - training loss: 0.2853, validation loss: 0.2230
2024-06-03 15:35:52 [INFO]: Epoch 070 - training loss: 0.2854, validation loss: 0.2222
2024-06-03 15:35:58 [INFO]: Epoch 071 - training loss: 0.2846, validation loss: 0.2219
2024-06-03 15:36:03 [INFO]: Epoch 072 - training loss: 0.2824, validation loss: 0.2236
2024-06-03 15:36:09 [INFO]: Epoch 073 - training loss: 0.2844, validation loss: 0.2258
2024-06-03 15:36:15 [INFO]: Epoch 074 - training loss: 0.2839, validation loss: 0.2221
2024-06-03 15:36:21 [INFO]: Epoch 075 - training loss: 0.2808, validation loss: 0.2241
2024-06-03 15:36:27 [INFO]: Epoch 076 - training loss: 0.2776, validation loss: 0.2185
2024-06-03 15:36:33 [INFO]: Epoch 077 - training loss: 0.2740, validation loss: 0.2181
2024-06-03 15:36:38 [INFO]: Epoch 078 - training loss: 0.2748, validation loss: 0.2183
2024-06-03 15:36:43 [INFO]: Epoch 079 - training loss: 0.2761, validation loss: 0.2213
2024-06-03 15:36:48 [INFO]: Epoch 080 - training loss: 0.2772, validation loss: 0.2192
2024-06-03 15:36:53 [INFO]: Epoch 081 - training loss: 0.2803, validation loss: 0.2263
2024-06-03 15:36:58 [INFO]: Epoch 082 - training loss: 0.2797, validation loss: 0.2209
2024-06-03 15:37:03 [INFO]: Epoch 083 - training loss: 0.2732, validation loss: 0.2161
2024-06-03 15:37:08 [INFO]: Epoch 084 - training loss: 0.2802, validation loss: 0.2220
2024-06-03 15:37:13 [INFO]: Epoch 085 - training loss: 0.2778, validation loss: 0.2145
2024-06-03 15:37:18 [INFO]: Epoch 086 - training loss: 0.2758, validation loss: 0.2197
2024-06-03 15:37:22 [INFO]: Epoch 087 - training loss: 0.2712, validation loss: 0.2199
2024-06-03 15:37:27 [INFO]: Epoch 088 - training loss: 0.2730, validation loss: 0.2195
2024-06-03 15:37:32 [INFO]: Epoch 089 - training loss: 0.2711, validation loss: 0.2172
2024-06-03 15:37:37 [INFO]: Epoch 090 - training loss: 0.2700, validation loss: 0.2207
2024-06-03 15:37:42 [INFO]: Epoch 091 - training loss: 0.2678, validation loss: 0.2159
2024-06-03 15:37:47 [INFO]: Epoch 092 - training loss: 0.2667, validation loss: 0.2171
2024-06-03 15:37:52 [INFO]: Epoch 093 - training loss: 0.2685, validation loss: 0.2173
2024-06-03 15:37:57 [INFO]: Epoch 094 - training loss: 0.2681, validation loss: 0.2173
2024-06-03 15:38:01 [INFO]: Epoch 095 - training loss: 0.2675, validation loss: 0.2181
2024-06-03 15:38:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:38:01 [INFO]: Finished training. The best model is from epoch#85.
2024-06-03 15:38:01 [INFO]: Saved the model to results_block_rate05/BeijingAir/Informer_BeijingAir/round_1/20240603_T152735/Informer.pypots
2024-06-03 15:38:04 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_1/imputation.pkl
2024-06-03 15:38:04 [INFO]: Round1 - Informer on BeijingAir: MAE=0.2171, MSE=0.2636, MRE=0.2934
2024-06-03 15:38:04 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 15:38:04 [INFO]: Using the given device: cuda:0
2024-06-03 15:38:04 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_2/20240603_T153804
2024-06-03 15:38:04 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_2/20240603_T153804/tensorboard
2024-06-03 15:38:05 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 15:38:09 [INFO]: Epoch 001 - training loss: 0.9836, validation loss: 0.4119
2024-06-03 15:38:14 [INFO]: Epoch 002 - training loss: 0.6348, validation loss: 0.3466
2024-06-03 15:38:20 [INFO]: Epoch 003 - training loss: 0.5493, validation loss: 0.3054
2024-06-03 15:38:24 [INFO]: Epoch 004 - training loss: 0.5000, validation loss: 0.2994
2024-06-03 15:38:29 [INFO]: Epoch 005 - training loss: 0.4804, validation loss: 0.2942
2024-06-03 15:38:34 [INFO]: Epoch 006 - training loss: 0.4718, validation loss: 0.2864
2024-06-03 15:38:39 [INFO]: Epoch 007 - training loss: 0.4509, validation loss: 0.2771
2024-06-03 15:38:44 [INFO]: Epoch 008 - training loss: 0.4426, validation loss: 0.2635
2024-06-03 15:38:49 [INFO]: Epoch 009 - training loss: 0.4342, validation loss: 0.2641
2024-06-03 15:38:52 [INFO]: Epoch 010 - training loss: 0.4190, validation loss: 0.2677
2024-06-03 15:38:55 [INFO]: Epoch 011 - training loss: 0.4107, validation loss: 0.2688
2024-06-03 15:38:58 [INFO]: Epoch 012 - training loss: 0.4083, validation loss: 0.2625
2024-06-03 15:39:01 [INFO]: Epoch 013 - training loss: 0.3996, validation loss: 0.2599
2024-06-03 15:39:04 [INFO]: Epoch 014 - training loss: 0.3923, validation loss: 0.2562
2024-06-03 15:39:08 [INFO]: Epoch 015 - training loss: 0.3879, validation loss: 0.2596
2024-06-03 15:39:11 [INFO]: Epoch 016 - training loss: 0.3818, validation loss: 0.2535
2024-06-03 15:39:14 [INFO]: Epoch 017 - training loss: 0.3800, validation loss: 0.2550
2024-06-03 15:39:17 [INFO]: Epoch 018 - training loss: 0.3776, validation loss: 0.2531
2024-06-03 15:39:20 [INFO]: Epoch 019 - training loss: 0.3777, validation loss: 0.2608
2024-06-03 15:39:23 [INFO]: Epoch 020 - training loss: 0.3755, validation loss: 0.2544
2024-06-03 15:39:26 [INFO]: Epoch 021 - training loss: 0.3674, validation loss: 0.2565
2024-06-03 15:39:29 [INFO]: Epoch 022 - training loss: 0.3629, validation loss: 0.2593
2024-06-03 15:39:32 [INFO]: Epoch 023 - training loss: 0.3675, validation loss: 0.2498
2024-06-03 15:39:36 [INFO]: Epoch 024 - training loss: 0.3589, validation loss: 0.2493
2024-06-03 15:39:39 [INFO]: Epoch 025 - training loss: 0.3547, validation loss: 0.2499
2024-06-03 15:39:42 [INFO]: Epoch 026 - training loss: 0.3542, validation loss: 0.2461
2024-06-03 15:39:45 [INFO]: Epoch 027 - training loss: 0.3521, validation loss: 0.2494
2024-06-03 15:39:48 [INFO]: Epoch 028 - training loss: 0.3477, validation loss: 0.2425
2024-06-03 15:39:51 [INFO]: Epoch 029 - training loss: 0.3474, validation loss: 0.2463
2024-06-03 15:39:54 [INFO]: Epoch 030 - training loss: 0.3434, validation loss: 0.2469
2024-06-03 15:39:57 [INFO]: Epoch 031 - training loss: 0.3447, validation loss: 0.2423
2024-06-03 15:40:00 [INFO]: Epoch 032 - training loss: 0.3428, validation loss: 0.2445
2024-06-03 15:40:03 [INFO]: Epoch 033 - training loss: 0.3371, validation loss: 0.2453
2024-06-03 15:40:06 [INFO]: Epoch 034 - training loss: 0.3401, validation loss: 0.2406
2024-06-03 15:40:10 [INFO]: Epoch 035 - training loss: 0.3388, validation loss: 0.2424
2024-06-03 15:40:12 [INFO]: Epoch 036 - training loss: 0.3330, validation loss: 0.2399
2024-06-03 15:40:16 [INFO]: Epoch 037 - training loss: 0.3260, validation loss: 0.2389
2024-06-03 15:40:19 [INFO]: Epoch 038 - training loss: 0.3261, validation loss: 0.2426
2024-06-03 15:40:22 [INFO]: Epoch 039 - training loss: 0.3239, validation loss: 0.2408
2024-06-03 15:40:25 [INFO]: Epoch 040 - training loss: 0.3235, validation loss: 0.2395
2024-06-03 15:40:28 [INFO]: Epoch 041 - training loss: 0.3238, validation loss: 0.2412
2024-06-03 15:40:31 [INFO]: Epoch 042 - training loss: 0.3203, validation loss: 0.2341
2024-06-03 15:40:34 [INFO]: Epoch 043 - training loss: 0.3155, validation loss: 0.2335
2024-06-03 15:40:37 [INFO]: Epoch 044 - training loss: 0.3164, validation loss: 0.2378
2024-06-03 15:40:40 [INFO]: Epoch 045 - training loss: 0.3170, validation loss: 0.2320
2024-06-03 15:40:44 [INFO]: Epoch 046 - training loss: 0.3137, validation loss: 0.2392
2024-06-03 15:40:47 [INFO]: Epoch 047 - training loss: 0.3170, validation loss: 0.2307
2024-06-03 15:40:50 [INFO]: Epoch 048 - training loss: 0.3179, validation loss: 0.2344
2024-06-03 15:40:53 [INFO]: Epoch 049 - training loss: 0.3183, validation loss: 0.2312
2024-06-03 15:40:56 [INFO]: Epoch 050 - training loss: 0.3070, validation loss: 0.2292
2024-06-03 15:41:00 [INFO]: Epoch 051 - training loss: 0.3023, validation loss: 0.2263
2024-06-03 15:41:03 [INFO]: Epoch 052 - training loss: 0.3041, validation loss: 0.2345
2024-06-03 15:41:06 [INFO]: Epoch 053 - training loss: 0.3016, validation loss: 0.2381
2024-06-03 15:41:09 [INFO]: Epoch 054 - training loss: 0.2994, validation loss: 0.2267
2024-06-03 15:41:12 [INFO]: Epoch 055 - training loss: 0.2955, validation loss: 0.2277
2024-06-03 15:41:15 [INFO]: Epoch 056 - training loss: 0.2997, validation loss: 0.2300
2024-06-03 15:41:18 [INFO]: Epoch 057 - training loss: 0.2990, validation loss: 0.2309
2024-06-03 15:41:22 [INFO]: Epoch 058 - training loss: 0.2941, validation loss: 0.2279
2024-06-03 15:41:25 [INFO]: Epoch 059 - training loss: 0.3010, validation loss: 0.2314
2024-06-03 15:41:28 [INFO]: Epoch 060 - training loss: 0.2968, validation loss: 0.2302
2024-06-03 15:41:31 [INFO]: Epoch 061 - training loss: 0.2926, validation loss: 0.2295
2024-06-03 15:41:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:41:31 [INFO]: Finished training. The best model is from epoch#51.
2024-06-03 15:41:31 [INFO]: Saved the model to results_block_rate05/BeijingAir/Informer_BeijingAir/round_2/20240603_T153804/Informer.pypots
2024-06-03 15:41:33 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_2/imputation.pkl
2024-06-03 15:41:33 [INFO]: Round2 - Informer on BeijingAir: MAE=0.2215, MSE=0.2760, MRE=0.2994
2024-06-03 15:41:33 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 15:41:33 [INFO]: Using the given device: cuda:0
2024-06-03 15:41:33 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_3/20240603_T154133
2024-06-03 15:41:33 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_3/20240603_T154133/tensorboard
2024-06-03 15:41:33 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 15:41:36 [INFO]: Epoch 001 - training loss: 0.9685, validation loss: 0.4156
2024-06-03 15:41:39 [INFO]: Epoch 002 - training loss: 0.6378, validation loss: 0.3445
2024-06-03 15:41:42 [INFO]: Epoch 003 - training loss: 0.5506, validation loss: 0.3169
2024-06-03 15:41:44 [INFO]: Epoch 004 - training loss: 0.5104, validation loss: 0.3030
2024-06-03 15:41:48 [INFO]: Epoch 005 - training loss: 0.4827, validation loss: 0.2861
2024-06-03 15:41:51 [INFO]: Epoch 006 - training loss: 0.4669, validation loss: 0.2889
2024-06-03 15:41:54 [INFO]: Epoch 007 - training loss: 0.4481, validation loss: 0.2812
2024-06-03 15:41:57 [INFO]: Epoch 008 - training loss: 0.4373, validation loss: 0.2721
2024-06-03 15:42:00 [INFO]: Epoch 009 - training loss: 0.4307, validation loss: 0.2675
2024-06-03 15:42:04 [INFO]: Epoch 010 - training loss: 0.4181, validation loss: 0.2748
2024-06-03 15:42:07 [INFO]: Epoch 011 - training loss: 0.4131, validation loss: 0.2597
2024-06-03 15:42:10 [INFO]: Epoch 012 - training loss: 0.4065, validation loss: 0.2648
2024-06-03 15:42:11 [INFO]: Epoch 013 - training loss: 0.3977, validation loss: 0.2685
2024-06-03 15:42:12 [INFO]: Epoch 014 - training loss: 0.3979, validation loss: 0.2650
2024-06-03 15:42:16 [INFO]: Epoch 015 - training loss: 0.3905, validation loss: 0.2541
2024-06-03 15:42:19 [INFO]: Epoch 016 - training loss: 0.3872, validation loss: 0.2540
2024-06-03 15:42:22 [INFO]: Epoch 017 - training loss: 0.3822, validation loss: 0.2584
2024-06-03 15:42:25 [INFO]: Epoch 018 - training loss: 0.3793, validation loss: 0.2598
2024-06-03 15:42:28 [INFO]: Epoch 019 - training loss: 0.3743, validation loss: 0.2545
2024-06-03 15:42:31 [INFO]: Epoch 020 - training loss: 0.3740, validation loss: 0.2515
2024-06-03 15:42:34 [INFO]: Epoch 021 - training loss: 0.3666, validation loss: 0.2554
2024-06-03 15:42:38 [INFO]: Epoch 022 - training loss: 0.3634, validation loss: 0.2510
2024-06-03 15:42:41 [INFO]: Epoch 023 - training loss: 0.3628, validation loss: 0.2555
2024-06-03 15:42:44 [INFO]: Epoch 024 - training loss: 0.3606, validation loss: 0.2488
2024-06-03 15:42:46 [INFO]: Epoch 025 - training loss: 0.3569, validation loss: 0.2453
2024-06-03 15:42:50 [INFO]: Epoch 026 - training loss: 0.3515, validation loss: 0.2459
2024-06-03 15:42:53 [INFO]: Epoch 027 - training loss: 0.3505, validation loss: 0.2469
2024-06-03 15:42:56 [INFO]: Epoch 028 - training loss: 0.3494, validation loss: 0.2450
2024-06-03 15:42:59 [INFO]: Epoch 029 - training loss: 0.3448, validation loss: 0.2436
2024-06-03 15:43:02 [INFO]: Epoch 030 - training loss: 0.3460, validation loss: 0.2506
2024-06-03 15:43:05 [INFO]: Epoch 031 - training loss: 0.3486, validation loss: 0.2413
2024-06-03 15:43:08 [INFO]: Epoch 032 - training loss: 0.3466, validation loss: 0.2472
2024-06-03 15:43:12 [INFO]: Epoch 033 - training loss: 0.3405, validation loss: 0.2442
2024-06-03 15:43:15 [INFO]: Epoch 034 - training loss: 0.3353, validation loss: 0.2408
2024-06-03 15:43:18 [INFO]: Epoch 035 - training loss: 0.3299, validation loss: 0.2411
2024-06-03 15:43:21 [INFO]: Epoch 036 - training loss: 0.3291, validation loss: 0.2439
2024-06-03 15:43:24 [INFO]: Epoch 037 - training loss: 0.3332, validation loss: 0.2398
2024-06-03 15:43:27 [INFO]: Epoch 038 - training loss: 0.3290, validation loss: 0.2440
2024-06-03 15:43:31 [INFO]: Epoch 039 - training loss: 0.3283, validation loss: 0.2317
2024-06-03 15:43:34 [INFO]: Epoch 040 - training loss: 0.3272, validation loss: 0.2359
2024-06-03 15:43:37 [INFO]: Epoch 041 - training loss: 0.3237, validation loss: 0.2374
2024-06-03 15:43:40 [INFO]: Epoch 042 - training loss: 0.3189, validation loss: 0.2388
2024-06-03 15:43:43 [INFO]: Epoch 043 - training loss: 0.3190, validation loss: 0.2308
2024-06-03 15:43:46 [INFO]: Epoch 044 - training loss: 0.3155, validation loss: 0.2346
2024-06-03 15:43:49 [INFO]: Epoch 045 - training loss: 0.3154, validation loss: 0.2346
2024-06-03 15:43:53 [INFO]: Epoch 046 - training loss: 0.3139, validation loss: 0.2337
2024-06-03 15:43:56 [INFO]: Epoch 047 - training loss: 0.3144, validation loss: 0.2306
2024-06-03 15:43:59 [INFO]: Epoch 048 - training loss: 0.3115, validation loss: 0.2329
2024-06-03 15:44:02 [INFO]: Epoch 049 - training loss: 0.3111, validation loss: 0.2338
2024-06-03 15:44:05 [INFO]: Epoch 050 - training loss: 0.3063, validation loss: 0.2376
2024-06-03 15:44:08 [INFO]: Epoch 051 - training loss: 0.3075, validation loss: 0.2299
2024-06-03 15:44:11 [INFO]: Epoch 052 - training loss: 0.3064, validation loss: 0.2307
2024-06-03 15:44:14 [INFO]: Epoch 053 - training loss: 0.3058, validation loss: 0.2304
2024-06-03 15:44:17 [INFO]: Epoch 054 - training loss: 0.3040, validation loss: 0.2325
2024-06-03 15:44:20 [INFO]: Epoch 055 - training loss: 0.3025, validation loss: 0.2372
2024-06-03 15:44:23 [INFO]: Epoch 056 - training loss: 0.3013, validation loss: 0.2323
2024-06-03 15:44:26 [INFO]: Epoch 057 - training loss: 0.2983, validation loss: 0.2258
2024-06-03 15:44:29 [INFO]: Epoch 058 - training loss: 0.2967, validation loss: 0.2282
2024-06-03 15:44:32 [INFO]: Epoch 059 - training loss: 0.2934, validation loss: 0.2220
2024-06-03 15:44:35 [INFO]: Epoch 060 - training loss: 0.2914, validation loss: 0.2217
2024-06-03 15:44:38 [INFO]: Epoch 061 - training loss: 0.2935, validation loss: 0.2304
2024-06-03 15:44:42 [INFO]: Epoch 062 - training loss: 0.2942, validation loss: 0.2275
2024-06-03 15:44:44 [INFO]: Epoch 063 - training loss: 0.2929, validation loss: 0.2255
2024-06-03 15:44:48 [INFO]: Epoch 064 - training loss: 0.2975, validation loss: 0.2299
2024-06-03 15:44:51 [INFO]: Epoch 065 - training loss: 0.2972, validation loss: 0.2259
2024-06-03 15:44:54 [INFO]: Epoch 066 - training loss: 0.2963, validation loss: 0.2285
2024-06-03 15:44:57 [INFO]: Epoch 067 - training loss: 0.2873, validation loss: 0.2222
2024-06-03 15:45:00 [INFO]: Epoch 068 - training loss: 0.2863, validation loss: 0.2228
2024-06-03 15:45:03 [INFO]: Epoch 069 - training loss: 0.2892, validation loss: 0.2239
2024-06-03 15:45:07 [INFO]: Epoch 070 - training loss: 0.2855, validation loss: 0.2265
2024-06-03 15:45:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:45:07 [INFO]: Finished training. The best model is from epoch#60.
2024-06-03 15:45:07 [INFO]: Saved the model to results_block_rate05/BeijingAir/Informer_BeijingAir/round_3/20240603_T154133/Informer.pypots
2024-06-03 15:45:08 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_3/imputation.pkl
2024-06-03 15:45:08 [INFO]: Round3 - Informer on BeijingAir: MAE=0.2198, MSE=0.2750, MRE=0.2972
2024-06-03 15:45:08 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 15:45:08 [INFO]: Using the given device: cuda:0
2024-06-03 15:45:08 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_4/20240603_T154508
2024-06-03 15:45:08 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_4/20240603_T154508/tensorboard
2024-06-03 15:45:08 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 15:45:12 [INFO]: Epoch 001 - training loss: 0.9994, validation loss: 0.4249
2024-06-03 15:45:15 [INFO]: Epoch 002 - training loss: 0.6510, validation loss: 0.3420
2024-06-03 15:45:18 [INFO]: Epoch 003 - training loss: 0.5533, validation loss: 0.3161
2024-06-03 15:45:21 [INFO]: Epoch 004 - training loss: 0.5046, validation loss: 0.2961
2024-06-03 15:45:25 [INFO]: Epoch 005 - training loss: 0.4813, validation loss: 0.2880
2024-06-03 15:45:28 [INFO]: Epoch 006 - training loss: 0.4728, validation loss: 0.2825
2024-06-03 15:45:31 [INFO]: Epoch 007 - training loss: 0.4501, validation loss: 0.2763
2024-06-03 15:45:34 [INFO]: Epoch 008 - training loss: 0.4385, validation loss: 0.2664
2024-06-03 15:45:37 [INFO]: Epoch 009 - training loss: 0.4299, validation loss: 0.2660
2024-06-03 15:45:40 [INFO]: Epoch 010 - training loss: 0.4211, validation loss: 0.2634
2024-06-03 15:45:44 [INFO]: Epoch 011 - training loss: 0.4102, validation loss: 0.2563
2024-06-03 15:45:47 [INFO]: Epoch 012 - training loss: 0.4016, validation loss: 0.2618
2024-06-03 15:45:50 [INFO]: Epoch 013 - training loss: 0.4041, validation loss: 0.2563
2024-06-03 15:45:53 [INFO]: Epoch 014 - training loss: 0.3915, validation loss: 0.2618
2024-06-03 15:45:57 [INFO]: Epoch 015 - training loss: 0.3868, validation loss: 0.2578
2024-06-03 15:46:00 [INFO]: Epoch 016 - training loss: 0.3795, validation loss: 0.2518
2024-06-03 15:46:03 [INFO]: Epoch 017 - training loss: 0.3775, validation loss: 0.2632
2024-06-03 15:46:05 [INFO]: Epoch 018 - training loss: 0.3793, validation loss: 0.2577
2024-06-03 15:46:08 [INFO]: Epoch 019 - training loss: 0.3732, validation loss: 0.2565
2024-06-03 15:46:11 [INFO]: Epoch 020 - training loss: 0.3682, validation loss: 0.2486
2024-06-03 15:46:14 [INFO]: Epoch 021 - training loss: 0.3664, validation loss: 0.2458
2024-06-03 15:46:17 [INFO]: Epoch 022 - training loss: 0.3654, validation loss: 0.2554
2024-06-03 15:46:20 [INFO]: Epoch 023 - training loss: 0.3580, validation loss: 0.2506
2024-06-03 15:46:24 [INFO]: Epoch 024 - training loss: 0.3556, validation loss: 0.2551
2024-06-03 15:46:27 [INFO]: Epoch 025 - training loss: 0.3536, validation loss: 0.2492
2024-06-03 15:46:30 [INFO]: Epoch 026 - training loss: 0.3514, validation loss: 0.2475
2024-06-03 15:46:33 [INFO]: Epoch 027 - training loss: 0.3542, validation loss: 0.2436
2024-06-03 15:46:36 [INFO]: Epoch 028 - training loss: 0.3443, validation loss: 0.2421
2024-06-03 15:46:39 [INFO]: Epoch 029 - training loss: 0.3426, validation loss: 0.2398
2024-06-03 15:46:42 [INFO]: Epoch 030 - training loss: 0.3454, validation loss: 0.2442
2024-06-03 15:46:46 [INFO]: Epoch 031 - training loss: 0.3431, validation loss: 0.2480
2024-06-03 15:46:49 [INFO]: Epoch 032 - training loss: 0.3388, validation loss: 0.2460
2024-06-03 15:46:52 [INFO]: Epoch 033 - training loss: 0.3381, validation loss: 0.2379
2024-06-03 15:46:55 [INFO]: Epoch 034 - training loss: 0.3337, validation loss: 0.2430
2024-06-03 15:46:58 [INFO]: Epoch 035 - training loss: 0.3366, validation loss: 0.2377
2024-06-03 15:47:01 [INFO]: Epoch 036 - training loss: 0.3336, validation loss: 0.2381
2024-06-03 15:47:04 [INFO]: Epoch 037 - training loss: 0.3288, validation loss: 0.2378
2024-06-03 15:47:07 [INFO]: Epoch 038 - training loss: 0.3248, validation loss: 0.2428
2024-06-03 15:47:10 [INFO]: Epoch 039 - training loss: 0.3242, validation loss: 0.2419
2024-06-03 15:47:14 [INFO]: Epoch 040 - training loss: 0.3213, validation loss: 0.2378
2024-06-03 15:47:17 [INFO]: Epoch 041 - training loss: 0.3298, validation loss: 0.2404
2024-06-03 15:47:20 [INFO]: Epoch 042 - training loss: 0.3276, validation loss: 0.2352
2024-06-03 15:47:23 [INFO]: Epoch 043 - training loss: 0.3201, validation loss: 0.2314
2024-06-03 15:47:26 [INFO]: Epoch 044 - training loss: 0.3138, validation loss: 0.2319
2024-06-03 15:47:29 [INFO]: Epoch 045 - training loss: 0.3152, validation loss: 0.2353
2024-06-03 15:47:32 [INFO]: Epoch 046 - training loss: 0.3134, validation loss: 0.2342
2024-06-03 15:47:35 [INFO]: Epoch 047 - training loss: 0.3087, validation loss: 0.2389
2024-06-03 15:47:38 [INFO]: Epoch 048 - training loss: 0.3085, validation loss: 0.2376
2024-06-03 15:47:41 [INFO]: Epoch 049 - training loss: 0.3065, validation loss: 0.2266
2024-06-03 15:47:44 [INFO]: Epoch 050 - training loss: 0.3122, validation loss: 0.2317
2024-06-03 15:47:48 [INFO]: Epoch 051 - training loss: 0.3071, validation loss: 0.2263
2024-06-03 15:47:51 [INFO]: Epoch 052 - training loss: 0.3051, validation loss: 0.2297
2024-06-03 15:47:54 [INFO]: Epoch 053 - training loss: 0.3020, validation loss: 0.2329
2024-06-03 15:47:57 [INFO]: Epoch 054 - training loss: 0.3042, validation loss: 0.2348
2024-06-03 15:48:00 [INFO]: Epoch 055 - training loss: 0.3032, validation loss: 0.2323
2024-06-03 15:48:03 [INFO]: Epoch 056 - training loss: 0.2986, validation loss: 0.2297
2024-06-03 15:48:06 [INFO]: Epoch 057 - training loss: 0.2969, validation loss: 0.2249
2024-06-03 15:48:09 [INFO]: Epoch 058 - training loss: 0.2956, validation loss: 0.2311
2024-06-03 15:48:12 [INFO]: Epoch 059 - training loss: 0.2952, validation loss: 0.2273
2024-06-03 15:48:15 [INFO]: Epoch 060 - training loss: 0.2966, validation loss: 0.2293
2024-06-03 15:48:18 [INFO]: Epoch 061 - training loss: 0.2946, validation loss: 0.2296
2024-06-03 15:48:21 [INFO]: Epoch 062 - training loss: 0.2919, validation loss: 0.2311
2024-06-03 15:48:24 [INFO]: Epoch 063 - training loss: 0.2913, validation loss: 0.2231
2024-06-03 15:48:27 [INFO]: Epoch 064 - training loss: 0.2873, validation loss: 0.2243
2024-06-03 15:48:31 [INFO]: Epoch 065 - training loss: 0.2836, validation loss: 0.2258
2024-06-03 15:48:34 [INFO]: Epoch 066 - training loss: 0.2843, validation loss: 0.2290
2024-06-03 15:48:37 [INFO]: Epoch 067 - training loss: 0.2830, validation loss: 0.2249
2024-06-03 15:48:40 [INFO]: Epoch 068 - training loss: 0.2815, validation loss: 0.2236
2024-06-03 15:48:44 [INFO]: Epoch 069 - training loss: 0.2843, validation loss: 0.2297
2024-06-03 15:48:47 [INFO]: Epoch 070 - training loss: 0.2879, validation loss: 0.2256
2024-06-03 15:48:50 [INFO]: Epoch 071 - training loss: 0.2870, validation loss: 0.2246
2024-06-03 15:48:53 [INFO]: Epoch 072 - training loss: 0.2849, validation loss: 0.2239
2024-06-03 15:48:56 [INFO]: Epoch 073 - training loss: 0.2810, validation loss: 0.2210
2024-06-03 15:48:59 [INFO]: Epoch 074 - training loss: 0.2844, validation loss: 0.2265
2024-06-03 15:49:02 [INFO]: Epoch 075 - training loss: 0.2833, validation loss: 0.2224
2024-06-03 15:49:05 [INFO]: Epoch 076 - training loss: 0.2892, validation loss: 0.2283
2024-06-03 15:49:09 [INFO]: Epoch 077 - training loss: 0.2814, validation loss: 0.2281
2024-06-03 15:49:12 [INFO]: Epoch 078 - training loss: 0.2812, validation loss: 0.2246
2024-06-03 15:49:15 [INFO]: Epoch 079 - training loss: 0.2750, validation loss: 0.2211
2024-06-03 15:49:18 [INFO]: Epoch 080 - training loss: 0.2738, validation loss: 0.2209
2024-06-03 15:49:22 [INFO]: Epoch 081 - training loss: 0.2735, validation loss: 0.2231
2024-06-03 15:49:25 [INFO]: Epoch 082 - training loss: 0.2739, validation loss: 0.2196
2024-06-03 15:49:28 [INFO]: Epoch 083 - training loss: 0.2736, validation loss: 0.2191
2024-06-03 15:49:31 [INFO]: Epoch 084 - training loss: 0.2726, validation loss: 0.2200
2024-06-03 15:49:34 [INFO]: Epoch 085 - training loss: 0.2717, validation loss: 0.2187
2024-06-03 15:49:37 [INFO]: Epoch 086 - training loss: 0.2728, validation loss: 0.2205
2024-06-03 15:49:40 [INFO]: Epoch 087 - training loss: 0.2718, validation loss: 0.2201
2024-06-03 15:49:44 [INFO]: Epoch 088 - training loss: 0.2743, validation loss: 0.2205
2024-06-03 15:49:47 [INFO]: Epoch 089 - training loss: 0.2742, validation loss: 0.2209
2024-06-03 15:49:50 [INFO]: Epoch 090 - training loss: 0.2694, validation loss: 0.2190
2024-06-03 15:49:53 [INFO]: Epoch 091 - training loss: 0.2686, validation loss: 0.2230
2024-06-03 15:49:56 [INFO]: Epoch 092 - training loss: 0.2679, validation loss: 0.2162
2024-06-03 15:49:59 [INFO]: Epoch 093 - training loss: 0.2661, validation loss: 0.2175
2024-06-03 15:50:02 [INFO]: Epoch 094 - training loss: 0.2688, validation loss: 0.2171
2024-06-03 15:50:05 [INFO]: Epoch 095 - training loss: 0.2652, validation loss: 0.2130
2024-06-03 15:50:08 [INFO]: Epoch 096 - training loss: 0.2674, validation loss: 0.2184
2024-06-03 15:50:11 [INFO]: Epoch 097 - training loss: 0.2674, validation loss: 0.2150
2024-06-03 15:50:14 [INFO]: Epoch 098 - training loss: 0.2667, validation loss: 0.2176
2024-06-03 15:50:18 [INFO]: Epoch 099 - training loss: 0.2629, validation loss: 0.2154
2024-06-03 15:50:21 [INFO]: Epoch 100 - training loss: 0.2608, validation loss: 0.2159
2024-06-03 15:50:21 [INFO]: Finished training. The best model is from epoch#95.
2024-06-03 15:50:21 [INFO]: Saved the model to results_block_rate05/BeijingAir/Informer_BeijingAir/round_4/20240603_T154508/Informer.pypots
2024-06-03 15:50:23 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Informer_BeijingAir/round_4/imputation.pkl
2024-06-03 15:50:23 [INFO]: Round4 - Informer on BeijingAir: MAE=0.2142, MSE=0.2618, MRE=0.2895
2024-06-03 15:50:23 [INFO]: Done! Final results:
Averaged Informer (6,706,308 params) on BeijingAir: MAE=0.2077 ± 0.0029382579408739054, MSE=0.2569 ± 0.0070217789848938976, MRE=0.2735 ± 0.003867547934349519, average inference time=0.59