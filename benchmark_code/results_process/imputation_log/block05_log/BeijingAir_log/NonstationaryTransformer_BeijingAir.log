2024-06-03 15:13:43 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 15:13:43 [INFO]: Using the given device: cuda:0
2024-06-03 15:13:43 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240603_T151343
2024-06-03 15:13:43 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240603_T151343/tensorboard
2024-06-03 15:13:44 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 15:14:01 [INFO]: Epoch 001 - training loss: 0.6287, validation loss: 0.4302
2024-06-03 15:14:07 [INFO]: Epoch 002 - training loss: 0.5537, validation loss: 0.4053
2024-06-03 15:14:12 [INFO]: Epoch 003 - training loss: 0.5395, validation loss: 0.3993
2024-06-03 15:14:18 [INFO]: Epoch 004 - training loss: 0.5345, validation loss: 0.3991
2024-06-03 15:14:24 [INFO]: Epoch 005 - training loss: 0.5285, validation loss: 0.3955
2024-06-03 15:14:30 [INFO]: Epoch 006 - training loss: 0.5232, validation loss: 0.3926
2024-06-03 15:14:36 [INFO]: Epoch 007 - training loss: 0.5174, validation loss: 0.3895
2024-06-03 15:14:42 [INFO]: Epoch 008 - training loss: 0.5133, validation loss: 0.3938
2024-06-03 15:14:48 [INFO]: Epoch 009 - training loss: 0.5100, validation loss: 0.3938
2024-06-03 15:14:53 [INFO]: Epoch 010 - training loss: 0.5076, validation loss: 0.3925
2024-06-03 15:14:59 [INFO]: Epoch 011 - training loss: 0.5044, validation loss: 0.3909
2024-06-03 15:15:05 [INFO]: Epoch 012 - training loss: 0.5030, validation loss: 0.3885
2024-06-03 15:15:11 [INFO]: Epoch 013 - training loss: 0.4985, validation loss: 0.3912
2024-06-03 15:15:16 [INFO]: Epoch 014 - training loss: 0.4976, validation loss: 0.3899
2024-06-03 15:15:22 [INFO]: Epoch 015 - training loss: 0.4963, validation loss: 0.3882
2024-06-03 15:15:27 [INFO]: Epoch 016 - training loss: 0.4940, validation loss: 0.3848
2024-06-03 15:15:33 [INFO]: Epoch 017 - training loss: 0.4922, validation loss: 0.3878
2024-06-03 15:15:38 [INFO]: Epoch 018 - training loss: 0.4883, validation loss: 0.3855
2024-06-03 15:15:44 [INFO]: Epoch 019 - training loss: 0.4869, validation loss: 0.3892
2024-06-03 15:15:50 [INFO]: Epoch 020 - training loss: 0.4860, validation loss: 0.3937
2024-06-03 15:15:56 [INFO]: Epoch 021 - training loss: 0.4839, validation loss: 0.3879
2024-06-03 15:16:01 [INFO]: Epoch 022 - training loss: 0.4819, validation loss: 0.3897
2024-06-03 15:16:07 [INFO]: Epoch 023 - training loss: 0.4803, validation loss: 0.3890
2024-06-03 15:16:13 [INFO]: Epoch 024 - training loss: 0.4795, validation loss: 0.3855
2024-06-03 15:16:19 [INFO]: Epoch 025 - training loss: 0.4770, validation loss: 0.3813
2024-06-03 15:16:24 [INFO]: Epoch 026 - training loss: 0.4766, validation loss: 0.3822
2024-06-03 15:16:30 [INFO]: Epoch 027 - training loss: 0.4759, validation loss: 0.3842
2024-06-03 15:16:36 [INFO]: Epoch 028 - training loss: 0.4727, validation loss: 0.3826
2024-06-03 15:16:41 [INFO]: Epoch 029 - training loss: 0.4726, validation loss: 0.3826
2024-06-03 15:16:47 [INFO]: Epoch 030 - training loss: 0.4699, validation loss: 0.3804
2024-06-03 15:16:53 [INFO]: Epoch 031 - training loss: 0.4698, validation loss: 0.3784
2024-06-03 15:16:59 [INFO]: Epoch 032 - training loss: 0.4687, validation loss: 0.3835
2024-06-03 15:17:04 [INFO]: Epoch 033 - training loss: 0.4660, validation loss: 0.3769
2024-06-03 15:17:10 [INFO]: Epoch 034 - training loss: 0.4649, validation loss: 0.3793
2024-06-03 15:17:16 [INFO]: Epoch 035 - training loss: 0.4635, validation loss: 0.3793
2024-06-03 15:17:22 [INFO]: Epoch 036 - training loss: 0.4635, validation loss: 0.3783
2024-06-03 15:17:27 [INFO]: Epoch 037 - training loss: 0.4627, validation loss: 0.3830
2024-06-03 15:17:33 [INFO]: Epoch 038 - training loss: 0.4603, validation loss: 0.3800
2024-06-03 15:17:38 [INFO]: Epoch 039 - training loss: 0.4612, validation loss: 0.3825
2024-06-03 15:17:44 [INFO]: Epoch 040 - training loss: 0.4598, validation loss: 0.3788
2024-06-03 15:17:50 [INFO]: Epoch 041 - training loss: 0.4582, validation loss: 0.3766
2024-06-03 15:17:56 [INFO]: Epoch 042 - training loss: 0.4558, validation loss: 0.3780
2024-06-03 15:18:01 [INFO]: Epoch 043 - training loss: 0.4549, validation loss: 0.3779
2024-06-03 15:18:07 [INFO]: Epoch 044 - training loss: 0.4545, validation loss: 0.3844
2024-06-03 15:18:13 [INFO]: Epoch 045 - training loss: 0.4543, validation loss: 0.3818
2024-06-03 15:18:18 [INFO]: Epoch 046 - training loss: 0.4535, validation loss: 0.3784
2024-06-03 15:18:24 [INFO]: Epoch 047 - training loss: 0.4536, validation loss: 0.3771
2024-06-03 15:18:29 [INFO]: Epoch 048 - training loss: 0.4537, validation loss: 0.3795
2024-06-03 15:18:35 [INFO]: Epoch 049 - training loss: 0.4502, validation loss: 0.3761
2024-06-03 15:18:40 [INFO]: Epoch 050 - training loss: 0.4505, validation loss: 0.3770
2024-06-03 15:18:46 [INFO]: Epoch 051 - training loss: 0.4474, validation loss: 0.3758
2024-06-03 15:18:52 [INFO]: Epoch 052 - training loss: 0.4491, validation loss: 0.3809
2024-06-03 15:18:58 [INFO]: Epoch 053 - training loss: 0.4476, validation loss: 0.3801
2024-06-03 15:19:04 [INFO]: Epoch 054 - training loss: 0.4473, validation loss: 0.3778
2024-06-03 15:19:10 [INFO]: Epoch 055 - training loss: 0.4479, validation loss: 0.3750
2024-06-03 15:19:16 [INFO]: Epoch 056 - training loss: 0.4474, validation loss: 0.3771
2024-06-03 15:19:21 [INFO]: Epoch 057 - training loss: 0.4466, validation loss: 0.3755
2024-06-03 15:19:27 [INFO]: Epoch 058 - training loss: 0.4454, validation loss: 0.3768
2024-06-03 15:19:33 [INFO]: Epoch 059 - training loss: 0.4456, validation loss: 0.3777
2024-06-03 15:19:39 [INFO]: Epoch 060 - training loss: 0.4436, validation loss: 0.3789
2024-06-03 15:19:44 [INFO]: Epoch 061 - training loss: 0.4436, validation loss: 0.3779
2024-06-03 15:19:50 [INFO]: Epoch 062 - training loss: 0.4417, validation loss: 0.3756
2024-06-03 15:19:55 [INFO]: Epoch 063 - training loss: 0.4401, validation loss: 0.3767
2024-06-03 15:20:00 [INFO]: Epoch 064 - training loss: 0.4401, validation loss: 0.3801
2024-06-03 15:20:06 [INFO]: Epoch 065 - training loss: 0.4411, validation loss: 0.3780
2024-06-03 15:20:06 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:20:06 [INFO]: Finished training. The best model is from epoch#55.
2024-06-03 15:20:06 [INFO]: Saved the model to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240603_T151343/NonstationaryTransformer.pypots
2024-06-03 15:20:09 [INFO]: Successfully saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/imputation.pkl
2024-06-03 15:20:09 [INFO]: Round0 - NonstationaryTransformer on BeijingAir: MAE=0.3094, MSE=0.3932, MRE=0.4182
2024-06-03 15:20:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 15:20:09 [INFO]: Using the given device: cuda:0
2024-06-03 15:20:09 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240603_T152009
2024-06-03 15:20:09 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240603_T152009/tensorboard
2024-06-03 15:20:09 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 15:20:15 [INFO]: Epoch 001 - training loss: 0.6273, validation loss: 0.4318
2024-06-03 15:20:20 [INFO]: Epoch 002 - training loss: 0.5583, validation loss: 0.4104
2024-06-03 15:20:26 [INFO]: Epoch 003 - training loss: 0.5418, validation loss: 0.4068
2024-06-03 15:20:32 [INFO]: Epoch 004 - training loss: 0.5312, validation loss: 0.4103
2024-06-03 15:20:38 [INFO]: Epoch 005 - training loss: 0.5274, validation loss: 0.4014
2024-06-03 15:20:43 [INFO]: Epoch 006 - training loss: 0.5207, validation loss: 0.3948
2024-06-03 15:20:49 [INFO]: Epoch 007 - training loss: 0.5159, validation loss: 0.3971
2024-06-03 15:20:54 [INFO]: Epoch 008 - training loss: 0.5139, validation loss: 0.3921
2024-06-03 15:21:00 [INFO]: Epoch 009 - training loss: 0.5106, validation loss: 0.3912
2024-06-03 15:21:06 [INFO]: Epoch 010 - training loss: 0.5098, validation loss: 0.3906
2024-06-03 15:21:12 [INFO]: Epoch 011 - training loss: 0.5036, validation loss: 0.3876
2024-06-03 15:21:18 [INFO]: Epoch 012 - training loss: 0.5010, validation loss: 0.3901
2024-06-03 15:21:23 [INFO]: Epoch 013 - training loss: 0.4990, validation loss: 0.3926
2024-06-03 15:21:29 [INFO]: Epoch 014 - training loss: 0.4950, validation loss: 0.3901
2024-06-03 15:21:34 [INFO]: Epoch 015 - training loss: 0.4937, validation loss: 0.3912
2024-06-03 15:21:40 [INFO]: Epoch 016 - training loss: 0.4947, validation loss: 0.3895
2024-06-03 15:21:45 [INFO]: Epoch 017 - training loss: 0.4892, validation loss: 0.3902
2024-06-03 15:21:51 [INFO]: Epoch 018 - training loss: 0.4888, validation loss: 0.3889
2024-06-03 15:21:57 [INFO]: Epoch 019 - training loss: 0.4859, validation loss: 0.3958
2024-06-03 15:22:02 [INFO]: Epoch 020 - training loss: 0.4872, validation loss: 0.3863
2024-06-03 15:22:08 [INFO]: Epoch 021 - training loss: 0.4822, validation loss: 0.3839
2024-06-03 15:22:14 [INFO]: Epoch 022 - training loss: 0.4820, validation loss: 0.3910
2024-06-03 15:22:20 [INFO]: Epoch 023 - training loss: 0.4800, validation loss: 0.3812
2024-06-03 15:22:25 [INFO]: Epoch 024 - training loss: 0.4781, validation loss: 0.3815
2024-06-03 15:22:31 [INFO]: Epoch 025 - training loss: 0.4766, validation loss: 0.3816
2024-06-03 15:22:37 [INFO]: Epoch 026 - training loss: 0.4753, validation loss: 0.3825
2024-06-03 15:22:42 [INFO]: Epoch 027 - training loss: 0.4738, validation loss: 0.3814
2024-06-03 15:22:47 [INFO]: Epoch 028 - training loss: 0.4731, validation loss: 0.3820
2024-06-03 15:22:52 [INFO]: Epoch 029 - training loss: 0.4717, validation loss: 0.3836
2024-06-03 15:22:57 [INFO]: Epoch 030 - training loss: 0.4701, validation loss: 0.3809
2024-06-03 15:23:02 [INFO]: Epoch 031 - training loss: 0.4698, validation loss: 0.3779
2024-06-03 15:23:07 [INFO]: Epoch 032 - training loss: 0.4680, validation loss: 0.3819
2024-06-03 15:23:12 [INFO]: Epoch 033 - training loss: 0.4653, validation loss: 0.3816
2024-06-03 15:23:17 [INFO]: Epoch 034 - training loss: 0.4644, validation loss: 0.3799
2024-06-03 15:23:21 [INFO]: Epoch 035 - training loss: 0.4642, validation loss: 0.3793
2024-06-03 15:23:26 [INFO]: Epoch 036 - training loss: 0.4624, validation loss: 0.3800
2024-06-03 15:23:31 [INFO]: Epoch 037 - training loss: 0.4591, validation loss: 0.3763
2024-06-03 15:23:36 [INFO]: Epoch 038 - training loss: 0.4618, validation loss: 0.3772
2024-06-03 15:23:41 [INFO]: Epoch 039 - training loss: 0.4607, validation loss: 0.3790
2024-06-03 15:23:46 [INFO]: Epoch 040 - training loss: 0.4596, validation loss: 0.3804
2024-06-03 15:23:51 [INFO]: Epoch 041 - training loss: 0.4586, validation loss: 0.3753
2024-06-03 15:23:56 [INFO]: Epoch 042 - training loss: 0.4548, validation loss: 0.3763
2024-06-03 15:24:02 [INFO]: Epoch 043 - training loss: 0.4563, validation loss: 0.3779
2024-06-03 15:24:07 [INFO]: Epoch 044 - training loss: 0.4544, validation loss: 0.3767
2024-06-03 15:24:12 [INFO]: Epoch 045 - training loss: 0.4550, validation loss: 0.3784
2024-06-03 15:24:17 [INFO]: Epoch 046 - training loss: 0.4530, validation loss: 0.3779
2024-06-03 15:24:22 [INFO]: Epoch 047 - training loss: 0.4517, validation loss: 0.3804
2024-06-03 15:24:27 [INFO]: Epoch 048 - training loss: 0.4509, validation loss: 0.3758
2024-06-03 15:24:31 [INFO]: Epoch 049 - training loss: 0.4495, validation loss: 0.3776
2024-06-03 15:24:37 [INFO]: Epoch 050 - training loss: 0.4505, validation loss: 0.3752
2024-06-03 15:24:43 [INFO]: Epoch 051 - training loss: 0.4489, validation loss: 0.3745
2024-06-03 15:24:47 [INFO]: Epoch 052 - training loss: 0.4496, validation loss: 0.3828
2024-06-03 15:24:52 [INFO]: Epoch 053 - training loss: 0.4503, validation loss: 0.3793
2024-06-03 15:24:57 [INFO]: Epoch 054 - training loss: 0.4477, validation loss: 0.3770
2024-06-03 15:25:03 [INFO]: Epoch 055 - training loss: 0.4482, validation loss: 0.3750
2024-06-03 15:25:08 [INFO]: Epoch 056 - training loss: 0.4458, validation loss: 0.3787
2024-06-03 15:25:13 [INFO]: Epoch 057 - training loss: 0.4453, validation loss: 0.3768
2024-06-03 15:25:18 [INFO]: Epoch 058 - training loss: 0.4449, validation loss: 0.3758
2024-06-03 15:25:23 [INFO]: Epoch 059 - training loss: 0.4432, validation loss: 0.3789
2024-06-03 15:25:28 [INFO]: Epoch 060 - training loss: 0.4434, validation loss: 0.3777
2024-06-03 15:25:33 [INFO]: Epoch 061 - training loss: 0.4436, validation loss: 0.3746
2024-06-03 15:25:33 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:25:33 [INFO]: Finished training. The best model is from epoch#51.
2024-06-03 15:25:33 [INFO]: Saved the model to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240603_T152009/NonstationaryTransformer.pypots
2024-06-03 15:25:35 [INFO]: Successfully saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/imputation.pkl
2024-06-03 15:25:35 [INFO]: Round1 - NonstationaryTransformer on BeijingAir: MAE=0.3088, MSE=0.3908, MRE=0.4175
2024-06-03 15:25:35 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 15:25:35 [INFO]: Using the given device: cuda:0
2024-06-03 15:25:35 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240603_T152535
2024-06-03 15:25:35 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240603_T152535/tensorboard
2024-06-03 15:25:36 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 15:25:41 [INFO]: Epoch 001 - training loss: 0.6248, validation loss: 0.4204
2024-06-03 15:25:46 [INFO]: Epoch 002 - training loss: 0.5541, validation loss: 0.4051
2024-06-03 15:25:51 [INFO]: Epoch 003 - training loss: 0.5402, validation loss: 0.3960
2024-06-03 15:25:56 [INFO]: Epoch 004 - training loss: 0.5332, validation loss: 0.3958
2024-06-03 15:26:01 [INFO]: Epoch 005 - training loss: 0.5254, validation loss: 0.3936
2024-06-03 15:26:06 [INFO]: Epoch 006 - training loss: 0.5205, validation loss: 0.3966
2024-06-03 15:26:11 [INFO]: Epoch 007 - training loss: 0.5189, validation loss: 0.3890
2024-06-03 15:26:16 [INFO]: Epoch 008 - training loss: 0.5155, validation loss: 0.3902
2024-06-03 15:26:21 [INFO]: Epoch 009 - training loss: 0.5103, validation loss: 0.3921
2024-06-03 15:26:26 [INFO]: Epoch 010 - training loss: 0.5080, validation loss: 0.3913
2024-06-03 15:26:31 [INFO]: Epoch 011 - training loss: 0.5033, validation loss: 0.3897
2024-06-03 15:26:36 [INFO]: Epoch 012 - training loss: 0.5011, validation loss: 0.3889
2024-06-03 15:26:41 [INFO]: Epoch 013 - training loss: 0.4987, validation loss: 0.3889
2024-06-03 15:26:46 [INFO]: Epoch 014 - training loss: 0.4960, validation loss: 0.3871
2024-06-03 15:26:50 [INFO]: Epoch 015 - training loss: 0.4929, validation loss: 0.3875
2024-06-03 15:26:55 [INFO]: Epoch 016 - training loss: 0.4948, validation loss: 0.3899
2024-06-03 15:27:00 [INFO]: Epoch 017 - training loss: 0.4915, validation loss: 0.3861
2024-06-03 15:27:04 [INFO]: Epoch 018 - training loss: 0.4885, validation loss: 0.3827
2024-06-03 15:27:09 [INFO]: Epoch 019 - training loss: 0.4885, validation loss: 0.3839
2024-06-03 15:27:14 [INFO]: Epoch 020 - training loss: 0.4873, validation loss: 0.3874
2024-06-03 15:27:19 [INFO]: Epoch 021 - training loss: 0.4847, validation loss: 0.3849
2024-06-03 15:27:24 [INFO]: Epoch 022 - training loss: 0.4826, validation loss: 0.3850
2024-06-03 15:27:29 [INFO]: Epoch 023 - training loss: 0.4798, validation loss: 0.3855
2024-06-03 15:27:34 [INFO]: Epoch 024 - training loss: 0.4807, validation loss: 0.3842
2024-06-03 15:27:39 [INFO]: Epoch 025 - training loss: 0.4781, validation loss: 0.3819
2024-06-03 15:27:44 [INFO]: Epoch 026 - training loss: 0.4747, validation loss: 0.3839
2024-06-03 15:27:49 [INFO]: Epoch 027 - training loss: 0.4747, validation loss: 0.3827
2024-06-03 15:27:54 [INFO]: Epoch 028 - training loss: 0.4735, validation loss: 0.3834
2024-06-03 15:27:59 [INFO]: Epoch 029 - training loss: 0.4713, validation loss: 0.3816
2024-06-03 15:28:04 [INFO]: Epoch 030 - training loss: 0.4701, validation loss: 0.3842
2024-06-03 15:28:09 [INFO]: Epoch 031 - training loss: 0.4694, validation loss: 0.3818
2024-06-03 15:28:14 [INFO]: Epoch 032 - training loss: 0.4676, validation loss: 0.3792
2024-06-03 15:28:19 [INFO]: Epoch 033 - training loss: 0.4653, validation loss: 0.3815
2024-06-03 15:28:24 [INFO]: Epoch 034 - training loss: 0.4674, validation loss: 0.3793
2024-06-03 15:28:29 [INFO]: Epoch 035 - training loss: 0.4641, validation loss: 0.3806
2024-06-03 15:28:34 [INFO]: Epoch 036 - training loss: 0.4639, validation loss: 0.3823
2024-06-03 15:28:40 [INFO]: Epoch 037 - training loss: 0.4616, validation loss: 0.3765
2024-06-03 15:28:45 [INFO]: Epoch 038 - training loss: 0.4597, validation loss: 0.3806
2024-06-03 15:28:49 [INFO]: Epoch 039 - training loss: 0.4620, validation loss: 0.3750
2024-06-03 15:28:55 [INFO]: Epoch 040 - training loss: 0.4593, validation loss: 0.3799
2024-06-03 15:29:00 [INFO]: Epoch 041 - training loss: 0.4587, validation loss: 0.3745
2024-06-03 15:29:05 [INFO]: Epoch 042 - training loss: 0.4558, validation loss: 0.3775
2024-06-03 15:29:09 [INFO]: Epoch 043 - training loss: 0.4562, validation loss: 0.3764
2024-06-03 15:29:14 [INFO]: Epoch 044 - training loss: 0.4556, validation loss: 0.3780
2024-06-03 15:29:19 [INFO]: Epoch 045 - training loss: 0.4541, validation loss: 0.3800
2024-06-03 15:29:24 [INFO]: Epoch 046 - training loss: 0.4542, validation loss: 0.3776
2024-06-03 15:29:29 [INFO]: Epoch 047 - training loss: 0.4532, validation loss: 0.3777
2024-06-03 15:29:34 [INFO]: Epoch 048 - training loss: 0.4518, validation loss: 0.3799
2024-06-03 15:29:39 [INFO]: Epoch 049 - training loss: 0.4514, validation loss: 0.3823
2024-06-03 15:29:44 [INFO]: Epoch 050 - training loss: 0.4498, validation loss: 0.3780
2024-06-03 15:29:50 [INFO]: Epoch 051 - training loss: 0.4503, validation loss: 0.3761
2024-06-03 15:29:50 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:29:50 [INFO]: Finished training. The best model is from epoch#41.
2024-06-03 15:29:50 [INFO]: Saved the model to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240603_T152535/NonstationaryTransformer.pypots
2024-06-03 15:29:52 [INFO]: Successfully saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/imputation.pkl
2024-06-03 15:29:52 [INFO]: Round2 - NonstationaryTransformer on BeijingAir: MAE=0.3102, MSE=0.3913, MRE=0.4193
2024-06-03 15:29:52 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 15:29:52 [INFO]: Using the given device: cuda:0
2024-06-03 15:29:52 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240603_T152952
2024-06-03 15:29:52 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240603_T152952/tensorboard
2024-06-03 15:29:52 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 15:29:57 [INFO]: Epoch 001 - training loss: 0.6236, validation loss: 0.4349
2024-06-03 15:30:02 [INFO]: Epoch 002 - training loss: 0.5533, validation loss: 0.4212
2024-06-03 15:30:07 [INFO]: Epoch 003 - training loss: 0.5385, validation loss: 0.4081
2024-06-03 15:30:12 [INFO]: Epoch 004 - training loss: 0.5323, validation loss: 0.4071
2024-06-03 15:30:16 [INFO]: Epoch 005 - training loss: 0.5264, validation loss: 0.4095
2024-06-03 15:30:21 [INFO]: Epoch 006 - training loss: 0.5212, validation loss: 0.4055
2024-06-03 15:30:26 [INFO]: Epoch 007 - training loss: 0.5166, validation loss: 0.4080
2024-06-03 15:30:31 [INFO]: Epoch 008 - training loss: 0.5127, validation loss: 0.4024
2024-06-03 15:30:36 [INFO]: Epoch 009 - training loss: 0.5109, validation loss: 0.4022
2024-06-03 15:30:41 [INFO]: Epoch 010 - training loss: 0.5084, validation loss: 0.4015
2024-06-03 15:30:46 [INFO]: Epoch 011 - training loss: 0.5039, validation loss: 0.4004
2024-06-03 15:30:51 [INFO]: Epoch 012 - training loss: 0.5015, validation loss: 0.4032
2024-06-03 15:30:56 [INFO]: Epoch 013 - training loss: 0.5003, validation loss: 0.4040
2024-06-03 15:31:00 [INFO]: Epoch 014 - training loss: 0.4976, validation loss: 0.4093
2024-06-03 15:31:05 [INFO]: Epoch 015 - training loss: 0.4973, validation loss: 0.4041
2024-06-03 15:31:10 [INFO]: Epoch 016 - training loss: 0.4922, validation loss: 0.4031
2024-06-03 15:31:15 [INFO]: Epoch 017 - training loss: 0.4904, validation loss: 0.3982
2024-06-03 15:31:20 [INFO]: Epoch 018 - training loss: 0.4882, validation loss: 0.3938
2024-06-03 15:31:25 [INFO]: Epoch 019 - training loss: 0.4870, validation loss: 0.3915
2024-06-03 15:31:30 [INFO]: Epoch 020 - training loss: 0.4851, validation loss: 0.3955
2024-06-03 15:31:35 [INFO]: Epoch 021 - training loss: 0.4849, validation loss: 0.3916
2024-06-03 15:31:40 [INFO]: Epoch 022 - training loss: 0.4828, validation loss: 0.3932
2024-06-03 15:31:45 [INFO]: Epoch 023 - training loss: 0.4795, validation loss: 0.3981
2024-06-03 15:31:50 [INFO]: Epoch 024 - training loss: 0.4787, validation loss: 0.3956
2024-06-03 15:31:55 [INFO]: Epoch 025 - training loss: 0.4762, validation loss: 0.3873
2024-06-03 15:32:00 [INFO]: Epoch 026 - training loss: 0.4756, validation loss: 0.3862
2024-06-03 15:32:05 [INFO]: Epoch 027 - training loss: 0.4770, validation loss: 0.3907
2024-06-03 15:32:09 [INFO]: Epoch 028 - training loss: 0.4735, validation loss: 0.3924
2024-06-03 15:32:14 [INFO]: Epoch 029 - training loss: 0.4712, validation loss: 0.3865
2024-06-03 15:32:19 [INFO]: Epoch 030 - training loss: 0.4709, validation loss: 0.3912
2024-06-03 15:32:25 [INFO]: Epoch 031 - training loss: 0.4688, validation loss: 0.3891
2024-06-03 15:32:29 [INFO]: Epoch 032 - training loss: 0.4682, validation loss: 0.3881
2024-06-03 15:32:34 [INFO]: Epoch 033 - training loss: 0.4675, validation loss: 0.3892
2024-06-03 15:32:40 [INFO]: Epoch 034 - training loss: 0.4668, validation loss: 0.3992
2024-06-03 15:32:44 [INFO]: Epoch 035 - training loss: 0.4635, validation loss: 0.3892
2024-06-03 15:32:49 [INFO]: Epoch 036 - training loss: 0.4632, validation loss: 0.3893
2024-06-03 15:32:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:32:49 [INFO]: Finished training. The best model is from epoch#26.
2024-06-03 15:32:49 [INFO]: Saved the model to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240603_T152952/NonstationaryTransformer.pypots
2024-06-03 15:32:51 [INFO]: Successfully saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/imputation.pkl
2024-06-03 15:32:51 [INFO]: Round3 - NonstationaryTransformer on BeijingAir: MAE=0.3118, MSE=0.4056, MRE=0.4214
2024-06-03 15:32:51 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 15:32:51 [INFO]: Using the given device: cuda:0
2024-06-03 15:32:51 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240603_T153251
2024-06-03 15:32:51 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240603_T153251/tensorboard
2024-06-03 15:32:51 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 15:32:56 [INFO]: Epoch 001 - training loss: 0.6264, validation loss: 0.4183
2024-06-03 15:33:01 [INFO]: Epoch 002 - training loss: 0.5561, validation loss: 0.4083
2024-06-03 15:33:06 [INFO]: Epoch 003 - training loss: 0.5396, validation loss: 0.3955
2024-06-03 15:33:11 [INFO]: Epoch 004 - training loss: 0.5341, validation loss: 0.4033
2024-06-03 15:33:15 [INFO]: Epoch 005 - training loss: 0.5252, validation loss: 0.4003
2024-06-03 15:33:20 [INFO]: Epoch 006 - training loss: 0.5213, validation loss: 0.3940
2024-06-03 15:33:25 [INFO]: Epoch 007 - training loss: 0.5186, validation loss: 0.3894
2024-06-03 15:33:29 [INFO]: Epoch 008 - training loss: 0.5128, validation loss: 0.3888
2024-06-03 15:33:34 [INFO]: Epoch 009 - training loss: 0.5098, validation loss: 0.3889
2024-06-03 15:33:39 [INFO]: Epoch 010 - training loss: 0.5073, validation loss: 0.3840
2024-06-03 15:33:43 [INFO]: Epoch 011 - training loss: 0.5043, validation loss: 0.3823
2024-06-03 15:33:48 [INFO]: Epoch 012 - training loss: 0.5012, validation loss: 0.3866
2024-06-03 15:33:52 [INFO]: Epoch 013 - training loss: 0.4999, validation loss: 0.3826
2024-06-03 15:33:57 [INFO]: Epoch 014 - training loss: 0.4962, validation loss: 0.3823
2024-06-03 15:34:02 [INFO]: Epoch 015 - training loss: 0.4956, validation loss: 0.3857
2024-06-03 15:34:06 [INFO]: Epoch 016 - training loss: 0.4936, validation loss: 0.3836
2024-06-03 15:34:11 [INFO]: Epoch 017 - training loss: 0.4921, validation loss: 0.3834
2024-06-03 15:34:15 [INFO]: Epoch 018 - training loss: 0.4888, validation loss: 0.3782
2024-06-03 15:34:20 [INFO]: Epoch 019 - training loss: 0.4876, validation loss: 0.3766
2024-06-03 15:34:24 [INFO]: Epoch 020 - training loss: 0.4869, validation loss: 0.3831
2024-06-03 15:34:29 [INFO]: Epoch 021 - training loss: 0.4838, validation loss: 0.3768
2024-06-03 15:34:33 [INFO]: Epoch 022 - training loss: 0.4801, validation loss: 0.3813
2024-06-03 15:34:37 [INFO]: Epoch 023 - training loss: 0.4796, validation loss: 0.3785
2024-06-03 15:34:41 [INFO]: Epoch 024 - training loss: 0.4783, validation loss: 0.3790
2024-06-03 15:34:45 [INFO]: Epoch 025 - training loss: 0.4754, validation loss: 0.3787
2024-06-03 15:34:49 [INFO]: Epoch 026 - training loss: 0.4753, validation loss: 0.3750
2024-06-03 15:34:52 [INFO]: Epoch 027 - training loss: 0.4756, validation loss: 0.3738
2024-06-03 15:34:56 [INFO]: Epoch 028 - training loss: 0.4733, validation loss: 0.3738
2024-06-03 15:35:00 [INFO]: Epoch 029 - training loss: 0.4701, validation loss: 0.3762
2024-06-03 15:35:03 [INFO]: Epoch 030 - training loss: 0.4710, validation loss: 0.3722
2024-06-03 15:35:07 [INFO]: Epoch 031 - training loss: 0.4673, validation loss: 0.3734
2024-06-03 15:35:11 [INFO]: Epoch 032 - training loss: 0.4683, validation loss: 0.3727
2024-06-03 15:35:14 [INFO]: Epoch 033 - training loss: 0.4673, validation loss: 0.3754
2024-06-03 15:35:18 [INFO]: Epoch 034 - training loss: 0.4654, validation loss: 0.3749
2024-06-03 15:35:22 [INFO]: Epoch 035 - training loss: 0.4667, validation loss: 0.3720
2024-06-03 15:35:26 [INFO]: Epoch 036 - training loss: 0.4633, validation loss: 0.3721
2024-06-03 15:35:29 [INFO]: Epoch 037 - training loss: 0.4617, validation loss: 0.3716
2024-06-03 15:35:33 [INFO]: Epoch 038 - training loss: 0.4604, validation loss: 0.3735
2024-06-03 15:35:36 [INFO]: Epoch 039 - training loss: 0.4608, validation loss: 0.3718
2024-06-03 15:35:40 [INFO]: Epoch 040 - training loss: 0.4570, validation loss: 0.3731
2024-06-03 15:35:44 [INFO]: Epoch 041 - training loss: 0.4567, validation loss: 0.3748
2024-06-03 15:35:47 [INFO]: Epoch 042 - training loss: 0.4580, validation loss: 0.3717
2024-06-03 15:35:51 [INFO]: Epoch 043 - training loss: 0.4575, validation loss: 0.3721
2024-06-03 15:35:55 [INFO]: Epoch 044 - training loss: 0.4554, validation loss: 0.3707
2024-06-03 15:35:58 [INFO]: Epoch 045 - training loss: 0.4549, validation loss: 0.3690
2024-06-03 15:36:02 [INFO]: Epoch 046 - training loss: 0.4541, validation loss: 0.3698
2024-06-03 15:36:06 [INFO]: Epoch 047 - training loss: 0.4545, validation loss: 0.3711
2024-06-03 15:36:09 [INFO]: Epoch 048 - training loss: 0.4516, validation loss: 0.3733
2024-06-03 15:36:13 [INFO]: Epoch 049 - training loss: 0.4499, validation loss: 0.3729
2024-06-03 15:36:17 [INFO]: Epoch 050 - training loss: 0.4512, validation loss: 0.3715
2024-06-03 15:36:20 [INFO]: Epoch 051 - training loss: 0.4503, validation loss: 0.3729
2024-06-03 15:36:24 [INFO]: Epoch 052 - training loss: 0.4497, validation loss: 0.3721
2024-06-03 15:36:28 [INFO]: Epoch 053 - training loss: 0.4474, validation loss: 0.3738
2024-06-03 15:36:31 [INFO]: Epoch 054 - training loss: 0.4471, validation loss: 0.3733
2024-06-03 15:36:35 [INFO]: Epoch 055 - training loss: 0.4456, validation loss: 0.3697
2024-06-03 15:36:35 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 15:36:35 [INFO]: Finished training. The best model is from epoch#45.
2024-06-03 15:36:35 [INFO]: Saved the model to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240603_T153251/NonstationaryTransformer.pypots
2024-06-03 15:36:37 [INFO]: Successfully saved to results_block_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/imputation.pkl
2024-06-03 15:36:37 [INFO]: Round4 - NonstationaryTransformer on BeijingAir: MAE=0.3088, MSE=0.3843, MRE=0.4175
2024-06-03 15:36:37 [INFO]: Done! Final results:
Averaged NonstationaryTransformer (6,978,068 params) on BeijingAir: MAE=0.2986 ± 0.0011743908806461294, MSE=0.3739 ± 0.007047944731412328, MRE=0.3930 ± 0.0015458183440528596, average inference time=0.36