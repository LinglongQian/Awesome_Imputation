2024-06-03 13:05:13 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 13:05:13 [INFO]: Using the given device: cuda:0
2024-06-03 13:05:13 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_0/20240603_T130513
2024-06-03 13:05:13 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_0/20240603_T130513/tensorboard
2024-06-03 13:05:13 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 244,833
2024-06-03 13:06:45 [INFO]: Epoch 001 - training loss: 0.5586, validation loss: 0.3688
2024-06-03 13:08:05 [INFO]: Epoch 002 - training loss: 0.3768, validation loss: 0.3311
2024-06-03 13:09:25 [INFO]: Epoch 003 - training loss: 0.3153, validation loss: 0.3270
2024-06-03 13:10:45 [INFO]: Epoch 004 - training loss: 0.3480, validation loss: 0.3164
2024-06-03 13:12:04 [INFO]: Epoch 005 - training loss: 0.3006, validation loss: 0.2981
2024-06-03 13:13:23 [INFO]: Epoch 006 - training loss: 0.2914, validation loss: 0.3082
2024-06-03 13:14:42 [INFO]: Epoch 007 - training loss: 0.2648, validation loss: 0.2428
2024-06-03 13:16:02 [INFO]: Epoch 008 - training loss: 0.2537, validation loss: 0.2498
2024-06-03 13:17:22 [INFO]: Epoch 009 - training loss: 0.2414, validation loss: 0.2338
2024-06-03 13:18:41 [INFO]: Epoch 010 - training loss: 0.2538, validation loss: 0.2127
2024-06-03 13:19:59 [INFO]: Epoch 011 - training loss: 0.2382, validation loss: 0.1952
2024-06-03 13:21:14 [INFO]: Epoch 012 - training loss: 0.2185, validation loss: 0.1944
2024-06-03 13:22:27 [INFO]: Epoch 013 - training loss: 0.2216, validation loss: 0.1875
2024-06-03 13:23:39 [INFO]: Epoch 014 - training loss: 0.2157, validation loss: 0.1810
2024-06-03 13:24:50 [INFO]: Epoch 015 - training loss: 0.2044, validation loss: 0.1872
2024-06-03 13:26:03 [INFO]: Epoch 016 - training loss: 0.1940, validation loss: 0.1749
2024-06-03 13:27:15 [INFO]: Epoch 017 - training loss: 0.2046, validation loss: 0.1779
2024-06-03 13:28:26 [INFO]: Epoch 018 - training loss: 0.2096, validation loss: 0.1719
2024-06-03 13:29:38 [INFO]: Epoch 019 - training loss: 0.2121, validation loss: 0.1699
2024-06-03 13:30:50 [INFO]: Epoch 020 - training loss: 0.1872, validation loss: 0.1692
2024-06-03 13:32:02 [INFO]: Epoch 021 - training loss: 0.1974, validation loss: 0.1762
2024-06-03 13:33:14 [INFO]: Epoch 022 - training loss: 0.1793, validation loss: 0.1732
2024-06-03 13:34:26 [INFO]: Epoch 023 - training loss: 0.1878, validation loss: 0.1657
2024-06-03 13:35:39 [INFO]: Epoch 024 - training loss: 0.1837, validation loss: 0.1615
2024-06-03 13:36:50 [INFO]: Epoch 025 - training loss: 0.1927, validation loss: 0.1589
2024-06-03 13:38:02 [INFO]: Epoch 026 - training loss: 0.1798, validation loss: 0.1624
2024-06-03 13:39:13 [INFO]: Epoch 027 - training loss: 0.1956, validation loss: 0.1629
2024-06-03 13:40:25 [INFO]: Epoch 028 - training loss: 0.2036, validation loss: 0.1609
2024-06-03 13:41:37 [INFO]: Epoch 029 - training loss: 0.1876, validation loss: 0.1645
2024-06-03 13:42:49 [INFO]: Epoch 030 - training loss: 0.1862, validation loss: 0.1579
2024-06-03 13:44:00 [INFO]: Epoch 031 - training loss: 0.1917, validation loss: 0.1540
2024-06-03 13:45:12 [INFO]: Epoch 032 - training loss: 0.1791, validation loss: 0.1599
2024-06-03 13:46:23 [INFO]: Epoch 033 - training loss: 0.1733, validation loss: 0.1515
2024-06-03 13:47:36 [INFO]: Epoch 034 - training loss: 0.1793, validation loss: 0.1520
2024-06-03 13:48:46 [INFO]: Epoch 035 - training loss: 0.1673, validation loss: 0.1503
2024-06-03 13:49:55 [INFO]: Epoch 036 - training loss: 0.1766, validation loss: 0.1539
2024-06-03 13:51:03 [INFO]: Epoch 037 - training loss: 0.1731, validation loss: 0.1472
2024-06-03 13:52:13 [INFO]: Epoch 038 - training loss: 0.1824, validation loss: 0.1511
2024-06-03 13:53:21 [INFO]: Epoch 039 - training loss: 0.1743, validation loss: 0.1524
2024-06-03 13:54:30 [INFO]: Epoch 040 - training loss: 0.1725, validation loss: 0.1479
2024-06-03 13:55:39 [INFO]: Epoch 041 - training loss: 0.1660, validation loss: 0.1456
2024-06-03 13:56:48 [INFO]: Epoch 042 - training loss: 0.1740, validation loss: 0.1452
2024-06-03 13:57:57 [INFO]: Epoch 043 - training loss: 0.1815, validation loss: 0.1418
2024-06-03 13:59:05 [INFO]: Epoch 044 - training loss: 0.1585, validation loss: 0.1434
2024-06-03 14:00:14 [INFO]: Epoch 045 - training loss: 0.1806, validation loss: 0.1418
2024-06-03 14:01:23 [INFO]: Epoch 046 - training loss: 0.1844, validation loss: 0.1448
2024-06-03 14:02:32 [INFO]: Epoch 047 - training loss: 0.1791, validation loss: 0.1412
2024-06-03 14:03:40 [INFO]: Epoch 048 - training loss: 0.1801, validation loss: 0.1398
2024-06-03 14:04:49 [INFO]: Epoch 049 - training loss: 0.1708, validation loss: 0.1427
2024-06-03 14:05:57 [INFO]: Epoch 050 - training loss: 0.1828, validation loss: 0.1422
2024-06-03 14:07:06 [INFO]: Epoch 051 - training loss: 0.1662, validation loss: 0.1443
2024-06-03 14:08:14 [INFO]: Epoch 052 - training loss: 0.1795, validation loss: 0.1423
2024-06-03 14:09:22 [INFO]: Epoch 053 - training loss: 0.1613, validation loss: 0.1375
2024-06-03 14:10:32 [INFO]: Epoch 054 - training loss: 0.1733, validation loss: 0.1390
2024-06-03 14:11:40 [INFO]: Epoch 055 - training loss: 0.1585, validation loss: 0.1343
2024-06-03 14:12:49 [INFO]: Epoch 056 - training loss: 0.1730, validation loss: 0.1375
2024-06-03 14:13:57 [INFO]: Epoch 057 - training loss: 0.1757, validation loss: 0.1373
2024-06-03 14:15:06 [INFO]: Epoch 058 - training loss: 0.1539, validation loss: 0.1471
2024-06-03 14:16:15 [INFO]: Epoch 059 - training loss: 0.1664, validation loss: 0.1409
2024-06-03 14:17:23 [INFO]: Epoch 060 - training loss: 0.1763, validation loss: 0.1356
2024-06-03 14:18:31 [INFO]: Epoch 061 - training loss: 0.1670, validation loss: 0.1350
2024-06-03 14:19:40 [INFO]: Epoch 062 - training loss: 0.1599, validation loss: 0.1339
2024-06-03 14:20:49 [INFO]: Epoch 063 - training loss: 0.1782, validation loss: 0.1332
2024-06-03 14:21:58 [INFO]: Epoch 064 - training loss: 0.1741, validation loss: 0.1342
2024-06-03 14:23:03 [INFO]: Epoch 065 - training loss: 0.1615, validation loss: 0.1306
2024-06-03 14:24:07 [INFO]: Epoch 066 - training loss: 0.1381, validation loss: 0.1306
2024-06-03 14:25:11 [INFO]: Epoch 067 - training loss: 0.1596, validation loss: 0.1331
2024-06-03 14:26:15 [INFO]: Epoch 068 - training loss: 0.1549, validation loss: 0.1305
2024-06-03 14:27:19 [INFO]: Epoch 069 - training loss: 0.1660, validation loss: 0.1316
2024-06-03 14:28:23 [INFO]: Epoch 070 - training loss: 0.1658, validation loss: 0.1315
2024-06-03 14:29:27 [INFO]: Epoch 071 - training loss: 0.1580, validation loss: 0.1318
2024-06-03 14:30:31 [INFO]: Epoch 072 - training loss: 0.1635, validation loss: 0.1315
2024-06-03 14:31:34 [INFO]: Epoch 073 - training loss: 0.1596, validation loss: 0.1296
2024-06-03 14:32:39 [INFO]: Epoch 074 - training loss: 0.1519, validation loss: 0.1291
2024-06-03 14:33:42 [INFO]: Epoch 075 - training loss: 0.1577, validation loss: 0.1348
2024-06-03 14:34:46 [INFO]: Epoch 076 - training loss: 0.1720, validation loss: 0.1287
2024-06-03 14:35:50 [INFO]: Epoch 077 - training loss: 0.1618, validation loss: 0.1288
2024-06-03 14:36:55 [INFO]: Epoch 078 - training loss: 0.1591, validation loss: 0.1287
2024-06-03 14:37:59 [INFO]: Epoch 079 - training loss: 0.1610, validation loss: 0.1363
2024-06-03 14:39:03 [INFO]: Epoch 080 - training loss: 0.1518, validation loss: 0.1297
2024-06-03 14:40:06 [INFO]: Epoch 081 - training loss: 0.1331, validation loss: 0.1305
2024-06-03 14:41:10 [INFO]: Epoch 082 - training loss: 0.1348, validation loss: 0.1300
2024-06-03 14:42:15 [INFO]: Epoch 083 - training loss: 0.1647, validation loss: 0.1299
2024-06-03 14:43:18 [INFO]: Epoch 084 - training loss: 0.1629, validation loss: 0.1285
2024-06-03 14:44:22 [INFO]: Epoch 085 - training loss: 0.1443, validation loss: 0.1321
2024-06-03 14:45:26 [INFO]: Epoch 086 - training loss: 0.1553, validation loss: 0.1257
2024-06-03 14:46:30 [INFO]: Epoch 087 - training loss: 0.1492, validation loss: 0.1253
2024-06-03 14:47:34 [INFO]: Epoch 088 - training loss: 0.1480, validation loss: 0.1266
2024-06-03 14:48:38 [INFO]: Epoch 089 - training loss: 0.1518, validation loss: 0.1256
2024-06-03 14:49:42 [INFO]: Epoch 090 - training loss: 0.1462, validation loss: 0.1288
2024-06-03 14:50:46 [INFO]: Epoch 091 - training loss: 0.1615, validation loss: 0.1288
2024-06-03 14:51:50 [INFO]: Epoch 092 - training loss: 0.1474, validation loss: 0.1274
2024-06-03 14:52:53 [INFO]: Epoch 093 - training loss: 0.1426, validation loss: 0.1268
2024-06-03 14:53:58 [INFO]: Epoch 094 - training loss: 0.1451, validation loss: 0.1240
2024-06-03 14:55:02 [INFO]: Epoch 095 - training loss: 0.1451, validation loss: 0.1266
2024-06-03 14:56:05 [INFO]: Epoch 096 - training loss: 0.1445, validation loss: 0.1261
2024-06-03 14:57:09 [INFO]: Epoch 097 - training loss: 0.1477, validation loss: 0.1221
2024-06-03 14:58:13 [INFO]: Epoch 098 - training loss: 0.1420, validation loss: 0.1232
2024-06-03 14:59:18 [INFO]: Epoch 099 - training loss: 0.1512, validation loss: 0.1240
2024-06-03 15:00:22 [INFO]: Epoch 100 - training loss: 0.1419, validation loss: 0.1259
2024-06-03 15:00:22 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 15:00:22 [INFO]: Saved the model to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_0/20240603_T130513/CSDI.pypots
2024-06-03 15:41:53 [INFO]: Successfully saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_0/imputation.pkl
2024-06-03 15:41:53 [INFO]: Round0 - CSDI on BeijingAir: MAE=0.1914, MSE=0.6131, MRE=0.2587
2024-06-03 15:41:53 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 15:41:53 [INFO]: Using the given device: cuda:0
2024-06-03 15:41:53 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_1/20240603_T154153
2024-06-03 15:41:53 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_1/20240603_T154153/tensorboard
2024-06-03 15:41:53 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 244,833
2024-06-03 15:42:57 [INFO]: Epoch 001 - training loss: 0.5399, validation loss: 0.3813
2024-06-03 15:44:02 [INFO]: Epoch 002 - training loss: 0.3626, validation loss: 0.3269
2024-06-03 15:45:05 [INFO]: Epoch 003 - training loss: 0.3417, validation loss: 0.3022
2024-06-03 15:46:09 [INFO]: Epoch 004 - training loss: 0.3064, validation loss: 0.2724
2024-06-03 15:47:13 [INFO]: Epoch 005 - training loss: 0.2734, validation loss: 0.2515
2024-06-03 15:48:17 [INFO]: Epoch 006 - training loss: 0.2584, validation loss: 0.2301
2024-06-03 15:49:21 [INFO]: Epoch 007 - training loss: 0.2563, validation loss: 0.2161
2024-06-03 15:50:25 [INFO]: Epoch 008 - training loss: 0.2302, validation loss: 0.2178
2024-06-03 15:51:29 [INFO]: Epoch 009 - training loss: 0.2245, validation loss: 0.2056
2024-06-03 15:52:33 [INFO]: Epoch 010 - training loss: 0.2190, validation loss: 0.1916
2024-06-03 15:53:37 [INFO]: Epoch 011 - training loss: 0.2251, validation loss: 0.1953
2024-06-03 15:54:41 [INFO]: Epoch 012 - training loss: 0.1996, validation loss: 0.1828
2024-06-03 15:55:45 [INFO]: Epoch 013 - training loss: 0.1967, validation loss: 0.1925
2024-06-03 15:56:49 [INFO]: Epoch 014 - training loss: 0.2161, validation loss: 0.1796
2024-06-03 15:57:53 [INFO]: Epoch 015 - training loss: 0.2057, validation loss: 0.1757
2024-06-03 15:58:57 [INFO]: Epoch 016 - training loss: 0.1946, validation loss: 0.1773
2024-06-03 16:00:01 [INFO]: Epoch 017 - training loss: 0.1980, validation loss: 0.1759
2024-06-03 16:01:05 [INFO]: Epoch 018 - training loss: 0.1905, validation loss: 0.1760
2024-06-03 16:02:09 [INFO]: Epoch 019 - training loss: 0.1978, validation loss: 0.1832
2024-06-03 16:03:13 [INFO]: Epoch 020 - training loss: 0.1967, validation loss: 0.1703
2024-06-03 16:04:17 [INFO]: Epoch 021 - training loss: 0.1994, validation loss: 0.1719
2024-06-03 16:05:22 [INFO]: Epoch 022 - training loss: 0.2017, validation loss: 0.1868
2024-06-03 16:06:26 [INFO]: Epoch 023 - training loss: 0.1903, validation loss: 0.1687
2024-06-03 16:07:30 [INFO]: Epoch 024 - training loss: 0.1733, validation loss: 0.1651
2024-06-03 16:08:34 [INFO]: Epoch 025 - training loss: 0.1776, validation loss: 0.1629
2024-06-03 16:09:38 [INFO]: Epoch 026 - training loss: 0.1858, validation loss: 0.1622
2024-06-03 16:10:42 [INFO]: Epoch 027 - training loss: 0.1811, validation loss: 0.1617
2024-06-03 16:11:46 [INFO]: Epoch 028 - training loss: 0.1677, validation loss: 0.1608
2024-06-03 16:12:50 [INFO]: Epoch 029 - training loss: 0.1879, validation loss: 0.1592
2024-06-03 16:13:54 [INFO]: Epoch 030 - training loss: 0.2063, validation loss: 0.1636
2024-06-03 16:14:58 [INFO]: Epoch 031 - training loss: 0.1994, validation loss: 0.1626
2024-06-03 16:16:02 [INFO]: Epoch 032 - training loss: 0.1776, validation loss: 0.1575
2024-06-03 16:17:06 [INFO]: Epoch 033 - training loss: 0.1846, validation loss: 0.1565
2024-06-03 16:18:10 [INFO]: Epoch 034 - training loss: 0.1701, validation loss: 0.1530
2024-06-03 16:19:14 [INFO]: Epoch 035 - training loss: 0.1699, validation loss: 0.1544
2024-06-03 16:20:18 [INFO]: Epoch 036 - training loss: 0.1918, validation loss: 0.1498
2024-06-03 16:21:22 [INFO]: Epoch 037 - training loss: 0.1732, validation loss: 0.1490
2024-06-03 16:22:26 [INFO]: Epoch 038 - training loss: 0.1683, validation loss: 0.1528
2024-06-03 16:23:30 [INFO]: Epoch 039 - training loss: 0.1804, validation loss: 0.1501
2024-06-03 16:24:35 [INFO]: Epoch 040 - training loss: 0.1674, validation loss: 0.1511
2024-06-03 16:25:39 [INFO]: Epoch 041 - training loss: 0.1871, validation loss: 0.1517
2024-06-03 16:26:43 [INFO]: Epoch 042 - training loss: 0.1848, validation loss: 0.1499
2024-06-03 16:27:47 [INFO]: Epoch 043 - training loss: 0.1710, validation loss: 0.1451
2024-06-03 16:28:52 [INFO]: Epoch 044 - training loss: 0.1821, validation loss: 0.1492
2024-06-03 16:29:55 [INFO]: Epoch 045 - training loss: 0.1627, validation loss: 0.1532
2024-06-03 16:30:59 [INFO]: Epoch 046 - training loss: 0.1536, validation loss: 0.1485
2024-06-03 16:32:04 [INFO]: Epoch 047 - training loss: 0.1643, validation loss: 0.1433
2024-06-03 16:33:08 [INFO]: Epoch 048 - training loss: 0.1760, validation loss: 0.1412
2024-06-03 16:34:11 [INFO]: Epoch 049 - training loss: 0.1627, validation loss: 0.1434
2024-06-03 16:35:15 [INFO]: Epoch 050 - training loss: 0.1595, validation loss: 0.1399
2024-06-03 16:36:19 [INFO]: Epoch 051 - training loss: 0.1573, validation loss: 0.1396
2024-06-03 16:37:23 [INFO]: Epoch 052 - training loss: 0.1645, validation loss: 0.1385
2024-06-03 16:38:27 [INFO]: Epoch 053 - training loss: 0.1535, validation loss: 0.1401
2024-06-03 16:39:31 [INFO]: Epoch 054 - training loss: 0.1604, validation loss: 0.1427
2024-06-03 16:40:35 [INFO]: Epoch 055 - training loss: 0.1689, validation loss: 0.1395
2024-06-03 16:41:39 [INFO]: Epoch 056 - training loss: 0.1553, validation loss: 0.1385
2024-06-03 16:42:44 [INFO]: Epoch 057 - training loss: 0.1741, validation loss: 0.1374
2024-06-03 16:43:47 [INFO]: Epoch 058 - training loss: 0.1651, validation loss: 0.1417
2024-06-03 16:44:52 [INFO]: Epoch 059 - training loss: 0.1780, validation loss: 0.1376
2024-06-03 16:45:56 [INFO]: Epoch 060 - training loss: 0.1629, validation loss: 0.1436
2024-06-03 16:47:00 [INFO]: Epoch 061 - training loss: 0.1657, validation loss: 0.1352
2024-06-03 16:48:04 [INFO]: Epoch 062 - training loss: 0.1560, validation loss: 0.1340
2024-06-03 16:49:08 [INFO]: Epoch 063 - training loss: 0.1694, validation loss: 0.1349
2024-06-03 16:50:12 [INFO]: Epoch 064 - training loss: 0.1689, validation loss: 0.1338
2024-06-03 16:51:16 [INFO]: Epoch 065 - training loss: 0.1692, validation loss: 0.1353
2024-06-03 16:52:21 [INFO]: Epoch 066 - training loss: 0.1729, validation loss: 0.1325
2024-06-03 16:53:24 [INFO]: Epoch 067 - training loss: 0.1635, validation loss: 0.1299
2024-06-03 16:54:28 [INFO]: Epoch 068 - training loss: 0.1526, validation loss: 0.1308
2024-06-03 16:55:32 [INFO]: Epoch 069 - training loss: 0.1724, validation loss: 0.1628
2024-06-03 16:56:37 [INFO]: Epoch 070 - training loss: 0.1611, validation loss: 0.1333
2024-06-03 16:57:41 [INFO]: Epoch 071 - training loss: 0.1723, validation loss: 0.1318
2024-06-03 16:58:45 [INFO]: Epoch 072 - training loss: 0.1491, validation loss: 0.1321
2024-06-03 16:59:49 [INFO]: Epoch 073 - training loss: 0.1645, validation loss: 0.1276
2024-06-03 17:00:54 [INFO]: Epoch 074 - training loss: 0.1647, validation loss: 0.1270
2024-06-03 17:01:58 [INFO]: Epoch 075 - training loss: 0.1630, validation loss: 0.1271
2024-06-03 17:03:02 [INFO]: Epoch 076 - training loss: 0.1589, validation loss: 0.1288
2024-06-03 17:03:54 [INFO]: Epoch 077 - training loss: 0.1492, validation loss: 0.1288
2024-06-03 17:04:26 [INFO]: Epoch 078 - training loss: 0.1709, validation loss: 0.1284
2024-06-03 17:04:59 [INFO]: Epoch 079 - training loss: 0.1723, validation loss: 0.1309
2024-06-03 17:05:31 [INFO]: Epoch 080 - training loss: 0.1704, validation loss: 0.1287
2024-06-03 17:06:04 [INFO]: Epoch 081 - training loss: 0.1687, validation loss: 0.1270
2024-06-03 17:06:36 [INFO]: Epoch 082 - training loss: 0.1523, validation loss: 0.1271
2024-06-03 17:07:09 [INFO]: Epoch 083 - training loss: 0.1549, validation loss: 0.1293
2024-06-03 17:07:41 [INFO]: Epoch 084 - training loss: 0.1583, validation loss: 0.1366
2024-06-03 17:07:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 17:07:41 [INFO]: Finished training. The best model is from epoch#74.
2024-06-03 17:07:41 [INFO]: Saved the model to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_1/20240603_T154153/CSDI.pypots
2024-06-03 17:25:36 [INFO]: Successfully saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_1/imputation.pkl
2024-06-03 17:25:36 [INFO]: Round1 - CSDI on BeijingAir: MAE=0.2460, MSE=0.6480, MRE=0.3326
2024-06-03 17:25:36 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 17:25:36 [INFO]: Using the given device: cuda:0
2024-06-03 17:25:36 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_2/20240603_T172536
2024-06-03 17:25:36 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_2/20240603_T172536/tensorboard
2024-06-03 17:25:36 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 244,833
2024-06-03 17:26:02 [INFO]: Epoch 001 - training loss: 0.5232, validation loss: 0.3660
2024-06-03 17:26:28 [INFO]: Epoch 002 - training loss: 0.3534, validation loss: 0.3437
2024-06-03 17:26:54 [INFO]: Epoch 003 - training loss: 0.3305, validation loss: 0.3318
2024-06-03 17:27:20 [INFO]: Epoch 004 - training loss: 0.2786, validation loss: 0.3046
2024-06-03 17:27:46 [INFO]: Epoch 005 - training loss: 0.2931, validation loss: 0.2811
2024-06-03 17:28:12 [INFO]: Epoch 006 - training loss: 0.3015, validation loss: 0.2726
2024-06-03 17:28:38 [INFO]: Epoch 007 - training loss: 0.2388, validation loss: 0.2481
2024-06-03 17:29:04 [INFO]: Epoch 008 - training loss: 0.2520, validation loss: 0.2346
2024-06-03 17:29:30 [INFO]: Epoch 009 - training loss: 0.2363, validation loss: 0.2168
2024-06-03 17:29:56 [INFO]: Epoch 010 - training loss: 0.2343, validation loss: 0.2035
2024-06-03 17:30:22 [INFO]: Epoch 011 - training loss: 0.2214, validation loss: 0.2049
2024-06-03 17:30:48 [INFO]: Epoch 012 - training loss: 0.2062, validation loss: 0.1964
2024-06-03 17:31:14 [INFO]: Epoch 013 - training loss: 0.2093, validation loss: 0.1971
2024-06-03 17:31:40 [INFO]: Epoch 014 - training loss: 0.2129, validation loss: 0.1911
2024-06-03 17:32:07 [INFO]: Epoch 015 - training loss: 0.2039, validation loss: 0.1815
2024-06-03 17:32:33 [INFO]: Epoch 016 - training loss: 0.2140, validation loss: 0.1779
2024-06-03 17:32:59 [INFO]: Epoch 017 - training loss: 0.2062, validation loss: 0.1745
2024-06-03 17:33:25 [INFO]: Epoch 018 - training loss: 0.2091, validation loss: 0.1759
2024-06-03 17:33:51 [INFO]: Epoch 019 - training loss: 0.1954, validation loss: 0.1715
2024-06-03 17:34:17 [INFO]: Epoch 020 - training loss: 0.1950, validation loss: 0.1724
2024-06-03 17:34:43 [INFO]: Epoch 021 - training loss: 0.2046, validation loss: 0.1763
2024-06-03 17:35:09 [INFO]: Epoch 022 - training loss: 0.1897, validation loss: 0.1672
2024-06-03 17:35:35 [INFO]: Epoch 023 - training loss: 0.1943, validation loss: 0.1690
2024-06-03 17:36:01 [INFO]: Epoch 024 - training loss: 0.1918, validation loss: 0.1640
2024-06-03 17:36:27 [INFO]: Epoch 025 - training loss: 0.2006, validation loss: 0.1664
2024-06-03 17:36:53 [INFO]: Epoch 026 - training loss: 0.1858, validation loss: 0.1614
2024-06-03 17:37:19 [INFO]: Epoch 027 - training loss: 0.1791, validation loss: 0.1588
2024-06-03 17:37:45 [INFO]: Epoch 028 - training loss: 0.1752, validation loss: 0.1609
2024-06-03 17:38:11 [INFO]: Epoch 029 - training loss: 0.1887, validation loss: 0.1592
2024-06-03 17:38:37 [INFO]: Epoch 030 - training loss: 0.1848, validation loss: 0.1579
2024-06-03 17:39:04 [INFO]: Epoch 031 - training loss: 0.1988, validation loss: 0.1541
2024-06-03 17:39:30 [INFO]: Epoch 032 - training loss: 0.1866, validation loss: 0.1540
2024-06-03 17:39:56 [INFO]: Epoch 033 - training loss: 0.1900, validation loss: 0.1545
2024-06-03 17:40:22 [INFO]: Epoch 034 - training loss: 0.1840, validation loss: 0.1522
2024-06-03 17:40:48 [INFO]: Epoch 035 - training loss: 0.1792, validation loss: 0.1537
2024-06-03 17:41:14 [INFO]: Epoch 036 - training loss: 0.1662, validation loss: 0.1504
2024-06-03 17:41:40 [INFO]: Epoch 037 - training loss: 0.1741, validation loss: 0.1524
2024-06-03 17:42:06 [INFO]: Epoch 038 - training loss: 0.1580, validation loss: 0.1591
2024-06-03 17:42:32 [INFO]: Epoch 039 - training loss: 0.1831, validation loss: 0.1481
2024-06-03 17:42:58 [INFO]: Epoch 040 - training loss: 0.1623, validation loss: 0.1496
2024-06-03 17:43:24 [INFO]: Epoch 041 - training loss: 0.1823, validation loss: 0.1454
2024-06-03 17:43:50 [INFO]: Epoch 042 - training loss: 0.1752, validation loss: 0.1454
2024-06-03 17:44:16 [INFO]: Epoch 043 - training loss: 0.1625, validation loss: 0.1440
2024-06-03 17:44:42 [INFO]: Epoch 044 - training loss: 0.1576, validation loss: 0.1434
2024-06-03 17:45:08 [INFO]: Epoch 045 - training loss: 0.1818, validation loss: 0.1449
2024-06-03 17:45:34 [INFO]: Epoch 046 - training loss: 0.1807, validation loss: 0.1447
2024-06-03 17:46:01 [INFO]: Epoch 047 - training loss: 0.1629, validation loss: 0.1401
2024-06-03 17:46:27 [INFO]: Epoch 048 - training loss: 0.1549, validation loss: 0.1439
2024-06-03 17:46:53 [INFO]: Epoch 049 - training loss: 0.1619, validation loss: 0.1394
2024-06-03 17:47:19 [INFO]: Epoch 050 - training loss: 0.1957, validation loss: 0.1421
2024-06-03 17:47:45 [INFO]: Epoch 051 - training loss: 0.1649, validation loss: 0.1494
2024-06-03 17:48:11 [INFO]: Epoch 052 - training loss: 0.1760, validation loss: 0.1399
2024-06-03 17:48:37 [INFO]: Epoch 053 - training loss: 0.1573, validation loss: 0.1362
2024-06-03 17:49:03 [INFO]: Epoch 054 - training loss: 0.1696, validation loss: 0.1400
2024-06-03 17:49:29 [INFO]: Epoch 055 - training loss: 0.1781, validation loss: 0.1413
2024-06-03 17:49:55 [INFO]: Epoch 056 - training loss: 0.1762, validation loss: 0.1354
2024-06-03 17:50:21 [INFO]: Epoch 057 - training loss: 0.1535, validation loss: 0.1345
2024-06-03 17:50:47 [INFO]: Epoch 058 - training loss: 0.1849, validation loss: 0.1350
2024-06-03 17:51:13 [INFO]: Epoch 059 - training loss: 0.1621, validation loss: 0.1374
2024-06-03 17:51:39 [INFO]: Epoch 060 - training loss: 0.1653, validation loss: 0.1365
2024-06-03 17:52:05 [INFO]: Epoch 061 - training loss: 0.1400, validation loss: 0.1400
2024-06-03 17:52:31 [INFO]: Epoch 062 - training loss: 0.1480, validation loss: 0.1397
2024-06-03 17:52:58 [INFO]: Epoch 063 - training loss: 0.1585, validation loss: 0.1375
2024-06-03 17:53:24 [INFO]: Epoch 064 - training loss: 0.1523, validation loss: 0.1312
2024-06-03 17:53:50 [INFO]: Epoch 065 - training loss: 0.1458, validation loss: 0.1321
2024-06-03 17:54:16 [INFO]: Epoch 066 - training loss: 0.1650, validation loss: 0.1325
2024-06-03 17:54:42 [INFO]: Epoch 067 - training loss: 0.1499, validation loss: 0.1312
2024-06-03 17:55:08 [INFO]: Epoch 068 - training loss: 0.1531, validation loss: 0.1317
2024-06-03 17:55:34 [INFO]: Epoch 069 - training loss: 0.1844, validation loss: 0.1320
2024-06-03 17:56:00 [INFO]: Epoch 070 - training loss: 0.1493, validation loss: 0.1357
2024-06-03 17:56:26 [INFO]: Epoch 071 - training loss: 0.1611, validation loss: 0.1295
2024-06-03 17:56:52 [INFO]: Epoch 072 - training loss: 0.1363, validation loss: 0.1304
2024-06-03 17:57:18 [INFO]: Epoch 073 - training loss: 0.1563, validation loss: 0.1281
2024-06-03 17:57:44 [INFO]: Epoch 074 - training loss: 0.1454, validation loss: 0.1285
2024-06-03 17:58:10 [INFO]: Epoch 075 - training loss: 0.1582, validation loss: 0.1283
2024-06-03 17:58:36 [INFO]: Epoch 076 - training loss: 0.1519, validation loss: 0.1279
2024-06-03 17:59:02 [INFO]: Epoch 077 - training loss: 0.1614, validation loss: 0.1301
2024-06-03 17:59:29 [INFO]: Epoch 078 - training loss: 0.1616, validation loss: 0.1256
2024-06-03 17:59:55 [INFO]: Epoch 079 - training loss: 0.1567, validation loss: 0.1289
2024-06-03 18:00:21 [INFO]: Epoch 080 - training loss: 0.1462, validation loss: 0.1275
2024-06-03 18:00:47 [INFO]: Epoch 081 - training loss: 0.1758, validation loss: 0.1299
2024-06-03 18:01:13 [INFO]: Epoch 082 - training loss: 0.1685, validation loss: 0.1265
2024-06-03 18:01:39 [INFO]: Epoch 083 - training loss: 0.1488, validation loss: 0.1273
2024-06-03 18:02:05 [INFO]: Epoch 084 - training loss: 0.1663, validation loss: 0.1253
2024-06-03 18:02:31 [INFO]: Epoch 085 - training loss: 0.1506, validation loss: 0.1256
2024-06-03 18:02:57 [INFO]: Epoch 086 - training loss: 0.1646, validation loss: 0.1250
2024-06-03 18:03:23 [INFO]: Epoch 087 - training loss: 0.1419, validation loss: 0.1243
2024-06-03 18:03:49 [INFO]: Epoch 088 - training loss: 0.1501, validation loss: 0.1259
2024-06-03 18:04:15 [INFO]: Epoch 089 - training loss: 0.1605, validation loss: 0.1272
2024-06-03 18:04:41 [INFO]: Epoch 090 - training loss: 0.1635, validation loss: 0.1242
2024-06-03 18:05:08 [INFO]: Epoch 091 - training loss: 0.1530, validation loss: 0.1221
2024-06-03 18:05:34 [INFO]: Epoch 092 - training loss: 0.1473, validation loss: 0.1240
2024-06-03 18:06:00 [INFO]: Epoch 093 - training loss: 0.1497, validation loss: 0.1226
2024-06-03 18:06:26 [INFO]: Epoch 094 - training loss: 0.1530, validation loss: 0.1239
2024-06-03 18:06:52 [INFO]: Epoch 095 - training loss: 0.1435, validation loss: 0.1245
2024-06-03 18:07:18 [INFO]: Epoch 096 - training loss: 0.1692, validation loss: 0.1270
2024-06-03 18:07:44 [INFO]: Epoch 097 - training loss: 0.1469, validation loss: 0.1246
2024-06-03 18:08:10 [INFO]: Epoch 098 - training loss: 0.1471, validation loss: 0.1302
2024-06-03 18:08:36 [INFO]: Epoch 099 - training loss: 0.1377, validation loss: 0.1233
2024-06-03 18:09:02 [INFO]: Epoch 100 - training loss: 0.1493, validation loss: 0.1255
2024-06-03 18:09:02 [INFO]: Finished training. The best model is from epoch#91.
2024-06-03 18:09:02 [INFO]: Saved the model to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_2/20240603_T172536/CSDI.pypots
2024-06-03 18:26:22 [INFO]: Successfully saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_2/imputation.pkl
2024-06-03 18:26:22 [INFO]: Round2 - CSDI on BeijingAir: MAE=0.1842, MSE=0.2983, MRE=0.2490
2024-06-03 18:26:22 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 18:26:22 [INFO]: Using the given device: cuda:0
2024-06-03 18:26:22 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_3/20240603_T182622
2024-06-03 18:26:22 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_3/20240603_T182622/tensorboard
2024-06-03 18:26:22 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 244,833
2024-06-03 18:26:48 [INFO]: Epoch 001 - training loss: 0.5520, validation loss: 0.4034
2024-06-03 18:27:14 [INFO]: Epoch 002 - training loss: 0.3704, validation loss: 0.3483
2024-06-03 18:27:41 [INFO]: Epoch 003 - training loss: 0.3350, validation loss: 0.3696
2024-06-03 18:28:07 [INFO]: Epoch 004 - training loss: 0.3162, validation loss: 0.3100
2024-06-03 18:28:33 [INFO]: Epoch 005 - training loss: 0.2723, validation loss: 0.2784
2024-06-03 18:28:59 [INFO]: Epoch 006 - training loss: 0.2610, validation loss: 0.2676
2024-06-03 18:29:25 [INFO]: Epoch 007 - training loss: 0.2612, validation loss: 0.2290
2024-06-03 18:29:51 [INFO]: Epoch 008 - training loss: 0.2379, validation loss: 0.2318
2024-06-03 18:30:17 [INFO]: Epoch 009 - training loss: 0.2230, validation loss: 0.2232
2024-06-03 18:30:43 [INFO]: Epoch 010 - training loss: 0.2213, validation loss: 0.2036
2024-06-03 18:31:09 [INFO]: Epoch 011 - training loss: 0.2119, validation loss: 0.1931
2024-06-03 18:31:35 [INFO]: Epoch 012 - training loss: 0.1968, validation loss: 0.1876
2024-06-03 18:32:01 [INFO]: Epoch 013 - training loss: 0.2208, validation loss: 0.1935
2024-06-03 18:32:27 [INFO]: Epoch 014 - training loss: 0.2138, validation loss: 0.1906
2024-06-03 18:32:53 [INFO]: Epoch 015 - training loss: 0.2039, validation loss: 0.1816
2024-06-03 18:33:20 [INFO]: Epoch 016 - training loss: 0.2030, validation loss: 0.1784
2024-06-03 18:33:46 [INFO]: Epoch 017 - training loss: 0.1994, validation loss: 0.1770
2024-06-03 18:34:12 [INFO]: Epoch 018 - training loss: 0.1783, validation loss: 0.1876
2024-06-03 18:34:38 [INFO]: Epoch 019 - training loss: 0.1963, validation loss: 0.1874
2024-06-03 18:35:04 [INFO]: Epoch 020 - training loss: 0.2029, validation loss: 0.1761
2024-06-03 18:35:30 [INFO]: Epoch 021 - training loss: 0.1854, validation loss: 0.1707
2024-06-03 18:35:56 [INFO]: Epoch 022 - training loss: 0.1695, validation loss: 0.1703
2024-06-03 18:36:22 [INFO]: Epoch 023 - training loss: 0.2012, validation loss: 0.1706
2024-06-03 18:36:48 [INFO]: Epoch 024 - training loss: 0.1916, validation loss: 0.1701
2024-06-03 18:37:14 [INFO]: Epoch 025 - training loss: 0.1670, validation loss: 0.1680
2024-06-03 18:37:40 [INFO]: Epoch 026 - training loss: 0.1756, validation loss: 0.1633
2024-06-03 18:38:06 [INFO]: Epoch 027 - training loss: 0.1858, validation loss: 0.1614
2024-06-03 18:38:32 [INFO]: Epoch 028 - training loss: 0.1850, validation loss: 0.1641
2024-06-03 18:38:59 [INFO]: Epoch 029 - training loss: 0.1752, validation loss: 0.1591
2024-06-03 18:39:25 [INFO]: Epoch 030 - training loss: 0.1827, validation loss: 0.1576
2024-06-03 18:39:51 [INFO]: Epoch 031 - training loss: 0.1866, validation loss: 0.1572
2024-06-03 18:40:17 [INFO]: Epoch 032 - training loss: 0.1956, validation loss: 0.1548
2024-06-03 18:40:43 [INFO]: Epoch 033 - training loss: 0.1805, validation loss: 0.1613
2024-06-03 18:41:09 [INFO]: Epoch 034 - training loss: 0.1767, validation loss: 0.1568
2024-06-03 18:41:35 [INFO]: Epoch 035 - training loss: 0.1828, validation loss: 0.1557
2024-06-03 18:42:01 [INFO]: Epoch 036 - training loss: 0.1845, validation loss: 0.1500
2024-06-03 18:42:27 [INFO]: Epoch 037 - training loss: 0.1830, validation loss: 0.1542
2024-06-03 18:42:53 [INFO]: Epoch 038 - training loss: 0.1711, validation loss: 0.1489
2024-06-03 18:43:19 [INFO]: Epoch 039 - training loss: 0.1614, validation loss: 0.1442
2024-06-03 18:43:45 [INFO]: Epoch 040 - training loss: 0.1782, validation loss: 0.1710
2024-06-03 18:44:11 [INFO]: Epoch 041 - training loss: 0.1831, validation loss: 0.1485
2024-06-03 18:44:38 [INFO]: Epoch 042 - training loss: 0.1454, validation loss: 0.1446
2024-06-03 18:45:04 [INFO]: Epoch 043 - training loss: 0.1820, validation loss: 0.1455
2024-06-03 18:45:30 [INFO]: Epoch 044 - training loss: 0.1643, validation loss: 0.1455
2024-06-03 18:45:56 [INFO]: Epoch 045 - training loss: 0.1697, validation loss: 0.1460
2024-06-03 18:46:22 [INFO]: Epoch 046 - training loss: 0.1660, validation loss: 0.1416
2024-06-03 18:46:48 [INFO]: Epoch 047 - training loss: 0.1760, validation loss: 0.1397
2024-06-03 18:47:14 [INFO]: Epoch 048 - training loss: 0.1526, validation loss: 0.1444
2024-06-03 18:47:40 [INFO]: Epoch 049 - training loss: 0.1764, validation loss: 0.1428
2024-06-03 18:48:06 [INFO]: Epoch 050 - training loss: 0.1641, validation loss: 0.1531
2024-06-03 18:48:32 [INFO]: Epoch 051 - training loss: 0.1485, validation loss: 0.1425
2024-06-03 18:48:58 [INFO]: Epoch 052 - training loss: 0.1786, validation loss: 0.1444
2024-06-03 18:49:24 [INFO]: Epoch 053 - training loss: 0.1519, validation loss: 0.1382
2024-06-03 18:49:50 [INFO]: Epoch 054 - training loss: 0.1593, validation loss: 0.1452
2024-06-03 18:50:17 [INFO]: Epoch 055 - training loss: 0.1722, validation loss: 0.1419
2024-06-03 18:50:43 [INFO]: Epoch 056 - training loss: 0.1421, validation loss: 0.1383
2024-06-03 18:51:09 [INFO]: Epoch 057 - training loss: 0.1602, validation loss: 0.1424
2024-06-03 18:51:35 [INFO]: Epoch 058 - training loss: 0.1518, validation loss: 0.1368
2024-06-03 18:52:01 [INFO]: Epoch 059 - training loss: 0.1776, validation loss: 0.1359
2024-06-03 18:52:27 [INFO]: Epoch 060 - training loss: 0.1564, validation loss: 0.1393
2024-06-03 18:52:53 [INFO]: Epoch 061 - training loss: 0.1711, validation loss: 0.1351
2024-06-03 18:53:19 [INFO]: Epoch 062 - training loss: 0.1836, validation loss: 0.1364
2024-06-03 18:53:45 [INFO]: Epoch 063 - training loss: 0.1648, validation loss: 0.1405
2024-06-03 18:54:11 [INFO]: Epoch 064 - training loss: 0.1635, validation loss: 0.1365
2024-06-03 18:54:37 [INFO]: Epoch 065 - training loss: 0.1641, validation loss: 0.1345
2024-06-03 18:55:03 [INFO]: Epoch 066 - training loss: 0.1659, validation loss: 0.1351
2024-06-03 18:55:29 [INFO]: Epoch 067 - training loss: 0.1636, validation loss: 0.1335
2024-06-03 18:55:55 [INFO]: Epoch 068 - training loss: 0.1632, validation loss: 0.1355
2024-06-03 18:56:22 [INFO]: Epoch 069 - training loss: 0.1464, validation loss: 0.1341
2024-06-03 18:56:48 [INFO]: Epoch 070 - training loss: 0.1551, validation loss: 0.1343
2024-06-03 18:57:14 [INFO]: Epoch 071 - training loss: 0.1771, validation loss: 0.1309
2024-06-03 18:57:40 [INFO]: Epoch 072 - training loss: 0.1518, validation loss: 0.1304
2024-06-03 18:58:06 [INFO]: Epoch 073 - training loss: 0.1589, validation loss: 0.1352
2024-06-03 18:58:32 [INFO]: Epoch 074 - training loss: 0.1618, validation loss: 0.1300
2024-06-03 18:58:58 [INFO]: Epoch 075 - training loss: 0.1696, validation loss: 0.1306
2024-06-03 18:59:24 [INFO]: Epoch 076 - training loss: 0.1583, validation loss: 0.1290
2024-06-03 18:59:50 [INFO]: Epoch 077 - training loss: 0.1612, validation loss: 0.1342
2024-06-03 19:00:16 [INFO]: Epoch 078 - training loss: 0.1475, validation loss: 0.1305
2024-06-03 19:00:42 [INFO]: Epoch 079 - training loss: 0.1710, validation loss: 0.1283
2024-06-03 19:01:08 [INFO]: Epoch 080 - training loss: 0.1544, validation loss: 0.1269
2024-06-03 19:01:34 [INFO]: Epoch 081 - training loss: 0.1441, validation loss: 0.1311
2024-06-03 19:02:01 [INFO]: Epoch 082 - training loss: 0.1502, validation loss: 0.1286
2024-06-03 19:02:27 [INFO]: Epoch 083 - training loss: 0.1464, validation loss: 0.1265
2024-06-03 19:02:53 [INFO]: Epoch 084 - training loss: 0.1689, validation loss: 0.1283
2024-06-03 19:03:19 [INFO]: Epoch 085 - training loss: 0.1491, validation loss: 0.1236
2024-06-03 19:03:45 [INFO]: Epoch 086 - training loss: 0.1497, validation loss: 0.1274
2024-06-03 19:04:11 [INFO]: Epoch 087 - training loss: 0.1692, validation loss: 0.1261
2024-06-03 19:04:37 [INFO]: Epoch 088 - training loss: 0.1472, validation loss: 0.1249
2024-06-03 19:05:03 [INFO]: Epoch 089 - training loss: 0.1625, validation loss: 0.1251
2024-06-03 19:05:29 [INFO]: Epoch 090 - training loss: 0.1512, validation loss: 0.1243
2024-06-03 19:05:55 [INFO]: Epoch 091 - training loss: 0.1381, validation loss: 0.1222
2024-06-03 19:06:22 [INFO]: Epoch 092 - training loss: 0.1582, validation loss: 0.1264
2024-06-03 19:06:48 [INFO]: Epoch 093 - training loss: 0.1371, validation loss: 0.1246
2024-06-03 19:07:14 [INFO]: Epoch 094 - training loss: 0.1648, validation loss: 0.1254
2024-06-03 19:07:40 [INFO]: Epoch 095 - training loss: 0.1411, validation loss: 0.1237
2024-06-03 19:08:06 [INFO]: Epoch 096 - training loss: 0.1460, validation loss: 0.1216
2024-06-03 19:08:32 [INFO]: Epoch 097 - training loss: 0.1481, validation loss: 0.1240
2024-06-03 19:08:58 [INFO]: Epoch 098 - training loss: 0.1548, validation loss: 0.1201
2024-06-03 19:09:24 [INFO]: Epoch 099 - training loss: 0.1529, validation loss: 0.1209
2024-06-03 19:09:50 [INFO]: Epoch 100 - training loss: 0.1658, validation loss: 0.1214
2024-06-03 19:09:50 [INFO]: Finished training. The best model is from epoch#98.
2024-06-03 19:09:50 [INFO]: Saved the model to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_3/20240603_T182622/CSDI.pypots
2024-06-03 19:27:12 [INFO]: Successfully saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_3/imputation.pkl
2024-06-03 19:27:12 [INFO]: Round3 - CSDI on BeijingAir: MAE=0.1816, MSE=0.2876, MRE=0.2455
2024-06-03 19:27:12 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 19:27:12 [INFO]: Using the given device: cuda:0
2024-06-03 19:27:12 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_4/20240603_T192712
2024-06-03 19:27:12 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_4/20240603_T192712/tensorboard
2024-06-03 19:27:12 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 244,833
2024-06-03 19:27:38 [INFO]: Epoch 001 - training loss: 0.5415, validation loss: 0.3930
2024-06-03 19:28:04 [INFO]: Epoch 002 - training loss: 0.3618, validation loss: 0.3374
2024-06-03 19:28:30 [INFO]: Epoch 003 - training loss: 0.3351, validation loss: 0.3282
2024-06-03 19:28:56 [INFO]: Epoch 004 - training loss: 0.3066, validation loss: 0.3014
2024-06-03 19:29:22 [INFO]: Epoch 005 - training loss: 0.2888, validation loss: 0.2663
2024-06-03 19:29:49 [INFO]: Epoch 006 - training loss: 0.2725, validation loss: 0.2586
2024-06-03 19:30:15 [INFO]: Epoch 007 - training loss: 0.2527, validation loss: 0.2300
2024-06-03 19:30:41 [INFO]: Epoch 008 - training loss: 0.2661, validation loss: 0.2140
2024-06-03 19:31:07 [INFO]: Epoch 009 - training loss: 0.2277, validation loss: 0.2041
2024-06-03 19:31:33 [INFO]: Epoch 010 - training loss: 0.2211, validation loss: 0.2066
2024-06-03 19:31:59 [INFO]: Epoch 011 - training loss: 0.2156, validation loss: 0.1931
2024-06-03 19:32:25 [INFO]: Epoch 012 - training loss: 0.2053, validation loss: 0.1921
2024-06-03 19:32:51 [INFO]: Epoch 013 - training loss: 0.2048, validation loss: 0.1830
2024-06-03 19:33:17 [INFO]: Epoch 014 - training loss: 0.2007, validation loss: 0.1778
2024-06-03 19:33:43 [INFO]: Epoch 015 - training loss: 0.2075, validation loss: 0.1802
2024-06-03 19:34:09 [INFO]: Epoch 016 - training loss: 0.2031, validation loss: 0.1797
2024-06-03 19:34:35 [INFO]: Epoch 017 - training loss: 0.2147, validation loss: 0.1704
2024-06-03 19:35:01 [INFO]: Epoch 018 - training loss: 0.1997, validation loss: 0.1741
2024-06-03 19:35:27 [INFO]: Epoch 019 - training loss: 0.2012, validation loss: 0.1697
2024-06-03 19:35:53 [INFO]: Epoch 020 - training loss: 0.1940, validation loss: 0.1649
2024-06-03 19:36:20 [INFO]: Epoch 021 - training loss: 0.1938, validation loss: 0.1647
2024-06-03 19:36:46 [INFO]: Epoch 022 - training loss: 0.1838, validation loss: 0.1624
2024-06-03 19:37:12 [INFO]: Epoch 023 - training loss: 0.1885, validation loss: 0.1594
2024-06-03 19:37:38 [INFO]: Epoch 024 - training loss: 0.2001, validation loss: 0.1600
2024-06-03 19:38:04 [INFO]: Epoch 025 - training loss: 0.1901, validation loss: 0.1567
2024-06-03 19:38:30 [INFO]: Epoch 026 - training loss: 0.1951, validation loss: 0.1555
2024-06-03 19:38:56 [INFO]: Epoch 027 - training loss: 0.1914, validation loss: 0.1692
2024-06-03 19:39:22 [INFO]: Epoch 028 - training loss: 0.1856, validation loss: 0.1525
2024-06-03 19:39:48 [INFO]: Epoch 029 - training loss: 0.1770, validation loss: 0.1593
2024-06-03 19:40:14 [INFO]: Epoch 030 - training loss: 0.2087, validation loss: 0.1737
2024-06-03 19:40:40 [INFO]: Epoch 031 - training loss: 0.1929, validation loss: 0.1583
2024-06-03 19:41:06 [INFO]: Epoch 032 - training loss: 0.1721, validation loss: 0.1515
2024-06-03 19:41:32 [INFO]: Epoch 033 - training loss: 0.1602, validation loss: 0.1478
2024-06-03 19:41:58 [INFO]: Epoch 034 - training loss: 0.1705, validation loss: 0.1472
2024-06-03 19:42:24 [INFO]: Epoch 035 - training loss: 0.1763, validation loss: 0.1530
2024-06-03 19:42:51 [INFO]: Epoch 036 - training loss: 0.1761, validation loss: 0.1475
2024-06-03 19:43:17 [INFO]: Epoch 037 - training loss: 0.1756, validation loss: 0.1484
2024-06-03 19:43:43 [INFO]: Epoch 038 - training loss: 0.1759, validation loss: 0.1495
2024-06-03 19:44:09 [INFO]: Epoch 039 - training loss: 0.1781, validation loss: 0.1484
2024-06-03 19:44:35 [INFO]: Epoch 040 - training loss: 0.1794, validation loss: 0.1501
2024-06-03 19:45:01 [INFO]: Epoch 041 - training loss: 0.1810, validation loss: 0.1478
2024-06-03 19:45:27 [INFO]: Epoch 042 - training loss: 0.1872, validation loss: 0.1448
2024-06-03 19:45:53 [INFO]: Epoch 043 - training loss: 0.1863, validation loss: 0.1455
2024-06-03 19:46:19 [INFO]: Epoch 044 - training loss: 0.1553, validation loss: 0.1401
2024-06-03 19:46:45 [INFO]: Epoch 045 - training loss: 0.1739, validation loss: 0.1428
2024-06-03 19:47:11 [INFO]: Epoch 046 - training loss: 0.1519, validation loss: 0.1406
2024-06-03 19:47:37 [INFO]: Epoch 047 - training loss: 0.1558, validation loss: 0.1400
2024-06-03 19:48:03 [INFO]: Epoch 048 - training loss: 0.1597, validation loss: 0.1375
2024-06-03 19:48:29 [INFO]: Epoch 049 - training loss: 0.1711, validation loss: 0.1382
2024-06-03 19:48:56 [INFO]: Epoch 050 - training loss: 0.1772, validation loss: 0.1490
2024-06-03 19:49:22 [INFO]: Epoch 051 - training loss: 0.1600, validation loss: 0.1369
2024-06-03 19:49:48 [INFO]: Epoch 052 - training loss: 0.1742, validation loss: 0.1378
2024-06-03 19:50:14 [INFO]: Epoch 053 - training loss: 0.1582, validation loss: 0.1361
2024-06-03 19:50:40 [INFO]: Epoch 054 - training loss: 0.1780, validation loss: 0.1358
2024-06-03 19:51:06 [INFO]: Epoch 055 - training loss: 0.1804, validation loss: 0.1380
2024-06-03 19:51:32 [INFO]: Epoch 056 - training loss: 0.1661, validation loss: 0.1343
2024-06-03 19:51:58 [INFO]: Epoch 057 - training loss: 0.1522, validation loss: 0.1360
2024-06-03 19:52:24 [INFO]: Epoch 058 - training loss: 0.1555, validation loss: 0.1342
2024-06-03 19:52:50 [INFO]: Epoch 059 - training loss: 0.1681, validation loss: 0.1319
2024-06-03 19:53:16 [INFO]: Epoch 060 - training loss: 0.1544, validation loss: 0.1345
2024-06-03 19:53:42 [INFO]: Epoch 061 - training loss: 0.1635, validation loss: 0.1331
2024-06-03 19:54:08 [INFO]: Epoch 062 - training loss: 0.1723, validation loss: 0.1424
2024-06-03 19:54:34 [INFO]: Epoch 063 - training loss: 0.1518, validation loss: 0.1354
2024-06-03 19:55:00 [INFO]: Epoch 064 - training loss: 0.1475, validation loss: 0.1330
2024-06-03 19:55:26 [INFO]: Epoch 065 - training loss: 0.1426, validation loss: 0.1281
2024-06-03 19:55:53 [INFO]: Epoch 066 - training loss: 0.1398, validation loss: 0.1308
2024-06-03 19:56:19 [INFO]: Epoch 067 - training loss: 0.1555, validation loss: 0.1315
2024-06-03 19:56:45 [INFO]: Epoch 068 - training loss: 0.1627, validation loss: 0.1328
2024-06-03 19:57:11 [INFO]: Epoch 069 - training loss: 0.1579, validation loss: 0.1288
2024-06-03 19:57:37 [INFO]: Epoch 070 - training loss: 0.1582, validation loss: 0.1279
2024-06-03 19:58:03 [INFO]: Epoch 071 - training loss: 0.1600, validation loss: 0.1307
2024-06-03 19:58:29 [INFO]: Epoch 072 - training loss: 0.1513, validation loss: 0.1299
2024-06-03 19:58:55 [INFO]: Epoch 073 - training loss: 0.1492, validation loss: 0.1297
2024-06-03 19:59:21 [INFO]: Epoch 074 - training loss: 0.1675, validation loss: 0.1308
2024-06-03 19:59:47 [INFO]: Epoch 075 - training loss: 0.1731, validation loss: 0.1346
2024-06-03 20:00:13 [INFO]: Epoch 076 - training loss: 0.1683, validation loss: 0.1342
2024-06-03 20:00:39 [INFO]: Epoch 077 - training loss: 0.1600, validation loss: 0.1282
2024-06-03 20:01:05 [INFO]: Epoch 078 - training loss: 0.1648, validation loss: 0.1270
2024-06-03 20:01:31 [INFO]: Epoch 079 - training loss: 0.1500, validation loss: 0.1283
2024-06-03 20:01:58 [INFO]: Epoch 080 - training loss: 0.1547, validation loss: 0.1246
2024-06-03 20:02:24 [INFO]: Epoch 081 - training loss: 0.1509, validation loss: 0.1291
2024-06-03 20:02:50 [INFO]: Epoch 082 - training loss: 0.1391, validation loss: 0.1247
2024-06-03 20:03:16 [INFO]: Epoch 083 - training loss: 0.1702, validation loss: 0.1266
2024-06-03 20:03:42 [INFO]: Epoch 084 - training loss: 0.1505, validation loss: 0.1262
2024-06-03 20:04:08 [INFO]: Epoch 085 - training loss: 0.1544, validation loss: 0.1262
2024-06-03 20:04:34 [INFO]: Epoch 086 - training loss: 0.1471, validation loss: 0.1264
2024-06-03 20:05:00 [INFO]: Epoch 087 - training loss: 0.1607, validation loss: 0.1254
2024-06-03 20:05:26 [INFO]: Epoch 088 - training loss: 0.1342, validation loss: 0.1257
2024-06-03 20:05:52 [INFO]: Epoch 089 - training loss: 0.1511, validation loss: 0.1283
2024-06-03 20:06:18 [INFO]: Epoch 090 - training loss: 0.1554, validation loss: 0.1264
2024-06-03 20:06:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 20:06:18 [INFO]: Finished training. The best model is from epoch#80.
2024-06-03 20:06:18 [INFO]: Saved the model to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_4/20240603_T192712/CSDI.pypots
2024-06-03 20:23:39 [INFO]: Successfully saved to results_block_rate05/BeijingAir/CSDI_BeijingAir/round_4/imputation.pkl
2024-06-03 20:23:39 [INFO]: Round4 - CSDI on BeijingAir: MAE=0.1838, MSE=0.2708, MRE=0.2485
2024-06-03 20:23:39 [INFO]: Done! Final results:
Averaged CSDI (244,833 params) on BeijingAir: MAE=0.1810 ± 0.025348625255299267, MSE=0.3985 ± 0.17186809212079865, MRE=0.2382 ± 0.03336569668746439, average inference time=283.28