2024-06-03 13:05:13 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 13:05:13 [INFO]: Using the given device: cuda:0
2024-06-03 13:05:13 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_0/20240603_T130513
2024-06-03 13:05:13 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_0/20240603_T130513/tensorboard
2024-06-03 13:05:15 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 6,700,164
2024-06-03 13:05:29 [INFO]: Epoch 001 - training loss: 1.6999, validation loss: 1.3357
2024-06-03 13:05:38 [INFO]: Epoch 002 - training loss: 1.6511, validation loss: 1.2636
2024-06-03 13:05:46 [INFO]: Epoch 003 - training loss: 1.6108, validation loss: 1.2059
2024-06-03 13:05:54 [INFO]: Epoch 004 - training loss: 1.5753, validation loss: 1.1622
2024-06-03 13:06:03 [INFO]: Epoch 005 - training loss: 1.5463, validation loss: 1.1319
2024-06-03 13:06:11 [INFO]: Epoch 006 - training loss: 1.5190, validation loss: 1.1116
2024-06-03 13:06:19 [INFO]: Epoch 007 - training loss: 1.5008, validation loss: 1.1005
2024-06-03 13:06:28 [INFO]: Epoch 008 - training loss: 1.4816, validation loss: 1.0941
2024-06-03 13:06:36 [INFO]: Epoch 009 - training loss: 1.4673, validation loss: 1.0885
2024-06-03 13:06:44 [INFO]: Epoch 010 - training loss: 1.4533, validation loss: 1.0846
2024-06-03 13:06:53 [INFO]: Epoch 011 - training loss: 1.4417, validation loss: 1.0814
2024-06-03 13:07:01 [INFO]: Epoch 012 - training loss: 1.4262, validation loss: 1.0785
2024-06-03 13:07:09 [INFO]: Epoch 013 - training loss: 1.4207, validation loss: 1.0754
2024-06-03 13:07:18 [INFO]: Epoch 014 - training loss: 1.4102, validation loss: 1.0715
2024-06-03 13:07:26 [INFO]: Epoch 015 - training loss: 1.4014, validation loss: 1.0686
2024-06-03 13:07:34 [INFO]: Epoch 016 - training loss: 1.3928, validation loss: 1.0653
2024-06-03 13:07:43 [INFO]: Epoch 017 - training loss: 1.3866, validation loss: 1.0619
2024-06-03 13:07:51 [INFO]: Epoch 018 - training loss: 1.3783, validation loss: 1.0596
2024-06-03 13:07:59 [INFO]: Epoch 019 - training loss: 1.3706, validation loss: 1.0568
2024-06-03 13:08:07 [INFO]: Epoch 020 - training loss: 1.3634, validation loss: 1.0551
2024-06-03 13:08:15 [INFO]: Epoch 021 - training loss: 1.3578, validation loss: 1.0527
2024-06-03 13:08:23 [INFO]: Epoch 022 - training loss: 1.3528, validation loss: 1.0509
2024-06-03 13:08:32 [INFO]: Epoch 023 - training loss: 1.3495, validation loss: 1.0490
2024-06-03 13:08:41 [INFO]: Epoch 024 - training loss: 1.3439, validation loss: 1.0473
2024-06-03 13:08:50 [INFO]: Epoch 025 - training loss: 1.3383, validation loss: 1.0459
2024-06-03 13:08:58 [INFO]: Epoch 026 - training loss: 1.3356, validation loss: 1.0447
2024-06-03 13:09:07 [INFO]: Epoch 027 - training loss: 1.3315, validation loss: 1.0431
2024-06-03 13:09:16 [INFO]: Epoch 028 - training loss: 1.3281, validation loss: 1.0421
2024-06-03 13:09:24 [INFO]: Epoch 029 - training loss: 1.3236, validation loss: 1.0408
2024-06-03 13:09:33 [INFO]: Epoch 030 - training loss: 1.3201, validation loss: 1.0397
2024-06-03 13:09:41 [INFO]: Epoch 031 - training loss: 1.3126, validation loss: 1.0383
2024-06-03 13:09:49 [INFO]: Epoch 032 - training loss: 1.3100, validation loss: 1.0381
2024-06-03 13:09:58 [INFO]: Epoch 033 - training loss: 1.3068, validation loss: 1.0370
2024-06-03 13:10:06 [INFO]: Epoch 034 - training loss: 1.3049, validation loss: 1.0358
2024-06-03 13:10:14 [INFO]: Epoch 035 - training loss: 1.3027, validation loss: 1.0353
2024-06-03 13:10:23 [INFO]: Epoch 036 - training loss: 1.2985, validation loss: 1.0345
2024-06-03 13:10:32 [INFO]: Epoch 037 - training loss: 1.2948, validation loss: 1.0334
2024-06-03 13:10:40 [INFO]: Epoch 038 - training loss: 1.2936, validation loss: 1.0332
2024-06-03 13:10:48 [INFO]: Epoch 039 - training loss: 1.2938, validation loss: 1.0327
2024-06-03 13:10:57 [INFO]: Epoch 040 - training loss: 1.2884, validation loss: 1.0321
2024-06-03 13:11:05 [INFO]: Epoch 041 - training loss: 1.2850, validation loss: 1.0316
2024-06-03 13:11:14 [INFO]: Epoch 042 - training loss: 1.2840, validation loss: 1.0312
2024-06-03 13:11:23 [INFO]: Epoch 043 - training loss: 1.2815, validation loss: 1.0311
2024-06-03 13:11:32 [INFO]: Epoch 044 - training loss: 1.2773, validation loss: 1.0306
2024-06-03 13:11:41 [INFO]: Epoch 045 - training loss: 1.2769, validation loss: 1.0304
2024-06-03 13:11:49 [INFO]: Epoch 046 - training loss: 1.2754, validation loss: 1.0298
2024-06-03 13:11:57 [INFO]: Epoch 047 - training loss: 1.2753, validation loss: 1.0292
2024-06-03 13:12:05 [INFO]: Epoch 048 - training loss: 1.2696, validation loss: 1.0290
2024-06-03 13:12:13 [INFO]: Epoch 049 - training loss: 1.2706, validation loss: 1.0290
2024-06-03 13:12:21 [INFO]: Epoch 050 - training loss: 1.2693, validation loss: 1.0287
2024-06-03 13:12:28 [INFO]: Epoch 051 - training loss: 1.2709, validation loss: 1.0286
2024-06-03 13:12:36 [INFO]: Epoch 052 - training loss: 1.2678, validation loss: 1.0283
2024-06-03 13:12:44 [INFO]: Epoch 053 - training loss: 1.2645, validation loss: 1.0282
2024-06-03 13:12:52 [INFO]: Epoch 054 - training loss: 1.2636, validation loss: 1.0273
2024-06-03 13:13:00 [INFO]: Epoch 055 - training loss: 1.2629, validation loss: 1.0273
2024-06-03 13:13:07 [INFO]: Epoch 056 - training loss: 1.2621, validation loss: 1.0272
2024-06-03 13:13:15 [INFO]: Epoch 057 - training loss: 1.2570, validation loss: 1.0272
2024-06-03 13:13:23 [INFO]: Epoch 058 - training loss: 1.2570, validation loss: 1.0272
2024-06-03 13:13:31 [INFO]: Epoch 059 - training loss: 1.2603, validation loss: 1.0272
2024-06-03 13:13:39 [INFO]: Epoch 060 - training loss: 1.2576, validation loss: 1.0272
2024-06-03 13:13:46 [INFO]: Epoch 061 - training loss: 1.2538, validation loss: 1.0272
2024-06-03 13:13:54 [INFO]: Epoch 062 - training loss: 1.2543, validation loss: 1.0270
2024-06-03 13:14:02 [INFO]: Epoch 063 - training loss: 1.2542, validation loss: 1.0268
2024-06-03 13:14:09 [INFO]: Epoch 064 - training loss: 1.2506, validation loss: 1.0266
2024-06-03 13:14:17 [INFO]: Epoch 065 - training loss: 1.2514, validation loss: 1.0267
2024-06-03 13:14:25 [INFO]: Epoch 066 - training loss: 1.2514, validation loss: 1.0269
2024-06-03 13:14:33 [INFO]: Epoch 067 - training loss: 1.2540, validation loss: 1.0270
2024-06-03 13:14:40 [INFO]: Epoch 068 - training loss: 1.2494, validation loss: 1.0267
2024-06-03 13:14:48 [INFO]: Epoch 069 - training loss: 1.2463, validation loss: 1.0271
2024-06-03 13:14:55 [INFO]: Epoch 070 - training loss: 1.2467, validation loss: 1.0274
2024-06-03 13:15:03 [INFO]: Epoch 071 - training loss: 1.2462, validation loss: 1.0271
2024-06-03 13:15:11 [INFO]: Epoch 072 - training loss: 1.2461, validation loss: 1.0273
2024-06-03 13:15:19 [INFO]: Epoch 073 - training loss: 1.2418, validation loss: 1.0268
2024-06-03 13:15:26 [INFO]: Epoch 074 - training loss: 1.2423, validation loss: 1.0273
2024-06-03 13:15:26 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 13:15:26 [INFO]: Finished training. The best model is from epoch#64.
2024-06-03 13:15:26 [INFO]: Saved the model to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_0/20240603_T130513/Autoformer.pypots
2024-06-03 13:15:28 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_0/imputation.pkl
2024-06-03 13:15:28 [INFO]: Round0 - Autoformer on BeijingAir: MAE=0.6816, MSE=1.0736, MRE=0.9214
2024-06-03 13:15:28 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 13:15:28 [INFO]: Using the given device: cuda:0
2024-06-03 13:15:28 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_1/20240603_T131528
2024-06-03 13:15:28 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_1/20240603_T131528/tensorboard
2024-06-03 13:15:28 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 6,700,164
2024-06-03 13:15:36 [INFO]: Epoch 001 - training loss: 1.7023, validation loss: 1.3389
2024-06-03 13:15:44 [INFO]: Epoch 002 - training loss: 1.6543, validation loss: 1.2659
2024-06-03 13:15:52 [INFO]: Epoch 003 - training loss: 1.6139, validation loss: 1.2085
2024-06-03 13:16:00 [INFO]: Epoch 004 - training loss: 1.5760, validation loss: 1.1622
2024-06-03 13:16:07 [INFO]: Epoch 005 - training loss: 1.5470, validation loss: 1.1295
2024-06-03 13:16:15 [INFO]: Epoch 006 - training loss: 1.5179, validation loss: 1.1091
2024-06-03 13:16:23 [INFO]: Epoch 007 - training loss: 1.4961, validation loss: 1.0985
2024-06-03 13:16:31 [INFO]: Epoch 008 - training loss: 1.4792, validation loss: 1.0927
2024-06-03 13:16:39 [INFO]: Epoch 009 - training loss: 1.4647, validation loss: 1.0896
2024-06-03 13:16:46 [INFO]: Epoch 010 - training loss: 1.4528, validation loss: 1.0867
2024-06-03 13:16:54 [INFO]: Epoch 011 - training loss: 1.4418, validation loss: 1.0840
2024-06-03 13:17:01 [INFO]: Epoch 012 - training loss: 1.4291, validation loss: 1.0809
2024-06-03 13:17:09 [INFO]: Epoch 013 - training loss: 1.4206, validation loss: 1.0775
2024-06-03 13:17:17 [INFO]: Epoch 014 - training loss: 1.4091, validation loss: 1.0754
2024-06-03 13:17:24 [INFO]: Epoch 015 - training loss: 1.3992, validation loss: 1.0717
2024-06-03 13:17:32 [INFO]: Epoch 016 - training loss: 1.3922, validation loss: 1.0690
2024-06-03 13:17:40 [INFO]: Epoch 017 - training loss: 1.3866, validation loss: 1.0655
2024-06-03 13:17:47 [INFO]: Epoch 018 - training loss: 1.3770, validation loss: 1.0626
2024-06-03 13:17:55 [INFO]: Epoch 019 - training loss: 1.3716, validation loss: 1.0601
2024-06-03 13:18:03 [INFO]: Epoch 020 - training loss: 1.3664, validation loss: 1.0581
2024-06-03 13:18:11 [INFO]: Epoch 021 - training loss: 1.3588, validation loss: 1.0554
2024-06-03 13:18:19 [INFO]: Epoch 022 - training loss: 1.3540, validation loss: 1.0533
2024-06-03 13:18:27 [INFO]: Epoch 023 - training loss: 1.3485, validation loss: 1.0515
2024-06-03 13:18:35 [INFO]: Epoch 024 - training loss: 1.3407, validation loss: 1.0497
2024-06-03 13:18:42 [INFO]: Epoch 025 - training loss: 1.3368, validation loss: 1.0479
2024-06-03 13:18:50 [INFO]: Epoch 026 - training loss: 1.3326, validation loss: 1.0469
2024-06-03 13:18:58 [INFO]: Epoch 027 - training loss: 1.3249, validation loss: 1.0452
2024-06-03 13:19:06 [INFO]: Epoch 028 - training loss: 1.3250, validation loss: 1.0435
2024-06-03 13:19:14 [INFO]: Epoch 029 - training loss: 1.3175, validation loss: 1.0422
2024-06-03 13:19:22 [INFO]: Epoch 030 - training loss: 1.3178, validation loss: 1.0409
2024-06-03 13:19:29 [INFO]: Epoch 031 - training loss: 1.3097, validation loss: 1.0396
2024-06-03 13:19:36 [INFO]: Epoch 032 - training loss: 1.3046, validation loss: 1.0387
2024-06-03 13:19:43 [INFO]: Epoch 033 - training loss: 1.3021, validation loss: 1.0379
2024-06-03 13:19:50 [INFO]: Epoch 034 - training loss: 1.3004, validation loss: 1.0370
2024-06-03 13:19:57 [INFO]: Epoch 035 - training loss: 1.2981, validation loss: 1.0362
2024-06-03 13:20:03 [INFO]: Epoch 036 - training loss: 1.2960, validation loss: 1.0356
2024-06-03 13:20:10 [INFO]: Epoch 037 - training loss: 1.2935, validation loss: 1.0348
2024-06-03 13:20:17 [INFO]: Epoch 038 - training loss: 1.2908, validation loss: 1.0340
2024-06-03 13:20:24 [INFO]: Epoch 039 - training loss: 1.2883, validation loss: 1.0334
2024-06-03 13:20:32 [INFO]: Epoch 040 - training loss: 1.2853, validation loss: 1.0330
2024-06-03 13:20:39 [INFO]: Epoch 041 - training loss: 1.2829, validation loss: 1.0321
2024-06-03 13:20:46 [INFO]: Epoch 042 - training loss: 1.2822, validation loss: 1.0319
2024-06-03 13:20:53 [INFO]: Epoch 043 - training loss: 1.2795, validation loss: 1.0317
2024-06-03 13:21:01 [INFO]: Epoch 044 - training loss: 1.2775, validation loss: 1.0313
2024-06-03 13:21:08 [INFO]: Epoch 045 - training loss: 1.2793, validation loss: 1.0308
2024-06-03 13:21:14 [INFO]: Epoch 046 - training loss: 1.2737, validation loss: 1.0301
2024-06-03 13:21:21 [INFO]: Epoch 047 - training loss: 1.2739, validation loss: 1.0296
2024-06-03 13:21:28 [INFO]: Epoch 048 - training loss: 1.2722, validation loss: 1.0295
2024-06-03 13:21:35 [INFO]: Epoch 049 - training loss: 1.2684, validation loss: 1.0292
2024-06-03 13:21:42 [INFO]: Epoch 050 - training loss: 1.2694, validation loss: 1.0288
2024-06-03 13:21:49 [INFO]: Epoch 051 - training loss: 1.2655, validation loss: 1.0286
2024-06-03 13:21:55 [INFO]: Epoch 052 - training loss: 1.2657, validation loss: 1.0285
2024-06-03 13:22:01 [INFO]: Epoch 053 - training loss: 1.2643, validation loss: 1.0286
2024-06-03 13:22:07 [INFO]: Epoch 054 - training loss: 1.2628, validation loss: 1.0280
2024-06-03 13:22:13 [INFO]: Epoch 055 - training loss: 1.2625, validation loss: 1.0281
2024-06-03 13:22:19 [INFO]: Epoch 056 - training loss: 1.2614, validation loss: 1.0276
2024-06-03 13:22:25 [INFO]: Epoch 057 - training loss: 1.2576, validation loss: 1.0276
2024-06-03 13:22:31 [INFO]: Epoch 058 - training loss: 1.2573, validation loss: 1.0276
2024-06-03 13:22:37 [INFO]: Epoch 059 - training loss: 1.2576, validation loss: 1.0271
2024-06-03 13:22:43 [INFO]: Epoch 060 - training loss: 1.2565, validation loss: 1.0268
2024-06-03 13:22:49 [INFO]: Epoch 061 - training loss: 1.2554, validation loss: 1.0268
2024-06-03 13:22:55 [INFO]: Epoch 062 - training loss: 1.2533, validation loss: 1.0268
2024-06-03 13:23:01 [INFO]: Epoch 063 - training loss: 1.2541, validation loss: 1.0271
2024-06-03 13:23:07 [INFO]: Epoch 064 - training loss: 1.2509, validation loss: 1.0267
2024-06-03 13:23:13 [INFO]: Epoch 065 - training loss: 1.2513, validation loss: 1.0264
2024-06-03 13:23:19 [INFO]: Epoch 066 - training loss: 1.2494, validation loss: 1.0269
2024-06-03 13:23:24 [INFO]: Epoch 067 - training loss: 1.2508, validation loss: 1.0264
2024-06-03 13:23:30 [INFO]: Epoch 068 - training loss: 1.2479, validation loss: 1.0266
2024-06-03 13:23:36 [INFO]: Epoch 069 - training loss: 1.2469, validation loss: 1.0264
2024-06-03 13:23:42 [INFO]: Epoch 070 - training loss: 1.2457, validation loss: 1.0266
2024-06-03 13:23:48 [INFO]: Epoch 071 - training loss: 1.2469, validation loss: 1.0263
2024-06-03 13:23:54 [INFO]: Epoch 072 - training loss: 1.2421, validation loss: 1.0268
2024-06-03 13:24:00 [INFO]: Epoch 073 - training loss: 1.2432, validation loss: 1.0268
2024-06-03 13:24:06 [INFO]: Epoch 074 - training loss: 1.2418, validation loss: 1.0266
2024-06-03 13:24:12 [INFO]: Epoch 075 - training loss: 1.2409, validation loss: 1.0270
2024-06-03 13:24:18 [INFO]: Epoch 076 - training loss: 1.2416, validation loss: 1.0271
2024-06-03 13:24:24 [INFO]: Epoch 077 - training loss: 1.2421, validation loss: 1.0272
2024-06-03 13:24:30 [INFO]: Epoch 078 - training loss: 1.2384, validation loss: 1.0271
2024-06-03 13:24:36 [INFO]: Epoch 079 - training loss: 1.2417, validation loss: 1.0269
2024-06-03 13:24:42 [INFO]: Epoch 080 - training loss: 1.2416, validation loss: 1.0272
2024-06-03 13:24:48 [INFO]: Epoch 081 - training loss: 1.2392, validation loss: 1.0275
2024-06-03 13:24:48 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 13:24:48 [INFO]: Finished training. The best model is from epoch#71.
2024-06-03 13:24:48 [INFO]: Saved the model to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_1/20240603_T131528/Autoformer.pypots
2024-06-03 13:24:49 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_1/imputation.pkl
2024-06-03 13:24:49 [INFO]: Round1 - Autoformer on BeijingAir: MAE=0.6810, MSE=1.0749, MRE=0.9206
2024-06-03 13:24:49 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 13:24:49 [INFO]: Using the given device: cuda:0
2024-06-03 13:24:49 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_2/20240603_T132449
2024-06-03 13:24:49 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_2/20240603_T132449/tensorboard
2024-06-03 13:24:49 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 6,700,164
2024-06-03 13:24:55 [INFO]: Epoch 001 - training loss: 1.7006, validation loss: 1.3356
2024-06-03 13:25:00 [INFO]: Epoch 002 - training loss: 1.6552, validation loss: 1.2651
2024-06-03 13:25:06 [INFO]: Epoch 003 - training loss: 1.6161, validation loss: 1.2091
2024-06-03 13:25:12 [INFO]: Epoch 004 - training loss: 1.5785, validation loss: 1.1665
2024-06-03 13:25:18 [INFO]: Epoch 005 - training loss: 1.5492, validation loss: 1.1343
2024-06-03 13:25:24 [INFO]: Epoch 006 - training loss: 1.5209, validation loss: 1.1146
2024-06-03 13:25:30 [INFO]: Epoch 007 - training loss: 1.5008, validation loss: 1.1025
2024-06-03 13:25:36 [INFO]: Epoch 008 - training loss: 1.4797, validation loss: 1.0951
2024-06-03 13:25:42 [INFO]: Epoch 009 - training loss: 1.4687, validation loss: 1.0904
2024-06-03 13:25:48 [INFO]: Epoch 010 - training loss: 1.4514, validation loss: 1.0868
2024-06-03 13:25:54 [INFO]: Epoch 011 - training loss: 1.4384, validation loss: 1.0838
2024-06-03 13:26:00 [INFO]: Epoch 012 - training loss: 1.4291, validation loss: 1.0811
2024-06-03 13:26:06 [INFO]: Epoch 013 - training loss: 1.4136, validation loss: 1.0783
2024-06-03 13:26:12 [INFO]: Epoch 014 - training loss: 1.4064, validation loss: 1.0753
2024-06-03 13:26:17 [INFO]: Epoch 015 - training loss: 1.3959, validation loss: 1.0723
2024-06-03 13:26:23 [INFO]: Epoch 016 - training loss: 1.3903, validation loss: 1.0692
2024-06-03 13:26:29 [INFO]: Epoch 017 - training loss: 1.3836, validation loss: 1.0660
2024-06-03 13:26:35 [INFO]: Epoch 018 - training loss: 1.3786, validation loss: 1.0637
2024-06-03 13:26:41 [INFO]: Epoch 019 - training loss: 1.3701, validation loss: 1.0615
2024-06-03 13:26:47 [INFO]: Epoch 020 - training loss: 1.3627, validation loss: 1.0592
2024-06-03 13:26:53 [INFO]: Epoch 021 - training loss: 1.3600, validation loss: 1.0575
2024-06-03 13:26:59 [INFO]: Epoch 022 - training loss: 1.3518, validation loss: 1.0556
2024-06-03 13:27:04 [INFO]: Epoch 023 - training loss: 1.3467, validation loss: 1.0540
2024-06-03 13:27:10 [INFO]: Epoch 024 - training loss: 1.3449, validation loss: 1.0525
2024-06-03 13:27:16 [INFO]: Epoch 025 - training loss: 1.3388, validation loss: 1.0508
2024-06-03 13:27:22 [INFO]: Epoch 026 - training loss: 1.3350, validation loss: 1.0494
2024-06-03 13:27:28 [INFO]: Epoch 027 - training loss: 1.3276, validation loss: 1.0483
2024-06-03 13:27:34 [INFO]: Epoch 028 - training loss: 1.3236, validation loss: 1.0475
2024-06-03 13:27:40 [INFO]: Epoch 029 - training loss: 1.3206, validation loss: 1.0457
2024-06-03 13:27:46 [INFO]: Epoch 030 - training loss: 1.3173, validation loss: 1.0449
2024-06-03 13:27:52 [INFO]: Epoch 031 - training loss: 1.3131, validation loss: 1.0441
2024-06-03 13:27:58 [INFO]: Epoch 032 - training loss: 1.3117, validation loss: 1.0435
2024-06-03 13:28:04 [INFO]: Epoch 033 - training loss: 1.3061, validation loss: 1.0423
2024-06-03 13:28:10 [INFO]: Epoch 034 - training loss: 1.3042, validation loss: 1.0416
2024-06-03 13:28:16 [INFO]: Epoch 035 - training loss: 1.3014, validation loss: 1.0410
2024-06-03 13:28:22 [INFO]: Epoch 036 - training loss: 1.2948, validation loss: 1.0399
2024-06-03 13:28:28 [INFO]: Epoch 037 - training loss: 1.2921, validation loss: 1.0394
2024-06-03 13:28:34 [INFO]: Epoch 038 - training loss: 1.2930, validation loss: 1.0382
2024-06-03 13:28:40 [INFO]: Epoch 039 - training loss: 1.2904, validation loss: 1.0378
2024-06-03 13:28:46 [INFO]: Epoch 040 - training loss: 1.2879, validation loss: 1.0374
2024-06-03 13:28:52 [INFO]: Epoch 041 - training loss: 1.2878, validation loss: 1.0367
2024-06-03 13:28:58 [INFO]: Epoch 042 - training loss: 1.2818, validation loss: 1.0363
2024-06-03 13:29:04 [INFO]: Epoch 043 - training loss: 1.2818, validation loss: 1.0361
2024-06-03 13:29:10 [INFO]: Epoch 044 - training loss: 1.2825, validation loss: 1.0354
2024-06-03 13:29:16 [INFO]: Epoch 045 - training loss: 1.2786, validation loss: 1.0352
2024-06-03 13:29:22 [INFO]: Epoch 046 - training loss: 1.2732, validation loss: 1.0351
2024-06-03 13:29:28 [INFO]: Epoch 047 - training loss: 1.2763, validation loss: 1.0349
2024-06-03 13:29:34 [INFO]: Epoch 048 - training loss: 1.2741, validation loss: 1.0343
2024-06-03 13:29:39 [INFO]: Epoch 049 - training loss: 1.2698, validation loss: 1.0343
2024-06-03 13:29:45 [INFO]: Epoch 050 - training loss: 1.2715, validation loss: 1.0338
2024-06-03 13:29:51 [INFO]: Epoch 051 - training loss: 1.2669, validation loss: 1.0336
2024-06-03 13:29:57 [INFO]: Epoch 052 - training loss: 1.2674, validation loss: 1.0339
2024-06-03 13:30:03 [INFO]: Epoch 053 - training loss: 1.2655, validation loss: 1.0336
2024-06-03 13:30:09 [INFO]: Epoch 054 - training loss: 1.2623, validation loss: 1.0333
2024-06-03 13:30:15 [INFO]: Epoch 055 - training loss: 1.2632, validation loss: 1.0331
2024-06-03 13:30:21 [INFO]: Epoch 056 - training loss: 1.2636, validation loss: 1.0333
2024-06-03 13:30:27 [INFO]: Epoch 057 - training loss: 1.2600, validation loss: 1.0335
2024-06-03 13:30:33 [INFO]: Epoch 058 - training loss: 1.2603, validation loss: 1.0332
2024-06-03 13:30:39 [INFO]: Epoch 059 - training loss: 1.2572, validation loss: 1.0330
2024-06-03 13:30:44 [INFO]: Epoch 060 - training loss: 1.2557, validation loss: 1.0332
2024-06-03 13:30:50 [INFO]: Epoch 061 - training loss: 1.2563, validation loss: 1.0331
2024-06-03 13:30:56 [INFO]: Epoch 062 - training loss: 1.2546, validation loss: 1.0333
2024-06-03 13:31:02 [INFO]: Epoch 063 - training loss: 1.2508, validation loss: 1.0328
2024-06-03 13:31:08 [INFO]: Epoch 064 - training loss: 1.2519, validation loss: 1.0331
2024-06-03 13:31:14 [INFO]: Epoch 065 - training loss: 1.2523, validation loss: 1.0327
2024-06-03 13:31:20 [INFO]: Epoch 066 - training loss: 1.2550, validation loss: 1.0332
2024-06-03 13:31:26 [INFO]: Epoch 067 - training loss: 1.2506, validation loss: 1.0329
2024-06-03 13:31:31 [INFO]: Epoch 068 - training loss: 1.2523, validation loss: 1.0335
2024-06-03 13:31:38 [INFO]: Epoch 069 - training loss: 1.2449, validation loss: 1.0335
2024-06-03 13:31:44 [INFO]: Epoch 070 - training loss: 1.2484, validation loss: 1.0332
2024-06-03 13:31:50 [INFO]: Epoch 071 - training loss: 1.2478, validation loss: 1.0336
2024-06-03 13:31:56 [INFO]: Epoch 072 - training loss: 1.2443, validation loss: 1.0333
2024-06-03 13:32:02 [INFO]: Epoch 073 - training loss: 1.2437, validation loss: 1.0333
2024-06-03 13:32:08 [INFO]: Epoch 074 - training loss: 1.2437, validation loss: 1.0334
2024-06-03 13:32:14 [INFO]: Epoch 075 - training loss: 1.2432, validation loss: 1.0333
2024-06-03 13:32:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 13:32:14 [INFO]: Finished training. The best model is from epoch#65.
2024-06-03 13:32:14 [INFO]: Saved the model to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_2/20240603_T132449/Autoformer.pypots
2024-06-03 13:32:15 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_2/imputation.pkl
2024-06-03 13:32:15 [INFO]: Round2 - Autoformer on BeijingAir: MAE=0.6840, MSE=1.0797, MRE=0.9247
2024-06-03 13:32:15 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 13:32:15 [INFO]: Using the given device: cuda:0
2024-06-03 13:32:15 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_3/20240603_T133215
2024-06-03 13:32:15 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_3/20240603_T133215/tensorboard
2024-06-03 13:32:15 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 6,700,164
2024-06-03 13:32:21 [INFO]: Epoch 001 - training loss: 1.7003, validation loss: 1.3416
2024-06-03 13:32:27 [INFO]: Epoch 002 - training loss: 1.6530, validation loss: 1.2684
2024-06-03 13:32:33 [INFO]: Epoch 003 - training loss: 1.6131, validation loss: 1.2076
2024-06-03 13:32:39 [INFO]: Epoch 004 - training loss: 1.5757, validation loss: 1.1598
2024-06-03 13:32:45 [INFO]: Epoch 005 - training loss: 1.5460, validation loss: 1.1263
2024-06-03 13:32:51 [INFO]: Epoch 006 - training loss: 1.5179, validation loss: 1.1070
2024-06-03 13:32:57 [INFO]: Epoch 007 - training loss: 1.4960, validation loss: 1.0980
2024-06-03 13:33:03 [INFO]: Epoch 008 - training loss: 1.4795, validation loss: 1.0934
2024-06-03 13:33:10 [INFO]: Epoch 009 - training loss: 1.4657, validation loss: 1.0898
2024-06-03 13:33:16 [INFO]: Epoch 010 - training loss: 1.4541, validation loss: 1.0878
2024-06-03 13:33:22 [INFO]: Epoch 011 - training loss: 1.4422, validation loss: 1.0850
2024-06-03 13:33:27 [INFO]: Epoch 012 - training loss: 1.4293, validation loss: 1.0831
2024-06-03 13:33:33 [INFO]: Epoch 013 - training loss: 1.4189, validation loss: 1.0791
2024-06-03 13:33:39 [INFO]: Epoch 014 - training loss: 1.4117, validation loss: 1.0758
2024-06-03 13:33:45 [INFO]: Epoch 015 - training loss: 1.4023, validation loss: 1.0725
2024-06-03 13:33:51 [INFO]: Epoch 016 - training loss: 1.3968, validation loss: 1.0693
2024-06-03 13:33:57 [INFO]: Epoch 017 - training loss: 1.3892, validation loss: 1.0664
2024-06-03 13:34:03 [INFO]: Epoch 018 - training loss: 1.3794, validation loss: 1.0634
2024-06-03 13:34:09 [INFO]: Epoch 019 - training loss: 1.3712, validation loss: 1.0607
2024-06-03 13:34:15 [INFO]: Epoch 020 - training loss: 1.3627, validation loss: 1.0580
2024-06-03 13:34:21 [INFO]: Epoch 021 - training loss: 1.3560, validation loss: 1.0556
2024-06-03 13:34:26 [INFO]: Epoch 022 - training loss: 1.3528, validation loss: 1.0533
2024-06-03 13:34:32 [INFO]: Epoch 023 - training loss: 1.3462, validation loss: 1.0519
2024-06-03 13:34:38 [INFO]: Epoch 024 - training loss: 1.3451, validation loss: 1.0499
2024-06-03 13:34:44 [INFO]: Epoch 025 - training loss: 1.3382, validation loss: 1.0481
2024-06-03 13:34:49 [INFO]: Epoch 026 - training loss: 1.3318, validation loss: 1.0464
2024-06-03 13:34:56 [INFO]: Epoch 027 - training loss: 1.3297, validation loss: 1.0451
2024-06-03 13:35:02 [INFO]: Epoch 028 - training loss: 1.3235, validation loss: 1.0440
2024-06-03 13:35:08 [INFO]: Epoch 029 - training loss: 1.3179, validation loss: 1.0427
2024-06-03 13:35:14 [INFO]: Epoch 030 - training loss: 1.3112, validation loss: 1.0412
2024-06-03 13:35:19 [INFO]: Epoch 031 - training loss: 1.3104, validation loss: 1.0401
2024-06-03 13:35:25 [INFO]: Epoch 032 - training loss: 1.3064, validation loss: 1.0393
2024-06-03 13:35:32 [INFO]: Epoch 033 - training loss: 1.3042, validation loss: 1.0383
2024-06-03 13:35:38 [INFO]: Epoch 034 - training loss: 1.3038, validation loss: 1.0376
2024-06-03 13:35:43 [INFO]: Epoch 035 - training loss: 1.2976, validation loss: 1.0369
2024-06-03 13:35:49 [INFO]: Epoch 036 - training loss: 1.2977, validation loss: 1.0359
2024-06-03 13:35:55 [INFO]: Epoch 037 - training loss: 1.2933, validation loss: 1.0352
2024-06-03 13:36:01 [INFO]: Epoch 038 - training loss: 1.2899, validation loss: 1.0346
2024-06-03 13:36:08 [INFO]: Epoch 039 - training loss: 1.2864, validation loss: 1.0338
2024-06-03 13:36:14 [INFO]: Epoch 040 - training loss: 1.2858, validation loss: 1.0331
2024-06-03 13:36:20 [INFO]: Epoch 041 - training loss: 1.2827, validation loss: 1.0329
2024-06-03 13:36:25 [INFO]: Epoch 042 - training loss: 1.2834, validation loss: 1.0321
2024-06-03 13:36:32 [INFO]: Epoch 043 - training loss: 1.2797, validation loss: 1.0319
2024-06-03 13:36:38 [INFO]: Epoch 044 - training loss: 1.2768, validation loss: 1.0316
2024-06-03 13:36:44 [INFO]: Epoch 045 - training loss: 1.2766, validation loss: 1.0309
2024-06-03 13:36:49 [INFO]: Epoch 046 - training loss: 1.2774, validation loss: 1.0308
2024-06-03 13:36:55 [INFO]: Epoch 047 - training loss: 1.2730, validation loss: 1.0305
2024-06-03 13:37:01 [INFO]: Epoch 048 - training loss: 1.2733, validation loss: 1.0301
2024-06-03 13:37:07 [INFO]: Epoch 049 - training loss: 1.2698, validation loss: 1.0297
2024-06-03 13:37:13 [INFO]: Epoch 050 - training loss: 1.2688, validation loss: 1.0291
2024-06-03 13:37:19 [INFO]: Epoch 051 - training loss: 1.2677, validation loss: 1.0288
2024-06-03 13:37:25 [INFO]: Epoch 052 - training loss: 1.2690, validation loss: 1.0288
2024-06-03 13:37:32 [INFO]: Epoch 053 - training loss: 1.2641, validation loss: 1.0286
2024-06-03 13:37:38 [INFO]: Epoch 054 - training loss: 1.2612, validation loss: 1.0283
2024-06-03 13:37:44 [INFO]: Epoch 055 - training loss: 1.2638, validation loss: 1.0288
2024-06-03 13:37:50 [INFO]: Epoch 056 - training loss: 1.2596, validation loss: 1.0284
2024-06-03 13:37:55 [INFO]: Epoch 057 - training loss: 1.2581, validation loss: 1.0278
2024-06-03 13:38:01 [INFO]: Epoch 058 - training loss: 1.2578, validation loss: 1.0280
2024-06-03 13:38:07 [INFO]: Epoch 059 - training loss: 1.2578, validation loss: 1.0282
2024-06-03 13:38:14 [INFO]: Epoch 060 - training loss: 1.2582, validation loss: 1.0276
2024-06-03 13:38:19 [INFO]: Epoch 061 - training loss: 1.2562, validation loss: 1.0276
2024-06-03 13:38:25 [INFO]: Epoch 062 - training loss: 1.2533, validation loss: 1.0276
2024-06-03 13:38:31 [INFO]: Epoch 063 - training loss: 1.2536, validation loss: 1.0277
2024-06-03 13:38:37 [INFO]: Epoch 064 - training loss: 1.2522, validation loss: 1.0278
2024-06-03 13:38:42 [INFO]: Epoch 065 - training loss: 1.2507, validation loss: 1.0276
2024-06-03 13:38:48 [INFO]: Epoch 066 - training loss: 1.2505, validation loss: 1.0275
2024-06-03 13:38:55 [INFO]: Epoch 067 - training loss: 1.2493, validation loss: 1.0271
2024-06-03 13:39:01 [INFO]: Epoch 068 - training loss: 1.2487, validation loss: 1.0272
2024-06-03 13:39:07 [INFO]: Epoch 069 - training loss: 1.2471, validation loss: 1.0275
2024-06-03 13:39:12 [INFO]: Epoch 070 - training loss: 1.2475, validation loss: 1.0275
2024-06-03 13:39:18 [INFO]: Epoch 071 - training loss: 1.2462, validation loss: 1.0272
2024-06-03 13:39:24 [INFO]: Epoch 072 - training loss: 1.2445, validation loss: 1.0273
2024-06-03 13:39:30 [INFO]: Epoch 073 - training loss: 1.2430, validation loss: 1.0275
2024-06-03 13:39:36 [INFO]: Epoch 074 - training loss: 1.2451, validation loss: 1.0273
2024-06-03 13:39:42 [INFO]: Epoch 075 - training loss: 1.2444, validation loss: 1.0274
2024-06-03 13:39:48 [INFO]: Epoch 076 - training loss: 1.2431, validation loss: 1.0280
2024-06-03 13:39:54 [INFO]: Epoch 077 - training loss: 1.2417, validation loss: 1.0282
2024-06-03 13:39:54 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 13:39:54 [INFO]: Finished training. The best model is from epoch#67.
2024-06-03 13:39:54 [INFO]: Saved the model to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_3/20240603_T133215/Autoformer.pypots
2024-06-03 13:39:55 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_3/imputation.pkl
2024-06-03 13:39:55 [INFO]: Round3 - Autoformer on BeijingAir: MAE=0.6818, MSE=1.0746, MRE=0.9217
2024-06-03 13:39:55 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 13:39:55 [INFO]: Using the given device: cuda:0
2024-06-03 13:39:55 [INFO]: Model files will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_4/20240603_T133955
2024-06-03 13:39:55 [INFO]: Tensorboard file will be saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_4/20240603_T133955/tensorboard
2024-06-03 13:39:55 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 6,700,164
2024-06-03 13:40:02 [INFO]: Epoch 001 - training loss: 1.7011, validation loss: 1.3432
2024-06-03 13:40:08 [INFO]: Epoch 002 - training loss: 1.6535, validation loss: 1.2701
2024-06-03 13:40:14 [INFO]: Epoch 003 - training loss: 1.6159, validation loss: 1.2133
2024-06-03 13:40:20 [INFO]: Epoch 004 - training loss: 1.5794, validation loss: 1.1684
2024-06-03 13:40:26 [INFO]: Epoch 005 - training loss: 1.5477, validation loss: 1.1358
2024-06-03 13:40:32 [INFO]: Epoch 006 - training loss: 1.5227, validation loss: 1.1152
2024-06-03 13:40:37 [INFO]: Epoch 007 - training loss: 1.4977, validation loss: 1.1024
2024-06-03 13:40:44 [INFO]: Epoch 008 - training loss: 1.4813, validation loss: 1.0942
2024-06-03 13:40:49 [INFO]: Epoch 009 - training loss: 1.4645, validation loss: 1.0893
2024-06-03 13:40:56 [INFO]: Epoch 010 - training loss: 1.4509, validation loss: 1.0860
2024-06-03 13:41:02 [INFO]: Epoch 011 - training loss: 1.4384, validation loss: 1.0836
2024-06-03 13:41:08 [INFO]: Epoch 012 - training loss: 1.4251, validation loss: 1.0805
2024-06-03 13:41:14 [INFO]: Epoch 013 - training loss: 1.4158, validation loss: 1.0774
2024-06-03 13:41:20 [INFO]: Epoch 014 - training loss: 1.4088, validation loss: 1.0739
2024-06-03 13:41:26 [INFO]: Epoch 015 - training loss: 1.3971, validation loss: 1.0712
2024-06-03 13:41:32 [INFO]: Epoch 016 - training loss: 1.3908, validation loss: 1.0682
2024-06-03 13:41:38 [INFO]: Epoch 017 - training loss: 1.3839, validation loss: 1.0650
2024-06-03 13:41:43 [INFO]: Epoch 018 - training loss: 1.3746, validation loss: 1.0625
2024-06-03 13:41:49 [INFO]: Epoch 019 - training loss: 1.3711, validation loss: 1.0595
2024-06-03 13:41:55 [INFO]: Epoch 020 - training loss: 1.3620, validation loss: 1.0575
2024-06-03 13:42:01 [INFO]: Epoch 021 - training loss: 1.3590, validation loss: 1.0550
2024-06-03 13:42:07 [INFO]: Epoch 022 - training loss: 1.3526, validation loss: 1.0530
2024-06-03 13:42:13 [INFO]: Epoch 023 - training loss: 1.3477, validation loss: 1.0513
2024-06-03 13:42:19 [INFO]: Epoch 024 - training loss: 1.3407, validation loss: 1.0495
2024-06-03 13:42:26 [INFO]: Epoch 025 - training loss: 1.3355, validation loss: 1.0478
2024-06-03 13:42:31 [INFO]: Epoch 026 - training loss: 1.3350, validation loss: 1.0465
2024-06-03 13:42:37 [INFO]: Epoch 027 - training loss: 1.3262, validation loss: 1.0450
2024-06-03 13:42:43 [INFO]: Epoch 028 - training loss: 1.3239, validation loss: 1.0437
2024-06-03 13:42:49 [INFO]: Epoch 029 - training loss: 1.3202, validation loss: 1.0420
2024-06-03 13:42:55 [INFO]: Epoch 030 - training loss: 1.3153, validation loss: 1.0416
2024-06-03 13:43:01 [INFO]: Epoch 031 - training loss: 1.3099, validation loss: 1.0404
2024-06-03 13:43:07 [INFO]: Epoch 032 - training loss: 1.3078, validation loss: 1.0398
2024-06-03 13:43:13 [INFO]: Epoch 033 - training loss: 1.3045, validation loss: 1.0384
2024-06-03 13:43:19 [INFO]: Epoch 034 - training loss: 1.2997, validation loss: 1.0374
2024-06-03 13:43:25 [INFO]: Epoch 035 - training loss: 1.2952, validation loss: 1.0370
2024-06-03 13:43:31 [INFO]: Epoch 036 - training loss: 1.2967, validation loss: 1.0362
2024-06-03 13:43:37 [INFO]: Epoch 037 - training loss: 1.2953, validation loss: 1.0353
2024-06-03 13:43:43 [INFO]: Epoch 038 - training loss: 1.2938, validation loss: 1.0344
2024-06-03 13:43:50 [INFO]: Epoch 039 - training loss: 1.2892, validation loss: 1.0341
2024-06-03 13:43:56 [INFO]: Epoch 040 - training loss: 1.2866, validation loss: 1.0337
2024-06-03 13:44:02 [INFO]: Epoch 041 - training loss: 1.2827, validation loss: 1.0332
2024-06-03 13:44:08 [INFO]: Epoch 042 - training loss: 1.2818, validation loss: 1.0328
2024-06-03 13:44:14 [INFO]: Epoch 043 - training loss: 1.2805, validation loss: 1.0326
2024-06-03 13:44:20 [INFO]: Epoch 044 - training loss: 1.2785, validation loss: 1.0319
2024-06-03 13:44:26 [INFO]: Epoch 045 - training loss: 1.2791, validation loss: 1.0317
2024-06-03 13:44:32 [INFO]: Epoch 046 - training loss: 1.2747, validation loss: 1.0312
2024-06-03 13:44:37 [INFO]: Epoch 047 - training loss: 1.2694, validation loss: 1.0309
2024-06-03 13:44:43 [INFO]: Epoch 048 - training loss: 1.2684, validation loss: 1.0306
2024-06-03 13:44:49 [INFO]: Epoch 049 - training loss: 1.2686, validation loss: 1.0299
2024-06-03 13:44:55 [INFO]: Epoch 050 - training loss: 1.2695, validation loss: 1.0299
2024-06-03 13:45:01 [INFO]: Epoch 051 - training loss: 1.2646, validation loss: 1.0294
2024-06-03 13:45:07 [INFO]: Epoch 052 - training loss: 1.2655, validation loss: 1.0298
2024-06-03 13:45:12 [INFO]: Epoch 053 - training loss: 1.2666, validation loss: 1.0295
2024-06-03 13:45:18 [INFO]: Epoch 054 - training loss: 1.2615, validation loss: 1.0288
2024-06-03 13:45:25 [INFO]: Epoch 055 - training loss: 1.2620, validation loss: 1.0287
2024-06-03 13:45:30 [INFO]: Epoch 056 - training loss: 1.2604, validation loss: 1.0285
2024-06-03 13:45:36 [INFO]: Epoch 057 - training loss: 1.2598, validation loss: 1.0285
2024-06-03 13:45:42 [INFO]: Epoch 058 - training loss: 1.2582, validation loss: 1.0288
2024-06-03 13:45:48 [INFO]: Epoch 059 - training loss: 1.2579, validation loss: 1.0285
2024-06-03 13:45:54 [INFO]: Epoch 060 - training loss: 1.2561, validation loss: 1.0283
2024-06-03 13:46:00 [INFO]: Epoch 061 - training loss: 1.2546, validation loss: 1.0284
2024-06-03 13:46:07 [INFO]: Epoch 062 - training loss: 1.2540, validation loss: 1.0278
2024-06-03 13:46:13 [INFO]: Epoch 063 - training loss: 1.2539, validation loss: 1.0284
2024-06-03 13:46:18 [INFO]: Epoch 064 - training loss: 1.2513, validation loss: 1.0285
2024-06-03 13:46:24 [INFO]: Epoch 065 - training loss: 1.2521, validation loss: 1.0277
2024-06-03 13:46:30 [INFO]: Epoch 066 - training loss: 1.2526, validation loss: 1.0281
2024-06-03 13:46:36 [INFO]: Epoch 067 - training loss: 1.2490, validation loss: 1.0284
2024-06-03 13:46:42 [INFO]: Epoch 068 - training loss: 1.2452, validation loss: 1.0281
2024-06-03 13:46:48 [INFO]: Epoch 069 - training loss: 1.2466, validation loss: 1.0284
2024-06-03 13:46:54 [INFO]: Epoch 070 - training loss: 1.2484, validation loss: 1.0281
2024-06-03 13:47:00 [INFO]: Epoch 071 - training loss: 1.2461, validation loss: 1.0281
2024-06-03 13:47:06 [INFO]: Epoch 072 - training loss: 1.2443, validation loss: 1.0280
2024-06-03 13:47:12 [INFO]: Epoch 073 - training loss: 1.2425, validation loss: 1.0276
2024-06-03 13:47:18 [INFO]: Epoch 074 - training loss: 1.2434, validation loss: 1.0283
2024-06-03 13:47:24 [INFO]: Epoch 075 - training loss: 1.2432, validation loss: 1.0290
2024-06-03 13:47:29 [INFO]: Epoch 076 - training loss: 1.2409, validation loss: 1.0282
2024-06-03 13:47:35 [INFO]: Epoch 077 - training loss: 1.2422, validation loss: 1.0287
2024-06-03 13:47:41 [INFO]: Epoch 078 - training loss: 1.2435, validation loss: 1.0290
2024-06-03 13:47:48 [INFO]: Epoch 079 - training loss: 1.2398, validation loss: 1.0289
2024-06-03 13:47:54 [INFO]: Epoch 080 - training loss: 1.2399, validation loss: 1.0296
2024-06-03 13:48:00 [INFO]: Epoch 081 - training loss: 1.2433, validation loss: 1.0287
2024-06-03 13:48:06 [INFO]: Epoch 082 - training loss: 1.2387, validation loss: 1.0294
2024-06-03 13:48:12 [INFO]: Epoch 083 - training loss: 1.2415, validation loss: 1.0294
2024-06-03 13:48:12 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 13:48:12 [INFO]: Finished training. The best model is from epoch#73.
2024-06-03 13:48:12 [INFO]: Saved the model to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_4/20240603_T133955/Autoformer.pypots
2024-06-03 13:48:13 [INFO]: Successfully saved to results_block_rate05/BeijingAir/Autoformer_BeijingAir/round_4/imputation.pkl
2024-06-03 13:48:13 [INFO]: Round4 - Autoformer on BeijingAir: MAE=0.6812, MSE=1.0764, MRE=0.9209
2024-06-03 13:48:13 [INFO]: Done! Final results:
Averaged Autoformer (6,700,164 params) on BeijingAir: MAE=0.6943 ± 0.001167204972713826, MSE=1.1007 ± 0.002216571676380904, MRE=0.9139 ± 0.001536359731521446, average inference time=0.25