2024-06-02 19:58:25 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:58:25 [INFO]: Using the given device: cuda:0
2024-06-02 19:58:26 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_0/20240602_T195826
2024-06-02 19:58:26 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_0/20240602_T195826/tensorboard
2024-06-02 19:58:27 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 1,058,311
2024-06-02 19:58:33 [INFO]: Epoch 001 - training loss: 1.5232, validation loss: 1.1756
2024-06-02 19:58:34 [INFO]: Epoch 002 - training loss: 0.9309, validation loss: 0.6125
2024-06-02 19:58:35 [INFO]: Epoch 003 - training loss: 0.7400, validation loss: 0.5041
2024-06-02 19:58:36 [INFO]: Epoch 004 - training loss: 0.6323, validation loss: 0.4930
2024-06-02 19:58:37 [INFO]: Epoch 005 - training loss: 0.5932, validation loss: 0.3374
2024-06-02 19:58:37 [INFO]: Epoch 006 - training loss: 0.5549, validation loss: 0.2677
2024-06-02 19:58:38 [INFO]: Epoch 007 - training loss: 0.5307, validation loss: 0.2341
2024-06-02 19:58:39 [INFO]: Epoch 008 - training loss: 0.5241, validation loss: 0.2047
2024-06-02 19:58:40 [INFO]: Epoch 009 - training loss: 0.4922, validation loss: 0.2039
2024-06-02 19:58:40 [INFO]: Epoch 010 - training loss: 0.4874, validation loss: 0.1908
2024-06-02 19:58:41 [INFO]: Epoch 011 - training loss: 0.4783, validation loss: 0.1666
2024-06-02 19:58:42 [INFO]: Epoch 012 - training loss: 0.4782, validation loss: 0.1628
2024-06-02 19:58:43 [INFO]: Epoch 013 - training loss: 0.4471, validation loss: 0.1571
2024-06-02 19:58:44 [INFO]: Epoch 014 - training loss: 0.4392, validation loss: 0.1577
2024-06-02 19:58:44 [INFO]: Epoch 015 - training loss: 0.4300, validation loss: 0.1490
2024-06-02 19:58:45 [INFO]: Epoch 016 - training loss: 0.4343, validation loss: 0.1769
2024-06-02 19:58:46 [INFO]: Epoch 017 - training loss: 0.4281, validation loss: 0.1528
2024-06-02 19:58:47 [INFO]: Epoch 018 - training loss: 0.4137, validation loss: 0.1572
2024-06-02 19:58:48 [INFO]: Epoch 019 - training loss: 0.4115, validation loss: 0.1500
2024-06-02 19:58:49 [INFO]: Epoch 020 - training loss: 0.4045, validation loss: 0.1624
2024-06-02 19:58:50 [INFO]: Epoch 021 - training loss: 0.4156, validation loss: 0.1508
2024-06-02 19:58:50 [INFO]: Epoch 022 - training loss: 0.4041, validation loss: 0.1598
2024-06-02 19:58:51 [INFO]: Epoch 023 - training loss: 0.3953, validation loss: 0.1389
2024-06-02 19:58:52 [INFO]: Epoch 024 - training loss: 0.3888, validation loss: 0.1534
2024-06-02 19:58:53 [INFO]: Epoch 025 - training loss: 0.3886, validation loss: 0.1588
2024-06-02 19:58:54 [INFO]: Epoch 026 - training loss: 0.3795, validation loss: 0.1368
2024-06-02 19:58:54 [INFO]: Epoch 027 - training loss: 0.3669, validation loss: 0.1617
2024-06-02 19:58:55 [INFO]: Epoch 028 - training loss: 0.3628, validation loss: 0.1402
2024-06-02 19:58:56 [INFO]: Epoch 029 - training loss: 0.3539, validation loss: 0.1428
2024-06-02 19:58:57 [INFO]: Epoch 030 - training loss: 0.3473, validation loss: 0.1384
2024-06-02 19:58:57 [INFO]: Epoch 031 - training loss: 0.3410, validation loss: 0.1467
2024-06-02 19:58:58 [INFO]: Epoch 032 - training loss: 0.3490, validation loss: 0.1285
2024-06-02 19:58:59 [INFO]: Epoch 033 - training loss: 0.3358, validation loss: 0.1267
2024-06-02 19:59:00 [INFO]: Epoch 034 - training loss: 0.3249, validation loss: 0.1379
2024-06-02 19:59:01 [INFO]: Epoch 035 - training loss: 0.3324, validation loss: 0.1486
2024-06-02 19:59:02 [INFO]: Epoch 036 - training loss: 0.3479, validation loss: 0.1310
2024-06-02 19:59:02 [INFO]: Epoch 037 - training loss: 0.3486, validation loss: 0.1340
2024-06-02 19:59:03 [INFO]: Epoch 038 - training loss: 0.3322, validation loss: 0.1339
2024-06-02 19:59:04 [INFO]: Epoch 039 - training loss: 0.3262, validation loss: 0.1262
2024-06-02 19:59:05 [INFO]: Epoch 040 - training loss: 0.3311, validation loss: 0.1244
2024-06-02 19:59:06 [INFO]: Epoch 041 - training loss: 0.3224, validation loss: 0.1387
2024-06-02 19:59:07 [INFO]: Epoch 042 - training loss: 0.3193, validation loss: 0.1315
2024-06-02 19:59:07 [INFO]: Epoch 043 - training loss: 0.3113, validation loss: 0.1166
2024-06-02 19:59:08 [INFO]: Epoch 044 - training loss: 0.3159, validation loss: 0.1247
2024-06-02 19:59:09 [INFO]: Epoch 045 - training loss: 0.3160, validation loss: 0.1243
2024-06-02 19:59:10 [INFO]: Epoch 046 - training loss: 0.3086, validation loss: 0.1268
2024-06-02 19:59:11 [INFO]: Epoch 047 - training loss: 0.2988, validation loss: 0.1141
2024-06-02 19:59:12 [INFO]: Epoch 048 - training loss: 0.2951, validation loss: 0.1244
2024-06-02 19:59:12 [INFO]: Epoch 049 - training loss: 0.2988, validation loss: 0.1197
2024-06-02 19:59:13 [INFO]: Epoch 050 - training loss: 0.2988, validation loss: 0.1126
2024-06-02 19:59:14 [INFO]: Epoch 051 - training loss: 0.2938, validation loss: 0.1277
2024-06-02 19:59:15 [INFO]: Epoch 052 - training loss: 0.2936, validation loss: 0.1309
2024-06-02 19:59:16 [INFO]: Epoch 053 - training loss: 0.3001, validation loss: 0.1106
2024-06-02 19:59:16 [INFO]: Epoch 054 - training loss: 0.2876, validation loss: 0.1204
2024-06-02 19:59:17 [INFO]: Epoch 055 - training loss: 0.2909, validation loss: 0.1218
2024-06-02 19:59:18 [INFO]: Epoch 056 - training loss: 0.2799, validation loss: 0.1110
2024-06-02 19:59:19 [INFO]: Epoch 057 - training loss: 0.2916, validation loss: 0.1227
2024-06-02 19:59:20 [INFO]: Epoch 058 - training loss: 0.2783, validation loss: 0.1146
2024-06-02 19:59:21 [INFO]: Epoch 059 - training loss: 0.2773, validation loss: 0.1225
2024-06-02 19:59:21 [INFO]: Epoch 060 - training loss: 0.2732, validation loss: 0.1155
2024-06-02 19:59:22 [INFO]: Epoch 061 - training loss: 0.2705, validation loss: 0.1157
2024-06-02 19:59:23 [INFO]: Epoch 062 - training loss: 0.2772, validation loss: 0.1134
2024-06-02 19:59:23 [INFO]: Epoch 063 - training loss: 0.2684, validation loss: 0.1115
2024-06-02 19:59:24 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:59:24 [INFO]: Finished training. The best model is from epoch#53.
2024-06-02 19:59:24 [INFO]: Saved the model to results_point_rate05/ETT_h1/Informer_ETT_h1/round_0/20240602_T195826/Informer.pypots
2024-06-02 19:59:24 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_0/imputation.pkl
2024-06-02 19:59:24 [INFO]: Round0 - Informer on ETT_h1: MAE=0.2752, MSE=0.1580, MRE=0.3256
2024-06-02 19:59:24 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 19:59:24 [INFO]: Using the given device: cuda:0
2024-06-02 19:59:24 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_1/20240602_T195924
2024-06-02 19:59:24 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_1/20240602_T195924/tensorboard
2024-06-02 19:59:24 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 1,058,311
2024-06-02 19:59:25 [INFO]: Epoch 001 - training loss: 1.5246, validation loss: 0.8947
2024-06-02 19:59:26 [INFO]: Epoch 002 - training loss: 0.9417, validation loss: 0.5484
2024-06-02 19:59:26 [INFO]: Epoch 003 - training loss: 0.7559, validation loss: 0.4429
2024-06-02 19:59:27 [INFO]: Epoch 004 - training loss: 0.6607, validation loss: 0.3898
2024-06-02 19:59:28 [INFO]: Epoch 005 - training loss: 0.6015, validation loss: 0.3136
2024-06-02 19:59:28 [INFO]: Epoch 006 - training loss: 0.5631, validation loss: 0.2921
2024-06-02 19:59:29 [INFO]: Epoch 007 - training loss: 0.5405, validation loss: 0.2730
2024-06-02 19:59:30 [INFO]: Epoch 008 - training loss: 0.5293, validation loss: 0.2723
2024-06-02 19:59:31 [INFO]: Epoch 009 - training loss: 0.5234, validation loss: 0.2609
2024-06-02 19:59:31 [INFO]: Epoch 010 - training loss: 0.5101, validation loss: 0.2203
2024-06-02 19:59:32 [INFO]: Epoch 011 - training loss: 0.4809, validation loss: 0.1960
2024-06-02 19:59:33 [INFO]: Epoch 012 - training loss: 0.4729, validation loss: 0.2031
2024-06-02 19:59:34 [INFO]: Epoch 013 - training loss: 0.4610, validation loss: 0.1935
2024-06-02 19:59:35 [INFO]: Epoch 014 - training loss: 0.4481, validation loss: 0.1822
2024-06-02 19:59:36 [INFO]: Epoch 015 - training loss: 0.4333, validation loss: 0.1569
2024-06-02 19:59:37 [INFO]: Epoch 016 - training loss: 0.4286, validation loss: 0.1581
2024-06-02 19:59:37 [INFO]: Epoch 017 - training loss: 0.4223, validation loss: 0.1651
2024-06-02 19:59:38 [INFO]: Epoch 018 - training loss: 0.4227, validation loss: 0.1539
2024-06-02 19:59:39 [INFO]: Epoch 019 - training loss: 0.4075, validation loss: 0.1610
2024-06-02 19:59:40 [INFO]: Epoch 020 - training loss: 0.4033, validation loss: 0.1530
2024-06-02 19:59:41 [INFO]: Epoch 021 - training loss: 0.3883, validation loss: 0.1488
2024-06-02 19:59:42 [INFO]: Epoch 022 - training loss: 0.3904, validation loss: 0.1572
2024-06-02 19:59:42 [INFO]: Epoch 023 - training loss: 0.3759, validation loss: 0.1507
2024-06-02 19:59:43 [INFO]: Epoch 024 - training loss: 0.3752, validation loss: 0.1535
2024-06-02 19:59:44 [INFO]: Epoch 025 - training loss: 0.3659, validation loss: 0.1474
2024-06-02 19:59:45 [INFO]: Epoch 026 - training loss: 0.3573, validation loss: 0.1447
2024-06-02 19:59:46 [INFO]: Epoch 027 - training loss: 0.3618, validation loss: 0.1474
2024-06-02 19:59:46 [INFO]: Epoch 028 - training loss: 0.3672, validation loss: 0.1378
2024-06-02 19:59:47 [INFO]: Epoch 029 - training loss: 0.3593, validation loss: 0.1388
2024-06-02 19:59:48 [INFO]: Epoch 030 - training loss: 0.3392, validation loss: 0.1373
2024-06-02 19:59:49 [INFO]: Epoch 031 - training loss: 0.3403, validation loss: 0.1370
2024-06-02 19:59:49 [INFO]: Epoch 032 - training loss: 0.3337, validation loss: 0.1394
2024-06-02 19:59:50 [INFO]: Epoch 033 - training loss: 0.3292, validation loss: 0.1330
2024-06-02 19:59:51 [INFO]: Epoch 034 - training loss: 0.3327, validation loss: 0.1413
2024-06-02 19:59:52 [INFO]: Epoch 035 - training loss: 0.3335, validation loss: 0.1475
2024-06-02 19:59:53 [INFO]: Epoch 036 - training loss: 0.3320, validation loss: 0.1382
2024-06-02 19:59:54 [INFO]: Epoch 037 - training loss: 0.3355, validation loss: 0.1456
2024-06-02 19:59:54 [INFO]: Epoch 038 - training loss: 0.3203, validation loss: 0.1224
2024-06-02 19:59:55 [INFO]: Epoch 039 - training loss: 0.3198, validation loss: 0.1457
2024-06-02 19:59:56 [INFO]: Epoch 040 - training loss: 0.3107, validation loss: 0.1366
2024-06-02 19:59:57 [INFO]: Epoch 041 - training loss: 0.3164, validation loss: 0.1407
2024-06-02 19:59:58 [INFO]: Epoch 042 - training loss: 0.3149, validation loss: 0.1148
2024-06-02 19:59:58 [INFO]: Epoch 043 - training loss: 0.3111, validation loss: 0.1265
2024-06-02 19:59:59 [INFO]: Epoch 044 - training loss: 0.3047, validation loss: 0.1254
2024-06-02 20:00:00 [INFO]: Epoch 045 - training loss: 0.2968, validation loss: 0.1184
2024-06-02 20:00:01 [INFO]: Epoch 046 - training loss: 0.3052, validation loss: 0.1266
2024-06-02 20:00:02 [INFO]: Epoch 047 - training loss: 0.3003, validation loss: 0.1256
2024-06-02 20:00:02 [INFO]: Epoch 048 - training loss: 0.2967, validation loss: 0.1188
2024-06-02 20:00:03 [INFO]: Epoch 049 - training loss: 0.2914, validation loss: 0.1129
2024-06-02 20:00:04 [INFO]: Epoch 050 - training loss: 0.2807, validation loss: 0.1180
2024-06-02 20:00:05 [INFO]: Epoch 051 - training loss: 0.2796, validation loss: 0.1167
2024-06-02 20:00:05 [INFO]: Epoch 052 - training loss: 0.2801, validation loss: 0.1261
2024-06-02 20:00:06 [INFO]: Epoch 053 - training loss: 0.2868, validation loss: 0.1293
2024-06-02 20:00:07 [INFO]: Epoch 054 - training loss: 0.2756, validation loss: 0.1291
2024-06-02 20:00:08 [INFO]: Epoch 055 - training loss: 0.2756, validation loss: 0.1081
2024-06-02 20:00:08 [INFO]: Epoch 056 - training loss: 0.2842, validation loss: 0.1159
2024-06-02 20:00:09 [INFO]: Epoch 057 - training loss: 0.2851, validation loss: 0.1230
2024-06-02 20:00:09 [INFO]: Epoch 058 - training loss: 0.2779, validation loss: 0.1169
2024-06-02 20:00:10 [INFO]: Epoch 059 - training loss: 0.2719, validation loss: 0.1190
2024-06-02 20:00:11 [INFO]: Epoch 060 - training loss: 0.2719, validation loss: 0.1127
2024-06-02 20:00:11 [INFO]: Epoch 061 - training loss: 0.2660, validation loss: 0.1120
2024-06-02 20:00:12 [INFO]: Epoch 062 - training loss: 0.2793, validation loss: 0.1170
2024-06-02 20:00:13 [INFO]: Epoch 063 - training loss: 0.2887, validation loss: 0.1261
2024-06-02 20:00:14 [INFO]: Epoch 064 - training loss: 0.2750, validation loss: 0.1160
2024-06-02 20:00:14 [INFO]: Epoch 065 - training loss: 0.2624, validation loss: 0.1259
2024-06-02 20:00:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:00:14 [INFO]: Finished training. The best model is from epoch#55.
2024-06-02 20:00:14 [INFO]: Saved the model to results_point_rate05/ETT_h1/Informer_ETT_h1/round_1/20240602_T195924/Informer.pypots
2024-06-02 20:00:15 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_1/imputation.pkl
2024-06-02 20:00:15 [INFO]: Round1 - Informer on ETT_h1: MAE=0.2866, MSE=0.1716, MRE=0.3391
2024-06-02 20:00:15 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 20:00:15 [INFO]: Using the given device: cuda:0
2024-06-02 20:00:15 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_2/20240602_T200015
2024-06-02 20:00:15 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_2/20240602_T200015/tensorboard
2024-06-02 20:00:15 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 1,058,311
2024-06-02 20:00:16 [INFO]: Epoch 001 - training loss: 1.5213, validation loss: 0.8285
2024-06-02 20:00:17 [INFO]: Epoch 002 - training loss: 0.9794, validation loss: 0.7044
2024-06-02 20:00:17 [INFO]: Epoch 003 - training loss: 0.7674, validation loss: 0.4930
2024-06-02 20:00:18 [INFO]: Epoch 004 - training loss: 0.6615, validation loss: 0.3822
2024-06-02 20:00:19 [INFO]: Epoch 005 - training loss: 0.6081, validation loss: 0.3627
2024-06-02 20:00:19 [INFO]: Epoch 006 - training loss: 0.5548, validation loss: 0.3039
2024-06-02 20:00:20 [INFO]: Epoch 007 - training loss: 0.5349, validation loss: 0.2821
2024-06-02 20:00:21 [INFO]: Epoch 008 - training loss: 0.5091, validation loss: 0.2417
2024-06-02 20:00:21 [INFO]: Epoch 009 - training loss: 0.4897, validation loss: 0.2298
2024-06-02 20:00:22 [INFO]: Epoch 010 - training loss: 0.4860, validation loss: 0.2068
2024-06-02 20:00:23 [INFO]: Epoch 011 - training loss: 0.4796, validation loss: 0.2048
2024-06-02 20:00:23 [INFO]: Epoch 012 - training loss: 0.4731, validation loss: 0.1835
2024-06-02 20:00:24 [INFO]: Epoch 013 - training loss: 0.4651, validation loss: 0.1700
2024-06-02 20:00:25 [INFO]: Epoch 014 - training loss: 0.4631, validation loss: 0.1572
2024-06-02 20:00:26 [INFO]: Epoch 015 - training loss: 0.4420, validation loss: 0.1526
2024-06-02 20:00:26 [INFO]: Epoch 016 - training loss: 0.4316, validation loss: 0.1568
2024-06-02 20:00:27 [INFO]: Epoch 017 - training loss: 0.4254, validation loss: 0.1452
2024-06-02 20:00:28 [INFO]: Epoch 018 - training loss: 0.4269, validation loss: 0.1419
2024-06-02 20:00:28 [INFO]: Epoch 019 - training loss: 0.4080, validation loss: 0.1546
2024-06-02 20:00:29 [INFO]: Epoch 020 - training loss: 0.3957, validation loss: 0.1473
2024-06-02 20:00:30 [INFO]: Epoch 021 - training loss: 0.4031, validation loss: 0.1516
2024-06-02 20:00:30 [INFO]: Epoch 022 - training loss: 0.3851, validation loss: 0.1583
2024-06-02 20:00:31 [INFO]: Epoch 023 - training loss: 0.3912, validation loss: 0.1408
2024-06-02 20:00:32 [INFO]: Epoch 024 - training loss: 0.3823, validation loss: 0.1321
2024-06-02 20:00:33 [INFO]: Epoch 025 - training loss: 0.3800, validation loss: 0.1659
2024-06-02 20:00:33 [INFO]: Epoch 026 - training loss: 0.3912, validation loss: 0.1481
2024-06-02 20:00:34 [INFO]: Epoch 027 - training loss: 0.3819, validation loss: 0.1302
2024-06-02 20:00:35 [INFO]: Epoch 028 - training loss: 0.3695, validation loss: 0.1364
2024-06-02 20:00:36 [INFO]: Epoch 029 - training loss: 0.3489, validation loss: 0.1402
2024-06-02 20:00:36 [INFO]: Epoch 030 - training loss: 0.3336, validation loss: 0.1306
2024-06-02 20:00:37 [INFO]: Epoch 031 - training loss: 0.3371, validation loss: 0.1327
2024-06-02 20:00:38 [INFO]: Epoch 032 - training loss: 0.3376, validation loss: 0.1146
2024-06-02 20:00:39 [INFO]: Epoch 033 - training loss: 0.3475, validation loss: 0.1165
2024-06-02 20:00:39 [INFO]: Epoch 034 - training loss: 0.3480, validation loss: 0.1355
2024-06-02 20:00:40 [INFO]: Epoch 035 - training loss: 0.3421, validation loss: 0.1220
2024-06-02 20:00:41 [INFO]: Epoch 036 - training loss: 0.3574, validation loss: 0.1423
2024-06-02 20:00:42 [INFO]: Epoch 037 - training loss: 0.3415, validation loss: 0.1250
2024-06-02 20:00:42 [INFO]: Epoch 038 - training loss: 0.3324, validation loss: 0.1256
2024-06-02 20:00:43 [INFO]: Epoch 039 - training loss: 0.3187, validation loss: 0.1267
2024-06-02 20:00:44 [INFO]: Epoch 040 - training loss: 0.3199, validation loss: 0.1209
2024-06-02 20:00:44 [INFO]: Epoch 041 - training loss: 0.3135, validation loss: 0.1147
2024-06-02 20:00:45 [INFO]: Epoch 042 - training loss: 0.3137, validation loss: 0.1074
2024-06-02 20:00:46 [INFO]: Epoch 043 - training loss: 0.3015, validation loss: 0.1232
2024-06-02 20:00:47 [INFO]: Epoch 044 - training loss: 0.2962, validation loss: 0.1125
2024-06-02 20:00:47 [INFO]: Epoch 045 - training loss: 0.2956, validation loss: 0.1130
2024-06-02 20:00:48 [INFO]: Epoch 046 - training loss: 0.2997, validation loss: 0.1147
2024-06-02 20:00:49 [INFO]: Epoch 047 - training loss: 0.2996, validation loss: 0.1042
2024-06-02 20:00:50 [INFO]: Epoch 048 - training loss: 0.2954, validation loss: 0.1038
2024-06-02 20:00:50 [INFO]: Epoch 049 - training loss: 0.3003, validation loss: 0.1141
2024-06-02 20:00:51 [INFO]: Epoch 050 - training loss: 0.2870, validation loss: 0.1162
2024-06-02 20:00:52 [INFO]: Epoch 051 - training loss: 0.2797, validation loss: 0.1171
2024-06-02 20:00:52 [INFO]: Epoch 052 - training loss: 0.2778, validation loss: 0.1095
2024-06-02 20:00:53 [INFO]: Epoch 053 - training loss: 0.2705, validation loss: 0.1033
2024-06-02 20:00:53 [INFO]: Epoch 054 - training loss: 0.2698, validation loss: 0.0996
2024-06-02 20:00:54 [INFO]: Epoch 055 - training loss: 0.2616, validation loss: 0.1022
2024-06-02 20:00:54 [INFO]: Epoch 056 - training loss: 0.2669, validation loss: 0.1039
2024-06-02 20:00:55 [INFO]: Epoch 057 - training loss: 0.2707, validation loss: 0.1170
2024-06-02 20:00:55 [INFO]: Epoch 058 - training loss: 0.2614, validation loss: 0.1021
2024-06-02 20:00:56 [INFO]: Epoch 059 - training loss: 0.2702, validation loss: 0.0996
2024-06-02 20:00:57 [INFO]: Epoch 060 - training loss: 0.2714, validation loss: 0.1018
2024-06-02 20:00:57 [INFO]: Epoch 061 - training loss: 0.2704, validation loss: 0.1046
2024-06-02 20:00:58 [INFO]: Epoch 062 - training loss: 0.2651, validation loss: 0.1055
2024-06-02 20:00:59 [INFO]: Epoch 063 - training loss: 0.2597, validation loss: 0.1019
2024-06-02 20:00:59 [INFO]: Epoch 064 - training loss: 0.2637, validation loss: 0.1001
2024-06-02 20:00:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:00:59 [INFO]: Finished training. The best model is from epoch#54.
2024-06-02 20:00:59 [INFO]: Saved the model to results_point_rate05/ETT_h1/Informer_ETT_h1/round_2/20240602_T200015/Informer.pypots
2024-06-02 20:01:00 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_2/imputation.pkl
2024-06-02 20:01:00 [INFO]: Round2 - Informer on ETT_h1: MAE=0.2684, MSE=0.1524, MRE=0.3176
2024-06-02 20:01:00 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 20:01:00 [INFO]: Using the given device: cuda:0
2024-06-02 20:01:00 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_3/20240602_T200100
2024-06-02 20:01:00 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_3/20240602_T200100/tensorboard
2024-06-02 20:01:00 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 1,058,311
2024-06-02 20:01:01 [INFO]: Epoch 001 - training loss: 1.5637, validation loss: 0.8283
2024-06-02 20:01:01 [INFO]: Epoch 002 - training loss: 0.9652, validation loss: 0.6007
2024-06-02 20:01:02 [INFO]: Epoch 003 - training loss: 0.7396, validation loss: 0.4705
2024-06-02 20:01:03 [INFO]: Epoch 004 - training loss: 0.6557, validation loss: 0.3362
2024-06-02 20:01:04 [INFO]: Epoch 005 - training loss: 0.6005, validation loss: 0.3001
2024-06-02 20:01:04 [INFO]: Epoch 006 - training loss: 0.5587, validation loss: 0.2623
2024-06-02 20:01:05 [INFO]: Epoch 007 - training loss: 0.5354, validation loss: 0.2114
2024-06-02 20:01:06 [INFO]: Epoch 008 - training loss: 0.5117, validation loss: 0.1981
2024-06-02 20:01:07 [INFO]: Epoch 009 - training loss: 0.4973, validation loss: 0.1801
2024-06-02 20:01:07 [INFO]: Epoch 010 - training loss: 0.4840, validation loss: 0.1729
2024-06-02 20:01:08 [INFO]: Epoch 011 - training loss: 0.4744, validation loss: 0.1562
2024-06-02 20:01:09 [INFO]: Epoch 012 - training loss: 0.4643, validation loss: 0.1480
2024-06-02 20:01:10 [INFO]: Epoch 013 - training loss: 0.4549, validation loss: 0.1504
2024-06-02 20:01:10 [INFO]: Epoch 014 - training loss: 0.4410, validation loss: 0.1484
2024-06-02 20:01:11 [INFO]: Epoch 015 - training loss: 0.4264, validation loss: 0.1499
2024-06-02 20:01:12 [INFO]: Epoch 016 - training loss: 0.4273, validation loss: 0.1447
2024-06-02 20:01:13 [INFO]: Epoch 017 - training loss: 0.4178, validation loss: 0.1405
2024-06-02 20:01:13 [INFO]: Epoch 018 - training loss: 0.4076, validation loss: 0.1371
2024-06-02 20:01:14 [INFO]: Epoch 019 - training loss: 0.4112, validation loss: 0.1396
2024-06-02 20:01:15 [INFO]: Epoch 020 - training loss: 0.4033, validation loss: 0.1462
2024-06-02 20:01:15 [INFO]: Epoch 021 - training loss: 0.4033, validation loss: 0.1368
2024-06-02 20:01:16 [INFO]: Epoch 022 - training loss: 0.3900, validation loss: 0.1497
2024-06-02 20:01:17 [INFO]: Epoch 023 - training loss: 0.3955, validation loss: 0.1335
2024-06-02 20:01:18 [INFO]: Epoch 024 - training loss: 0.3808, validation loss: 0.1511
2024-06-02 20:01:18 [INFO]: Epoch 025 - training loss: 0.3684, validation loss: 0.1415
2024-06-02 20:01:19 [INFO]: Epoch 026 - training loss: 0.3842, validation loss: 0.1499
2024-06-02 20:01:20 [INFO]: Epoch 027 - training loss: 0.3763, validation loss: 0.1435
2024-06-02 20:01:20 [INFO]: Epoch 028 - training loss: 0.3614, validation loss: 0.1371
2024-06-02 20:01:21 [INFO]: Epoch 029 - training loss: 0.3474, validation loss: 0.1350
2024-06-02 20:01:22 [INFO]: Epoch 030 - training loss: 0.3480, validation loss: 0.1385
2024-06-02 20:01:23 [INFO]: Epoch 031 - training loss: 0.3485, validation loss: 0.1286
2024-06-02 20:01:23 [INFO]: Epoch 032 - training loss: 0.3355, validation loss: 0.1309
2024-06-02 20:01:24 [INFO]: Epoch 033 - training loss: 0.3256, validation loss: 0.1263
2024-06-02 20:01:25 [INFO]: Epoch 034 - training loss: 0.3318, validation loss: 0.1351
2024-06-02 20:01:26 [INFO]: Epoch 035 - training loss: 0.3317, validation loss: 0.1354
2024-06-02 20:01:26 [INFO]: Epoch 036 - training loss: 0.3311, validation loss: 0.1368
2024-06-02 20:01:27 [INFO]: Epoch 037 - training loss: 0.3243, validation loss: 0.1300
2024-06-02 20:01:28 [INFO]: Epoch 038 - training loss: 0.3262, validation loss: 0.1363
2024-06-02 20:01:29 [INFO]: Epoch 039 - training loss: 0.3264, validation loss: 0.1379
2024-06-02 20:01:29 [INFO]: Epoch 040 - training loss: 0.3331, validation loss: 0.1255
2024-06-02 20:01:30 [INFO]: Epoch 041 - training loss: 0.3264, validation loss: 0.1300
2024-06-02 20:01:30 [INFO]: Epoch 042 - training loss: 0.3092, validation loss: 0.1169
2024-06-02 20:01:31 [INFO]: Epoch 043 - training loss: 0.3086, validation loss: 0.1165
2024-06-02 20:01:31 [INFO]: Epoch 044 - training loss: 0.2981, validation loss: 0.1122
2024-06-02 20:01:31 [INFO]: Epoch 045 - training loss: 0.3015, validation loss: 0.1204
2024-06-02 20:01:32 [INFO]: Epoch 046 - training loss: 0.3004, validation loss: 0.1169
2024-06-02 20:01:32 [INFO]: Epoch 047 - training loss: 0.3031, validation loss: 0.1121
2024-06-02 20:01:33 [INFO]: Epoch 048 - training loss: 0.3079, validation loss: 0.1069
2024-06-02 20:01:34 [INFO]: Epoch 049 - training loss: 0.2961, validation loss: 0.1232
2024-06-02 20:01:34 [INFO]: Epoch 050 - training loss: 0.3028, validation loss: 0.1172
2024-06-02 20:01:35 [INFO]: Epoch 051 - training loss: 0.2893, validation loss: 0.1097
2024-06-02 20:01:35 [INFO]: Epoch 052 - training loss: 0.2937, validation loss: 0.1114
2024-06-02 20:01:36 [INFO]: Epoch 053 - training loss: 0.2931, validation loss: 0.1167
2024-06-02 20:01:37 [INFO]: Epoch 054 - training loss: 0.2971, validation loss: 0.1171
2024-06-02 20:01:37 [INFO]: Epoch 055 - training loss: 0.2926, validation loss: 0.1141
2024-06-02 20:01:38 [INFO]: Epoch 056 - training loss: 0.2865, validation loss: 0.1139
2024-06-02 20:01:38 [INFO]: Epoch 057 - training loss: 0.2844, validation loss: 0.1137
2024-06-02 20:01:39 [INFO]: Epoch 058 - training loss: 0.2803, validation loss: 0.1190
2024-06-02 20:01:39 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:01:39 [INFO]: Finished training. The best model is from epoch#48.
2024-06-02 20:01:39 [INFO]: Saved the model to results_point_rate05/ETT_h1/Informer_ETT_h1/round_3/20240602_T200100/Informer.pypots
2024-06-02 20:01:40 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_3/imputation.pkl
2024-06-02 20:01:40 [INFO]: Round3 - Informer on ETT_h1: MAE=0.2756, MSE=0.1585, MRE=0.3260
2024-06-02 20:01:40 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 20:01:40 [INFO]: Using the given device: cuda:0
2024-06-02 20:01:40 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_4/20240602_T200140
2024-06-02 20:01:40 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_4/20240602_T200140/tensorboard
2024-06-02 20:01:40 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 1,058,311
2024-06-02 20:01:40 [INFO]: Epoch 001 - training loss: 1.5009, validation loss: 0.8276
2024-06-02 20:01:41 [INFO]: Epoch 002 - training loss: 0.9275, validation loss: 0.4783
2024-06-02 20:01:41 [INFO]: Epoch 003 - training loss: 0.7346, validation loss: 0.4434
2024-06-02 20:01:42 [INFO]: Epoch 004 - training loss: 0.6589, validation loss: 0.3321
2024-06-02 20:01:42 [INFO]: Epoch 005 - training loss: 0.5946, validation loss: 0.2717
2024-06-02 20:01:43 [INFO]: Epoch 006 - training loss: 0.5554, validation loss: 0.2620
2024-06-02 20:01:44 [INFO]: Epoch 007 - training loss: 0.5345, validation loss: 0.2260
2024-06-02 20:01:44 [INFO]: Epoch 008 - training loss: 0.5097, validation loss: 0.2038
2024-06-02 20:01:44 [INFO]: Epoch 009 - training loss: 0.4941, validation loss: 0.1933
2024-06-02 20:01:45 [INFO]: Epoch 010 - training loss: 0.4818, validation loss: 0.1788
2024-06-02 20:01:45 [INFO]: Epoch 011 - training loss: 0.4684, validation loss: 0.1783
2024-06-02 20:01:45 [INFO]: Epoch 012 - training loss: 0.4588, validation loss: 0.1667
2024-06-02 20:01:46 [INFO]: Epoch 013 - training loss: 0.4587, validation loss: 0.1598
2024-06-02 20:01:46 [INFO]: Epoch 014 - training loss: 0.4445, validation loss: 0.1737
2024-06-02 20:01:46 [INFO]: Epoch 015 - training loss: 0.4392, validation loss: 0.1544
2024-06-02 20:01:47 [INFO]: Epoch 016 - training loss: 0.4277, validation loss: 0.1474
2024-06-02 20:01:47 [INFO]: Epoch 017 - training loss: 0.4187, validation loss: 0.1547
2024-06-02 20:01:47 [INFO]: Epoch 018 - training loss: 0.4059, validation loss: 0.1424
2024-06-02 20:01:48 [INFO]: Epoch 019 - training loss: 0.3949, validation loss: 0.1528
2024-06-02 20:01:48 [INFO]: Epoch 020 - training loss: 0.4002, validation loss: 0.1496
2024-06-02 20:01:49 [INFO]: Epoch 021 - training loss: 0.4174, validation loss: 0.1657
2024-06-02 20:01:49 [INFO]: Epoch 022 - training loss: 0.4253, validation loss: 0.1509
2024-06-02 20:01:50 [INFO]: Epoch 023 - training loss: 0.4221, validation loss: 0.1632
2024-06-02 20:01:50 [INFO]: Epoch 024 - training loss: 0.4094, validation loss: 0.1548
2024-06-02 20:01:50 [INFO]: Epoch 025 - training loss: 0.3852, validation loss: 0.1334
2024-06-02 20:01:51 [INFO]: Epoch 026 - training loss: 0.3772, validation loss: 0.1454
2024-06-02 20:01:51 [INFO]: Epoch 027 - training loss: 0.3724, validation loss: 0.1387
2024-06-02 20:01:51 [INFO]: Epoch 028 - training loss: 0.3525, validation loss: 0.1318
2024-06-02 20:01:52 [INFO]: Epoch 029 - training loss: 0.3526, validation loss: 0.1311
2024-06-02 20:01:52 [INFO]: Epoch 030 - training loss: 0.3486, validation loss: 0.1370
2024-06-02 20:01:53 [INFO]: Epoch 031 - training loss: 0.3341, validation loss: 0.1261
2024-06-02 20:01:53 [INFO]: Epoch 032 - training loss: 0.3334, validation loss: 0.1223
2024-06-02 20:01:54 [INFO]: Epoch 033 - training loss: 0.3328, validation loss: 0.1249
2024-06-02 20:01:54 [INFO]: Epoch 034 - training loss: 0.3208, validation loss: 0.1196
2024-06-02 20:01:55 [INFO]: Epoch 035 - training loss: 0.3264, validation loss: 0.1262
2024-06-02 20:01:55 [INFO]: Epoch 036 - training loss: 0.3249, validation loss: 0.1252
2024-06-02 20:01:56 [INFO]: Epoch 037 - training loss: 0.3173, validation loss: 0.1270
2024-06-02 20:01:56 [INFO]: Epoch 038 - training loss: 0.3254, validation loss: 0.1227
2024-06-02 20:01:56 [INFO]: Epoch 039 - training loss: 0.3261, validation loss: 0.1218
2024-06-02 20:01:57 [INFO]: Epoch 040 - training loss: 0.3214, validation loss: 0.1204
2024-06-02 20:01:57 [INFO]: Epoch 041 - training loss: 0.3152, validation loss: 0.1229
2024-06-02 20:01:58 [INFO]: Epoch 042 - training loss: 0.3242, validation loss: 0.1185
2024-06-02 20:01:58 [INFO]: Epoch 043 - training loss: 0.3184, validation loss: 0.1407
2024-06-02 20:01:58 [INFO]: Epoch 044 - training loss: 0.3420, validation loss: 0.1204
2024-06-02 20:01:59 [INFO]: Epoch 045 - training loss: 0.3415, validation loss: 0.1100
2024-06-02 20:01:59 [INFO]: Epoch 046 - training loss: 0.3166, validation loss: 0.1278
2024-06-02 20:02:00 [INFO]: Epoch 047 - training loss: 0.3174, validation loss: 0.1175
2024-06-02 20:02:00 [INFO]: Epoch 048 - training loss: 0.3075, validation loss: 0.1091
2024-06-02 20:02:00 [INFO]: Epoch 049 - training loss: 0.2983, validation loss: 0.1112
2024-06-02 20:02:01 [INFO]: Epoch 050 - training loss: 0.2833, validation loss: 0.1129
2024-06-02 20:02:01 [INFO]: Epoch 051 - training loss: 0.2810, validation loss: 0.1158
2024-06-02 20:02:02 [INFO]: Epoch 052 - training loss: 0.2763, validation loss: 0.1139
2024-06-02 20:02:02 [INFO]: Epoch 053 - training loss: 0.2817, validation loss: 0.1136
2024-06-02 20:02:02 [INFO]: Epoch 054 - training loss: 0.2774, validation loss: 0.1191
2024-06-02 20:02:03 [INFO]: Epoch 055 - training loss: 0.2829, validation loss: 0.1178
2024-06-02 20:02:03 [INFO]: Epoch 056 - training loss: 0.2766, validation loss: 0.1157
2024-06-02 20:02:03 [INFO]: Epoch 057 - training loss: 0.2851, validation loss: 0.1175
2024-06-02 20:02:03 [INFO]: Epoch 058 - training loss: 0.2714, validation loss: 0.1113
2024-06-02 20:02:03 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:02:03 [INFO]: Finished training. The best model is from epoch#48.
2024-06-02 20:02:03 [INFO]: Saved the model to results_point_rate05/ETT_h1/Informer_ETT_h1/round_4/20240602_T200140/Informer.pypots
2024-06-02 20:02:04 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Informer_ETT_h1/round_4/imputation.pkl
2024-06-02 20:02:04 [INFO]: Round4 - Informer on ETT_h1: MAE=0.2896, MSE=0.1703, MRE=0.3426
2024-06-02 20:02:04 [INFO]: Done! Final results:
Averaged Informer (1,058,311 params) on ETT_h1: MAE=0.2791 ± 0.007840699712476397, MSE=0.1622 ± 0.007488046029880699, MRE=0.3302 ± 0.009275522260319594, average inference time=0.11
