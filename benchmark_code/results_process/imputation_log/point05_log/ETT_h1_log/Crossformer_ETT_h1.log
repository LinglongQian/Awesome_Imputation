2024-06-02 19:39:17 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:39:17 [INFO]: Using the given device: cuda:0
2024-06-02 19:39:17 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_0/20240602_T193917
2024-06-02 19:39:17 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_0/20240602_T193917/tensorboard
2024-06-02 19:39:18 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 223,479
2024-06-02 19:39:21 [INFO]: Epoch 001 - training loss: 1.5326, validation loss: 0.9388
2024-06-02 19:39:22 [INFO]: Epoch 002 - training loss: 1.4602, validation loss: 1.0073
2024-06-02 19:39:22 [INFO]: Epoch 003 - training loss: 1.4613, validation loss: 0.8971
2024-06-02 19:39:23 [INFO]: Epoch 004 - training loss: 1.4182, validation loss: 0.8783
2024-06-02 19:39:24 [INFO]: Epoch 005 - training loss: 1.2308, validation loss: 0.7215
2024-06-02 19:39:24 [INFO]: Epoch 006 - training loss: 0.9401, validation loss: 0.4667
2024-06-02 19:39:25 [INFO]: Epoch 007 - training loss: 0.8401, validation loss: 0.3946
2024-06-02 19:39:25 [INFO]: Epoch 008 - training loss: 0.7216, validation loss: 0.3202
2024-06-02 19:39:26 [INFO]: Epoch 009 - training loss: 0.6474, validation loss: 0.2309
2024-06-02 19:39:27 [INFO]: Epoch 010 - training loss: 0.6277, validation loss: 0.1942
2024-06-02 19:39:27 [INFO]: Epoch 011 - training loss: 0.5920, validation loss: 0.1981
2024-06-02 19:39:28 [INFO]: Epoch 012 - training loss: 0.5548, validation loss: 0.1891
2024-06-02 19:39:28 [INFO]: Epoch 013 - training loss: 0.5312, validation loss: 0.1887
2024-06-02 19:39:29 [INFO]: Epoch 014 - training loss: 0.5234, validation loss: 0.1783
2024-06-02 19:39:29 [INFO]: Epoch 015 - training loss: 0.5198, validation loss: 0.1784
2024-06-02 19:39:30 [INFO]: Epoch 016 - training loss: 0.4970, validation loss: 0.1645
2024-06-02 19:39:31 [INFO]: Epoch 017 - training loss: 0.5011, validation loss: 0.1835
2024-06-02 19:39:31 [INFO]: Epoch 018 - training loss: 0.4963, validation loss: 0.1617
2024-06-02 19:39:32 [INFO]: Epoch 019 - training loss: 0.4991, validation loss: 0.1643
2024-06-02 19:39:33 [INFO]: Epoch 020 - training loss: 0.4735, validation loss: 0.1479
2024-06-02 19:39:34 [INFO]: Epoch 021 - training loss: 0.4741, validation loss: 0.1551
2024-06-02 19:39:34 [INFO]: Epoch 022 - training loss: 0.4627, validation loss: 0.1564
2024-06-02 19:39:35 [INFO]: Epoch 023 - training loss: 0.4635, validation loss: 0.1540
2024-06-02 19:39:36 [INFO]: Epoch 024 - training loss: 0.4672, validation loss: 0.1544
2024-06-02 19:39:36 [INFO]: Epoch 025 - training loss: 0.4506, validation loss: 0.1496
2024-06-02 19:39:37 [INFO]: Epoch 026 - training loss: 0.4468, validation loss: 0.1473
2024-06-02 19:39:38 [INFO]: Epoch 027 - training loss: 0.4483, validation loss: 0.1464
2024-06-02 19:39:38 [INFO]: Epoch 028 - training loss: 0.4501, validation loss: 0.1489
2024-06-02 19:39:39 [INFO]: Epoch 029 - training loss: 0.4405, validation loss: 0.1502
2024-06-02 19:39:40 [INFO]: Epoch 030 - training loss: 0.4423, validation loss: 0.1467
2024-06-02 19:39:41 [INFO]: Epoch 031 - training loss: 0.4339, validation loss: 0.1443
2024-06-02 19:39:41 [INFO]: Epoch 032 - training loss: 0.4309, validation loss: 0.1492
2024-06-02 19:39:42 [INFO]: Epoch 033 - training loss: 0.4269, validation loss: 0.1454
2024-06-02 19:39:43 [INFO]: Epoch 034 - training loss: 0.4299, validation loss: 0.1506
2024-06-02 19:39:43 [INFO]: Epoch 035 - training loss: 0.4276, validation loss: 0.1508
2024-06-02 19:39:44 [INFO]: Epoch 036 - training loss: 0.4302, validation loss: 0.1464
2024-06-02 19:39:44 [INFO]: Epoch 037 - training loss: 0.4355, validation loss: 0.1579
2024-06-02 19:39:45 [INFO]: Epoch 038 - training loss: 0.4288, validation loss: 0.1642
2024-06-02 19:39:46 [INFO]: Epoch 039 - training loss: 0.4342, validation loss: 0.1559
2024-06-02 19:39:46 [INFO]: Epoch 040 - training loss: 0.4306, validation loss: 0.1517
2024-06-02 19:39:47 [INFO]: Epoch 041 - training loss: 0.4161, validation loss: 0.1458
2024-06-02 19:39:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:39:47 [INFO]: Finished training. The best model is from epoch#31.
2024-06-02 19:39:47 [INFO]: Saved the model to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_0/20240602_T193917/Crossformer.pypots
2024-06-02 19:39:47 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_0/imputation.pkl
2024-06-02 19:39:47 [INFO]: Round0 - Crossformer on ETT_h1: MAE=0.3084, MSE=0.1776, MRE=0.3648
2024-06-02 19:39:47 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 19:39:47 [INFO]: Using the given device: cuda:0
2024-06-02 19:39:47 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_1/20240602_T193947
2024-06-02 19:39:47 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_1/20240602_T193947/tensorboard
2024-06-02 19:39:47 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 223,479
2024-06-02 19:39:48 [INFO]: Epoch 001 - training loss: 1.5106, validation loss: 0.9160
2024-06-02 19:39:49 [INFO]: Epoch 002 - training loss: 1.4567, validation loss: 0.8997
2024-06-02 19:39:49 [INFO]: Epoch 003 - training loss: 1.4064, validation loss: 0.7208
2024-06-02 19:39:50 [INFO]: Epoch 004 - training loss: 1.1037, validation loss: 0.5169
2024-06-02 19:39:51 [INFO]: Epoch 005 - training loss: 0.8887, validation loss: 0.3870
2024-06-02 19:39:51 [INFO]: Epoch 006 - training loss: 0.7655, validation loss: 0.2999
2024-06-02 19:39:52 [INFO]: Epoch 007 - training loss: 0.6722, validation loss: 0.2415
2024-06-02 19:39:53 [INFO]: Epoch 008 - training loss: 0.6191, validation loss: 0.2179
2024-06-02 19:39:53 [INFO]: Epoch 009 - training loss: 0.5928, validation loss: 0.1926
2024-06-02 19:39:54 [INFO]: Epoch 010 - training loss: 0.5716, validation loss: 0.1940
2024-06-02 19:39:55 [INFO]: Epoch 011 - training loss: 0.5517, validation loss: 0.1911
2024-06-02 19:39:55 [INFO]: Epoch 012 - training loss: 0.5317, validation loss: 0.1719
2024-06-02 19:39:56 [INFO]: Epoch 013 - training loss: 0.5209, validation loss: 0.1679
2024-06-02 19:39:56 [INFO]: Epoch 014 - training loss: 0.5119, validation loss: 0.1716
2024-06-02 19:39:57 [INFO]: Epoch 015 - training loss: 0.4993, validation loss: 0.1661
2024-06-02 19:39:58 [INFO]: Epoch 016 - training loss: 0.4984, validation loss: 0.1553
2024-06-02 19:39:58 [INFO]: Epoch 017 - training loss: 0.4895, validation loss: 0.1612
2024-06-02 19:39:59 [INFO]: Epoch 018 - training loss: 0.4798, validation loss: 0.1698
2024-06-02 19:39:59 [INFO]: Epoch 019 - training loss: 0.4767, validation loss: 0.1569
2024-06-02 19:40:00 [INFO]: Epoch 020 - training loss: 0.4716, validation loss: 0.1577
2024-06-02 19:40:01 [INFO]: Epoch 021 - training loss: 0.4752, validation loss: 0.1510
2024-06-02 19:40:01 [INFO]: Epoch 022 - training loss: 0.4715, validation loss: 0.1600
2024-06-02 19:40:02 [INFO]: Epoch 023 - training loss: 0.4642, validation loss: 0.1566
2024-06-02 19:40:02 [INFO]: Epoch 024 - training loss: 0.4529, validation loss: 0.1510
2024-06-02 19:40:03 [INFO]: Epoch 025 - training loss: 0.4570, validation loss: 0.1465
2024-06-02 19:40:04 [INFO]: Epoch 026 - training loss: 0.4545, validation loss: 0.1547
2024-06-02 19:40:04 [INFO]: Epoch 027 - training loss: 0.4494, validation loss: 0.1541
2024-06-02 19:40:05 [INFO]: Epoch 028 - training loss: 0.4382, validation loss: 0.1583
2024-06-02 19:40:06 [INFO]: Epoch 029 - training loss: 0.4415, validation loss: 0.1464
2024-06-02 19:40:06 [INFO]: Epoch 030 - training loss: 0.4403, validation loss: 0.1519
2024-06-02 19:40:07 [INFO]: Epoch 031 - training loss: 0.4299, validation loss: 0.1449
2024-06-02 19:40:08 [INFO]: Epoch 032 - training loss: 0.4327, validation loss: 0.1420
2024-06-02 19:40:08 [INFO]: Epoch 033 - training loss: 0.4323, validation loss: 0.1409
2024-06-02 19:40:09 [INFO]: Epoch 034 - training loss: 0.4285, validation loss: 0.1538
2024-06-02 19:40:10 [INFO]: Epoch 035 - training loss: 0.4301, validation loss: 0.1437
2024-06-02 19:40:10 [INFO]: Epoch 036 - training loss: 0.4214, validation loss: 0.1418
2024-06-02 19:40:11 [INFO]: Epoch 037 - training loss: 0.4269, validation loss: 0.1395
2024-06-02 19:40:12 [INFO]: Epoch 038 - training loss: 0.4294, validation loss: 0.1550
2024-06-02 19:40:12 [INFO]: Epoch 039 - training loss: 0.4259, validation loss: 0.1507
2024-06-02 19:40:13 [INFO]: Epoch 040 - training loss: 0.4190, validation loss: 0.1425
2024-06-02 19:40:14 [INFO]: Epoch 041 - training loss: 0.4120, validation loss: 0.1391
2024-06-02 19:40:14 [INFO]: Epoch 042 - training loss: 0.4039, validation loss: 0.1350
2024-06-02 19:40:15 [INFO]: Epoch 043 - training loss: 0.4071, validation loss: 0.1318
2024-06-02 19:40:16 [INFO]: Epoch 044 - training loss: 0.4009, validation loss: 0.1351
2024-06-02 19:40:16 [INFO]: Epoch 045 - training loss: 0.4121, validation loss: 0.1353
2024-06-02 19:40:17 [INFO]: Epoch 046 - training loss: 0.4029, validation loss: 0.1386
2024-06-02 19:40:17 [INFO]: Epoch 047 - training loss: 0.4028, validation loss: 0.1283
2024-06-02 19:40:18 [INFO]: Epoch 048 - training loss: 0.3986, validation loss: 0.1304
2024-06-02 19:40:19 [INFO]: Epoch 049 - training loss: 0.3944, validation loss: 0.1292
2024-06-02 19:40:20 [INFO]: Epoch 050 - training loss: 0.3886, validation loss: 0.1279
2024-06-02 19:40:20 [INFO]: Epoch 051 - training loss: 0.3882, validation loss: 0.1280
2024-06-02 19:40:21 [INFO]: Epoch 052 - training loss: 0.3928, validation loss: 0.1243
2024-06-02 19:40:21 [INFO]: Epoch 053 - training loss: 0.3944, validation loss: 0.1251
2024-06-02 19:40:22 [INFO]: Epoch 054 - training loss: 0.3811, validation loss: 0.1277
2024-06-02 19:40:23 [INFO]: Epoch 055 - training loss: 0.3758, validation loss: 0.1241
2024-06-02 19:40:23 [INFO]: Epoch 056 - training loss: 0.3771, validation loss: 0.1280
2024-06-02 19:40:24 [INFO]: Epoch 057 - training loss: 0.3810, validation loss: 0.1260
2024-06-02 19:40:25 [INFO]: Epoch 058 - training loss: 0.3811, validation loss: 0.1219
2024-06-02 19:40:25 [INFO]: Epoch 059 - training loss: 0.3758, validation loss: 0.1254
2024-06-02 19:40:26 [INFO]: Epoch 060 - training loss: 0.3749, validation loss: 0.1237
2024-06-02 19:40:26 [INFO]: Epoch 061 - training loss: 0.3707, validation loss: 0.1231
2024-06-02 19:40:27 [INFO]: Epoch 062 - training loss: 0.3699, validation loss: 0.1214
2024-06-02 19:40:27 [INFO]: Epoch 063 - training loss: 0.3732, validation loss: 0.1178
2024-06-02 19:40:28 [INFO]: Epoch 064 - training loss: 0.3663, validation loss: 0.1269
2024-06-02 19:40:29 [INFO]: Epoch 065 - training loss: 0.3669, validation loss: 0.1213
2024-06-02 19:40:29 [INFO]: Epoch 066 - training loss: 0.3668, validation loss: 0.1204
2024-06-02 19:40:30 [INFO]: Epoch 067 - training loss: 0.3606, validation loss: 0.1216
2024-06-02 19:40:30 [INFO]: Epoch 068 - training loss: 0.3638, validation loss: 0.1171
2024-06-02 19:40:31 [INFO]: Epoch 069 - training loss: 0.3560, validation loss: 0.1168
2024-06-02 19:40:31 [INFO]: Epoch 070 - training loss: 0.3618, validation loss: 0.1221
2024-06-02 19:40:32 [INFO]: Epoch 071 - training loss: 0.3532, validation loss: 0.1170
2024-06-02 19:40:33 [INFO]: Epoch 072 - training loss: 0.3512, validation loss: 0.1142
2024-06-02 19:40:33 [INFO]: Epoch 073 - training loss: 0.3608, validation loss: 0.1187
2024-06-02 19:40:34 [INFO]: Epoch 074 - training loss: 0.3457, validation loss: 0.1218
2024-06-02 19:40:34 [INFO]: Epoch 075 - training loss: 0.3551, validation loss: 0.1196
2024-06-02 19:40:35 [INFO]: Epoch 076 - training loss: 0.3558, validation loss: 0.1109
2024-06-02 19:40:36 [INFO]: Epoch 077 - training loss: 0.3496, validation loss: 0.1142
2024-06-02 19:40:36 [INFO]: Epoch 078 - training loss: 0.3537, validation loss: 0.1130
2024-06-02 19:40:37 [INFO]: Epoch 079 - training loss: 0.3591, validation loss: 0.1278
2024-06-02 19:40:37 [INFO]: Epoch 080 - training loss: 0.3595, validation loss: 0.1126
2024-06-02 19:40:38 [INFO]: Epoch 081 - training loss: 0.3456, validation loss: 0.1081
2024-06-02 19:40:38 [INFO]: Epoch 082 - training loss: 0.3515, validation loss: 0.1099
2024-06-02 19:40:39 [INFO]: Epoch 083 - training loss: 0.3442, validation loss: 0.1088
2024-06-02 19:40:40 [INFO]: Epoch 084 - training loss: 0.3395, validation loss: 0.1095
2024-06-02 19:40:40 [INFO]: Epoch 085 - training loss: 0.3508, validation loss: 0.1101
2024-06-02 19:40:41 [INFO]: Epoch 086 - training loss: 0.3497, validation loss: 0.1105
2024-06-02 19:40:41 [INFO]: Epoch 087 - training loss: 0.3424, validation loss: 0.1092
2024-06-02 19:40:42 [INFO]: Epoch 088 - training loss: 0.3488, validation loss: 0.1128
2024-06-02 19:40:43 [INFO]: Epoch 089 - training loss: 0.3497, validation loss: 0.1117
2024-06-02 19:40:43 [INFO]: Epoch 090 - training loss: 0.3439, validation loss: 0.1097
2024-06-02 19:40:44 [INFO]: Epoch 091 - training loss: 0.3332, validation loss: 0.1071
2024-06-02 19:40:44 [INFO]: Epoch 092 - training loss: 0.3382, validation loss: 0.1122
2024-06-02 19:40:45 [INFO]: Epoch 093 - training loss: 0.3406, validation loss: 0.1109
2024-06-02 19:40:45 [INFO]: Epoch 094 - training loss: 0.3416, validation loss: 0.1081
2024-06-02 19:40:46 [INFO]: Epoch 095 - training loss: 0.3428, validation loss: 0.1084
2024-06-02 19:40:46 [INFO]: Epoch 096 - training loss: 0.3359, validation loss: 0.1071
2024-06-02 19:40:47 [INFO]: Epoch 097 - training loss: 0.3406, validation loss: 0.0995
2024-06-02 19:40:47 [INFO]: Epoch 098 - training loss: 0.3414, validation loss: 0.1076
2024-06-02 19:40:48 [INFO]: Epoch 099 - training loss: 0.3345, validation loss: 0.1000
2024-06-02 19:40:49 [INFO]: Epoch 100 - training loss: 0.3361, validation loss: 0.0998
2024-06-02 19:40:49 [INFO]: Finished training. The best model is from epoch#97.
2024-06-02 19:40:49 [INFO]: Saved the model to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_1/20240602_T193947/Crossformer.pypots
2024-06-02 19:40:49 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_1/imputation.pkl
2024-06-02 19:40:49 [INFO]: Round1 - Crossformer on ETT_h1: MAE=0.2511, MSE=0.1306, MRE=0.2971
2024-06-02 19:40:49 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 19:40:49 [INFO]: Using the given device: cuda:0
2024-06-02 19:40:49 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_2/20240602_T194049
2024-06-02 19:40:49 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_2/20240602_T194049/tensorboard
2024-06-02 19:40:49 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 223,479
2024-06-02 19:40:50 [INFO]: Epoch 001 - training loss: 1.5192, validation loss: 0.9215
2024-06-02 19:40:50 [INFO]: Epoch 002 - training loss: 1.3883, validation loss: 0.9084
2024-06-02 19:40:51 [INFO]: Epoch 003 - training loss: 1.1195, validation loss: 0.6071
2024-06-02 19:40:51 [INFO]: Epoch 004 - training loss: 0.9444, validation loss: 0.5345
2024-06-02 19:40:52 [INFO]: Epoch 005 - training loss: 0.8759, validation loss: 0.3784
2024-06-02 19:40:53 [INFO]: Epoch 006 - training loss: 0.7297, validation loss: 0.2430
2024-06-02 19:40:53 [INFO]: Epoch 007 - training loss: 0.6416, validation loss: 0.2314
2024-06-02 19:40:54 [INFO]: Epoch 008 - training loss: 0.5983, validation loss: 0.1980
2024-06-02 19:40:54 [INFO]: Epoch 009 - training loss: 0.5937, validation loss: 0.1853
2024-06-02 19:40:55 [INFO]: Epoch 010 - training loss: 0.5492, validation loss: 0.1876
2024-06-02 19:40:56 [INFO]: Epoch 011 - training loss: 0.5508, validation loss: 0.1796
2024-06-02 19:40:56 [INFO]: Epoch 012 - training loss: 0.5234, validation loss: 0.1736
2024-06-02 19:40:57 [INFO]: Epoch 013 - training loss: 0.5122, validation loss: 0.1669
2024-06-02 19:40:57 [INFO]: Epoch 014 - training loss: 0.5168, validation loss: 0.1652
2024-06-02 19:40:58 [INFO]: Epoch 015 - training loss: 0.5136, validation loss: 0.1659
2024-06-02 19:40:58 [INFO]: Epoch 016 - training loss: 0.4970, validation loss: 0.1561
2024-06-02 19:40:59 [INFO]: Epoch 017 - training loss: 0.4899, validation loss: 0.1614
2024-06-02 19:41:00 [INFO]: Epoch 018 - training loss: 0.4809, validation loss: 0.1584
2024-06-02 19:41:00 [INFO]: Epoch 019 - training loss: 0.4759, validation loss: 0.1579
2024-06-02 19:41:01 [INFO]: Epoch 020 - training loss: 0.4755, validation loss: 0.1577
2024-06-02 19:41:01 [INFO]: Epoch 021 - training loss: 0.4687, validation loss: 0.1547
2024-06-02 19:41:02 [INFO]: Epoch 022 - training loss: 0.4664, validation loss: 0.1549
2024-06-02 19:41:02 [INFO]: Epoch 023 - training loss: 0.4615, validation loss: 0.1639
2024-06-02 19:41:03 [INFO]: Epoch 024 - training loss: 0.4581, validation loss: 0.1609
2024-06-02 19:41:03 [INFO]: Epoch 025 - training loss: 0.4575, validation loss: 0.1478
2024-06-02 19:41:04 [INFO]: Epoch 026 - training loss: 0.4556, validation loss: 0.1591
2024-06-02 19:41:04 [INFO]: Epoch 027 - training loss: 0.4440, validation loss: 0.1542
2024-06-02 19:41:05 [INFO]: Epoch 028 - training loss: 0.4438, validation loss: 0.1593
2024-06-02 19:41:05 [INFO]: Epoch 029 - training loss: 0.4442, validation loss: 0.1513
2024-06-02 19:41:06 [INFO]: Epoch 030 - training loss: 0.4423, validation loss: 0.1474
2024-06-02 19:41:07 [INFO]: Epoch 031 - training loss: 0.4475, validation loss: 0.1547
2024-06-02 19:41:07 [INFO]: Epoch 032 - training loss: 0.4386, validation loss: 0.1543
2024-06-02 19:41:08 [INFO]: Epoch 033 - training loss: 0.4384, validation loss: 0.1684
2024-06-02 19:41:09 [INFO]: Epoch 034 - training loss: 0.4384, validation loss: 0.1630
2024-06-02 19:41:09 [INFO]: Epoch 035 - training loss: 0.4332, validation loss: 0.1608
2024-06-02 19:41:10 [INFO]: Epoch 036 - training loss: 0.4277, validation loss: 0.1491
2024-06-02 19:41:10 [INFO]: Epoch 037 - training loss: 0.4288, validation loss: 0.1511
2024-06-02 19:41:11 [INFO]: Epoch 038 - training loss: 0.4135, validation loss: 0.1532
2024-06-02 19:41:11 [INFO]: Epoch 039 - training loss: 0.4138, validation loss: 0.1446
2024-06-02 19:41:12 [INFO]: Epoch 040 - training loss: 0.4133, validation loss: 0.1560
2024-06-02 19:41:12 [INFO]: Epoch 041 - training loss: 0.4167, validation loss: 0.1392
2024-06-02 19:41:12 [INFO]: Epoch 042 - training loss: 0.4066, validation loss: 0.1442
2024-06-02 19:41:13 [INFO]: Epoch 043 - training loss: 0.4054, validation loss: 0.1414
2024-06-02 19:41:13 [INFO]: Epoch 044 - training loss: 0.4012, validation loss: 0.1542
2024-06-02 19:41:14 [INFO]: Epoch 045 - training loss: 0.4094, validation loss: 0.1413
2024-06-02 19:41:14 [INFO]: Epoch 046 - training loss: 0.4004, validation loss: 0.1412
2024-06-02 19:41:15 [INFO]: Epoch 047 - training loss: 0.4050, validation loss: 0.1451
2024-06-02 19:41:15 [INFO]: Epoch 048 - training loss: 0.4010, validation loss: 0.1393
2024-06-02 19:41:16 [INFO]: Epoch 049 - training loss: 0.3943, validation loss: 0.1333
2024-06-02 19:41:16 [INFO]: Epoch 050 - training loss: 0.3931, validation loss: 0.1362
2024-06-02 19:41:17 [INFO]: Epoch 051 - training loss: 0.3912, validation loss: 0.1422
2024-06-02 19:41:17 [INFO]: Epoch 052 - training loss: 0.3876, validation loss: 0.1440
2024-06-02 19:41:17 [INFO]: Epoch 053 - training loss: 0.3808, validation loss: 0.1336
2024-06-02 19:41:18 [INFO]: Epoch 054 - training loss: 0.3819, validation loss: 0.1376
2024-06-02 19:41:18 [INFO]: Epoch 055 - training loss: 0.3851, validation loss: 0.1513
2024-06-02 19:41:19 [INFO]: Epoch 056 - training loss: 0.3888, validation loss: 0.1394
2024-06-02 19:41:19 [INFO]: Epoch 057 - training loss: 0.3907, validation loss: 0.1443
2024-06-02 19:41:20 [INFO]: Epoch 058 - training loss: 0.3853, validation loss: 0.1404
2024-06-02 19:41:20 [INFO]: Epoch 059 - training loss: 0.3822, validation loss: 0.1281
2024-06-02 19:41:21 [INFO]: Epoch 060 - training loss: 0.3805, validation loss: 0.1373
2024-06-02 19:41:21 [INFO]: Epoch 061 - training loss: 0.3797, validation loss: 0.1330
2024-06-02 19:41:22 [INFO]: Epoch 062 - training loss: 0.3778, validation loss: 0.1363
2024-06-02 19:41:22 [INFO]: Epoch 063 - training loss: 0.3759, validation loss: 0.1502
2024-06-02 19:41:23 [INFO]: Epoch 064 - training loss: 0.3748, validation loss: 0.1329
2024-06-02 19:41:23 [INFO]: Epoch 065 - training loss: 0.3677, validation loss: 0.1372
2024-06-02 19:41:24 [INFO]: Epoch 066 - training loss: 0.3729, validation loss: 0.1414
2024-06-02 19:41:24 [INFO]: Epoch 067 - training loss: 0.3665, validation loss: 0.1284
2024-06-02 19:41:25 [INFO]: Epoch 068 - training loss: 0.3654, validation loss: 0.1261
2024-06-02 19:41:25 [INFO]: Epoch 069 - training loss: 0.3653, validation loss: 0.1344
2024-06-02 19:41:26 [INFO]: Epoch 070 - training loss: 0.3697, validation loss: 0.1317
2024-06-02 19:41:26 [INFO]: Epoch 071 - training loss: 0.3659, validation loss: 0.1321
2024-06-02 19:41:27 [INFO]: Epoch 072 - training loss: 0.3621, validation loss: 0.1305
2024-06-02 19:41:27 [INFO]: Epoch 073 - training loss: 0.3638, validation loss: 0.1285
2024-06-02 19:41:27 [INFO]: Epoch 074 - training loss: 0.3640, validation loss: 0.1362
2024-06-02 19:41:28 [INFO]: Epoch 075 - training loss: 0.3573, validation loss: 0.1323
2024-06-02 19:41:28 [INFO]: Epoch 076 - training loss: 0.3575, validation loss: 0.1428
2024-06-02 19:41:29 [INFO]: Epoch 077 - training loss: 0.3608, validation loss: 0.1210
2024-06-02 19:41:29 [INFO]: Epoch 078 - training loss: 0.3538, validation loss: 0.1221
2024-06-02 19:41:29 [INFO]: Epoch 079 - training loss: 0.3490, validation loss: 0.1297
2024-06-02 19:41:30 [INFO]: Epoch 080 - training loss: 0.3466, validation loss: 0.1203
2024-06-02 19:41:30 [INFO]: Epoch 081 - training loss: 0.3481, validation loss: 0.1269
2024-06-02 19:41:31 [INFO]: Epoch 082 - training loss: 0.3505, validation loss: 0.1277
2024-06-02 19:41:31 [INFO]: Epoch 083 - training loss: 0.3534, validation loss: 0.1162
2024-06-02 19:41:31 [INFO]: Epoch 084 - training loss: 0.3582, validation loss: 0.1267
2024-06-02 19:41:32 [INFO]: Epoch 085 - training loss: 0.3604, validation loss: 0.1263
2024-06-02 19:41:32 [INFO]: Epoch 086 - training loss: 0.3526, validation loss: 0.1278
2024-06-02 19:41:33 [INFO]: Epoch 087 - training loss: 0.3526, validation loss: 0.1308
2024-06-02 19:41:33 [INFO]: Epoch 088 - training loss: 0.3581, validation loss: 0.1242
2024-06-02 19:41:34 [INFO]: Epoch 089 - training loss: 0.3447, validation loss: 0.1226
2024-06-02 19:41:34 [INFO]: Epoch 090 - training loss: 0.3482, validation loss: 0.1200
2024-06-02 19:41:35 [INFO]: Epoch 091 - training loss: 0.3489, validation loss: 0.1167
2024-06-02 19:41:35 [INFO]: Epoch 092 - training loss: 0.3535, validation loss: 0.1264
2024-06-02 19:41:35 [INFO]: Epoch 093 - training loss: 0.3504, validation loss: 0.1184
2024-06-02 19:41:35 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:41:35 [INFO]: Finished training. The best model is from epoch#83.
2024-06-02 19:41:35 [INFO]: Saved the model to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_2/20240602_T194049/Crossformer.pypots
2024-06-02 19:41:36 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_2/imputation.pkl
2024-06-02 19:41:36 [INFO]: Round2 - Crossformer on ETT_h1: MAE=0.2725, MSE=0.1465, MRE=0.3223
2024-06-02 19:41:36 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 19:41:36 [INFO]: Using the given device: cuda:0
2024-06-02 19:41:36 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_3/20240602_T194136
2024-06-02 19:41:36 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_3/20240602_T194136/tensorboard
2024-06-02 19:41:36 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 223,479
2024-06-02 19:41:36 [INFO]: Epoch 001 - training loss: 1.4990, validation loss: 0.9001
2024-06-02 19:41:37 [INFO]: Epoch 002 - training loss: 1.4189, validation loss: 0.7272
2024-06-02 19:41:37 [INFO]: Epoch 003 - training loss: 1.1130, validation loss: 0.5804
2024-06-02 19:41:38 [INFO]: Epoch 004 - training loss: 0.9331, validation loss: 0.4349
2024-06-02 19:41:38 [INFO]: Epoch 005 - training loss: 0.7761, validation loss: 0.2770
2024-06-02 19:41:39 [INFO]: Epoch 006 - training loss: 0.6947, validation loss: 0.2297
2024-06-02 19:41:39 [INFO]: Epoch 007 - training loss: 0.6570, validation loss: 0.2138
2024-06-02 19:41:39 [INFO]: Epoch 008 - training loss: 0.6132, validation loss: 0.2018
2024-06-02 19:41:40 [INFO]: Epoch 009 - training loss: 0.5951, validation loss: 0.1952
2024-06-02 19:41:40 [INFO]: Epoch 010 - training loss: 0.5696, validation loss: 0.1893
2024-06-02 19:41:41 [INFO]: Epoch 011 - training loss: 0.5417, validation loss: 0.1788
2024-06-02 19:41:41 [INFO]: Epoch 012 - training loss: 0.5410, validation loss: 0.1614
2024-06-02 19:41:42 [INFO]: Epoch 013 - training loss: 0.5228, validation loss: 0.1675
2024-06-02 19:41:42 [INFO]: Epoch 014 - training loss: 0.5123, validation loss: 0.1660
2024-06-02 19:41:42 [INFO]: Epoch 015 - training loss: 0.4993, validation loss: 0.1576
2024-06-02 19:41:43 [INFO]: Epoch 016 - training loss: 0.4959, validation loss: 0.1594
2024-06-02 19:41:43 [INFO]: Epoch 017 - training loss: 0.4861, validation loss: 0.1593
2024-06-02 19:41:43 [INFO]: Epoch 018 - training loss: 0.4791, validation loss: 0.1598
2024-06-02 19:41:44 [INFO]: Epoch 019 - training loss: 0.4786, validation loss: 0.1453
2024-06-02 19:41:44 [INFO]: Epoch 020 - training loss: 0.4735, validation loss: 0.1479
2024-06-02 19:41:45 [INFO]: Epoch 021 - training loss: 0.4670, validation loss: 0.1539
2024-06-02 19:41:45 [INFO]: Epoch 022 - training loss: 0.4622, validation loss: 0.1535
2024-06-02 19:41:45 [INFO]: Epoch 023 - training loss: 0.4510, validation loss: 0.1515
2024-06-02 19:41:46 [INFO]: Epoch 024 - training loss: 0.4535, validation loss: 0.1487
2024-06-02 19:41:47 [INFO]: Epoch 025 - training loss: 0.4366, validation loss: 0.1445
2024-06-02 19:41:47 [INFO]: Epoch 026 - training loss: 0.4485, validation loss: 0.1388
2024-06-02 19:41:47 [INFO]: Epoch 027 - training loss: 0.4400, validation loss: 0.1390
2024-06-02 19:41:48 [INFO]: Epoch 028 - training loss: 0.4423, validation loss: 0.1473
2024-06-02 19:41:48 [INFO]: Epoch 029 - training loss: 0.4378, validation loss: 0.1442
2024-06-02 19:41:49 [INFO]: Epoch 030 - training loss: 0.4291, validation loss: 0.1440
2024-06-02 19:41:49 [INFO]: Epoch 031 - training loss: 0.4397, validation loss: 0.1387
2024-06-02 19:41:49 [INFO]: Epoch 032 - training loss: 0.4302, validation loss: 0.1406
2024-06-02 19:41:50 [INFO]: Epoch 033 - training loss: 0.4281, validation loss: 0.1333
2024-06-02 19:41:50 [INFO]: Epoch 034 - training loss: 0.4265, validation loss: 0.1489
2024-06-02 19:41:50 [INFO]: Epoch 035 - training loss: 0.4211, validation loss: 0.1329
2024-06-02 19:41:51 [INFO]: Epoch 036 - training loss: 0.4156, validation loss: 0.1470
2024-06-02 19:41:51 [INFO]: Epoch 037 - training loss: 0.4133, validation loss: 0.1406
2024-06-02 19:41:51 [INFO]: Epoch 038 - training loss: 0.4142, validation loss: 0.1462
2024-06-02 19:41:52 [INFO]: Epoch 039 - training loss: 0.4090, validation loss: 0.1399
2024-06-02 19:41:52 [INFO]: Epoch 040 - training loss: 0.4109, validation loss: 0.1297
2024-06-02 19:41:52 [INFO]: Epoch 041 - training loss: 0.4061, validation loss: 0.1296
2024-06-02 19:41:53 [INFO]: Epoch 042 - training loss: 0.3946, validation loss: 0.1258
2024-06-02 19:41:53 [INFO]: Epoch 043 - training loss: 0.4068, validation loss: 0.1419
2024-06-02 19:41:53 [INFO]: Epoch 044 - training loss: 0.4006, validation loss: 0.1424
2024-06-02 19:41:54 [INFO]: Epoch 045 - training loss: 0.3959, validation loss: 0.1396
2024-06-02 19:41:54 [INFO]: Epoch 046 - training loss: 0.4089, validation loss: 0.1375
2024-06-02 19:41:54 [INFO]: Epoch 047 - training loss: 0.4002, validation loss: 0.1245
2024-06-02 19:41:55 [INFO]: Epoch 048 - training loss: 0.3865, validation loss: 0.1335
2024-06-02 19:41:55 [INFO]: Epoch 049 - training loss: 0.3926, validation loss: 0.1368
2024-06-02 19:41:55 [INFO]: Epoch 050 - training loss: 0.3922, validation loss: 0.1290
2024-06-02 19:41:56 [INFO]: Epoch 051 - training loss: 0.3896, validation loss: 0.1303
2024-06-02 19:41:56 [INFO]: Epoch 052 - training loss: 0.3801, validation loss: 0.1302
2024-06-02 19:41:56 [INFO]: Epoch 053 - training loss: 0.3838, validation loss: 0.1217
2024-06-02 19:41:56 [INFO]: Epoch 054 - training loss: 0.3844, validation loss: 0.1209
2024-06-02 19:41:57 [INFO]: Epoch 055 - training loss: 0.3771, validation loss: 0.1259
2024-06-02 19:41:57 [INFO]: Epoch 056 - training loss: 0.3828, validation loss: 0.1294
2024-06-02 19:41:57 [INFO]: Epoch 057 - training loss: 0.3822, validation loss: 0.1192
2024-06-02 19:41:58 [INFO]: Epoch 058 - training loss: 0.3756, validation loss: 0.1339
2024-06-02 19:41:58 [INFO]: Epoch 059 - training loss: 0.3835, validation loss: 0.1277
2024-06-02 19:41:58 [INFO]: Epoch 060 - training loss: 0.3820, validation loss: 0.1251
2024-06-02 19:41:59 [INFO]: Epoch 061 - training loss: 0.3832, validation loss: 0.1209
2024-06-02 19:41:59 [INFO]: Epoch 062 - training loss: 0.3749, validation loss: 0.1261
2024-06-02 19:41:59 [INFO]: Epoch 063 - training loss: 0.3706, validation loss: 0.1209
2024-06-02 19:42:00 [INFO]: Epoch 064 - training loss: 0.3772, validation loss: 0.1159
2024-06-02 19:42:00 [INFO]: Epoch 065 - training loss: 0.3586, validation loss: 0.1242
2024-06-02 19:42:00 [INFO]: Epoch 066 - training loss: 0.3666, validation loss: 0.1246
2024-06-02 19:42:01 [INFO]: Epoch 067 - training loss: 0.3621, validation loss: 0.1203
2024-06-02 19:42:01 [INFO]: Epoch 068 - training loss: 0.3679, validation loss: 0.1160
2024-06-02 19:42:01 [INFO]: Epoch 069 - training loss: 0.3643, validation loss: 0.1221
2024-06-02 19:42:02 [INFO]: Epoch 070 - training loss: 0.3629, validation loss: 0.1184
2024-06-02 19:42:02 [INFO]: Epoch 071 - training loss: 0.3576, validation loss: 0.1162
2024-06-02 19:42:02 [INFO]: Epoch 072 - training loss: 0.3714, validation loss: 0.1229
2024-06-02 19:42:03 [INFO]: Epoch 073 - training loss: 0.3590, validation loss: 0.1186
2024-06-02 19:42:03 [INFO]: Epoch 074 - training loss: 0.3575, validation loss: 0.1131
2024-06-02 19:42:03 [INFO]: Epoch 075 - training loss: 0.3603, validation loss: 0.1183
2024-06-02 19:42:04 [INFO]: Epoch 076 - training loss: 0.3585, validation loss: 0.1135
2024-06-02 19:42:04 [INFO]: Epoch 077 - training loss: 0.3588, validation loss: 0.1341
2024-06-02 19:42:05 [INFO]: Epoch 078 - training loss: 0.3515, validation loss: 0.1147
2024-06-02 19:42:05 [INFO]: Epoch 079 - training loss: 0.3534, validation loss: 0.1224
2024-06-02 19:42:05 [INFO]: Epoch 080 - training loss: 0.3521, validation loss: 0.1185
2024-06-02 19:42:06 [INFO]: Epoch 081 - training loss: 0.3566, validation loss: 0.1203
2024-06-02 19:42:06 [INFO]: Epoch 082 - training loss: 0.3404, validation loss: 0.1191
2024-06-02 19:42:06 [INFO]: Epoch 083 - training loss: 0.3484, validation loss: 0.1241
2024-06-02 19:42:07 [INFO]: Epoch 084 - training loss: 0.3438, validation loss: 0.1138
2024-06-02 19:42:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:42:07 [INFO]: Finished training. The best model is from epoch#74.
2024-06-02 19:42:07 [INFO]: Saved the model to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_3/20240602_T194136/Crossformer.pypots
2024-06-02 19:42:07 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_3/imputation.pkl
2024-06-02 19:42:07 [INFO]: Round3 - Crossformer on ETT_h1: MAE=0.2619, MSE=0.1396, MRE=0.3098
2024-06-02 19:42:07 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 19:42:07 [INFO]: Using the given device: cuda:0
2024-06-02 19:42:07 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_4/20240602_T194207
2024-06-02 19:42:07 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_4/20240602_T194207/tensorboard
2024-06-02 19:42:07 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 223,479
2024-06-02 19:42:08 [INFO]: Epoch 001 - training loss: 1.5193, validation loss: 0.9149
2024-06-02 19:42:08 [INFO]: Epoch 002 - training loss: 1.3160, validation loss: 0.6590
2024-06-02 19:42:08 [INFO]: Epoch 003 - training loss: 0.9925, validation loss: 0.4784
2024-06-02 19:42:09 [INFO]: Epoch 004 - training loss: 0.8562, validation loss: 0.4029
2024-06-02 19:42:09 [INFO]: Epoch 005 - training loss: 0.7525, validation loss: 0.3148
2024-06-02 19:42:09 [INFO]: Epoch 006 - training loss: 0.6797, validation loss: 0.2310
2024-06-02 19:42:09 [INFO]: Epoch 007 - training loss: 0.6097, validation loss: 0.2050
2024-06-02 19:42:10 [INFO]: Epoch 008 - training loss: 0.5806, validation loss: 0.2042
2024-06-02 19:42:10 [INFO]: Epoch 009 - training loss: 0.5679, validation loss: 0.1894
2024-06-02 19:42:10 [INFO]: Epoch 010 - training loss: 0.5472, validation loss: 0.1757
2024-06-02 19:42:11 [INFO]: Epoch 011 - training loss: 0.5348, validation loss: 0.1755
2024-06-02 19:42:11 [INFO]: Epoch 012 - training loss: 0.5261, validation loss: 0.1735
2024-06-02 19:42:12 [INFO]: Epoch 013 - training loss: 0.5254, validation loss: 0.1780
2024-06-02 19:42:12 [INFO]: Epoch 014 - training loss: 0.5078, validation loss: 0.1594
2024-06-02 19:42:12 [INFO]: Epoch 015 - training loss: 0.4948, validation loss: 0.1721
2024-06-02 19:42:12 [INFO]: Epoch 016 - training loss: 0.4947, validation loss: 0.1615
2024-06-02 19:42:13 [INFO]: Epoch 017 - training loss: 0.4795, validation loss: 0.1607
2024-06-02 19:42:13 [INFO]: Epoch 018 - training loss: 0.4754, validation loss: 0.1598
2024-06-02 19:42:14 [INFO]: Epoch 019 - training loss: 0.4756, validation loss: 0.1628
2024-06-02 19:42:14 [INFO]: Epoch 020 - training loss: 0.4742, validation loss: 0.1710
2024-06-02 19:42:14 [INFO]: Epoch 021 - training loss: 0.4659, validation loss: 0.1683
2024-06-02 19:42:15 [INFO]: Epoch 022 - training loss: 0.4629, validation loss: 0.1644
2024-06-02 19:42:15 [INFO]: Epoch 023 - training loss: 0.4659, validation loss: 0.1678
2024-06-02 19:42:15 [INFO]: Epoch 024 - training loss: 0.4467, validation loss: 0.1590
2024-06-02 19:42:16 [INFO]: Epoch 025 - training loss: 0.4518, validation loss: 0.1527
2024-06-02 19:42:16 [INFO]: Epoch 026 - training loss: 0.4597, validation loss: 0.1529
2024-06-02 19:42:16 [INFO]: Epoch 027 - training loss: 0.4541, validation loss: 0.1523
2024-06-02 19:42:17 [INFO]: Epoch 028 - training loss: 0.4426, validation loss: 0.1578
2024-06-02 19:42:17 [INFO]: Epoch 029 - training loss: 0.4462, validation loss: 0.1600
2024-06-02 19:42:18 [INFO]: Epoch 030 - training loss: 0.4383, validation loss: 0.1507
2024-06-02 19:42:18 [INFO]: Epoch 031 - training loss: 0.4406, validation loss: 0.1575
2024-06-02 19:42:18 [INFO]: Epoch 032 - training loss: 0.4345, validation loss: 0.1471
2024-06-02 19:42:19 [INFO]: Epoch 033 - training loss: 0.4303, validation loss: 0.1418
2024-06-02 19:42:19 [INFO]: Epoch 034 - training loss: 0.4266, validation loss: 0.1439
2024-06-02 19:42:19 [INFO]: Epoch 035 - training loss: 0.4174, validation loss: 0.1484
2024-06-02 19:42:20 [INFO]: Epoch 036 - training loss: 0.4215, validation loss: 0.1400
2024-06-02 19:42:20 [INFO]: Epoch 037 - training loss: 0.4223, validation loss: 0.1432
2024-06-02 19:42:20 [INFO]: Epoch 038 - training loss: 0.4219, validation loss: 0.1373
2024-06-02 19:42:21 [INFO]: Epoch 039 - training loss: 0.4113, validation loss: 0.1482
2024-06-02 19:42:21 [INFO]: Epoch 040 - training loss: 0.4150, validation loss: 0.1317
2024-06-02 19:42:21 [INFO]: Epoch 041 - training loss: 0.4027, validation loss: 0.1335
2024-06-02 19:42:22 [INFO]: Epoch 042 - training loss: 0.3959, validation loss: 0.1422
2024-06-02 19:42:22 [INFO]: Epoch 043 - training loss: 0.3960, validation loss: 0.1296
2024-06-02 19:42:23 [INFO]: Epoch 044 - training loss: 0.3946, validation loss: 0.1357
2024-06-02 19:42:23 [INFO]: Epoch 045 - training loss: 0.3892, validation loss: 0.1281
2024-06-02 19:42:23 [INFO]: Epoch 046 - training loss: 0.3938, validation loss: 0.1221
2024-06-02 19:42:24 [INFO]: Epoch 047 - training loss: 0.3873, validation loss: 0.1321
2024-06-02 19:42:24 [INFO]: Epoch 048 - training loss: 0.3853, validation loss: 0.1263
2024-06-02 19:42:24 [INFO]: Epoch 049 - training loss: 0.3926, validation loss: 0.1360
2024-06-02 19:42:25 [INFO]: Epoch 050 - training loss: 0.3936, validation loss: 0.1485
2024-06-02 19:42:25 [INFO]: Epoch 051 - training loss: 0.3954, validation loss: 0.1281
2024-06-02 19:42:25 [INFO]: Epoch 052 - training loss: 0.3858, validation loss: 0.1196
2024-06-02 19:42:25 [INFO]: Epoch 053 - training loss: 0.3911, validation loss: 0.1257
2024-06-02 19:42:26 [INFO]: Epoch 054 - training loss: 0.3805, validation loss: 0.1181
2024-06-02 19:42:26 [INFO]: Epoch 055 - training loss: 0.3836, validation loss: 0.1278
2024-06-02 19:42:26 [INFO]: Epoch 056 - training loss: 0.3799, validation loss: 0.1312
2024-06-02 19:42:27 [INFO]: Epoch 057 - training loss: 0.3775, validation loss: 0.1221
2024-06-02 19:42:27 [INFO]: Epoch 058 - training loss: 0.3766, validation loss: 0.1280
2024-06-02 19:42:27 [INFO]: Epoch 059 - training loss: 0.3808, validation loss: 0.1265
2024-06-02 19:42:27 [INFO]: Epoch 060 - training loss: 0.3709, validation loss: 0.1180
2024-06-02 19:42:28 [INFO]: Epoch 061 - training loss: 0.3661, validation loss: 0.1180
2024-06-02 19:42:28 [INFO]: Epoch 062 - training loss: 0.3650, validation loss: 0.1158
2024-06-02 19:42:28 [INFO]: Epoch 063 - training loss: 0.3658, validation loss: 0.1239
2024-06-02 19:42:29 [INFO]: Epoch 064 - training loss: 0.3702, validation loss: 0.1201
2024-06-02 19:42:29 [INFO]: Epoch 065 - training loss: 0.3660, validation loss: 0.1164
2024-06-02 19:42:29 [INFO]: Epoch 066 - training loss: 0.3637, validation loss: 0.1182
2024-06-02 19:42:30 [INFO]: Epoch 067 - training loss: 0.3572, validation loss: 0.1239
2024-06-02 19:42:30 [INFO]: Epoch 068 - training loss: 0.3586, validation loss: 0.1214
2024-06-02 19:42:30 [INFO]: Epoch 069 - training loss: 0.3590, validation loss: 0.1143
2024-06-02 19:42:31 [INFO]: Epoch 070 - training loss: 0.3656, validation loss: 0.1211
2024-06-02 19:42:31 [INFO]: Epoch 071 - training loss: 0.3717, validation loss: 0.1146
2024-06-02 19:42:31 [INFO]: Epoch 072 - training loss: 0.3685, validation loss: 0.1152
2024-06-02 19:42:32 [INFO]: Epoch 073 - training loss: 0.3596, validation loss: 0.1214
2024-06-02 19:42:32 [INFO]: Epoch 074 - training loss: 0.3556, validation loss: 0.1163
2024-06-02 19:42:32 [INFO]: Epoch 075 - training loss: 0.3535, validation loss: 0.1125
2024-06-02 19:42:33 [INFO]: Epoch 076 - training loss: 0.3561, validation loss: 0.1102
2024-06-02 19:42:33 [INFO]: Epoch 077 - training loss: 0.3531, validation loss: 0.1122
2024-06-02 19:42:33 [INFO]: Epoch 078 - training loss: 0.3497, validation loss: 0.1109
2024-06-02 19:42:34 [INFO]: Epoch 079 - training loss: 0.3366, validation loss: 0.1100
2024-06-02 19:42:34 [INFO]: Epoch 080 - training loss: 0.3446, validation loss: 0.1156
2024-06-02 19:42:34 [INFO]: Epoch 081 - training loss: 0.3497, validation loss: 0.1099
2024-06-02 19:42:35 [INFO]: Epoch 082 - training loss: 0.3457, validation loss: 0.1128
2024-06-02 19:42:35 [INFO]: Epoch 083 - training loss: 0.3509, validation loss: 0.1102
2024-06-02 19:42:36 [INFO]: Epoch 084 - training loss: 0.3419, validation loss: 0.1182
2024-06-02 19:42:36 [INFO]: Epoch 085 - training loss: 0.3406, validation loss: 0.1083
2024-06-02 19:42:36 [INFO]: Epoch 086 - training loss: 0.3396, validation loss: 0.1094
2024-06-02 19:42:36 [INFO]: Epoch 087 - training loss: 0.3367, validation loss: 0.1041
2024-06-02 19:42:37 [INFO]: Epoch 088 - training loss: 0.3357, validation loss: 0.1142
2024-06-02 19:42:37 [INFO]: Epoch 089 - training loss: 0.3359, validation loss: 0.1070
2024-06-02 19:42:37 [INFO]: Epoch 090 - training loss: 0.3352, validation loss: 0.1109
2024-06-02 19:42:37 [INFO]: Epoch 091 - training loss: 0.3356, validation loss: 0.1032
2024-06-02 19:42:38 [INFO]: Epoch 092 - training loss: 0.3395, validation loss: 0.1044
2024-06-02 19:42:38 [INFO]: Epoch 093 - training loss: 0.3408, validation loss: 0.0988
2024-06-02 19:42:39 [INFO]: Epoch 094 - training loss: 0.3405, validation loss: 0.1081
2024-06-02 19:42:39 [INFO]: Epoch 095 - training loss: 0.3307, validation loss: 0.1097
2024-06-02 19:42:39 [INFO]: Epoch 096 - training loss: 0.3281, validation loss: 0.1041
2024-06-02 19:42:40 [INFO]: Epoch 097 - training loss: 0.3258, validation loss: 0.1141
2024-06-02 19:42:40 [INFO]: Epoch 098 - training loss: 0.3260, validation loss: 0.1071
2024-06-02 19:42:40 [INFO]: Epoch 099 - training loss: 0.3336, validation loss: 0.1027
2024-06-02 19:42:41 [INFO]: Epoch 100 - training loss: 0.3310, validation loss: 0.1074
2024-06-02 19:42:41 [INFO]: Finished training. The best model is from epoch#93.
2024-06-02 19:42:41 [INFO]: Saved the model to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_4/20240602_T194207/Crossformer.pypots
2024-06-02 19:42:41 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Crossformer_ETT_h1/round_4/imputation.pkl
2024-06-02 19:42:41 [INFO]: Round4 - Crossformer on ETT_h1: MAE=0.2539, MSE=0.1346, MRE=0.3004
2024-06-02 19:42:41 [INFO]: Done! Final results:
Averaged Crossformer (223,479 params) on ETT_h1: MAE=0.2696 ± 0.020788725408193882, MSE=0.1458 ± 0.016771650130690048, MRE=0.3189 ± 0.02459299454875716, average inference time=0.04
