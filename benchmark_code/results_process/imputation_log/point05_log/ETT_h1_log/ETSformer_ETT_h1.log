2024-06-02 19:39:17 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:39:17 [INFO]: Using the given device: cuda:0
2024-06-02 19:39:17 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_0/20240602_T193917
2024-06-02 19:39:17 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_0/20240602_T193917/tensorboard
2024-06-02 19:39:18 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 809,057
2024-06-02 19:39:24 [INFO]: Epoch 001 - training loss: 1.2225, validation loss: 0.7648
2024-06-02 19:39:25 [INFO]: Epoch 002 - training loss: 1.0732, validation loss: 0.6493
2024-06-02 19:39:26 [INFO]: Epoch 003 - training loss: 0.9627, validation loss: 0.6572
2024-06-02 19:39:27 [INFO]: Epoch 004 - training loss: 0.8590, validation loss: 0.5980
2024-06-02 19:39:29 [INFO]: Epoch 005 - training loss: 0.7984, validation loss: 0.5980
2024-06-02 19:39:30 [INFO]: Epoch 006 - training loss: 0.7530, validation loss: 0.5528
2024-06-02 19:39:31 [INFO]: Epoch 007 - training loss: 0.7353, validation loss: 0.4854
2024-06-02 19:39:32 [INFO]: Epoch 008 - training loss: 0.7197, validation loss: 0.5169
2024-06-02 19:39:34 [INFO]: Epoch 009 - training loss: 0.6883, validation loss: 0.4459
2024-06-02 19:39:35 [INFO]: Epoch 010 - training loss: 0.6792, validation loss: 0.4616
2024-06-02 19:39:36 [INFO]: Epoch 011 - training loss: 0.6637, validation loss: 0.4483
2024-06-02 19:39:38 [INFO]: Epoch 012 - training loss: 0.6673, validation loss: 0.4329
2024-06-02 19:39:39 [INFO]: Epoch 013 - training loss: 0.6553, validation loss: 0.4068
2024-06-02 19:39:40 [INFO]: Epoch 014 - training loss: 0.6425, validation loss: 0.3849
2024-06-02 19:39:41 [INFO]: Epoch 015 - training loss: 0.6495, validation loss: 0.4043
2024-06-02 19:39:42 [INFO]: Epoch 016 - training loss: 0.6384, validation loss: 0.3958
2024-06-02 19:39:43 [INFO]: Epoch 017 - training loss: 0.6339, validation loss: 0.3992
2024-06-02 19:39:45 [INFO]: Epoch 018 - training loss: 0.6236, validation loss: 0.3755
2024-06-02 19:39:46 [INFO]: Epoch 019 - training loss: 0.6178, validation loss: 0.3641
2024-06-02 19:39:47 [INFO]: Epoch 020 - training loss: 0.6117, validation loss: 0.3789
2024-06-02 19:39:49 [INFO]: Epoch 021 - training loss: 0.6172, validation loss: 0.3489
2024-06-02 19:39:50 [INFO]: Epoch 022 - training loss: 0.6132, validation loss: 0.3428
2024-06-02 19:39:51 [INFO]: Epoch 023 - training loss: 0.6064, validation loss: 0.3328
2024-06-02 19:39:52 [INFO]: Epoch 024 - training loss: 0.6110, validation loss: 0.3447
2024-06-02 19:39:53 [INFO]: Epoch 025 - training loss: 0.5988, validation loss: 0.3276
2024-06-02 19:39:54 [INFO]: Epoch 026 - training loss: 0.6052, validation loss: 0.3286
2024-06-02 19:39:56 [INFO]: Epoch 027 - training loss: 0.5943, validation loss: 0.3132
2024-06-02 19:39:57 [INFO]: Epoch 028 - training loss: 0.5832, validation loss: 0.3086
2024-06-02 19:39:58 [INFO]: Epoch 029 - training loss: 0.5779, validation loss: 0.3161
2024-06-02 19:40:00 [INFO]: Epoch 030 - training loss: 0.5927, validation loss: 0.3160
2024-06-02 19:40:01 [INFO]: Epoch 031 - training loss: 0.5813, validation loss: 0.3070
2024-06-02 19:40:02 [INFO]: Epoch 032 - training loss: 0.5891, validation loss: 0.2926
2024-06-02 19:40:03 [INFO]: Epoch 033 - training loss: 0.5789, validation loss: 0.3078
2024-06-02 19:40:04 [INFO]: Epoch 034 - training loss: 0.5845, validation loss: 0.3131
2024-06-02 19:40:06 [INFO]: Epoch 035 - training loss: 0.5778, validation loss: 0.2909
2024-06-02 19:40:07 [INFO]: Epoch 036 - training loss: 0.5829, validation loss: 0.2804
2024-06-02 19:40:08 [INFO]: Epoch 037 - training loss: 0.5772, validation loss: 0.2873
2024-06-02 19:40:10 [INFO]: Epoch 038 - training loss: 0.5730, validation loss: 0.2904
2024-06-02 19:40:11 [INFO]: Epoch 039 - training loss: 0.5792, validation loss: 0.2727
2024-06-02 19:40:12 [INFO]: Epoch 040 - training loss: 0.5651, validation loss: 0.2814
2024-06-02 19:40:13 [INFO]: Epoch 041 - training loss: 0.5778, validation loss: 0.2810
2024-06-02 19:40:14 [INFO]: Epoch 042 - training loss: 0.5706, validation loss: 0.2735
2024-06-02 19:40:15 [INFO]: Epoch 043 - training loss: 0.5672, validation loss: 0.2618
2024-06-02 19:40:17 [INFO]: Epoch 044 - training loss: 0.5669, validation loss: 0.2631
2024-06-02 19:40:18 [INFO]: Epoch 045 - training loss: 0.5627, validation loss: 0.2825
2024-06-02 19:40:19 [INFO]: Epoch 046 - training loss: 0.5619, validation loss: 0.2716
2024-06-02 19:40:20 [INFO]: Epoch 047 - training loss: 0.5600, validation loss: 0.2629
2024-06-02 19:40:21 [INFO]: Epoch 048 - training loss: 0.5615, validation loss: 0.2554
2024-06-02 19:40:22 [INFO]: Epoch 049 - training loss: 0.5536, validation loss: 0.2598
2024-06-02 19:40:23 [INFO]: Epoch 050 - training loss: 0.5683, validation loss: 0.2656
2024-06-02 19:40:24 [INFO]: Epoch 051 - training loss: 0.5663, validation loss: 0.2560
2024-06-02 19:40:25 [INFO]: Epoch 052 - training loss: 0.5640, validation loss: 0.2592
2024-06-02 19:40:26 [INFO]: Epoch 053 - training loss: 0.5620, validation loss: 0.2552
2024-06-02 19:40:27 [INFO]: Epoch 054 - training loss: 0.5572, validation loss: 0.2531
2024-06-02 19:40:28 [INFO]: Epoch 055 - training loss: 0.5633, validation loss: 0.2589
2024-06-02 19:40:30 [INFO]: Epoch 056 - training loss: 0.5591, validation loss: 0.2504
2024-06-02 19:40:31 [INFO]: Epoch 057 - training loss: 0.5485, validation loss: 0.2471
2024-06-02 19:40:32 [INFO]: Epoch 058 - training loss: 0.5501, validation loss: 0.2407
2024-06-02 19:40:33 [INFO]: Epoch 059 - training loss: 0.5512, validation loss: 0.2414
2024-06-02 19:40:34 [INFO]: Epoch 060 - training loss: 0.5588, validation loss: 0.2426
2024-06-02 19:40:35 [INFO]: Epoch 061 - training loss: 0.5522, validation loss: 0.2505
2024-06-02 19:40:36 [INFO]: Epoch 062 - training loss: 0.5516, validation loss: 0.2463
2024-06-02 19:40:37 [INFO]: Epoch 063 - training loss: 0.5561, validation loss: 0.2345
2024-06-02 19:40:38 [INFO]: Epoch 064 - training loss: 0.5550, validation loss: 0.2344
2024-06-02 19:40:39 [INFO]: Epoch 065 - training loss: 0.5484, validation loss: 0.2478
2024-06-02 19:40:41 [INFO]: Epoch 066 - training loss: 0.5456, validation loss: 0.2326
2024-06-02 19:40:42 [INFO]: Epoch 067 - training loss: 0.5458, validation loss: 0.2213
2024-06-02 19:40:43 [INFO]: Epoch 068 - training loss: 0.5472, validation loss: 0.2245
2024-06-02 19:40:44 [INFO]: Epoch 069 - training loss: 0.5469, validation loss: 0.2339
2024-06-02 19:40:45 [INFO]: Epoch 070 - training loss: 0.5632, validation loss: 0.2291
2024-06-02 19:40:46 [INFO]: Epoch 071 - training loss: 0.5400, validation loss: 0.2272
2024-06-02 19:40:47 [INFO]: Epoch 072 - training loss: 0.5437, validation loss: 0.2329
2024-06-02 19:40:48 [INFO]: Epoch 073 - training loss: 0.5491, validation loss: 0.2359
2024-06-02 19:40:49 [INFO]: Epoch 074 - training loss: 0.5449, validation loss: 0.2284
2024-06-02 19:40:51 [INFO]: Epoch 075 - training loss: 0.5363, validation loss: 0.2308
2024-06-02 19:40:52 [INFO]: Epoch 076 - training loss: 0.5426, validation loss: 0.2395
2024-06-02 19:40:53 [INFO]: Epoch 077 - training loss: 0.5549, validation loss: 0.2251
2024-06-02 19:40:53 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:40:53 [INFO]: Finished training. The best model is from epoch#67.
2024-06-02 19:40:53 [INFO]: Saved the model to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_0/20240602_T193917/ETSformer.pypots
2024-06-02 19:40:54 [INFO]: Successfully saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_0/imputation.pkl
2024-06-02 19:40:54 [INFO]: Round0 - ETSformer on ETT_h1: MAE=0.3666, MSE=0.2806, MRE=0.4336
2024-06-02 19:40:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 19:40:54 [INFO]: Using the given device: cuda:0
2024-06-02 19:40:54 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_1/20240602_T194054
2024-06-02 19:40:54 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_1/20240602_T194054/tensorboard
2024-06-02 19:40:54 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 809,057
2024-06-02 19:40:55 [INFO]: Epoch 001 - training loss: 1.2807, validation loss: 0.7357
2024-06-02 19:40:56 [INFO]: Epoch 002 - training loss: 1.0871, validation loss: 0.6414
2024-06-02 19:40:57 [INFO]: Epoch 003 - training loss: 0.9842, validation loss: 0.6136
2024-06-02 19:40:58 [INFO]: Epoch 004 - training loss: 0.8899, validation loss: 0.6169
2024-06-02 19:40:59 [INFO]: Epoch 005 - training loss: 0.8311, validation loss: 0.6360
2024-06-02 19:41:00 [INFO]: Epoch 006 - training loss: 0.7804, validation loss: 0.6065
2024-06-02 19:41:02 [INFO]: Epoch 007 - training loss: 0.7487, validation loss: 0.5160
2024-06-02 19:41:03 [INFO]: Epoch 008 - training loss: 0.7254, validation loss: 0.5204
2024-06-02 19:41:03 [INFO]: Epoch 009 - training loss: 0.7158, validation loss: 0.5057
2024-06-02 19:41:04 [INFO]: Epoch 010 - training loss: 0.6832, validation loss: 0.4882
2024-06-02 19:41:05 [INFO]: Epoch 011 - training loss: 0.6488, validation loss: 0.4638
2024-06-02 19:41:07 [INFO]: Epoch 012 - training loss: 0.6608, validation loss: 0.4672
2024-06-02 19:41:08 [INFO]: Epoch 013 - training loss: 0.6386, validation loss: 0.4453
2024-06-02 19:41:09 [INFO]: Epoch 014 - training loss: 0.6360, validation loss: 0.4223
2024-06-02 19:41:10 [INFO]: Epoch 015 - training loss: 0.6227, validation loss: 0.4193
2024-06-02 19:41:11 [INFO]: Epoch 016 - training loss: 0.6208, validation loss: 0.4225
2024-06-02 19:41:12 [INFO]: Epoch 017 - training loss: 0.6203, validation loss: 0.3752
2024-06-02 19:41:13 [INFO]: Epoch 018 - training loss: 0.6171, validation loss: 0.3577
2024-06-02 19:41:14 [INFO]: Epoch 019 - training loss: 0.6160, validation loss: 0.3722
2024-06-02 19:41:15 [INFO]: Epoch 020 - training loss: 0.5981, validation loss: 0.3711
2024-06-02 19:41:16 [INFO]: Epoch 021 - training loss: 0.5968, validation loss: 0.3584
2024-06-02 19:41:17 [INFO]: Epoch 022 - training loss: 0.5828, validation loss: 0.3437
2024-06-02 19:41:18 [INFO]: Epoch 023 - training loss: 0.5910, validation loss: 0.3217
2024-06-02 19:41:19 [INFO]: Epoch 024 - training loss: 0.6019, validation loss: 0.3271
2024-06-02 19:41:20 [INFO]: Epoch 025 - training loss: 0.5864, validation loss: 0.3197
2024-06-02 19:41:22 [INFO]: Epoch 026 - training loss: 0.5775, validation loss: 0.3235
2024-06-02 19:41:23 [INFO]: Epoch 027 - training loss: 0.5784, validation loss: 0.3064
2024-06-02 19:41:23 [INFO]: Epoch 028 - training loss: 0.5713, validation loss: 0.2981
2024-06-02 19:41:24 [INFO]: Epoch 029 - training loss: 0.5866, validation loss: 0.2907
2024-06-02 19:41:25 [INFO]: Epoch 030 - training loss: 0.5696, validation loss: 0.2971
2024-06-02 19:41:26 [INFO]: Epoch 031 - training loss: 0.5698, validation loss: 0.3097
2024-06-02 19:41:27 [INFO]: Epoch 032 - training loss: 0.5744, validation loss: 0.2842
2024-06-02 19:41:27 [INFO]: Epoch 033 - training loss: 0.5670, validation loss: 0.2731
2024-06-02 19:41:28 [INFO]: Epoch 034 - training loss: 0.5661, validation loss: 0.2687
2024-06-02 19:41:29 [INFO]: Epoch 035 - training loss: 0.5577, validation loss: 0.2657
2024-06-02 19:41:30 [INFO]: Epoch 036 - training loss: 0.5682, validation loss: 0.2663
2024-06-02 19:41:31 [INFO]: Epoch 037 - training loss: 0.5598, validation loss: 0.2731
2024-06-02 19:41:32 [INFO]: Epoch 038 - training loss: 0.5714, validation loss: 0.2760
2024-06-02 19:41:33 [INFO]: Epoch 039 - training loss: 0.5684, validation loss: 0.2670
2024-06-02 19:41:34 [INFO]: Epoch 040 - training loss: 0.5622, validation loss: 0.2572
2024-06-02 19:41:34 [INFO]: Epoch 041 - training loss: 0.5570, validation loss: 0.2497
2024-06-02 19:41:35 [INFO]: Epoch 042 - training loss: 0.5588, validation loss: 0.2446
2024-06-02 19:41:36 [INFO]: Epoch 043 - training loss: 0.5588, validation loss: 0.2514
2024-06-02 19:41:37 [INFO]: Epoch 044 - training loss: 0.5597, validation loss: 0.2507
2024-06-02 19:41:38 [INFO]: Epoch 045 - training loss: 0.5529, validation loss: 0.2533
2024-06-02 19:41:39 [INFO]: Epoch 046 - training loss: 0.5409, validation loss: 0.2409
2024-06-02 19:41:40 [INFO]: Epoch 047 - training loss: 0.5532, validation loss: 0.2349
2024-06-02 19:41:40 [INFO]: Epoch 048 - training loss: 0.5452, validation loss: 0.2328
2024-06-02 19:41:41 [INFO]: Epoch 049 - training loss: 0.5463, validation loss: 0.2316
2024-06-02 19:41:42 [INFO]: Epoch 050 - training loss: 0.5595, validation loss: 0.2340
2024-06-02 19:41:43 [INFO]: Epoch 051 - training loss: 0.5522, validation loss: 0.2285
2024-06-02 19:41:44 [INFO]: Epoch 052 - training loss: 0.5402, validation loss: 0.2299
2024-06-02 19:41:45 [INFO]: Epoch 053 - training loss: 0.5302, validation loss: 0.2235
2024-06-02 19:41:45 [INFO]: Epoch 054 - training loss: 0.5456, validation loss: 0.2213
2024-06-02 19:41:46 [INFO]: Epoch 055 - training loss: 0.5362, validation loss: 0.2209
2024-06-02 19:41:47 [INFO]: Epoch 056 - training loss: 0.5411, validation loss: 0.2206
2024-06-02 19:41:48 [INFO]: Epoch 057 - training loss: 0.5304, validation loss: 0.2270
2024-06-02 19:41:48 [INFO]: Epoch 058 - training loss: 0.5356, validation loss: 0.2117
2024-06-02 19:41:49 [INFO]: Epoch 059 - training loss: 0.5422, validation loss: 0.2151
2024-06-02 19:41:50 [INFO]: Epoch 060 - training loss: 0.5380, validation loss: 0.2190
2024-06-02 19:41:50 [INFO]: Epoch 061 - training loss: 0.5375, validation loss: 0.2170
2024-06-02 19:41:51 [INFO]: Epoch 062 - training loss: 0.5323, validation loss: 0.2205
2024-06-02 19:41:51 [INFO]: Epoch 063 - training loss: 0.5270, validation loss: 0.2146
2024-06-02 19:41:52 [INFO]: Epoch 064 - training loss: 0.5327, validation loss: 0.2058
2024-06-02 19:41:52 [INFO]: Epoch 065 - training loss: 0.5267, validation loss: 0.2090
2024-06-02 19:41:53 [INFO]: Epoch 066 - training loss: 0.5373, validation loss: 0.2172
2024-06-02 19:41:53 [INFO]: Epoch 067 - training loss: 0.5365, validation loss: 0.2180
2024-06-02 19:41:54 [INFO]: Epoch 068 - training loss: 0.5380, validation loss: 0.2104
2024-06-02 19:41:55 [INFO]: Epoch 069 - training loss: 0.5202, validation loss: 0.2041
2024-06-02 19:41:56 [INFO]: Epoch 070 - training loss: 0.5225, validation loss: 0.2054
2024-06-02 19:41:57 [INFO]: Epoch 071 - training loss: 0.5173, validation loss: 0.2086
2024-06-02 19:41:57 [INFO]: Epoch 072 - training loss: 0.5202, validation loss: 0.2085
2024-06-02 19:41:58 [INFO]: Epoch 073 - training loss: 0.5160, validation loss: 0.2048
2024-06-02 19:41:59 [INFO]: Epoch 074 - training loss: 0.5200, validation loss: 0.1960
2024-06-02 19:41:59 [INFO]: Epoch 075 - training loss: 0.5213, validation loss: 0.2050
2024-06-02 19:42:00 [INFO]: Epoch 076 - training loss: 0.5303, validation loss: 0.2057
2024-06-02 19:42:00 [INFO]: Epoch 077 - training loss: 0.5150, validation loss: 0.2062
2024-06-02 19:42:01 [INFO]: Epoch 078 - training loss: 0.5191, validation loss: 0.2004
2024-06-02 19:42:01 [INFO]: Epoch 079 - training loss: 0.5118, validation loss: 0.1962
2024-06-02 19:42:02 [INFO]: Epoch 080 - training loss: 0.5180, validation loss: 0.2023
2024-06-02 19:42:03 [INFO]: Epoch 081 - training loss: 0.5152, validation loss: 0.2011
2024-06-02 19:42:03 [INFO]: Epoch 082 - training loss: 0.5082, validation loss: 0.1948
2024-06-02 19:42:04 [INFO]: Epoch 083 - training loss: 0.5101, validation loss: 0.1990
2024-06-02 19:42:04 [INFO]: Epoch 084 - training loss: 0.5089, validation loss: 0.1971
2024-06-02 19:42:05 [INFO]: Epoch 085 - training loss: 0.5167, validation loss: 0.1950
2024-06-02 19:42:05 [INFO]: Epoch 086 - training loss: 0.5086, validation loss: 0.1922
2024-06-02 19:42:06 [INFO]: Epoch 087 - training loss: 0.5099, validation loss: 0.1871
2024-06-02 19:42:07 [INFO]: Epoch 088 - training loss: 0.5145, validation loss: 0.1923
2024-06-02 19:42:07 [INFO]: Epoch 089 - training loss: 0.5104, validation loss: 0.1967
2024-06-02 19:42:08 [INFO]: Epoch 090 - training loss: 0.5222, validation loss: 0.1896
2024-06-02 19:42:09 [INFO]: Epoch 091 - training loss: 0.5104, validation loss: 0.1940
2024-06-02 19:42:09 [INFO]: Epoch 092 - training loss: 0.5132, validation loss: 0.1899
2024-06-02 19:42:10 [INFO]: Epoch 093 - training loss: 0.5132, validation loss: 0.1847
2024-06-02 19:42:11 [INFO]: Epoch 094 - training loss: 0.5075, validation loss: 0.1849
2024-06-02 19:42:12 [INFO]: Epoch 095 - training loss: 0.5130, validation loss: 0.1767
2024-06-02 19:42:12 [INFO]: Epoch 096 - training loss: 0.5020, validation loss: 0.1842
2024-06-02 19:42:13 [INFO]: Epoch 097 - training loss: 0.5038, validation loss: 0.1871
2024-06-02 19:42:13 [INFO]: Epoch 098 - training loss: 0.5108, validation loss: 0.1873
2024-06-02 19:42:14 [INFO]: Epoch 099 - training loss: 0.5010, validation loss: 0.1833
2024-06-02 19:42:14 [INFO]: Epoch 100 - training loss: 0.5041, validation loss: 0.1780
2024-06-02 19:42:14 [INFO]: Finished training. The best model is from epoch#95.
2024-06-02 19:42:15 [INFO]: Saved the model to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_1/20240602_T194054/ETSformer.pypots
2024-06-02 19:42:15 [INFO]: Successfully saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_1/imputation.pkl
2024-06-02 19:42:15 [INFO]: Round1 - ETSformer on ETT_h1: MAE=0.3453, MSE=0.2297, MRE=0.4085
2024-06-02 19:42:15 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 19:42:15 [INFO]: Using the given device: cuda:0
2024-06-02 19:42:15 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_2/20240602_T194215
2024-06-02 19:42:15 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_2/20240602_T194215/tensorboard
2024-06-02 19:42:15 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 809,057
2024-06-02 19:42:16 [INFO]: Epoch 001 - training loss: 1.2561, validation loss: 0.7180
2024-06-02 19:42:16 [INFO]: Epoch 002 - training loss: 1.0928, validation loss: 0.7279
2024-06-02 19:42:17 [INFO]: Epoch 003 - training loss: 0.9814, validation loss: 0.6093
2024-06-02 19:42:17 [INFO]: Epoch 004 - training loss: 0.8864, validation loss: 0.6242
2024-06-02 19:42:18 [INFO]: Epoch 005 - training loss: 0.8232, validation loss: 0.5680
2024-06-02 19:42:18 [INFO]: Epoch 006 - training loss: 0.7823, validation loss: 0.5288
2024-06-02 19:42:19 [INFO]: Epoch 007 - training loss: 0.7327, validation loss: 0.5148
2024-06-02 19:42:20 [INFO]: Epoch 008 - training loss: 0.7414, validation loss: 0.4782
2024-06-02 19:42:20 [INFO]: Epoch 009 - training loss: 0.6956, validation loss: 0.4865
2024-06-02 19:42:21 [INFO]: Epoch 010 - training loss: 0.6837, validation loss: 0.4187
2024-06-02 19:42:21 [INFO]: Epoch 011 - training loss: 0.6653, validation loss: 0.4470
2024-06-02 19:42:22 [INFO]: Epoch 012 - training loss: 0.6486, validation loss: 0.4270
2024-06-02 19:42:23 [INFO]: Epoch 013 - training loss: 0.6595, validation loss: 0.4210
2024-06-02 19:42:23 [INFO]: Epoch 014 - training loss: 0.6584, validation loss: 0.4070
2024-06-02 19:42:24 [INFO]: Epoch 015 - training loss: 0.6529, validation loss: 0.3888
2024-06-02 19:42:25 [INFO]: Epoch 016 - training loss: 0.6345, validation loss: 0.3728
2024-06-02 19:42:25 [INFO]: Epoch 017 - training loss: 0.6235, validation loss: 0.3858
2024-06-02 19:42:26 [INFO]: Epoch 018 - training loss: 0.6248, validation loss: 0.3834
2024-06-02 19:42:27 [INFO]: Epoch 019 - training loss: 0.6273, validation loss: 0.3634
2024-06-02 19:42:27 [INFO]: Epoch 020 - training loss: 0.6220, validation loss: 0.3733
2024-06-02 19:42:28 [INFO]: Epoch 021 - training loss: 0.6202, validation loss: 0.3659
2024-06-02 19:42:29 [INFO]: Epoch 022 - training loss: 0.6177, validation loss: 0.3530
2024-06-02 19:42:30 [INFO]: Epoch 023 - training loss: 0.6067, validation loss: 0.3322
2024-06-02 19:42:30 [INFO]: Epoch 024 - training loss: 0.6003, validation loss: 0.3529
2024-06-02 19:42:31 [INFO]: Epoch 025 - training loss: 0.6043, validation loss: 0.3410
2024-06-02 19:42:31 [INFO]: Epoch 026 - training loss: 0.5994, validation loss: 0.3186
2024-06-02 19:42:32 [INFO]: Epoch 027 - training loss: 0.6103, validation loss: 0.3331
2024-06-02 19:42:33 [INFO]: Epoch 028 - training loss: 0.5896, validation loss: 0.3271
2024-06-02 19:42:33 [INFO]: Epoch 029 - training loss: 0.5914, validation loss: 0.3179
2024-06-02 19:42:34 [INFO]: Epoch 030 - training loss: 0.5853, validation loss: 0.3093
2024-06-02 19:42:34 [INFO]: Epoch 031 - training loss: 0.5819, validation loss: 0.3147
2024-06-02 19:42:35 [INFO]: Epoch 032 - training loss: 0.5795, validation loss: 0.3083
2024-06-02 19:42:36 [INFO]: Epoch 033 - training loss: 0.5787, validation loss: 0.3094
2024-06-02 19:42:37 [INFO]: Epoch 034 - training loss: 0.5771, validation loss: 0.3032
2024-06-02 19:42:37 [INFO]: Epoch 035 - training loss: 0.5770, validation loss: 0.2867
2024-06-02 19:42:38 [INFO]: Epoch 036 - training loss: 0.5748, validation loss: 0.2952
2024-06-02 19:42:39 [INFO]: Epoch 037 - training loss: 0.5649, validation loss: 0.3011
2024-06-02 19:42:40 [INFO]: Epoch 038 - training loss: 0.5827, validation loss: 0.2928
2024-06-02 19:42:40 [INFO]: Epoch 039 - training loss: 0.5780, validation loss: 0.2889
2024-06-02 19:42:41 [INFO]: Epoch 040 - training loss: 0.5674, validation loss: 0.2732
2024-06-02 19:42:41 [INFO]: Epoch 041 - training loss: 0.5684, validation loss: 0.2814
2024-06-02 19:42:42 [INFO]: Epoch 042 - training loss: 0.5654, validation loss: 0.2864
2024-06-02 19:42:42 [INFO]: Epoch 043 - training loss: 0.5661, validation loss: 0.2780
2024-06-02 19:42:43 [INFO]: Epoch 044 - training loss: 0.5682, validation loss: 0.2650
2024-06-02 19:42:43 [INFO]: Epoch 045 - training loss: 0.5576, validation loss: 0.2747
2024-06-02 19:42:43 [INFO]: Epoch 046 - training loss: 0.5678, validation loss: 0.2700
2024-06-02 19:42:44 [INFO]: Epoch 047 - training loss: 0.5661, validation loss: 0.2662
2024-06-02 19:42:45 [INFO]: Epoch 048 - training loss: 0.5540, validation loss: 0.2628
2024-06-02 19:42:45 [INFO]: Epoch 049 - training loss: 0.5584, validation loss: 0.2595
2024-06-02 19:42:46 [INFO]: Epoch 050 - training loss: 0.5787, validation loss: 0.2603
2024-06-02 19:42:46 [INFO]: Epoch 051 - training loss: 0.5580, validation loss: 0.2605
2024-06-02 19:42:47 [INFO]: Epoch 052 - training loss: 0.5500, validation loss: 0.2542
2024-06-02 19:42:47 [INFO]: Epoch 053 - training loss: 0.5624, validation loss: 0.2566
2024-06-02 19:42:48 [INFO]: Epoch 054 - training loss: 0.5569, validation loss: 0.2483
2024-06-02 19:42:48 [INFO]: Epoch 055 - training loss: 0.5529, validation loss: 0.2601
2024-06-02 19:42:49 [INFO]: Epoch 056 - training loss: 0.5469, validation loss: 0.2579
2024-06-02 19:42:49 [INFO]: Epoch 057 - training loss: 0.5540, validation loss: 0.2380
2024-06-02 19:42:50 [INFO]: Epoch 058 - training loss: 0.5462, validation loss: 0.2429
2024-06-02 19:42:51 [INFO]: Epoch 059 - training loss: 0.5525, validation loss: 0.2603
2024-06-02 19:42:51 [INFO]: Epoch 060 - training loss: 0.5608, validation loss: 0.2618
2024-06-02 19:42:52 [INFO]: Epoch 061 - training loss: 0.5635, validation loss: 0.2448
2024-06-02 19:42:52 [INFO]: Epoch 062 - training loss: 0.5408, validation loss: 0.2409
2024-06-02 19:42:53 [INFO]: Epoch 063 - training loss: 0.5480, validation loss: 0.2470
2024-06-02 19:42:53 [INFO]: Epoch 064 - training loss: 0.5509, validation loss: 0.2458
2024-06-02 19:42:54 [INFO]: Epoch 065 - training loss: 0.5404, validation loss: 0.2376
2024-06-02 19:42:54 [INFO]: Epoch 066 - training loss: 0.5512, validation loss: 0.2393
2024-06-02 19:42:55 [INFO]: Epoch 067 - training loss: 0.5569, validation loss: 0.2308
2024-06-02 19:42:56 [INFO]: Epoch 068 - training loss: 0.5443, validation loss: 0.2309
2024-06-02 19:42:56 [INFO]: Epoch 069 - training loss: 0.5421, validation loss: 0.2406
2024-06-02 19:42:57 [INFO]: Epoch 070 - training loss: 0.5416, validation loss: 0.2291
2024-06-02 19:42:57 [INFO]: Epoch 071 - training loss: 0.5380, validation loss: 0.2331
2024-06-02 19:42:58 [INFO]: Epoch 072 - training loss: 0.5349, validation loss: 0.2292
2024-06-02 19:42:58 [INFO]: Epoch 073 - training loss: 0.5445, validation loss: 0.2262
2024-06-02 19:42:59 [INFO]: Epoch 074 - training loss: 0.5470, validation loss: 0.2382
2024-06-02 19:43:00 [INFO]: Epoch 075 - training loss: 0.5400, validation loss: 0.2217
2024-06-02 19:43:00 [INFO]: Epoch 076 - training loss: 0.5396, validation loss: 0.2242
2024-06-02 19:43:01 [INFO]: Epoch 077 - training loss: 0.5379, validation loss: 0.2393
2024-06-02 19:43:01 [INFO]: Epoch 078 - training loss: 0.5398, validation loss: 0.2399
2024-06-02 19:43:02 [INFO]: Epoch 079 - training loss: 0.5383, validation loss: 0.2230
2024-06-02 19:43:02 [INFO]: Epoch 080 - training loss: 0.5407, validation loss: 0.2256
2024-06-02 19:43:03 [INFO]: Epoch 081 - training loss: 0.5363, validation loss: 0.2193
2024-06-02 19:43:04 [INFO]: Epoch 082 - training loss: 0.5356, validation loss: 0.2225
2024-06-02 19:43:04 [INFO]: Epoch 083 - training loss: 0.5249, validation loss: 0.2167
2024-06-02 19:43:05 [INFO]: Epoch 084 - training loss: 0.5275, validation loss: 0.2231
2024-06-02 19:43:05 [INFO]: Epoch 085 - training loss: 0.5342, validation loss: 0.2146
2024-06-02 19:43:06 [INFO]: Epoch 086 - training loss: 0.5365, validation loss: 0.2121
2024-06-02 19:43:06 [INFO]: Epoch 087 - training loss: 0.5384, validation loss: 0.2217
2024-06-02 19:43:07 [INFO]: Epoch 088 - training loss: 0.5368, validation loss: 0.2178
2024-06-02 19:43:08 [INFO]: Epoch 089 - training loss: 0.5297, validation loss: 0.2193
2024-06-02 19:43:08 [INFO]: Epoch 090 - training loss: 0.5288, validation loss: 0.2154
2024-06-02 19:43:09 [INFO]: Epoch 091 - training loss: 0.5342, validation loss: 0.2137
2024-06-02 19:43:10 [INFO]: Epoch 092 - training loss: 0.5355, validation loss: 0.2133
2024-06-02 19:43:10 [INFO]: Epoch 093 - training loss: 0.5308, validation loss: 0.2147
2024-06-02 19:43:11 [INFO]: Epoch 094 - training loss: 0.5316, validation loss: 0.2089
2024-06-02 19:43:11 [INFO]: Epoch 095 - training loss: 0.5318, validation loss: 0.2060
2024-06-02 19:43:12 [INFO]: Epoch 096 - training loss: 0.5345, validation loss: 0.2099
2024-06-02 19:43:12 [INFO]: Epoch 097 - training loss: 0.5337, validation loss: 0.2127
2024-06-02 19:43:13 [INFO]: Epoch 098 - training loss: 0.5308, validation loss: 0.2190
2024-06-02 19:43:13 [INFO]: Epoch 099 - training loss: 0.5413, validation loss: 0.2109
2024-06-02 19:43:14 [INFO]: Epoch 100 - training loss: 0.5270, validation loss: 0.2130
2024-06-02 19:43:14 [INFO]: Finished training. The best model is from epoch#95.
2024-06-02 19:43:14 [INFO]: Saved the model to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_2/20240602_T194215/ETSformer.pypots
2024-06-02 19:43:14 [INFO]: Successfully saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_2/imputation.pkl
2024-06-02 19:43:14 [INFO]: Round2 - ETSformer on ETT_h1: MAE=0.3669, MSE=0.2665, MRE=0.4340
2024-06-02 19:43:14 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 19:43:14 [INFO]: Using the given device: cuda:0
2024-06-02 19:43:14 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_3/20240602_T194314
2024-06-02 19:43:14 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_3/20240602_T194314/tensorboard
2024-06-02 19:43:14 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 809,057
2024-06-02 19:43:15 [INFO]: Epoch 001 - training loss: 1.2153, validation loss: 0.7361
2024-06-02 19:43:16 [INFO]: Epoch 002 - training loss: 1.0671, validation loss: 0.6509
2024-06-02 19:43:16 [INFO]: Epoch 003 - training loss: 0.9593, validation loss: 0.6683
2024-06-02 19:43:17 [INFO]: Epoch 004 - training loss: 0.8845, validation loss: 0.6752
2024-06-02 19:43:17 [INFO]: Epoch 005 - training loss: 0.8121, validation loss: 0.5917
2024-06-02 19:43:18 [INFO]: Epoch 006 - training loss: 0.7751, validation loss: 0.5893
2024-06-02 19:43:18 [INFO]: Epoch 007 - training loss: 0.7522, validation loss: 0.5301
2024-06-02 19:43:19 [INFO]: Epoch 008 - training loss: 0.7110, validation loss: 0.5310
2024-06-02 19:43:20 [INFO]: Epoch 009 - training loss: 0.7059, validation loss: 0.4981
2024-06-02 19:43:20 [INFO]: Epoch 010 - training loss: 0.6900, validation loss: 0.4885
2024-06-02 19:43:21 [INFO]: Epoch 011 - training loss: 0.6602, validation loss: 0.4786
2024-06-02 19:43:21 [INFO]: Epoch 012 - training loss: 0.6518, validation loss: 0.4638
2024-06-02 19:43:22 [INFO]: Epoch 013 - training loss: 0.6550, validation loss: 0.4626
2024-06-02 19:43:22 [INFO]: Epoch 014 - training loss: 0.6440, validation loss: 0.4497
2024-06-02 19:43:23 [INFO]: Epoch 015 - training loss: 0.6372, validation loss: 0.4228
2024-06-02 19:43:24 [INFO]: Epoch 016 - training loss: 0.6348, validation loss: 0.4229
2024-06-02 19:43:24 [INFO]: Epoch 017 - training loss: 0.6337, validation loss: 0.4025
2024-06-02 19:43:24 [INFO]: Epoch 018 - training loss: 0.6386, validation loss: 0.4139
2024-06-02 19:43:25 [INFO]: Epoch 019 - training loss: 0.6212, validation loss: 0.4219
2024-06-02 19:43:26 [INFO]: Epoch 020 - training loss: 0.6223, validation loss: 0.3784
2024-06-02 19:43:27 [INFO]: Epoch 021 - training loss: 0.6147, validation loss: 0.3848
2024-06-02 19:43:27 [INFO]: Epoch 022 - training loss: 0.6124, validation loss: 0.3831
2024-06-02 19:43:28 [INFO]: Epoch 023 - training loss: 0.6093, validation loss: 0.3567
2024-06-02 19:43:28 [INFO]: Epoch 024 - training loss: 0.5949, validation loss: 0.3622
2024-06-02 19:43:29 [INFO]: Epoch 025 - training loss: 0.5899, validation loss: 0.3518
2024-06-02 19:43:30 [INFO]: Epoch 026 - training loss: 0.5934, validation loss: 0.3487
2024-06-02 19:43:30 [INFO]: Epoch 027 - training loss: 0.5848, validation loss: 0.3449
2024-06-02 19:43:31 [INFO]: Epoch 028 - training loss: 0.5790, validation loss: 0.3304
2024-06-02 19:43:31 [INFO]: Epoch 029 - training loss: 0.5718, validation loss: 0.3085
2024-06-02 19:43:32 [INFO]: Epoch 030 - training loss: 0.5703, validation loss: 0.2976
2024-06-02 19:43:33 [INFO]: Epoch 031 - training loss: 0.5902, validation loss: 0.3203
2024-06-02 19:43:33 [INFO]: Epoch 032 - training loss: 0.5754, validation loss: 0.3177
2024-06-02 19:43:34 [INFO]: Epoch 033 - training loss: 0.5892, validation loss: 0.3076
2024-06-02 19:43:34 [INFO]: Epoch 034 - training loss: 0.5763, validation loss: 0.2945
2024-06-02 19:43:35 [INFO]: Epoch 035 - training loss: 0.5704, validation loss: 0.2902
2024-06-02 19:43:36 [INFO]: Epoch 036 - training loss: 0.5621, validation loss: 0.2793
2024-06-02 19:43:36 [INFO]: Epoch 037 - training loss: 0.5784, validation loss: 0.2831
2024-06-02 19:43:37 [INFO]: Epoch 038 - training loss: 0.5622, validation loss: 0.2981
2024-06-02 19:43:37 [INFO]: Epoch 039 - training loss: 0.5643, validation loss: 0.2781
2024-06-02 19:43:38 [INFO]: Epoch 040 - training loss: 0.5673, validation loss: 0.2741
2024-06-02 19:43:38 [INFO]: Epoch 041 - training loss: 0.5621, validation loss: 0.2649
2024-06-02 19:43:39 [INFO]: Epoch 042 - training loss: 0.5600, validation loss: 0.2720
2024-06-02 19:43:39 [INFO]: Epoch 043 - training loss: 0.5584, validation loss: 0.2681
2024-06-02 19:43:40 [INFO]: Epoch 044 - training loss: 0.5588, validation loss: 0.2619
2024-06-02 19:43:41 [INFO]: Epoch 045 - training loss: 0.5571, validation loss: 0.2532
2024-06-02 19:43:41 [INFO]: Epoch 046 - training loss: 0.5597, validation loss: 0.2563
2024-06-02 19:43:42 [INFO]: Epoch 047 - training loss: 0.5451, validation loss: 0.2627
2024-06-02 19:43:42 [INFO]: Epoch 048 - training loss: 0.5537, validation loss: 0.2555
2024-06-02 19:43:43 [INFO]: Epoch 049 - training loss: 0.5453, validation loss: 0.2478
2024-06-02 19:43:44 [INFO]: Epoch 050 - training loss: 0.5501, validation loss: 0.2428
2024-06-02 19:43:44 [INFO]: Epoch 051 - training loss: 0.5433, validation loss: 0.2367
2024-06-02 19:43:45 [INFO]: Epoch 052 - training loss: 0.5523, validation loss: 0.2428
2024-06-02 19:43:46 [INFO]: Epoch 053 - training loss: 0.5565, validation loss: 0.2484
2024-06-02 19:43:47 [INFO]: Epoch 054 - training loss: 0.5540, validation loss: 0.2482
2024-06-02 19:43:47 [INFO]: Epoch 055 - training loss: 0.5534, validation loss: 0.2437
2024-06-02 19:43:48 [INFO]: Epoch 056 - training loss: 0.5390, validation loss: 0.2483
2024-06-02 19:43:49 [INFO]: Epoch 057 - training loss: 0.5333, validation loss: 0.2448
2024-06-02 19:43:49 [INFO]: Epoch 058 - training loss: 0.5484, validation loss: 0.2410
2024-06-02 19:43:50 [INFO]: Epoch 059 - training loss: 0.5366, validation loss: 0.2387
2024-06-02 19:43:51 [INFO]: Epoch 060 - training loss: 0.5427, validation loss: 0.2427
2024-06-02 19:43:51 [INFO]: Epoch 061 - training loss: 0.5327, validation loss: 0.2382
2024-06-02 19:43:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:43:51 [INFO]: Finished training. The best model is from epoch#51.
2024-06-02 19:43:51 [INFO]: Saved the model to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_3/20240602_T194314/ETSformer.pypots
2024-06-02 19:43:52 [INFO]: Successfully saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_3/imputation.pkl
2024-06-02 19:43:52 [INFO]: Round3 - ETSformer on ETT_h1: MAE=0.3848, MSE=0.2971, MRE=0.4552
2024-06-02 19:43:52 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 19:43:52 [INFO]: Using the given device: cuda:0
2024-06-02 19:43:52 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_4/20240602_T194352
2024-06-02 19:43:52 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_4/20240602_T194352/tensorboard
2024-06-02 19:43:52 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 809,057
2024-06-02 19:43:53 [INFO]: Epoch 001 - training loss: 1.2748, validation loss: 0.7554
2024-06-02 19:43:53 [INFO]: Epoch 002 - training loss: 1.0704, validation loss: 0.6871
2024-06-02 19:43:54 [INFO]: Epoch 003 - training loss: 0.9720, validation loss: 0.6449
2024-06-02 19:43:55 [INFO]: Epoch 004 - training loss: 0.9028, validation loss: 0.6098
2024-06-02 19:43:55 [INFO]: Epoch 005 - training loss: 0.8157, validation loss: 0.5846
2024-06-02 19:43:56 [INFO]: Epoch 006 - training loss: 0.7649, validation loss: 0.5446
2024-06-02 19:43:57 [INFO]: Epoch 007 - training loss: 0.7406, validation loss: 0.5004
2024-06-02 19:43:57 [INFO]: Epoch 008 - training loss: 0.7054, validation loss: 0.4946
2024-06-02 19:43:58 [INFO]: Epoch 009 - training loss: 0.6920, validation loss: 0.4940
2024-06-02 19:43:59 [INFO]: Epoch 010 - training loss: 0.6869, validation loss: 0.5032
2024-06-02 19:43:59 [INFO]: Epoch 011 - training loss: 0.6837, validation loss: 0.4531
2024-06-02 19:44:00 [INFO]: Epoch 012 - training loss: 0.6582, validation loss: 0.4376
2024-06-02 19:44:01 [INFO]: Epoch 013 - training loss: 0.6595, validation loss: 0.4636
2024-06-02 19:44:01 [INFO]: Epoch 014 - training loss: 0.6511, validation loss: 0.4246
2024-06-02 19:44:02 [INFO]: Epoch 015 - training loss: 0.6597, validation loss: 0.4260
2024-06-02 19:44:03 [INFO]: Epoch 016 - training loss: 0.6448, validation loss: 0.4058
2024-06-02 19:44:04 [INFO]: Epoch 017 - training loss: 0.6251, validation loss: 0.4034
2024-06-02 19:44:04 [INFO]: Epoch 018 - training loss: 0.6232, validation loss: 0.3891
2024-06-02 19:44:05 [INFO]: Epoch 019 - training loss: 0.6240, validation loss: 0.3911
2024-06-02 19:44:06 [INFO]: Epoch 020 - training loss: 0.6050, validation loss: 0.3690
2024-06-02 19:44:06 [INFO]: Epoch 021 - training loss: 0.6109, validation loss: 0.3512
2024-06-02 19:44:07 [INFO]: Epoch 022 - training loss: 0.6005, validation loss: 0.3403
2024-06-02 19:44:08 [INFO]: Epoch 023 - training loss: 0.5897, validation loss: 0.3367
2024-06-02 19:44:09 [INFO]: Epoch 024 - training loss: 0.5930, validation loss: 0.3562
2024-06-02 19:44:10 [INFO]: Epoch 025 - training loss: 0.5954, validation loss: 0.3510
2024-06-02 19:44:10 [INFO]: Epoch 026 - training loss: 0.5948, validation loss: 0.3253
2024-06-02 19:44:11 [INFO]: Epoch 027 - training loss: 0.5977, validation loss: 0.3133
2024-06-02 19:44:11 [INFO]: Epoch 028 - training loss: 0.5860, validation loss: 0.3044
2024-06-02 19:44:12 [INFO]: Epoch 029 - training loss: 0.5743, validation loss: 0.3147
2024-06-02 19:44:13 [INFO]: Epoch 030 - training loss: 0.5823, validation loss: 0.2979
2024-06-02 19:44:13 [INFO]: Epoch 031 - training loss: 0.5830, validation loss: 0.3029
2024-06-02 19:44:14 [INFO]: Epoch 032 - training loss: 0.5865, validation loss: 0.3000
2024-06-02 19:44:15 [INFO]: Epoch 033 - training loss: 0.5731, validation loss: 0.2888
2024-06-02 19:44:15 [INFO]: Epoch 034 - training loss: 0.5835, validation loss: 0.2869
2024-06-02 19:44:16 [INFO]: Epoch 035 - training loss: 0.5874, validation loss: 0.2931
2024-06-02 19:44:17 [INFO]: Epoch 036 - training loss: 0.5772, validation loss: 0.2937
2024-06-02 19:44:17 [INFO]: Epoch 037 - training loss: 0.5721, validation loss: 0.2806
2024-06-02 19:44:18 [INFO]: Epoch 038 - training loss: 0.5694, validation loss: 0.2742
2024-06-02 19:44:18 [INFO]: Epoch 039 - training loss: 0.5776, validation loss: 0.2681
2024-06-02 19:44:19 [INFO]: Epoch 040 - training loss: 0.5728, validation loss: 0.2718
2024-06-02 19:44:20 [INFO]: Epoch 041 - training loss: 0.5656, validation loss: 0.2801
2024-06-02 19:44:21 [INFO]: Epoch 042 - training loss: 0.5665, validation loss: 0.2640
2024-06-02 19:44:21 [INFO]: Epoch 043 - training loss: 0.5587, validation loss: 0.2605
2024-06-02 19:44:22 [INFO]: Epoch 044 - training loss: 0.5697, validation loss: 0.2536
2024-06-02 19:44:22 [INFO]: Epoch 045 - training loss: 0.5564, validation loss: 0.2575
2024-06-02 19:44:23 [INFO]: Epoch 046 - training loss: 0.5591, validation loss: 0.2644
2024-06-02 19:44:24 [INFO]: Epoch 047 - training loss: 0.5509, validation loss: 0.2663
2024-06-02 19:44:25 [INFO]: Epoch 048 - training loss: 0.5597, validation loss: 0.2606
2024-06-02 19:44:25 [INFO]: Epoch 049 - training loss: 0.5603, validation loss: 0.2579
2024-06-02 19:44:26 [INFO]: Epoch 050 - training loss: 0.5555, validation loss: 0.2580
2024-06-02 19:44:26 [INFO]: Epoch 051 - training loss: 0.5583, validation loss: 0.2507
2024-06-02 19:44:27 [INFO]: Epoch 052 - training loss: 0.5524, validation loss: 0.2504
2024-06-02 19:44:27 [INFO]: Epoch 053 - training loss: 0.5527, validation loss: 0.2491
2024-06-02 19:44:28 [INFO]: Epoch 054 - training loss: 0.5583, validation loss: 0.2489
2024-06-02 19:44:28 [INFO]: Epoch 055 - training loss: 0.5539, validation loss: 0.2504
2024-06-02 19:44:29 [INFO]: Epoch 056 - training loss: 0.5486, validation loss: 0.2521
2024-06-02 19:44:29 [INFO]: Epoch 057 - training loss: 0.5510, validation loss: 0.2495
2024-06-02 19:44:30 [INFO]: Epoch 058 - training loss: 0.5450, validation loss: 0.2443
2024-06-02 19:44:30 [INFO]: Epoch 059 - training loss: 0.5523, validation loss: 0.2460
2024-06-02 19:44:31 [INFO]: Epoch 060 - training loss: 0.5568, validation loss: 0.2462
2024-06-02 19:44:31 [INFO]: Epoch 061 - training loss: 0.5535, validation loss: 0.2423
2024-06-02 19:44:32 [INFO]: Epoch 062 - training loss: 0.5554, validation loss: 0.2362
2024-06-02 19:44:32 [INFO]: Epoch 063 - training loss: 0.5566, validation loss: 0.2375
2024-06-02 19:44:33 [INFO]: Epoch 064 - training loss: 0.5526, validation loss: 0.2378
2024-06-02 19:44:33 [INFO]: Epoch 065 - training loss: 0.5415, validation loss: 0.2468
2024-06-02 19:44:34 [INFO]: Epoch 066 - training loss: 0.5544, validation loss: 0.2393
2024-06-02 19:44:34 [INFO]: Epoch 067 - training loss: 0.5453, validation loss: 0.2347
2024-06-02 19:44:35 [INFO]: Epoch 068 - training loss: 0.5385, validation loss: 0.2313
2024-06-02 19:44:35 [INFO]: Epoch 069 - training loss: 0.5456, validation loss: 0.2396
2024-06-02 19:44:36 [INFO]: Epoch 070 - training loss: 0.5500, validation loss: 0.2386
2024-06-02 19:44:36 [INFO]: Epoch 071 - training loss: 0.5315, validation loss: 0.2292
2024-06-02 19:44:37 [INFO]: Epoch 072 - training loss: 0.5361, validation loss: 0.2294
2024-06-02 19:44:37 [INFO]: Epoch 073 - training loss: 0.5291, validation loss: 0.2369
2024-06-02 19:44:38 [INFO]: Epoch 074 - training loss: 0.5403, validation loss: 0.2267
2024-06-02 19:44:39 [INFO]: Epoch 075 - training loss: 0.5383, validation loss: 0.2290
2024-06-02 19:44:39 [INFO]: Epoch 076 - training loss: 0.5353, validation loss: 0.2318
2024-06-02 19:44:40 [INFO]: Epoch 077 - training loss: 0.5365, validation loss: 0.2257
2024-06-02 19:44:40 [INFO]: Epoch 078 - training loss: 0.5348, validation loss: 0.2220
2024-06-02 19:44:41 [INFO]: Epoch 079 - training loss: 0.5394, validation loss: 0.2202
2024-06-02 19:44:41 [INFO]: Epoch 080 - training loss: 0.5354, validation loss: 0.2210
2024-06-02 19:44:42 [INFO]: Epoch 081 - training loss: 0.5357, validation loss: 0.2260
2024-06-02 19:44:43 [INFO]: Epoch 082 - training loss: 0.5359, validation loss: 0.2254
2024-06-02 19:44:43 [INFO]: Epoch 083 - training loss: 0.5410, validation loss: 0.2180
2024-06-02 19:44:44 [INFO]: Epoch 084 - training loss: 0.5433, validation loss: 0.2166
2024-06-02 19:44:45 [INFO]: Epoch 085 - training loss: 0.5406, validation loss: 0.2203
2024-06-02 19:44:45 [INFO]: Epoch 086 - training loss: 0.5351, validation loss: 0.2331
2024-06-02 19:44:46 [INFO]: Epoch 087 - training loss: 0.5381, validation loss: 0.2222
2024-06-02 19:44:46 [INFO]: Epoch 088 - training loss: 0.5284, validation loss: 0.2156
2024-06-02 19:44:47 [INFO]: Epoch 089 - training loss: 0.5296, validation loss: 0.2195
2024-06-02 19:44:48 [INFO]: Epoch 090 - training loss: 0.5339, validation loss: 0.2178
2024-06-02 19:44:48 [INFO]: Epoch 091 - training loss: 0.5364, validation loss: 0.2201
2024-06-02 19:44:49 [INFO]: Epoch 092 - training loss: 0.5270, validation loss: 0.2189
2024-06-02 19:44:49 [INFO]: Epoch 093 - training loss: 0.5286, validation loss: 0.2187
2024-06-02 19:44:50 [INFO]: Epoch 094 - training loss: 0.5226, validation loss: 0.2171
2024-06-02 19:44:51 [INFO]: Epoch 095 - training loss: 0.5334, validation loss: 0.2151
2024-06-02 19:44:51 [INFO]: Epoch 096 - training loss: 0.5375, validation loss: 0.2193
2024-06-02 19:44:52 [INFO]: Epoch 097 - training loss: 0.5318, validation loss: 0.2188
2024-06-02 19:44:52 [INFO]: Epoch 098 - training loss: 0.5293, validation loss: 0.2115
2024-06-02 19:44:53 [INFO]: Epoch 099 - training loss: 0.5191, validation loss: 0.2074
2024-06-02 19:44:53 [INFO]: Epoch 100 - training loss: 0.5318, validation loss: 0.2155
2024-06-02 19:44:53 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 19:44:53 [INFO]: Saved the model to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_4/20240602_T194352/ETSformer.pypots
2024-06-02 19:44:54 [INFO]: Successfully saved to results_point_rate05/ETT_h1/ETSformer_ETT_h1/round_4/imputation.pkl
2024-06-02 19:44:54 [INFO]: Round4 - ETSformer on ETT_h1: MAE=0.3580, MSE=0.2691, MRE=0.4235
2024-06-02 19:44:54 [INFO]: Done! Final results:
Averaged ETSformer (809,057 params) on ETT_h1: MAE=0.3643 ± 0.012904148381436565, MSE=0.2686 ± 0.022264463801184117, MRE=0.4310 ± 0.015265565568341143, average inference time=0.14
