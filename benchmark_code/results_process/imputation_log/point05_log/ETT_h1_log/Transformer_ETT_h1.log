2024-06-02 20:03:16 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 20:03:16 [INFO]: Using the given device: cuda:0
2024-06-02 20:03:17 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_0/20240602_T200317
2024-06-02 20:03:17 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_0/20240602_T200317/tensorboard
2024-06-02 20:03:17 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=2, d_k=512
2024-06-02 20:03:17 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (2) * d_k (512)
2024-06-02 20:03:19 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 5,800,199
2024-06-02 20:03:21 [INFO]: Epoch 001 - training loss: 2.1795, validation loss: 0.8256
2024-06-02 20:03:21 [INFO]: Epoch 002 - training loss: 1.1813, validation loss: 0.6547
2024-06-02 20:03:21 [INFO]: Epoch 003 - training loss: 0.9685, validation loss: 0.5561
2024-06-02 20:03:21 [INFO]: Epoch 004 - training loss: 0.8753, validation loss: 0.4835
2024-06-02 20:03:22 [INFO]: Epoch 005 - training loss: 0.8021, validation loss: 0.3980
2024-06-02 20:03:22 [INFO]: Epoch 006 - training loss: 0.7491, validation loss: 0.3154
2024-06-02 20:03:23 [INFO]: Epoch 007 - training loss: 0.7304, validation loss: 0.3450
2024-06-02 20:03:23 [INFO]: Epoch 008 - training loss: 0.7146, validation loss: 0.3151
2024-06-02 20:03:24 [INFO]: Epoch 009 - training loss: 0.7133, validation loss: 0.2944
2024-06-02 20:03:25 [INFO]: Epoch 010 - training loss: 0.6534, validation loss: 0.2514
2024-06-02 20:03:27 [INFO]: Epoch 011 - training loss: 0.6346, validation loss: 0.2335
2024-06-02 20:03:28 [INFO]: Epoch 012 - training loss: 0.6113, validation loss: 0.1892
2024-06-02 20:03:29 [INFO]: Epoch 013 - training loss: 0.5859, validation loss: 0.1714
2024-06-02 20:03:30 [INFO]: Epoch 014 - training loss: 0.5677, validation loss: 0.1919
2024-06-02 20:03:31 [INFO]: Epoch 015 - training loss: 0.5560, validation loss: 0.1722
2024-06-02 20:03:32 [INFO]: Epoch 016 - training loss: 0.5618, validation loss: 0.1560
2024-06-02 20:03:33 [INFO]: Epoch 017 - training loss: 0.5599, validation loss: 0.1502
2024-06-02 20:03:34 [INFO]: Epoch 018 - training loss: 0.5497, validation loss: 0.1788
2024-06-02 20:03:35 [INFO]: Epoch 019 - training loss: 0.5431, validation loss: 0.1451
2024-06-02 20:03:37 [INFO]: Epoch 020 - training loss: 0.5286, validation loss: 0.1842
2024-06-02 20:03:38 [INFO]: Epoch 021 - training loss: 0.5326, validation loss: 0.1395
2024-06-02 20:03:39 [INFO]: Epoch 022 - training loss: 0.5113, validation loss: 0.1250
2024-06-02 20:03:40 [INFO]: Epoch 023 - training loss: 0.4937, validation loss: 0.1429
2024-06-02 20:03:41 [INFO]: Epoch 024 - training loss: 0.4789, validation loss: 0.1155
2024-06-02 20:03:42 [INFO]: Epoch 025 - training loss: 0.4848, validation loss: 0.1158
2024-06-02 20:03:43 [INFO]: Epoch 026 - training loss: 0.4765, validation loss: 0.1226
2024-06-02 20:03:44 [INFO]: Epoch 027 - training loss: 0.4700, validation loss: 0.1293
2024-06-02 20:03:45 [INFO]: Epoch 028 - training loss: 0.4679, validation loss: 0.1300
2024-06-02 20:03:46 [INFO]: Epoch 029 - training loss: 0.4778, validation loss: 0.1013
2024-06-02 20:03:47 [INFO]: Epoch 030 - training loss: 0.4611, validation loss: 0.1239
2024-06-02 20:03:48 [INFO]: Epoch 031 - training loss: 0.4501, validation loss: 0.1053
2024-06-02 20:03:49 [INFO]: Epoch 032 - training loss: 0.4482, validation loss: 0.1000
2024-06-02 20:03:50 [INFO]: Epoch 033 - training loss: 0.4320, validation loss: 0.1098
2024-06-02 20:03:51 [INFO]: Epoch 034 - training loss: 0.4605, validation loss: 0.1502
2024-06-02 20:03:52 [INFO]: Epoch 035 - training loss: 0.4778, validation loss: 0.1286
2024-06-02 20:03:54 [INFO]: Epoch 036 - training loss: 0.4972, validation loss: 0.1203
2024-06-02 20:03:55 [INFO]: Epoch 037 - training loss: 0.4614, validation loss: 0.1163
2024-06-02 20:03:56 [INFO]: Epoch 038 - training loss: 0.4527, validation loss: 0.1041
2024-06-02 20:03:57 [INFO]: Epoch 039 - training loss: 0.4399, validation loss: 0.1191
2024-06-02 20:03:58 [INFO]: Epoch 040 - training loss: 0.4392, validation loss: 0.1172
2024-06-02 20:03:59 [INFO]: Epoch 041 - training loss: 0.4311, validation loss: 0.1157
2024-06-02 20:04:00 [INFO]: Epoch 042 - training loss: 0.4382, validation loss: 0.1105
2024-06-02 20:04:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:04:00 [INFO]: Finished training. The best model is from epoch#32.
2024-06-02 20:04:01 [INFO]: Saved the model to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_0/20240602_T200317/Transformer.pypots
2024-06-02 20:04:01 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_0/imputation.pkl
2024-06-02 20:04:01 [INFO]: Round0 - Transformer on ETT_h1: MAE=0.2743, MSE=0.1512, MRE=0.3245
2024-06-02 20:04:01 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 20:04:01 [INFO]: Using the given device: cuda:0
2024-06-02 20:04:01 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_1/20240602_T200401
2024-06-02 20:04:01 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_1/20240602_T200401/tensorboard
2024-06-02 20:04:01 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=2, d_k=512
2024-06-02 20:04:01 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (2) * d_k (512)
2024-06-02 20:04:01 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 5,800,199
2024-06-02 20:04:02 [INFO]: Epoch 001 - training loss: 2.2138, validation loss: 1.2198
2024-06-02 20:04:04 [INFO]: Epoch 002 - training loss: 1.1936, validation loss: 0.6965
2024-06-02 20:04:05 [INFO]: Epoch 003 - training loss: 0.9857, validation loss: 0.4549
2024-06-02 20:04:06 [INFO]: Epoch 004 - training loss: 0.8311, validation loss: 0.4100
2024-06-02 20:04:07 [INFO]: Epoch 005 - training loss: 0.7626, validation loss: 0.3011
2024-06-02 20:04:08 [INFO]: Epoch 006 - training loss: 0.7419, validation loss: 0.3484
2024-06-02 20:04:09 [INFO]: Epoch 007 - training loss: 0.6953, validation loss: 0.2833
2024-06-02 20:04:11 [INFO]: Epoch 008 - training loss: 0.6722, validation loss: 0.2490
2024-06-02 20:04:12 [INFO]: Epoch 009 - training loss: 0.6423, validation loss: 0.2190
2024-06-02 20:04:13 [INFO]: Epoch 010 - training loss: 0.6289, validation loss: 0.2358
2024-06-02 20:04:14 [INFO]: Epoch 011 - training loss: 0.6142, validation loss: 0.1787
2024-06-02 20:04:15 [INFO]: Epoch 012 - training loss: 0.6070, validation loss: 0.1970
2024-06-02 20:04:16 [INFO]: Epoch 013 - training loss: 0.5934, validation loss: 0.1762
2024-06-02 20:04:17 [INFO]: Epoch 014 - training loss: 0.5825, validation loss: 0.1748
2024-06-02 20:04:19 [INFO]: Epoch 015 - training loss: 0.5604, validation loss: 0.1465
2024-06-02 20:04:19 [INFO]: Epoch 016 - training loss: 0.5479, validation loss: 0.1706
2024-06-02 20:04:21 [INFO]: Epoch 017 - training loss: 0.5343, validation loss: 0.1545
2024-06-02 20:04:22 [INFO]: Epoch 018 - training loss: 0.5330, validation loss: 0.1477
2024-06-02 20:04:23 [INFO]: Epoch 019 - training loss: 0.5239, validation loss: 0.1543
2024-06-02 20:04:24 [INFO]: Epoch 020 - training loss: 0.5212, validation loss: 0.1904
2024-06-02 20:04:25 [INFO]: Epoch 021 - training loss: 0.5599, validation loss: 0.1462
2024-06-02 20:04:26 [INFO]: Epoch 022 - training loss: 0.5493, validation loss: 0.1334
2024-06-02 20:04:27 [INFO]: Epoch 023 - training loss: 0.5220, validation loss: 0.1330
2024-06-02 20:04:28 [INFO]: Epoch 024 - training loss: 0.5172, validation loss: 0.1337
2024-06-02 20:04:29 [INFO]: Epoch 025 - training loss: 0.5074, validation loss: 0.1968
2024-06-02 20:04:30 [INFO]: Epoch 026 - training loss: 0.5133, validation loss: 0.1390
2024-06-02 20:04:32 [INFO]: Epoch 027 - training loss: 0.5061, validation loss: 0.1465
2024-06-02 20:04:33 [INFO]: Epoch 028 - training loss: 0.5060, validation loss: 0.1289
2024-06-02 20:04:34 [INFO]: Epoch 029 - training loss: 0.4857, validation loss: 0.1526
2024-06-02 20:04:35 [INFO]: Epoch 030 - training loss: 0.4856, validation loss: 0.1080
2024-06-02 20:04:36 [INFO]: Epoch 031 - training loss: 0.4667, validation loss: 0.1229
2024-06-02 20:04:37 [INFO]: Epoch 032 - training loss: 0.4643, validation loss: 0.1144
2024-06-02 20:04:38 [INFO]: Epoch 033 - training loss: 0.4639, validation loss: 0.1193
2024-06-02 20:04:40 [INFO]: Epoch 034 - training loss: 0.4521, validation loss: 0.1284
2024-06-02 20:04:41 [INFO]: Epoch 035 - training loss: 0.4525, validation loss: 0.1246
2024-06-02 20:04:42 [INFO]: Epoch 036 - training loss: 0.4627, validation loss: 0.0993
2024-06-02 20:04:43 [INFO]: Epoch 037 - training loss: 0.4569, validation loss: 0.1422
2024-06-02 20:04:44 [INFO]: Epoch 038 - training loss: 0.4599, validation loss: 0.1242
2024-06-02 20:04:45 [INFO]: Epoch 039 - training loss: 0.4371, validation loss: 0.1177
2024-06-02 20:04:47 [INFO]: Epoch 040 - training loss: 0.4374, validation loss: 0.1135
2024-06-02 20:04:48 [INFO]: Epoch 041 - training loss: 0.4255, validation loss: 0.1291
2024-06-02 20:04:49 [INFO]: Epoch 042 - training loss: 0.4192, validation loss: 0.1015
2024-06-02 20:04:50 [INFO]: Epoch 043 - training loss: 0.4144, validation loss: 0.1172
2024-06-02 20:04:51 [INFO]: Epoch 044 - training loss: 0.4136, validation loss: 0.1123
2024-06-02 20:04:52 [INFO]: Epoch 045 - training loss: 0.4139, validation loss: 0.1265
2024-06-02 20:04:53 [INFO]: Epoch 046 - training loss: 0.4263, validation loss: 0.1071
2024-06-02 20:04:53 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:04:53 [INFO]: Finished training. The best model is from epoch#36.
2024-06-02 20:04:53 [INFO]: Saved the model to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_1/20240602_T200401/Transformer.pypots
2024-06-02 20:04:53 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_1/imputation.pkl
2024-06-02 20:04:53 [INFO]: Round1 - Transformer on ETT_h1: MAE=0.2610, MSE=0.1421, MRE=0.3088
2024-06-02 20:04:53 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 20:04:53 [INFO]: Using the given device: cuda:0
2024-06-02 20:04:53 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_2/20240602_T200453
2024-06-02 20:04:53 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_2/20240602_T200453/tensorboard
2024-06-02 20:04:53 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=2, d_k=512
2024-06-02 20:04:53 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (2) * d_k (512)
2024-06-02 20:04:54 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 5,800,199
2024-06-02 20:04:55 [INFO]: Epoch 001 - training loss: 2.1869, validation loss: 1.0729
2024-06-02 20:04:56 [INFO]: Epoch 002 - training loss: 1.1732, validation loss: 0.5666
2024-06-02 20:04:57 [INFO]: Epoch 003 - training loss: 0.9726, validation loss: 0.5156
2024-06-02 20:04:58 [INFO]: Epoch 004 - training loss: 0.8721, validation loss: 0.4083
2024-06-02 20:04:59 [INFO]: Epoch 005 - training loss: 0.8060, validation loss: 0.3390
2024-06-02 20:05:00 [INFO]: Epoch 006 - training loss: 0.7618, validation loss: 0.3538
2024-06-02 20:05:01 [INFO]: Epoch 007 - training loss: 0.7421, validation loss: 0.2771
2024-06-02 20:05:02 [INFO]: Epoch 008 - training loss: 0.7118, validation loss: 0.2973
2024-06-02 20:05:03 [INFO]: Epoch 009 - training loss: 0.6809, validation loss: 0.2581
2024-06-02 20:05:05 [INFO]: Epoch 010 - training loss: 0.6685, validation loss: 0.2638
2024-06-02 20:05:06 [INFO]: Epoch 011 - training loss: 0.6612, validation loss: 0.1941
2024-06-02 20:05:07 [INFO]: Epoch 012 - training loss: 0.6279, validation loss: 0.2203
2024-06-02 20:05:08 [INFO]: Epoch 013 - training loss: 0.6081, validation loss: 0.1905
2024-06-02 20:05:09 [INFO]: Epoch 014 - training loss: 0.5843, validation loss: 0.1664
2024-06-02 20:05:10 [INFO]: Epoch 015 - training loss: 0.5821, validation loss: 0.1844
2024-06-02 20:05:12 [INFO]: Epoch 016 - training loss: 0.5761, validation loss: 0.1698
2024-06-02 20:05:13 [INFO]: Epoch 017 - training loss: 0.5611, validation loss: 0.1880
2024-06-02 20:05:14 [INFO]: Epoch 018 - training loss: 0.5527, validation loss: 0.1832
2024-06-02 20:05:15 [INFO]: Epoch 019 - training loss: 0.5389, validation loss: 0.1574
2024-06-02 20:05:16 [INFO]: Epoch 020 - training loss: 0.5329, validation loss: 0.1460
2024-06-02 20:05:17 [INFO]: Epoch 021 - training loss: 0.5435, validation loss: 0.1923
2024-06-02 20:05:18 [INFO]: Epoch 022 - training loss: 0.5249, validation loss: 0.1670
2024-06-02 20:05:19 [INFO]: Epoch 023 - training loss: 0.5196, validation loss: 0.1527
2024-06-02 20:05:21 [INFO]: Epoch 024 - training loss: 0.4967, validation loss: 0.1390
2024-06-02 20:05:21 [INFO]: Epoch 025 - training loss: 0.5056, validation loss: 0.1713
2024-06-02 20:05:22 [INFO]: Epoch 026 - training loss: 0.5292, validation loss: 0.1407
2024-06-02 20:05:23 [INFO]: Epoch 027 - training loss: 0.5025, validation loss: 0.1232
2024-06-02 20:05:24 [INFO]: Epoch 028 - training loss: 0.4788, validation loss: 0.1814
2024-06-02 20:05:25 [INFO]: Epoch 029 - training loss: 0.4759, validation loss: 0.1218
2024-06-02 20:05:25 [INFO]: Epoch 030 - training loss: 0.4844, validation loss: 0.1341
2024-06-02 20:05:26 [INFO]: Epoch 031 - training loss: 0.4727, validation loss: 0.1380
2024-06-02 20:05:28 [INFO]: Epoch 032 - training loss: 0.4922, validation loss: 0.1626
2024-06-02 20:05:29 [INFO]: Epoch 033 - training loss: 0.4915, validation loss: 0.1388
2024-06-02 20:05:30 [INFO]: Epoch 034 - training loss: 0.4831, validation loss: 0.1516
2024-06-02 20:05:31 [INFO]: Epoch 035 - training loss: 0.4675, validation loss: 0.1268
2024-06-02 20:05:32 [INFO]: Epoch 036 - training loss: 0.4500, validation loss: 0.1242
2024-06-02 20:05:33 [INFO]: Epoch 037 - training loss: 0.4460, validation loss: 0.1191
2024-06-02 20:05:34 [INFO]: Epoch 038 - training loss: 0.4463, validation loss: 0.1267
2024-06-02 20:05:36 [INFO]: Epoch 039 - training loss: 0.4435, validation loss: 0.1195
2024-06-02 20:05:37 [INFO]: Epoch 040 - training loss: 0.4415, validation loss: 0.1366
2024-06-02 20:05:38 [INFO]: Epoch 041 - training loss: 0.4409, validation loss: 0.1198
2024-06-02 20:05:39 [INFO]: Epoch 042 - training loss: 0.4374, validation loss: 0.1435
2024-06-02 20:05:40 [INFO]: Epoch 043 - training loss: 0.4352, validation loss: 0.1059
2024-06-02 20:05:41 [INFO]: Epoch 044 - training loss: 0.4252, validation loss: 0.1251
2024-06-02 20:05:42 [INFO]: Epoch 045 - training loss: 0.4255, validation loss: 0.1043
2024-06-02 20:05:43 [INFO]: Epoch 046 - training loss: 0.4331, validation loss: 0.1195
2024-06-02 20:05:45 [INFO]: Epoch 047 - training loss: 0.4232, validation loss: 0.1062
2024-06-02 20:05:46 [INFO]: Epoch 048 - training loss: 0.4147, validation loss: 0.1390
2024-06-02 20:05:47 [INFO]: Epoch 049 - training loss: 0.4245, validation loss: 0.1057
2024-06-02 20:05:48 [INFO]: Epoch 050 - training loss: 0.4049, validation loss: 0.1068
2024-06-02 20:05:49 [INFO]: Epoch 051 - training loss: 0.3918, validation loss: 0.0926
2024-06-02 20:05:50 [INFO]: Epoch 052 - training loss: 0.3995, validation loss: 0.1057
2024-06-02 20:05:51 [INFO]: Epoch 053 - training loss: 0.4003, validation loss: 0.1012
2024-06-02 20:05:52 [INFO]: Epoch 054 - training loss: 0.3959, validation loss: 0.1026
2024-06-02 20:05:53 [INFO]: Epoch 055 - training loss: 0.3848, validation loss: 0.1029
2024-06-02 20:05:54 [INFO]: Epoch 056 - training loss: 0.3913, validation loss: 0.1019
2024-06-02 20:05:56 [INFO]: Epoch 057 - training loss: 0.3891, validation loss: 0.1150
2024-06-02 20:05:57 [INFO]: Epoch 058 - training loss: 0.4100, validation loss: 0.1251
2024-06-02 20:05:58 [INFO]: Epoch 059 - training loss: 0.4055, validation loss: 0.1212
2024-06-02 20:05:59 [INFO]: Epoch 060 - training loss: 0.4001, validation loss: 0.1035
2024-06-02 20:06:00 [INFO]: Epoch 061 - training loss: 0.3912, validation loss: 0.1095
2024-06-02 20:06:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:06:00 [INFO]: Finished training. The best model is from epoch#51.
2024-06-02 20:06:00 [INFO]: Saved the model to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_2/20240602_T200453/Transformer.pypots
2024-06-02 20:06:01 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_2/imputation.pkl
2024-06-02 20:06:01 [INFO]: Round2 - Transformer on ETT_h1: MAE=0.2640, MSE=0.1524, MRE=0.3123
2024-06-02 20:06:01 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 20:06:01 [INFO]: Using the given device: cuda:0
2024-06-02 20:06:01 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_3/20240602_T200601
2024-06-02 20:06:01 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_3/20240602_T200601/tensorboard
2024-06-02 20:06:01 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=2, d_k=512
2024-06-02 20:06:01 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (2) * d_k (512)
2024-06-02 20:06:01 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 5,800,199
2024-06-02 20:06:02 [INFO]: Epoch 001 - training loss: 2.3376, validation loss: 1.0017
2024-06-02 20:06:03 [INFO]: Epoch 002 - training loss: 1.3243, validation loss: 0.9260
2024-06-02 20:06:04 [INFO]: Epoch 003 - training loss: 1.0173, validation loss: 0.5332
2024-06-02 20:06:05 [INFO]: Epoch 004 - training loss: 0.8880, validation loss: 0.4244
2024-06-02 20:06:06 [INFO]: Epoch 005 - training loss: 0.8279, validation loss: 0.3478
2024-06-02 20:06:08 [INFO]: Epoch 006 - training loss: 0.7660, validation loss: 0.3280
2024-06-02 20:06:09 [INFO]: Epoch 007 - training loss: 0.7471, validation loss: 0.3594
2024-06-02 20:06:10 [INFO]: Epoch 008 - training loss: 0.7370, validation loss: 0.2884
2024-06-02 20:06:11 [INFO]: Epoch 009 - training loss: 0.6976, validation loss: 0.2489
2024-06-02 20:06:12 [INFO]: Epoch 010 - training loss: 0.6732, validation loss: 0.2519
2024-06-02 20:06:13 [INFO]: Epoch 011 - training loss: 0.6745, validation loss: 0.2341
2024-06-02 20:06:14 [INFO]: Epoch 012 - training loss: 0.6456, validation loss: 0.2129
2024-06-02 20:06:15 [INFO]: Epoch 013 - training loss: 0.6477, validation loss: 0.1968
2024-06-02 20:06:17 [INFO]: Epoch 014 - training loss: 0.6076, validation loss: 0.2013
2024-06-02 20:06:18 [INFO]: Epoch 015 - training loss: 0.5953, validation loss: 0.1923
2024-06-02 20:06:19 [INFO]: Epoch 016 - training loss: 0.5687, validation loss: 0.1542
2024-06-02 20:06:20 [INFO]: Epoch 017 - training loss: 0.5477, validation loss: 0.1677
2024-06-02 20:06:21 [INFO]: Epoch 018 - training loss: 0.5369, validation loss: 0.1637
2024-06-02 20:06:23 [INFO]: Epoch 019 - training loss: 0.5476, validation loss: 0.1361
2024-06-02 20:06:24 [INFO]: Epoch 020 - training loss: 0.5303, validation loss: 0.1617
2024-06-02 20:06:25 [INFO]: Epoch 021 - training loss: 0.5334, validation loss: 0.1728
2024-06-02 20:06:26 [INFO]: Epoch 022 - training loss: 0.5138, validation loss: 0.1822
2024-06-02 20:06:27 [INFO]: Epoch 023 - training loss: 0.5060, validation loss: 0.1545
2024-06-02 20:06:28 [INFO]: Epoch 024 - training loss: 0.5023, validation loss: 0.1573
2024-06-02 20:06:29 [INFO]: Epoch 025 - training loss: 0.5037, validation loss: 0.1357
2024-06-02 20:06:30 [INFO]: Epoch 026 - training loss: 0.4899, validation loss: 0.1464
2024-06-02 20:06:32 [INFO]: Epoch 027 - training loss: 0.4976, validation loss: 0.1604
2024-06-02 20:06:33 [INFO]: Epoch 028 - training loss: 0.4800, validation loss: 0.1292
2024-06-02 20:06:34 [INFO]: Epoch 029 - training loss: 0.4692, validation loss: 0.1150
2024-06-02 20:06:35 [INFO]: Epoch 030 - training loss: 0.4695, validation loss: 0.1179
2024-06-02 20:06:36 [INFO]: Epoch 031 - training loss: 0.4657, validation loss: 0.1100
2024-06-02 20:06:37 [INFO]: Epoch 032 - training loss: 0.4649, validation loss: 0.1216
2024-06-02 20:06:38 [INFO]: Epoch 033 - training loss: 0.4508, validation loss: 0.1306
2024-06-02 20:06:39 [INFO]: Epoch 034 - training loss: 0.4660, validation loss: 0.1632
2024-06-02 20:06:40 [INFO]: Epoch 035 - training loss: 0.4564, validation loss: 0.1257
2024-06-02 20:06:42 [INFO]: Epoch 036 - training loss: 0.4528, validation loss: 0.1053
2024-06-02 20:06:43 [INFO]: Epoch 037 - training loss: 0.4432, validation loss: 0.1167
2024-06-02 20:06:44 [INFO]: Epoch 038 - training loss: 0.4454, validation loss: 0.1148
2024-06-02 20:06:45 [INFO]: Epoch 039 - training loss: 0.4500, validation loss: 0.1263
2024-06-02 20:06:46 [INFO]: Epoch 040 - training loss: 0.4326, validation loss: 0.1162
2024-06-02 20:06:47 [INFO]: Epoch 041 - training loss: 0.4266, validation loss: 0.1087
2024-06-02 20:06:48 [INFO]: Epoch 042 - training loss: 0.4203, validation loss: 0.1263
2024-06-02 20:06:49 [INFO]: Epoch 043 - training loss: 0.4237, validation loss: 0.1223
2024-06-02 20:06:50 [INFO]: Epoch 044 - training loss: 0.4243, validation loss: 0.1122
2024-06-02 20:06:51 [INFO]: Epoch 045 - training loss: 0.4224, validation loss: 0.1317
2024-06-02 20:06:52 [INFO]: Epoch 046 - training loss: 0.4156, validation loss: 0.1349
2024-06-02 20:06:52 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:06:52 [INFO]: Finished training. The best model is from epoch#36.
2024-06-02 20:06:52 [INFO]: Saved the model to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_3/20240602_T200601/Transformer.pypots
2024-06-02 20:06:52 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_3/imputation.pkl
2024-06-02 20:06:52 [INFO]: Round3 - Transformer on ETT_h1: MAE=0.2781, MSE=0.1738, MRE=0.3290
2024-06-02 20:06:52 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 20:06:52 [INFO]: Using the given device: cuda:0
2024-06-02 20:06:52 [INFO]: Model files will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_4/20240602_T200652
2024-06-02 20:06:52 [INFO]: Tensorboard file will be saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_4/20240602_T200652/tensorboard
2024-06-02 20:06:52 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=2, d_k=512
2024-06-02 20:06:52 [WARNING]: ⚠️ d_model is reset to 1024 = n_heads (2) * d_k (512)
2024-06-02 20:06:52 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 5,800,199
2024-06-02 20:06:53 [INFO]: Epoch 001 - training loss: 2.3573, validation loss: 0.8947
2024-06-02 20:06:54 [INFO]: Epoch 002 - training loss: 1.3458, validation loss: 0.9582
2024-06-02 20:06:55 [INFO]: Epoch 003 - training loss: 1.0550, validation loss: 0.7446
2024-06-02 20:06:56 [INFO]: Epoch 004 - training loss: 0.9541, validation loss: 0.5717
2024-06-02 20:06:57 [INFO]: Epoch 005 - training loss: 0.8616, validation loss: 0.5029
2024-06-02 20:06:58 [INFO]: Epoch 006 - training loss: 0.7950, validation loss: 0.4337
2024-06-02 20:06:59 [INFO]: Epoch 007 - training loss: 0.7655, validation loss: 0.4053
2024-06-02 20:07:00 [INFO]: Epoch 008 - training loss: 0.7590, validation loss: 0.3511
2024-06-02 20:07:01 [INFO]: Epoch 009 - training loss: 0.7073, validation loss: 0.3162
2024-06-02 20:07:02 [INFO]: Epoch 010 - training loss: 0.6993, validation loss: 0.2509
2024-06-02 20:07:02 [INFO]: Epoch 011 - training loss: 0.6702, validation loss: 0.2643
2024-06-02 20:07:03 [INFO]: Epoch 012 - training loss: 0.6550, validation loss: 0.2328
2024-06-02 20:07:04 [INFO]: Epoch 013 - training loss: 0.6279, validation loss: 0.2236
2024-06-02 20:07:05 [INFO]: Epoch 014 - training loss: 0.6090, validation loss: 0.1949
2024-06-02 20:07:05 [INFO]: Epoch 015 - training loss: 0.6143, validation loss: 0.2017
2024-06-02 20:07:06 [INFO]: Epoch 016 - training loss: 0.6230, validation loss: 0.1895
2024-06-02 20:07:07 [INFO]: Epoch 017 - training loss: 0.5976, validation loss: 0.2315
2024-06-02 20:07:08 [INFO]: Epoch 018 - training loss: 0.5653, validation loss: 0.1865
2024-06-02 20:07:09 [INFO]: Epoch 019 - training loss: 0.5640, validation loss: 0.1949
2024-06-02 20:07:10 [INFO]: Epoch 020 - training loss: 0.5752, validation loss: 0.1903
2024-06-02 20:07:11 [INFO]: Epoch 021 - training loss: 0.5710, validation loss: 0.1888
2024-06-02 20:07:12 [INFO]: Epoch 022 - training loss: 0.5542, validation loss: 0.1603
2024-06-02 20:07:13 [INFO]: Epoch 023 - training loss: 0.5550, validation loss: 0.1485
2024-06-02 20:07:14 [INFO]: Epoch 024 - training loss: 0.5484, validation loss: 0.1780
2024-06-02 20:07:15 [INFO]: Epoch 025 - training loss: 0.5281, validation loss: 0.1925
2024-06-02 20:07:16 [INFO]: Epoch 026 - training loss: 0.5678, validation loss: 0.2182
2024-06-02 20:07:17 [INFO]: Epoch 027 - training loss: 0.5422, validation loss: 0.1708
2024-06-02 20:07:18 [INFO]: Epoch 028 - training loss: 0.5170, validation loss: 0.1669
2024-06-02 20:07:19 [INFO]: Epoch 029 - training loss: 0.5131, validation loss: 0.1514
2024-06-02 20:07:19 [INFO]: Epoch 030 - training loss: 0.4979, validation loss: 0.1649
2024-06-02 20:07:20 [INFO]: Epoch 031 - training loss: 0.5193, validation loss: 0.1616
2024-06-02 20:07:21 [INFO]: Epoch 032 - training loss: 0.5219, validation loss: 0.1669
2024-06-02 20:07:22 [INFO]: Epoch 033 - training loss: 0.5352, validation loss: 0.1363
2024-06-02 20:07:23 [INFO]: Epoch 034 - training loss: 0.5111, validation loss: 0.1834
2024-06-02 20:07:24 [INFO]: Epoch 035 - training loss: 0.4967, validation loss: 0.1655
2024-06-02 20:07:25 [INFO]: Epoch 036 - training loss: 0.4846, validation loss: 0.1623
2024-06-02 20:07:26 [INFO]: Epoch 037 - training loss: 0.4784, validation loss: 0.1269
2024-06-02 20:07:27 [INFO]: Epoch 038 - training loss: 0.4952, validation loss: 0.1527
2024-06-02 20:07:28 [INFO]: Epoch 039 - training loss: 0.4940, validation loss: 0.1373
2024-06-02 20:07:30 [INFO]: Epoch 040 - training loss: 0.4898, validation loss: 0.1281
2024-06-02 20:07:31 [INFO]: Epoch 041 - training loss: 0.4748, validation loss: 0.1423
2024-06-02 20:07:32 [INFO]: Epoch 042 - training loss: 0.4625, validation loss: 0.1613
2024-06-02 20:07:32 [INFO]: Epoch 043 - training loss: 0.4690, validation loss: 0.1241
2024-06-02 20:07:33 [INFO]: Epoch 044 - training loss: 0.4537, validation loss: 0.1425
2024-06-02 20:07:34 [INFO]: Epoch 045 - training loss: 0.4541, validation loss: 0.1590
2024-06-02 20:07:36 [INFO]: Epoch 046 - training loss: 0.4469, validation loss: 0.1375
2024-06-02 20:07:37 [INFO]: Epoch 047 - training loss: 0.4377, validation loss: 0.1259
2024-06-02 20:07:38 [INFO]: Epoch 048 - training loss: 0.4442, validation loss: 0.1454
2024-06-02 20:07:39 [INFO]: Epoch 049 - training loss: 0.4365, validation loss: 0.1229
2024-06-02 20:07:40 [INFO]: Epoch 050 - training loss: 0.4412, validation loss: 0.1244
2024-06-02 20:07:41 [INFO]: Epoch 051 - training loss: 0.4322, validation loss: 0.1358
2024-06-02 20:07:42 [INFO]: Epoch 052 - training loss: 0.4408, validation loss: 0.1048
2024-06-02 20:07:43 [INFO]: Epoch 053 - training loss: 0.4291, validation loss: 0.1234
2024-06-02 20:07:44 [INFO]: Epoch 054 - training loss: 0.4130, validation loss: 0.1249
2024-06-02 20:07:45 [INFO]: Epoch 055 - training loss: 0.4292, validation loss: 0.1238
2024-06-02 20:07:46 [INFO]: Epoch 056 - training loss: 0.4318, validation loss: 0.1353
2024-06-02 20:07:47 [INFO]: Epoch 057 - training loss: 0.4174, validation loss: 0.1262
2024-06-02 20:07:48 [INFO]: Epoch 058 - training loss: 0.4123, validation loss: 0.1422
2024-06-02 20:07:49 [INFO]: Epoch 059 - training loss: 0.4094, validation loss: 0.1218
2024-06-02 20:07:50 [INFO]: Epoch 060 - training loss: 0.4146, validation loss: 0.1237
2024-06-02 20:07:51 [INFO]: Epoch 061 - training loss: 0.3994, validation loss: 0.1139
2024-06-02 20:07:52 [INFO]: Epoch 062 - training loss: 0.4102, validation loss: 0.1182
2024-06-02 20:07:52 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:07:52 [INFO]: Finished training. The best model is from epoch#52.
2024-06-02 20:07:52 [INFO]: Saved the model to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_4/20240602_T200652/Transformer.pypots
2024-06-02 20:07:52 [INFO]: Successfully saved to results_point_rate05/ETT_h1/Transformer_ETT_h1/round_4/imputation.pkl
2024-06-02 20:07:52 [INFO]: Round4 - Transformer on ETT_h1: MAE=0.2944, MSE=0.1884, MRE=0.3482
2024-06-02 20:07:52 [INFO]: Done! Final results:
Averaged Transformer (5,800,199 params) on ETT_h1: MAE=0.2744 ± 0.011820261629509983, MSE=0.1616 ± 0.0169746636431725, MRE=0.3246 ± 0.013983331065830759, average inference time=0.08
