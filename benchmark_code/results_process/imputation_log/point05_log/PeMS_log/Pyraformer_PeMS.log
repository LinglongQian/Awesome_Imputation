2024-06-03 02:07:41 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 02:07:41 [INFO]: Using the given device: cuda:0
2024-06-03 02:07:41 [INFO]: Model files will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_0/20240603_T020741
2024-06-03 02:07:41 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_0/20240603_T020741/tensorboard
2024-06-03 02:07:42 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 02:07:53 [INFO]: Epoch 001 - training loss: 0.9398, validation loss: 0.5761
2024-06-03 02:07:55 [INFO]: Epoch 002 - training loss: 0.5839, validation loss: 0.5157
2024-06-03 02:07:57 [INFO]: Epoch 003 - training loss: 0.5207, validation loss: 0.5071
2024-06-03 02:07:59 [INFO]: Epoch 004 - training loss: 0.4906, validation loss: 0.4935
2024-06-03 02:08:01 [INFO]: Epoch 005 - training loss: 0.4742, validation loss: 0.4857
2024-06-03 02:08:04 [INFO]: Epoch 006 - training loss: 0.4610, validation loss: 0.4887
2024-06-03 02:08:06 [INFO]: Epoch 007 - training loss: 0.4404, validation loss: 0.4749
2024-06-03 02:08:09 [INFO]: Epoch 008 - training loss: 0.4354, validation loss: 0.4712
2024-06-03 02:08:12 [INFO]: Epoch 009 - training loss: 0.4284, validation loss: 0.4726
2024-06-03 02:08:15 [INFO]: Epoch 010 - training loss: 0.4185, validation loss: 0.4697
2024-06-03 02:08:18 [INFO]: Epoch 011 - training loss: 0.4080, validation loss: 0.4608
2024-06-03 02:08:21 [INFO]: Epoch 012 - training loss: 0.4006, validation loss: 0.4570
2024-06-03 02:08:24 [INFO]: Epoch 013 - training loss: 0.3923, validation loss: 0.4593
2024-06-03 02:08:28 [INFO]: Epoch 014 - training loss: 0.3881, validation loss: 0.4573
2024-06-03 02:08:31 [INFO]: Epoch 015 - training loss: 0.3810, validation loss: 0.4535
2024-06-03 02:08:34 [INFO]: Epoch 016 - training loss: 0.3804, validation loss: 0.4514
2024-06-03 02:08:37 [INFO]: Epoch 017 - training loss: 0.3778, validation loss: 0.4602
2024-06-03 02:08:40 [INFO]: Epoch 018 - training loss: 0.3812, validation loss: 0.4514
2024-06-03 02:08:43 [INFO]: Epoch 019 - training loss: 0.3728, validation loss: 0.4450
2024-06-03 02:08:47 [INFO]: Epoch 020 - training loss: 0.3689, validation loss: 0.4473
2024-06-03 02:08:50 [INFO]: Epoch 021 - training loss: 0.3574, validation loss: 0.4443
2024-06-03 02:08:53 [INFO]: Epoch 022 - training loss: 0.3588, validation loss: 0.4437
2024-06-03 02:08:56 [INFO]: Epoch 023 - training loss: 0.3554, validation loss: 0.4401
2024-06-03 02:08:59 [INFO]: Epoch 024 - training loss: 0.3524, validation loss: 0.4389
2024-06-03 02:09:02 [INFO]: Epoch 025 - training loss: 0.3521, validation loss: 0.4406
2024-06-03 02:09:05 [INFO]: Epoch 026 - training loss: 0.3456, validation loss: 0.4397
2024-06-03 02:09:08 [INFO]: Epoch 027 - training loss: 0.3461, validation loss: 0.4444
2024-06-03 02:09:12 [INFO]: Epoch 028 - training loss: 0.3472, validation loss: 0.4353
2024-06-03 02:09:15 [INFO]: Epoch 029 - training loss: 0.3373, validation loss: 0.4370
2024-06-03 02:09:18 [INFO]: Epoch 030 - training loss: 0.3360, validation loss: 0.4339
2024-06-03 02:09:22 [INFO]: Epoch 031 - training loss: 0.3339, validation loss: 0.4338
2024-06-03 02:09:25 [INFO]: Epoch 032 - training loss: 0.3354, validation loss: 0.4327
2024-06-03 02:09:28 [INFO]: Epoch 033 - training loss: 0.3344, validation loss: 0.4325
2024-06-03 02:09:31 [INFO]: Epoch 034 - training loss: 0.3290, validation loss: 0.4351
2024-06-03 02:09:34 [INFO]: Epoch 035 - training loss: 0.3300, validation loss: 0.4338
2024-06-03 02:09:37 [INFO]: Epoch 036 - training loss: 0.3270, validation loss: 0.4277
2024-06-03 02:09:40 [INFO]: Epoch 037 - training loss: 0.3207, validation loss: 0.4254
2024-06-03 02:09:43 [INFO]: Epoch 038 - training loss: 0.3205, validation loss: 0.4277
2024-06-03 02:09:46 [INFO]: Epoch 039 - training loss: 0.3256, validation loss: 0.4279
2024-06-03 02:09:49 [INFO]: Epoch 040 - training loss: 0.3234, validation loss: 0.4325
2024-06-03 02:09:52 [INFO]: Epoch 041 - training loss: 0.3188, validation loss: 0.4268
2024-06-03 02:09:56 [INFO]: Epoch 042 - training loss: 0.3135, validation loss: 0.4242
2024-06-03 02:09:59 [INFO]: Epoch 043 - training loss: 0.3077, validation loss: 0.4205
2024-06-03 02:10:02 [INFO]: Epoch 044 - training loss: 0.3116, validation loss: 0.4223
2024-06-03 02:10:05 [INFO]: Epoch 045 - training loss: 0.3079, validation loss: 0.4226
2024-06-03 02:10:08 [INFO]: Epoch 046 - training loss: 0.3099, validation loss: 0.4205
2024-06-03 02:10:12 [INFO]: Epoch 047 - training loss: 0.3103, validation loss: 0.4203
2024-06-03 02:10:15 [INFO]: Epoch 048 - training loss: 0.3057, validation loss: 0.4222
2024-06-03 02:10:18 [INFO]: Epoch 049 - training loss: 0.3068, validation loss: 0.4203
2024-06-03 02:10:21 [INFO]: Epoch 050 - training loss: 0.3034, validation loss: 0.4221
2024-06-03 02:10:24 [INFO]: Epoch 051 - training loss: 0.3014, validation loss: 0.4179
2024-06-03 02:10:28 [INFO]: Epoch 052 - training loss: 0.2990, validation loss: 0.4220
2024-06-03 02:10:31 [INFO]: Epoch 053 - training loss: 0.3055, validation loss: 0.4193
2024-06-03 02:10:34 [INFO]: Epoch 054 - training loss: 0.3021, validation loss: 0.4138
2024-06-03 02:10:37 [INFO]: Epoch 055 - training loss: 0.3006, validation loss: 0.4201
2024-06-03 02:10:40 [INFO]: Epoch 056 - training loss: 0.2988, validation loss: 0.4165
2024-06-03 02:10:43 [INFO]: Epoch 057 - training loss: 0.2942, validation loss: 0.4213
2024-06-03 02:10:47 [INFO]: Epoch 058 - training loss: 0.2962, validation loss: 0.4203
2024-06-03 02:10:50 [INFO]: Epoch 059 - training loss: 0.3046, validation loss: 0.4165
2024-06-03 02:10:53 [INFO]: Epoch 060 - training loss: 0.2954, validation loss: 0.4154
2024-06-03 02:10:56 [INFO]: Epoch 061 - training loss: 0.2878, validation loss: 0.4125
2024-06-03 02:10:59 [INFO]: Epoch 062 - training loss: 0.2893, validation loss: 0.4145
2024-06-03 02:11:03 [INFO]: Epoch 063 - training loss: 0.2952, validation loss: 0.4163
2024-06-03 02:11:06 [INFO]: Epoch 064 - training loss: 0.2871, validation loss: 0.4125
2024-06-03 02:11:09 [INFO]: Epoch 065 - training loss: 0.2859, validation loss: 0.4142
2024-06-03 02:11:12 [INFO]: Epoch 066 - training loss: 0.2830, validation loss: 0.4175
2024-06-03 02:11:15 [INFO]: Epoch 067 - training loss: 0.2852, validation loss: 0.4131
2024-06-03 02:11:18 [INFO]: Epoch 068 - training loss: 0.2827, validation loss: 0.4125
2024-06-03 02:11:22 [INFO]: Epoch 069 - training loss: 0.2821, validation loss: 0.4123
2024-06-03 02:11:25 [INFO]: Epoch 070 - training loss: 0.2830, validation loss: 0.4137
2024-06-03 02:11:28 [INFO]: Epoch 071 - training loss: 0.2819, validation loss: 0.4122
2024-06-03 02:11:31 [INFO]: Epoch 072 - training loss: 0.2790, validation loss: 0.4181
2024-06-03 02:11:33 [INFO]: Epoch 073 - training loss: 0.2784, validation loss: 0.4148
2024-06-03 02:11:37 [INFO]: Epoch 074 - training loss: 0.2771, validation loss: 0.4108
2024-06-03 02:11:40 [INFO]: Epoch 075 - training loss: 0.2785, validation loss: 0.4113
2024-06-03 02:11:43 [INFO]: Epoch 076 - training loss: 0.2752, validation loss: 0.4070
2024-06-03 02:11:46 [INFO]: Epoch 077 - training loss: 0.2724, validation loss: 0.4077
2024-06-03 02:11:49 [INFO]: Epoch 078 - training loss: 0.2761, validation loss: 0.4100
2024-06-03 02:11:52 [INFO]: Epoch 079 - training loss: 0.2771, validation loss: 0.4111
2024-06-03 02:11:55 [INFO]: Epoch 080 - training loss: 0.2794, validation loss: 0.4094
2024-06-03 02:11:58 [INFO]: Epoch 081 - training loss: 0.2769, validation loss: 0.4107
2024-06-03 02:12:02 [INFO]: Epoch 082 - training loss: 0.2748, validation loss: 0.4075
2024-06-03 02:12:05 [INFO]: Epoch 083 - training loss: 0.2735, validation loss: 0.4131
2024-06-03 02:12:08 [INFO]: Epoch 084 - training loss: 0.2752, validation loss: 0.4078
2024-06-03 02:12:11 [INFO]: Epoch 085 - training loss: 0.2713, validation loss: 0.4079
2024-06-03 02:12:14 [INFO]: Epoch 086 - training loss: 0.2729, validation loss: 0.4068
2024-06-03 02:12:17 [INFO]: Epoch 087 - training loss: 0.2685, validation loss: 0.4104
2024-06-03 02:12:21 [INFO]: Epoch 088 - training loss: 0.2702, validation loss: 0.4086
2024-06-03 02:12:24 [INFO]: Epoch 089 - training loss: 0.2677, validation loss: 0.4116
2024-06-03 02:12:27 [INFO]: Epoch 090 - training loss: 0.2655, validation loss: 0.4054
2024-06-03 02:12:30 [INFO]: Epoch 091 - training loss: 0.2644, validation loss: 0.4086
2024-06-03 02:12:34 [INFO]: Epoch 092 - training loss: 0.2670, validation loss: 0.4053
2024-06-03 02:12:37 [INFO]: Epoch 093 - training loss: 0.2685, validation loss: 0.4069
2024-06-03 02:12:40 [INFO]: Epoch 094 - training loss: 0.2652, validation loss: 0.4090
2024-06-03 02:12:43 [INFO]: Epoch 095 - training loss: 0.2646, validation loss: 0.4084
2024-06-03 02:12:46 [INFO]: Epoch 096 - training loss: 0.2652, validation loss: 0.4061
2024-06-03 02:12:49 [INFO]: Epoch 097 - training loss: 0.2650, validation loss: 0.4089
2024-06-03 02:12:52 [INFO]: Epoch 098 - training loss: 0.2621, validation loss: 0.4099
2024-06-03 02:12:56 [INFO]: Epoch 099 - training loss: 0.2577, validation loss: 0.4091
2024-06-03 02:12:59 [INFO]: Epoch 100 - training loss: 0.2609, validation loss: 0.4075
2024-06-03 02:12:59 [INFO]: Finished training. The best model is from epoch#92.
2024-06-03 02:12:59 [INFO]: Saved the model to results_point_rate05/PeMS/Pyraformer_PeMS/round_0/20240603_T020741/Pyraformer.pypots
2024-06-03 02:13:01 [INFO]: Successfully saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_0/imputation.pkl
2024-06-03 02:13:01 [INFO]: Round0 - Pyraformer on PeMS: MAE=0.3073, MSE=0.5793, MRE=0.3814
2024-06-03 02:13:01 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 02:13:01 [INFO]: Using the given device: cuda:0
2024-06-03 02:13:01 [INFO]: Model files will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_1/20240603_T021301
2024-06-03 02:13:01 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_1/20240603_T021301/tensorboard
2024-06-03 02:13:01 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 02:13:04 [INFO]: Epoch 001 - training loss: 0.9473, validation loss: 0.5896
2024-06-03 02:13:07 [INFO]: Epoch 002 - training loss: 0.6008, validation loss: 0.5227
2024-06-03 02:13:10 [INFO]: Epoch 003 - training loss: 0.5215, validation loss: 0.5021
2024-06-03 02:13:13 [INFO]: Epoch 004 - training loss: 0.4847, validation loss: 0.4935
2024-06-03 02:13:17 [INFO]: Epoch 005 - training loss: 0.4762, validation loss: 0.4860
2024-06-03 02:13:20 [INFO]: Epoch 006 - training loss: 0.4588, validation loss: 0.4836
2024-06-03 02:13:23 [INFO]: Epoch 007 - training loss: 0.4363, validation loss: 0.4761
2024-06-03 02:13:26 [INFO]: Epoch 008 - training loss: 0.4250, validation loss: 0.4787
2024-06-03 02:13:29 [INFO]: Epoch 009 - training loss: 0.4213, validation loss: 0.4723
2024-06-03 02:13:32 [INFO]: Epoch 010 - training loss: 0.4153, validation loss: 0.4666
2024-06-03 02:13:35 [INFO]: Epoch 011 - training loss: 0.4065, validation loss: 0.4717
2024-06-03 02:13:38 [INFO]: Epoch 012 - training loss: 0.4063, validation loss: 0.4781
2024-06-03 02:13:41 [INFO]: Epoch 013 - training loss: 0.4121, validation loss: 0.4589
2024-06-03 02:13:45 [INFO]: Epoch 014 - training loss: 0.3947, validation loss: 0.4589
2024-06-03 02:13:48 [INFO]: Epoch 015 - training loss: 0.3878, validation loss: 0.4588
2024-06-03 02:13:51 [INFO]: Epoch 016 - training loss: 0.3786, validation loss: 0.4624
2024-06-03 02:13:54 [INFO]: Epoch 017 - training loss: 0.3819, validation loss: 0.4525
2024-06-03 02:13:57 [INFO]: Epoch 018 - training loss: 0.3708, validation loss: 0.4563
2024-06-03 02:14:00 [INFO]: Epoch 019 - training loss: 0.3699, validation loss: 0.4523
2024-06-03 02:14:03 [INFO]: Epoch 020 - training loss: 0.3685, validation loss: 0.4452
2024-06-03 02:14:06 [INFO]: Epoch 021 - training loss: 0.3602, validation loss: 0.4442
2024-06-03 02:14:09 [INFO]: Epoch 022 - training loss: 0.3614, validation loss: 0.4440
2024-06-03 02:14:13 [INFO]: Epoch 023 - training loss: 0.3577, validation loss: 0.4430
2024-06-03 02:14:16 [INFO]: Epoch 024 - training loss: 0.3591, validation loss: 0.4470
2024-06-03 02:14:19 [INFO]: Epoch 025 - training loss: 0.3530, validation loss: 0.4376
2024-06-03 02:14:22 [INFO]: Epoch 026 - training loss: 0.3494, validation loss: 0.4399
2024-06-03 02:14:25 [INFO]: Epoch 027 - training loss: 0.3498, validation loss: 0.4425
2024-06-03 02:14:28 [INFO]: Epoch 028 - training loss: 0.3419, validation loss: 0.4332
2024-06-03 02:14:31 [INFO]: Epoch 029 - training loss: 0.3392, validation loss: 0.4340
2024-06-03 02:14:34 [INFO]: Epoch 030 - training loss: 0.3342, validation loss: 0.4328
2024-06-03 02:14:37 [INFO]: Epoch 031 - training loss: 0.3334, validation loss: 0.4397
2024-06-03 02:14:40 [INFO]: Epoch 032 - training loss: 0.3355, validation loss: 0.4328
2024-06-03 02:14:44 [INFO]: Epoch 033 - training loss: 0.3314, validation loss: 0.4371
2024-06-03 02:14:47 [INFO]: Epoch 034 - training loss: 0.3278, validation loss: 0.4299
2024-06-03 02:14:50 [INFO]: Epoch 035 - training loss: 0.3273, validation loss: 0.4357
2024-06-03 02:14:53 [INFO]: Epoch 036 - training loss: 0.3277, validation loss: 0.4319
2024-06-03 02:14:56 [INFO]: Epoch 037 - training loss: 0.3283, validation loss: 0.4335
2024-06-03 02:14:59 [INFO]: Epoch 038 - training loss: 0.3287, validation loss: 0.4302
2024-06-03 02:15:02 [INFO]: Epoch 039 - training loss: 0.3202, validation loss: 0.4272
2024-06-03 02:15:05 [INFO]: Epoch 040 - training loss: 0.3235, validation loss: 0.4292
2024-06-03 02:15:08 [INFO]: Epoch 041 - training loss: 0.3183, validation loss: 0.4287
2024-06-03 02:15:11 [INFO]: Epoch 042 - training loss: 0.3115, validation loss: 0.4279
2024-06-03 02:15:14 [INFO]: Epoch 043 - training loss: 0.3174, validation loss: 0.4273
2024-06-03 02:15:17 [INFO]: Epoch 044 - training loss: 0.3105, validation loss: 0.4261
2024-06-03 02:15:20 [INFO]: Epoch 045 - training loss: 0.3080, validation loss: 0.4280
2024-06-03 02:15:22 [INFO]: Epoch 046 - training loss: 0.3120, validation loss: 0.4340
2024-06-03 02:15:25 [INFO]: Epoch 047 - training loss: 0.3132, validation loss: 0.4238
2024-06-03 02:15:29 [INFO]: Epoch 048 - training loss: 0.3060, validation loss: 0.4290
2024-06-03 02:15:32 [INFO]: Epoch 049 - training loss: 0.3040, validation loss: 0.4223
2024-06-03 02:15:35 [INFO]: Epoch 050 - training loss: 0.3047, validation loss: 0.4207
2024-06-03 02:15:38 [INFO]: Epoch 051 - training loss: 0.3033, validation loss: 0.4208
2024-06-03 02:15:41 [INFO]: Epoch 052 - training loss: 0.3027, validation loss: 0.4219
2024-06-03 02:15:44 [INFO]: Epoch 053 - training loss: 0.3050, validation loss: 0.4182
2024-06-03 02:15:47 [INFO]: Epoch 054 - training loss: 0.3019, validation loss: 0.4176
2024-06-03 02:15:50 [INFO]: Epoch 055 - training loss: 0.2984, validation loss: 0.4211
2024-06-03 02:15:54 [INFO]: Epoch 056 - training loss: 0.3052, validation loss: 0.4193
2024-06-03 02:15:57 [INFO]: Epoch 057 - training loss: 0.3002, validation loss: 0.4189
2024-06-03 02:16:00 [INFO]: Epoch 058 - training loss: 0.2944, validation loss: 0.4165
2024-06-03 02:16:04 [INFO]: Epoch 059 - training loss: 0.2937, validation loss: 0.4150
2024-06-03 02:16:07 [INFO]: Epoch 060 - training loss: 0.2897, validation loss: 0.4196
2024-06-03 02:16:10 [INFO]: Epoch 061 - training loss: 0.2910, validation loss: 0.4170
2024-06-03 02:16:13 [INFO]: Epoch 062 - training loss: 0.2873, validation loss: 0.4169
2024-06-03 02:16:17 [INFO]: Epoch 063 - training loss: 0.2915, validation loss: 0.4163
2024-06-03 02:16:20 [INFO]: Epoch 064 - training loss: 0.2878, validation loss: 0.4172
2024-06-03 02:16:22 [INFO]: Epoch 065 - training loss: 0.2868, validation loss: 0.4162
2024-06-03 02:16:26 [INFO]: Epoch 066 - training loss: 0.2860, validation loss: 0.4209
2024-06-03 02:16:28 [INFO]: Epoch 067 - training loss: 0.2863, validation loss: 0.4159
2024-06-03 02:16:32 [INFO]: Epoch 068 - training loss: 0.2837, validation loss: 0.4202
2024-06-03 02:16:35 [INFO]: Epoch 069 - training loss: 0.2844, validation loss: 0.4118
2024-06-03 02:16:38 [INFO]: Epoch 070 - training loss: 0.2841, validation loss: 0.4167
2024-06-03 02:16:40 [INFO]: Epoch 071 - training loss: 0.2831, validation loss: 0.4193
2024-06-03 02:16:43 [INFO]: Epoch 072 - training loss: 0.2853, validation loss: 0.4125
2024-06-03 02:16:47 [INFO]: Epoch 073 - training loss: 0.2823, validation loss: 0.4156
2024-06-03 02:16:49 [INFO]: Epoch 074 - training loss: 0.2836, validation loss: 0.4175
2024-06-03 02:16:52 [INFO]: Epoch 075 - training loss: 0.2790, validation loss: 0.4117
2024-06-03 02:16:55 [INFO]: Epoch 076 - training loss: 0.2759, validation loss: 0.4159
2024-06-03 02:16:59 [INFO]: Epoch 077 - training loss: 0.2766, validation loss: 0.4148
2024-06-03 02:17:01 [INFO]: Epoch 078 - training loss: 0.2753, validation loss: 0.4123
2024-06-03 02:17:05 [INFO]: Epoch 079 - training loss: 0.2755, validation loss: 0.4105
2024-06-03 02:17:08 [INFO]: Epoch 080 - training loss: 0.2715, validation loss: 0.4108
2024-06-03 02:17:11 [INFO]: Epoch 081 - training loss: 0.2708, validation loss: 0.4086
2024-06-03 02:17:14 [INFO]: Epoch 082 - training loss: 0.2726, validation loss: 0.4085
2024-06-03 02:17:17 [INFO]: Epoch 083 - training loss: 0.2723, validation loss: 0.4067
2024-06-03 02:17:21 [INFO]: Epoch 084 - training loss: 0.2704, validation loss: 0.4101
2024-06-03 02:17:24 [INFO]: Epoch 085 - training loss: 0.2735, validation loss: 0.4156
2024-06-03 02:17:27 [INFO]: Epoch 086 - training loss: 0.2845, validation loss: 0.4107
2024-06-03 02:17:30 [INFO]: Epoch 087 - training loss: 0.2759, validation loss: 0.4102
2024-06-03 02:17:33 [INFO]: Epoch 088 - training loss: 0.2712, validation loss: 0.4075
2024-06-03 02:17:36 [INFO]: Epoch 089 - training loss: 0.2696, validation loss: 0.4090
2024-06-03 02:17:40 [INFO]: Epoch 090 - training loss: 0.2667, validation loss: 0.4103
2024-06-03 02:17:43 [INFO]: Epoch 091 - training loss: 0.2687, validation loss: 0.4084
2024-06-03 02:17:46 [INFO]: Epoch 092 - training loss: 0.2689, validation loss: 0.4058
2024-06-03 02:17:49 [INFO]: Epoch 093 - training loss: 0.2677, validation loss: 0.4074
2024-06-03 02:17:52 [INFO]: Epoch 094 - training loss: 0.2639, validation loss: 0.4059
2024-06-03 02:17:55 [INFO]: Epoch 095 - training loss: 0.2644, validation loss: 0.4089
2024-06-03 02:17:58 [INFO]: Epoch 096 - training loss: 0.2668, validation loss: 0.4069
2024-06-03 02:18:01 [INFO]: Epoch 097 - training loss: 0.2671, validation loss: 0.4117
2024-06-03 02:18:04 [INFO]: Epoch 098 - training loss: 0.2625, validation loss: 0.4110
2024-06-03 02:18:07 [INFO]: Epoch 099 - training loss: 0.2627, validation loss: 0.4096
2024-06-03 02:18:10 [INFO]: Epoch 100 - training loss: 0.2643, validation loss: 0.4096
2024-06-03 02:18:10 [INFO]: Finished training. The best model is from epoch#92.
2024-06-03 02:18:10 [INFO]: Saved the model to results_point_rate05/PeMS/Pyraformer_PeMS/round_1/20240603_T021301/Pyraformer.pypots
2024-06-03 02:18:11 [INFO]: Successfully saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_1/imputation.pkl
2024-06-03 02:18:11 [INFO]: Round1 - Pyraformer on PeMS: MAE=0.3077, MSE=0.5833, MRE=0.3819
2024-06-03 02:18:11 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 02:18:11 [INFO]: Using the given device: cuda:0
2024-06-03 02:18:11 [INFO]: Model files will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_2/20240603_T021811
2024-06-03 02:18:11 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_2/20240603_T021811/tensorboard
2024-06-03 02:18:12 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 02:18:14 [INFO]: Epoch 001 - training loss: 0.9288, validation loss: 0.5769
2024-06-03 02:18:17 [INFO]: Epoch 002 - training loss: 0.5821, validation loss: 0.5230
2024-06-03 02:18:20 [INFO]: Epoch 003 - training loss: 0.5237, validation loss: 0.5031
2024-06-03 02:18:23 [INFO]: Epoch 004 - training loss: 0.4866, validation loss: 0.4952
2024-06-03 02:18:26 [INFO]: Epoch 005 - training loss: 0.4662, validation loss: 0.4857
2024-06-03 02:18:29 [INFO]: Epoch 006 - training loss: 0.4599, validation loss: 0.4819
2024-06-03 02:18:32 [INFO]: Epoch 007 - training loss: 0.4507, validation loss: 0.4785
2024-06-03 02:18:35 [INFO]: Epoch 008 - training loss: 0.4326, validation loss: 0.4763
2024-06-03 02:18:38 [INFO]: Epoch 009 - training loss: 0.4299, validation loss: 0.4703
2024-06-03 02:18:40 [INFO]: Epoch 010 - training loss: 0.4222, validation loss: 0.4700
2024-06-03 02:18:43 [INFO]: Epoch 011 - training loss: 0.4239, validation loss: 0.4733
2024-06-03 02:18:46 [INFO]: Epoch 012 - training loss: 0.4141, validation loss: 0.4659
2024-06-03 02:18:48 [INFO]: Epoch 013 - training loss: 0.4016, validation loss: 0.4622
2024-06-03 02:18:50 [INFO]: Epoch 014 - training loss: 0.3913, validation loss: 0.4535
2024-06-03 02:18:51 [INFO]: Epoch 015 - training loss: 0.3918, validation loss: 0.4571
2024-06-03 02:18:53 [INFO]: Epoch 016 - training loss: 0.3864, validation loss: 0.4499
2024-06-03 02:18:55 [INFO]: Epoch 017 - training loss: 0.3782, validation loss: 0.4519
2024-06-03 02:18:57 [INFO]: Epoch 018 - training loss: 0.3715, validation loss: 0.4503
2024-06-03 02:18:59 [INFO]: Epoch 019 - training loss: 0.3717, validation loss: 0.4496
2024-06-03 02:19:01 [INFO]: Epoch 020 - training loss: 0.3695, validation loss: 0.4491
2024-06-03 02:19:02 [INFO]: Epoch 021 - training loss: 0.3689, validation loss: 0.4515
2024-06-03 02:19:04 [INFO]: Epoch 022 - training loss: 0.3533, validation loss: 0.4386
2024-06-03 02:19:06 [INFO]: Epoch 023 - training loss: 0.3532, validation loss: 0.4426
2024-06-03 02:19:08 [INFO]: Epoch 024 - training loss: 0.3497, validation loss: 0.4422
2024-06-03 02:19:10 [INFO]: Epoch 025 - training loss: 0.3474, validation loss: 0.4391
2024-06-03 02:19:12 [INFO]: Epoch 026 - training loss: 0.3482, validation loss: 0.4383
2024-06-03 02:19:14 [INFO]: Epoch 027 - training loss: 0.3461, validation loss: 0.4413
2024-06-03 02:19:16 [INFO]: Epoch 028 - training loss: 0.3411, validation loss: 0.4375
2024-06-03 02:19:19 [INFO]: Epoch 029 - training loss: 0.3368, validation loss: 0.4320
2024-06-03 02:19:20 [INFO]: Epoch 030 - training loss: 0.3380, validation loss: 0.4394
2024-06-03 02:19:22 [INFO]: Epoch 031 - training loss: 0.3343, validation loss: 0.4383
2024-06-03 02:19:24 [INFO]: Epoch 032 - training loss: 0.3346, validation loss: 0.4367
2024-06-03 02:19:26 [INFO]: Epoch 033 - training loss: 0.3352, validation loss: 0.4381
2024-06-03 02:19:28 [INFO]: Epoch 034 - training loss: 0.3319, validation loss: 0.4329
2024-06-03 02:19:29 [INFO]: Epoch 035 - training loss: 0.3274, validation loss: 0.4338
2024-06-03 02:19:31 [INFO]: Epoch 036 - training loss: 0.3216, validation loss: 0.4334
2024-06-03 02:19:33 [INFO]: Epoch 037 - training loss: 0.3241, validation loss: 0.4286
2024-06-03 02:19:35 [INFO]: Epoch 038 - training loss: 0.3218, validation loss: 0.4295
2024-06-03 02:19:36 [INFO]: Epoch 039 - training loss: 0.3152, validation loss: 0.4266
2024-06-03 02:19:38 [INFO]: Epoch 040 - training loss: 0.3150, validation loss: 0.4267
2024-06-03 02:19:40 [INFO]: Epoch 041 - training loss: 0.3141, validation loss: 0.4259
2024-06-03 02:19:42 [INFO]: Epoch 042 - training loss: 0.3125, validation loss: 0.4263
2024-06-03 02:19:44 [INFO]: Epoch 043 - training loss: 0.3106, validation loss: 0.4195
2024-06-03 02:19:46 [INFO]: Epoch 044 - training loss: 0.3116, validation loss: 0.4253
2024-06-03 02:19:48 [INFO]: Epoch 045 - training loss: 0.3097, validation loss: 0.4268
2024-06-03 02:19:50 [INFO]: Epoch 046 - training loss: 0.3112, validation loss: 0.4204
2024-06-03 02:19:52 [INFO]: Epoch 047 - training loss: 0.3060, validation loss: 0.4263
2024-06-03 02:19:54 [INFO]: Epoch 048 - training loss: 0.3043, validation loss: 0.4166
2024-06-03 02:19:56 [INFO]: Epoch 049 - training loss: 0.3009, validation loss: 0.4214
2024-06-03 02:19:59 [INFO]: Epoch 050 - training loss: 0.2999, validation loss: 0.4197
2024-06-03 02:20:02 [INFO]: Epoch 051 - training loss: 0.3005, validation loss: 0.4187
2024-06-03 02:20:05 [INFO]: Epoch 052 - training loss: 0.3003, validation loss: 0.4196
2024-06-03 02:20:08 [INFO]: Epoch 053 - training loss: 0.2986, validation loss: 0.4185
2024-06-03 02:20:11 [INFO]: Epoch 054 - training loss: 0.2983, validation loss: 0.4231
2024-06-03 02:20:13 [INFO]: Epoch 055 - training loss: 0.2940, validation loss: 0.4203
2024-06-03 02:20:16 [INFO]: Epoch 056 - training loss: 0.2930, validation loss: 0.4148
2024-06-03 02:20:19 [INFO]: Epoch 057 - training loss: 0.2918, validation loss: 0.4191
2024-06-03 02:20:22 [INFO]: Epoch 058 - training loss: 0.2946, validation loss: 0.4169
2024-06-03 02:20:25 [INFO]: Epoch 059 - training loss: 0.2935, validation loss: 0.4179
2024-06-03 02:20:28 [INFO]: Epoch 060 - training loss: 0.2908, validation loss: 0.4144
2024-06-03 02:20:31 [INFO]: Epoch 061 - training loss: 0.2886, validation loss: 0.4183
2024-06-03 02:20:34 [INFO]: Epoch 062 - training loss: 0.2867, validation loss: 0.4150
2024-06-03 02:20:37 [INFO]: Epoch 063 - training loss: 0.2842, validation loss: 0.4177
2024-06-03 02:20:40 [INFO]: Epoch 064 - training loss: 0.2855, validation loss: 0.4155
2024-06-03 02:20:42 [INFO]: Epoch 065 - training loss: 0.2859, validation loss: 0.4126
2024-06-03 02:20:45 [INFO]: Epoch 066 - training loss: 0.2854, validation loss: 0.4144
2024-06-03 02:20:48 [INFO]: Epoch 067 - training loss: 0.2877, validation loss: 0.4117
2024-06-03 02:20:51 [INFO]: Epoch 068 - training loss: 0.2849, validation loss: 0.4194
2024-06-03 02:20:53 [INFO]: Epoch 069 - training loss: 0.2849, validation loss: 0.4117
2024-06-03 02:20:56 [INFO]: Epoch 070 - training loss: 0.2826, validation loss: 0.4134
2024-06-03 02:20:59 [INFO]: Epoch 071 - training loss: 0.2774, validation loss: 0.4167
2024-06-03 02:21:02 [INFO]: Epoch 072 - training loss: 0.2861, validation loss: 0.4120
2024-06-03 02:21:05 [INFO]: Epoch 073 - training loss: 0.2822, validation loss: 0.4090
2024-06-03 02:21:07 [INFO]: Epoch 074 - training loss: 0.2807, validation loss: 0.4149
2024-06-03 02:21:10 [INFO]: Epoch 075 - training loss: 0.2768, validation loss: 0.4110
2024-06-03 02:21:13 [INFO]: Epoch 076 - training loss: 0.2725, validation loss: 0.4127
2024-06-03 02:21:16 [INFO]: Epoch 077 - training loss: 0.2753, validation loss: 0.4128
2024-06-03 02:21:19 [INFO]: Epoch 078 - training loss: 0.2727, validation loss: 0.4100
2024-06-03 02:21:22 [INFO]: Epoch 079 - training loss: 0.2726, validation loss: 0.4126
2024-06-03 02:21:25 [INFO]: Epoch 080 - training loss: 0.2735, validation loss: 0.4117
2024-06-03 02:21:27 [INFO]: Epoch 081 - training loss: 0.2749, validation loss: 0.4091
2024-06-03 02:21:30 [INFO]: Epoch 082 - training loss: 0.2706, validation loss: 0.4084
2024-06-03 02:21:33 [INFO]: Epoch 083 - training loss: 0.2733, validation loss: 0.4082
2024-06-03 02:21:36 [INFO]: Epoch 084 - training loss: 0.2705, validation loss: 0.4078
2024-06-03 02:21:39 [INFO]: Epoch 085 - training loss: 0.2726, validation loss: 0.4087
2024-06-03 02:21:42 [INFO]: Epoch 086 - training loss: 0.2679, validation loss: 0.4085
2024-06-03 02:21:45 [INFO]: Epoch 087 - training loss: 0.2677, validation loss: 0.4059
2024-06-03 02:21:48 [INFO]: Epoch 088 - training loss: 0.2680, validation loss: 0.4088
2024-06-03 02:21:51 [INFO]: Epoch 089 - training loss: 0.2651, validation loss: 0.4056
2024-06-03 02:21:54 [INFO]: Epoch 090 - training loss: 0.2682, validation loss: 0.4081
2024-06-03 02:21:57 [INFO]: Epoch 091 - training loss: 0.2689, validation loss: 0.4069
2024-06-03 02:21:59 [INFO]: Epoch 092 - training loss: 0.2669, validation loss: 0.4117
2024-06-03 02:22:02 [INFO]: Epoch 093 - training loss: 0.2617, validation loss: 0.4095
2024-06-03 02:22:05 [INFO]: Epoch 094 - training loss: 0.2627, validation loss: 0.4080
2024-06-03 02:22:08 [INFO]: Epoch 095 - training loss: 0.2621, validation loss: 0.4109
2024-06-03 02:22:10 [INFO]: Epoch 096 - training loss: 0.2611, validation loss: 0.4103
2024-06-03 02:22:13 [INFO]: Epoch 097 - training loss: 0.2651, validation loss: 0.4075
2024-06-03 02:22:16 [INFO]: Epoch 098 - training loss: 0.2621, validation loss: 0.4088
2024-06-03 02:22:19 [INFO]: Epoch 099 - training loss: 0.2589, validation loss: 0.4061
2024-06-03 02:22:19 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:22:19 [INFO]: Finished training. The best model is from epoch#89.
2024-06-03 02:22:19 [INFO]: Saved the model to results_point_rate05/PeMS/Pyraformer_PeMS/round_2/20240603_T021811/Pyraformer.pypots
2024-06-03 02:22:20 [INFO]: Successfully saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_2/imputation.pkl
2024-06-03 02:22:20 [INFO]: Round2 - Pyraformer on PeMS: MAE=0.3028, MSE=0.5758, MRE=0.3758
2024-06-03 02:22:20 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 02:22:20 [INFO]: Using the given device: cuda:0
2024-06-03 02:22:20 [INFO]: Model files will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_3/20240603_T022220
2024-06-03 02:22:20 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_3/20240603_T022220/tensorboard
2024-06-03 02:22:20 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 02:22:23 [INFO]: Epoch 001 - training loss: 0.9489, validation loss: 0.5791
2024-06-03 02:22:26 [INFO]: Epoch 002 - training loss: 0.5852, validation loss: 0.5280
2024-06-03 02:22:29 [INFO]: Epoch 003 - training loss: 0.5289, validation loss: 0.5110
2024-06-03 02:22:32 [INFO]: Epoch 004 - training loss: 0.4889, validation loss: 0.5073
2024-06-03 02:22:35 [INFO]: Epoch 005 - training loss: 0.4835, validation loss: 0.4957
2024-06-03 02:22:38 [INFO]: Epoch 006 - training loss: 0.4636, validation loss: 0.4849
2024-06-03 02:22:40 [INFO]: Epoch 007 - training loss: 0.4497, validation loss: 0.4840
2024-06-03 02:22:43 [INFO]: Epoch 008 - training loss: 0.4383, validation loss: 0.4774
2024-06-03 02:22:46 [INFO]: Epoch 009 - training loss: 0.4217, validation loss: 0.4810
2024-06-03 02:22:49 [INFO]: Epoch 010 - training loss: 0.4225, validation loss: 0.4725
2024-06-03 02:22:52 [INFO]: Epoch 011 - training loss: 0.4089, validation loss: 0.4645
2024-06-03 02:22:55 [INFO]: Epoch 012 - training loss: 0.4136, validation loss: 0.4582
2024-06-03 02:22:57 [INFO]: Epoch 013 - training loss: 0.3974, validation loss: 0.4631
2024-06-03 02:23:00 [INFO]: Epoch 014 - training loss: 0.3917, validation loss: 0.4655
2024-06-03 02:23:03 [INFO]: Epoch 015 - training loss: 0.3869, validation loss: 0.4594
2024-06-03 02:23:06 [INFO]: Epoch 016 - training loss: 0.3942, validation loss: 0.4632
2024-06-03 02:23:09 [INFO]: Epoch 017 - training loss: 0.3841, validation loss: 0.4492
2024-06-03 02:23:12 [INFO]: Epoch 018 - training loss: 0.3747, validation loss: 0.4523
2024-06-03 02:23:14 [INFO]: Epoch 019 - training loss: 0.3667, validation loss: 0.4536
2024-06-03 02:23:17 [INFO]: Epoch 020 - training loss: 0.3637, validation loss: 0.4522
2024-06-03 02:23:20 [INFO]: Epoch 021 - training loss: 0.3605, validation loss: 0.4521
2024-06-03 02:23:23 [INFO]: Epoch 022 - training loss: 0.3630, validation loss: 0.4459
2024-06-03 02:23:26 [INFO]: Epoch 023 - training loss: 0.3592, validation loss: 0.4426
2024-06-03 02:23:28 [INFO]: Epoch 024 - training loss: 0.3545, validation loss: 0.4421
2024-06-03 02:23:31 [INFO]: Epoch 025 - training loss: 0.3499, validation loss: 0.4455
2024-06-03 02:23:33 [INFO]: Epoch 026 - training loss: 0.3490, validation loss: 0.4476
2024-06-03 02:23:36 [INFO]: Epoch 027 - training loss: 0.3453, validation loss: 0.4434
2024-06-03 02:23:38 [INFO]: Epoch 028 - training loss: 0.3439, validation loss: 0.4409
2024-06-03 02:23:40 [INFO]: Epoch 029 - training loss: 0.3428, validation loss: 0.4366
2024-06-03 02:23:43 [INFO]: Epoch 030 - training loss: 0.3398, validation loss: 0.4400
2024-06-03 02:23:45 [INFO]: Epoch 031 - training loss: 0.3368, validation loss: 0.4357
2024-06-03 02:23:47 [INFO]: Epoch 032 - training loss: 0.3352, validation loss: 0.4348
2024-06-03 02:23:50 [INFO]: Epoch 033 - training loss: 0.3302, validation loss: 0.4338
2024-06-03 02:23:52 [INFO]: Epoch 034 - training loss: 0.3270, validation loss: 0.4348
2024-06-03 02:23:54 [INFO]: Epoch 035 - training loss: 0.3364, validation loss: 0.4361
2024-06-03 02:23:57 [INFO]: Epoch 036 - training loss: 0.3312, validation loss: 0.4355
2024-06-03 02:23:59 [INFO]: Epoch 037 - training loss: 0.3262, validation loss: 0.4307
2024-06-03 02:24:01 [INFO]: Epoch 038 - training loss: 0.3267, validation loss: 0.4314
2024-06-03 02:24:04 [INFO]: Epoch 039 - training loss: 0.3209, validation loss: 0.4308
2024-06-03 02:24:06 [INFO]: Epoch 040 - training loss: 0.3209, validation loss: 0.4290
2024-06-03 02:24:08 [INFO]: Epoch 041 - training loss: 0.3188, validation loss: 0.4283
2024-06-03 02:24:11 [INFO]: Epoch 042 - training loss: 0.3202, validation loss: 0.4257
2024-06-03 02:24:13 [INFO]: Epoch 043 - training loss: 0.3123, validation loss: 0.4265
2024-06-03 02:24:16 [INFO]: Epoch 044 - training loss: 0.3182, validation loss: 0.4234
2024-06-03 02:24:18 [INFO]: Epoch 045 - training loss: 0.3133, validation loss: 0.4267
2024-06-03 02:24:21 [INFO]: Epoch 046 - training loss: 0.3086, validation loss: 0.4238
2024-06-03 02:24:23 [INFO]: Epoch 047 - training loss: 0.3106, validation loss: 0.4211
2024-06-03 02:24:26 [INFO]: Epoch 048 - training loss: 0.3071, validation loss: 0.4221
2024-06-03 02:24:28 [INFO]: Epoch 049 - training loss: 0.3078, validation loss: 0.4206
2024-06-03 02:24:30 [INFO]: Epoch 050 - training loss: 0.3053, validation loss: 0.4226
2024-06-03 02:24:33 [INFO]: Epoch 051 - training loss: 0.3047, validation loss: 0.4223
2024-06-03 02:24:35 [INFO]: Epoch 052 - training loss: 0.3017, validation loss: 0.4212
2024-06-03 02:24:37 [INFO]: Epoch 053 - training loss: 0.3003, validation loss: 0.4256
2024-06-03 02:24:40 [INFO]: Epoch 054 - training loss: 0.3011, validation loss: 0.4272
2024-06-03 02:24:42 [INFO]: Epoch 055 - training loss: 0.3003, validation loss: 0.4231
2024-06-03 02:24:44 [INFO]: Epoch 056 - training loss: 0.2998, validation loss: 0.4225
2024-06-03 02:24:47 [INFO]: Epoch 057 - training loss: 0.2969, validation loss: 0.4194
2024-06-03 02:24:49 [INFO]: Epoch 058 - training loss: 0.2971, validation loss: 0.4132
2024-06-03 02:24:51 [INFO]: Epoch 059 - training loss: 0.2966, validation loss: 0.4149
2024-06-03 02:24:54 [INFO]: Epoch 060 - training loss: 0.2911, validation loss: 0.4199
2024-06-03 02:24:56 [INFO]: Epoch 061 - training loss: 0.2897, validation loss: 0.4168
2024-06-03 02:24:58 [INFO]: Epoch 062 - training loss: 0.2920, validation loss: 0.4158
2024-06-03 02:25:01 [INFO]: Epoch 063 - training loss: 0.2889, validation loss: 0.4210
2024-06-03 02:25:03 [INFO]: Epoch 064 - training loss: 0.2925, validation loss: 0.4163
2024-06-03 02:25:06 [INFO]: Epoch 065 - training loss: 0.2891, validation loss: 0.4178
2024-06-03 02:25:08 [INFO]: Epoch 066 - training loss: 0.2876, validation loss: 0.4156
2024-06-03 02:25:11 [INFO]: Epoch 067 - training loss: 0.2864, validation loss: 0.4175
2024-06-03 02:25:13 [INFO]: Epoch 068 - training loss: 0.2843, validation loss: 0.4172
2024-06-03 02:25:13 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:25:13 [INFO]: Finished training. The best model is from epoch#58.
2024-06-03 02:25:13 [INFO]: Saved the model to results_point_rate05/PeMS/Pyraformer_PeMS/round_3/20240603_T022220/Pyraformer.pypots
2024-06-03 02:25:14 [INFO]: Successfully saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_3/imputation.pkl
2024-06-03 02:25:14 [INFO]: Round3 - Pyraformer on PeMS: MAE=0.3042, MSE=0.5861, MRE=0.3774
2024-06-03 02:25:14 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 02:25:14 [INFO]: Using the given device: cuda:0
2024-06-03 02:25:14 [INFO]: Model files will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_4/20240603_T022514
2024-06-03 02:25:14 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_4/20240603_T022514/tensorboard
2024-06-03 02:25:15 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 02:25:17 [INFO]: Epoch 001 - training loss: 0.9233, validation loss: 0.5853
2024-06-03 02:25:19 [INFO]: Epoch 002 - training loss: 0.5878, validation loss: 0.5228
2024-06-03 02:25:22 [INFO]: Epoch 003 - training loss: 0.5223, validation loss: 0.5016
2024-06-03 02:25:24 [INFO]: Epoch 004 - training loss: 0.4908, validation loss: 0.5054
2024-06-03 02:25:26 [INFO]: Epoch 005 - training loss: 0.4663, validation loss: 0.4896
2024-06-03 02:25:29 [INFO]: Epoch 006 - training loss: 0.4532, validation loss: 0.4885
2024-06-03 02:25:31 [INFO]: Epoch 007 - training loss: 0.4440, validation loss: 0.4807
2024-06-03 02:25:33 [INFO]: Epoch 008 - training loss: 0.4372, validation loss: 0.4772
2024-06-03 02:25:36 [INFO]: Epoch 009 - training loss: 0.4240, validation loss: 0.4683
2024-06-03 02:25:38 [INFO]: Epoch 010 - training loss: 0.4130, validation loss: 0.4691
2024-06-03 02:25:40 [INFO]: Epoch 011 - training loss: 0.4096, validation loss: 0.4640
2024-06-03 02:25:43 [INFO]: Epoch 012 - training loss: 0.4020, validation loss: 0.4546
2024-06-03 02:25:45 [INFO]: Epoch 013 - training loss: 0.3993, validation loss: 0.4609
2024-06-03 02:25:47 [INFO]: Epoch 014 - training loss: 0.3903, validation loss: 0.4544
2024-06-03 02:25:49 [INFO]: Epoch 015 - training loss: 0.3807, validation loss: 0.4521
2024-06-03 02:25:52 [INFO]: Epoch 016 - training loss: 0.3790, validation loss: 0.4571
2024-06-03 02:25:54 [INFO]: Epoch 017 - training loss: 0.3841, validation loss: 0.4506
2024-06-03 02:25:56 [INFO]: Epoch 018 - training loss: 0.3712, validation loss: 0.4470
2024-06-03 02:25:58 [INFO]: Epoch 019 - training loss: 0.3703, validation loss: 0.4476
2024-06-03 02:26:01 [INFO]: Epoch 020 - training loss: 0.3643, validation loss: 0.4438
2024-06-03 02:26:03 [INFO]: Epoch 021 - training loss: 0.3585, validation loss: 0.4454
2024-06-03 02:26:05 [INFO]: Epoch 022 - training loss: 0.3592, validation loss: 0.4406
2024-06-03 02:26:07 [INFO]: Epoch 023 - training loss: 0.3535, validation loss: 0.4427
2024-06-03 02:26:10 [INFO]: Epoch 024 - training loss: 0.3571, validation loss: 0.4395
2024-06-03 02:26:12 [INFO]: Epoch 025 - training loss: 0.3545, validation loss: 0.4384
2024-06-03 02:26:14 [INFO]: Epoch 026 - training loss: 0.3471, validation loss: 0.4391
2024-06-03 02:26:17 [INFO]: Epoch 027 - training loss: 0.3455, validation loss: 0.4366
2024-06-03 02:26:19 [INFO]: Epoch 028 - training loss: 0.3397, validation loss: 0.4370
2024-06-03 02:26:21 [INFO]: Epoch 029 - training loss: 0.3394, validation loss: 0.4326
2024-06-03 02:26:23 [INFO]: Epoch 030 - training loss: 0.3329, validation loss: 0.4297
2024-06-03 02:26:26 [INFO]: Epoch 031 - training loss: 0.3367, validation loss: 0.4284
2024-06-03 02:26:28 [INFO]: Epoch 032 - training loss: 0.3293, validation loss: 0.4342
2024-06-03 02:26:30 [INFO]: Epoch 033 - training loss: 0.3288, validation loss: 0.4305
2024-06-03 02:26:32 [INFO]: Epoch 034 - training loss: 0.3272, validation loss: 0.4259
2024-06-03 02:26:35 [INFO]: Epoch 035 - training loss: 0.3275, validation loss: 0.4251
2024-06-03 02:26:37 [INFO]: Epoch 036 - training loss: 0.3249, validation loss: 0.4228
2024-06-03 02:26:39 [INFO]: Epoch 037 - training loss: 0.3240, validation loss: 0.4250
2024-06-03 02:26:42 [INFO]: Epoch 038 - training loss: 0.3201, validation loss: 0.4254
2024-06-03 02:26:44 [INFO]: Epoch 039 - training loss: 0.3196, validation loss: 0.4251
2024-06-03 02:26:46 [INFO]: Epoch 040 - training loss: 0.3176, validation loss: 0.4250
2024-06-03 02:26:48 [INFO]: Epoch 041 - training loss: 0.3179, validation loss: 0.4245
2024-06-03 02:26:51 [INFO]: Epoch 042 - training loss: 0.3169, validation loss: 0.4284
2024-06-03 02:26:53 [INFO]: Epoch 043 - training loss: 0.3129, validation loss: 0.4239
2024-06-03 02:26:55 [INFO]: Epoch 044 - training loss: 0.3103, validation loss: 0.4258
2024-06-03 02:26:57 [INFO]: Epoch 045 - training loss: 0.3102, validation loss: 0.4217
2024-06-03 02:27:00 [INFO]: Epoch 046 - training loss: 0.3096, validation loss: 0.4212
2024-06-03 02:27:02 [INFO]: Epoch 047 - training loss: 0.3023, validation loss: 0.4211
2024-06-03 02:27:04 [INFO]: Epoch 048 - training loss: 0.3079, validation loss: 0.4131
2024-06-03 02:27:05 [INFO]: Epoch 049 - training loss: 0.3035, validation loss: 0.4171
2024-06-03 02:27:07 [INFO]: Epoch 050 - training loss: 0.3055, validation loss: 0.4175
2024-06-03 02:27:09 [INFO]: Epoch 051 - training loss: 0.3044, validation loss: 0.4201
2024-06-03 02:27:11 [INFO]: Epoch 052 - training loss: 0.3021, validation loss: 0.4206
2024-06-03 02:27:13 [INFO]: Epoch 053 - training loss: 0.2967, validation loss: 0.4169
2024-06-03 02:27:15 [INFO]: Epoch 054 - training loss: 0.2940, validation loss: 0.4139
2024-06-03 02:27:16 [INFO]: Epoch 055 - training loss: 0.2977, validation loss: 0.4141
2024-06-03 02:27:18 [INFO]: Epoch 056 - training loss: 0.2954, validation loss: 0.4152
2024-06-03 02:27:20 [INFO]: Epoch 057 - training loss: 0.2933, validation loss: 0.4157
2024-06-03 02:27:22 [INFO]: Epoch 058 - training loss: 0.2944, validation loss: 0.4130
2024-06-03 02:27:23 [INFO]: Epoch 059 - training loss: 0.2909, validation loss: 0.4192
2024-06-03 02:27:25 [INFO]: Epoch 060 - training loss: 0.2880, validation loss: 0.4131
2024-06-03 02:27:27 [INFO]: Epoch 061 - training loss: 0.2901, validation loss: 0.4134
2024-06-03 02:27:29 [INFO]: Epoch 062 - training loss: 0.2869, validation loss: 0.4156
2024-06-03 02:27:31 [INFO]: Epoch 063 - training loss: 0.2902, validation loss: 0.4114
2024-06-03 02:27:32 [INFO]: Epoch 064 - training loss: 0.2863, validation loss: 0.4167
2024-06-03 02:27:34 [INFO]: Epoch 065 - training loss: 0.2888, validation loss: 0.4147
2024-06-03 02:27:36 [INFO]: Epoch 066 - training loss: 0.2829, validation loss: 0.4093
2024-06-03 02:27:37 [INFO]: Epoch 067 - training loss: 0.2826, validation loss: 0.4101
2024-06-03 02:27:39 [INFO]: Epoch 068 - training loss: 0.2815, validation loss: 0.4103
2024-06-03 02:27:41 [INFO]: Epoch 069 - training loss: 0.2795, validation loss: 0.4123
2024-06-03 02:27:43 [INFO]: Epoch 070 - training loss: 0.2799, validation loss: 0.4151
2024-06-03 02:27:45 [INFO]: Epoch 071 - training loss: 0.2796, validation loss: 0.4183
2024-06-03 02:27:47 [INFO]: Epoch 072 - training loss: 0.2849, validation loss: 0.4114
2024-06-03 02:27:49 [INFO]: Epoch 073 - training loss: 0.2849, validation loss: 0.4081
2024-06-03 02:27:50 [INFO]: Epoch 074 - training loss: 0.2800, validation loss: 0.4092
2024-06-03 02:27:52 [INFO]: Epoch 075 - training loss: 0.2768, validation loss: 0.4094
2024-06-03 02:27:54 [INFO]: Epoch 076 - training loss: 0.2743, validation loss: 0.4087
2024-06-03 02:27:56 [INFO]: Epoch 077 - training loss: 0.2731, validation loss: 0.4073
2024-06-03 02:27:58 [INFO]: Epoch 078 - training loss: 0.2724, validation loss: 0.4079
2024-06-03 02:27:59 [INFO]: Epoch 079 - training loss: 0.2715, validation loss: 0.4072
2024-06-03 02:28:01 [INFO]: Epoch 080 - training loss: 0.2707, validation loss: 0.4095
2024-06-03 02:28:03 [INFO]: Epoch 081 - training loss: 0.2667, validation loss: 0.4085
2024-06-03 02:28:05 [INFO]: Epoch 082 - training loss: 0.2703, validation loss: 0.4071
2024-06-03 02:28:07 [INFO]: Epoch 083 - training loss: 0.2699, validation loss: 0.4095
2024-06-03 02:28:09 [INFO]: Epoch 084 - training loss: 0.2677, validation loss: 0.4089
2024-06-03 02:28:11 [INFO]: Epoch 085 - training loss: 0.2657, validation loss: 0.4061
2024-06-03 02:28:12 [INFO]: Epoch 086 - training loss: 0.2656, validation loss: 0.4078
2024-06-03 02:28:14 [INFO]: Epoch 087 - training loss: 0.2677, validation loss: 0.4070
2024-06-03 02:28:15 [INFO]: Epoch 088 - training loss: 0.2691, validation loss: 0.4107
2024-06-03 02:28:17 [INFO]: Epoch 089 - training loss: 0.2665, validation loss: 0.4043
2024-06-03 02:28:18 [INFO]: Epoch 090 - training loss: 0.2646, validation loss: 0.4067
2024-06-03 02:28:20 [INFO]: Epoch 091 - training loss: 0.2672, validation loss: 0.4072
2024-06-03 02:28:21 [INFO]: Epoch 092 - training loss: 0.2644, validation loss: 0.4099
2024-06-03 02:28:23 [INFO]: Epoch 093 - training loss: 0.2673, validation loss: 0.4065
2024-06-03 02:28:24 [INFO]: Epoch 094 - training loss: 0.2625, validation loss: 0.4047
2024-06-03 02:28:26 [INFO]: Epoch 095 - training loss: 0.2617, validation loss: 0.4063
2024-06-03 02:28:27 [INFO]: Epoch 096 - training loss: 0.2599, validation loss: 0.4072
2024-06-03 02:28:29 [INFO]: Epoch 097 - training loss: 0.2622, validation loss: 0.4072
2024-06-03 02:28:30 [INFO]: Epoch 098 - training loss: 0.2629, validation loss: 0.4084
2024-06-03 02:28:32 [INFO]: Epoch 099 - training loss: 0.2608, validation loss: 0.4068
2024-06-03 02:28:32 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:28:32 [INFO]: Finished training. The best model is from epoch#89.
2024-06-03 02:28:32 [INFO]: Saved the model to results_point_rate05/PeMS/Pyraformer_PeMS/round_4/20240603_T022514/Pyraformer.pypots
2024-06-03 02:28:32 [INFO]: Successfully saved to results_point_rate05/PeMS/Pyraformer_PeMS/round_4/imputation.pkl
2024-06-03 02:28:32 [INFO]: Round4 - Pyraformer on PeMS: MAE=0.3032, MSE=0.5750, MRE=0.3762
2024-06-03 02:28:32 [INFO]: Done! Final results:
Averaged Pyraformer (4,048,606 params) on PeMS: MAE=0.3050 ± 0.002078463190464043, MSE=0.5799 ± 0.004270683188479191, MRE=0.3785 ± 0.0025791965126067693, average inference time=0.21
