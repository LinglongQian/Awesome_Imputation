2024-06-03 02:07:41 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 02:07:41 [INFO]: Using the given device: cuda:0
2024-06-03 02:07:41 [INFO]: Model files will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_0/20240603_T020741
2024-06-03 02:07:41 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_0/20240603_T020741/tensorboard
2024-06-03 02:07:42 [INFO]: StemGNN initialized with the given hyperparameters, the number of trainable parameters: 2,386,294
2024-06-03 02:07:52 [INFO]: Epoch 001 - training loss: 1.2131, validation loss: 1.0147
2024-06-03 02:07:55 [INFO]: Epoch 002 - training loss: 0.8604, validation loss: 0.8574
2024-06-03 02:07:58 [INFO]: Epoch 003 - training loss: 0.7371, validation loss: 0.8044
2024-06-03 02:08:00 [INFO]: Epoch 004 - training loss: 0.6535, validation loss: 0.7561
2024-06-03 02:08:03 [INFO]: Epoch 005 - training loss: 0.6147, validation loss: 0.7462
2024-06-03 02:08:07 [INFO]: Epoch 006 - training loss: 0.5785, validation loss: 0.7059
2024-06-03 02:08:11 [INFO]: Epoch 007 - training loss: 0.5524, validation loss: 0.6694
2024-06-03 02:08:15 [INFO]: Epoch 008 - training loss: 0.5332, validation loss: 0.6616
2024-06-03 02:08:19 [INFO]: Epoch 009 - training loss: 0.5253, validation loss: 0.6526
2024-06-03 02:08:23 [INFO]: Epoch 010 - training loss: 0.5155, validation loss: 0.6486
2024-06-03 02:08:26 [INFO]: Epoch 011 - training loss: 0.5046, validation loss: 0.6395
2024-06-03 02:08:30 [INFO]: Epoch 012 - training loss: 0.4939, validation loss: 0.6265
2024-06-03 02:08:34 [INFO]: Epoch 013 - training loss: 0.4869, validation loss: 0.6289
2024-06-03 02:08:38 [INFO]: Epoch 014 - training loss: 0.4885, validation loss: 0.6218
2024-06-03 02:08:42 [INFO]: Epoch 015 - training loss: 0.4770, validation loss: 0.6127
2024-06-03 02:08:46 [INFO]: Epoch 016 - training loss: 0.4727, validation loss: 0.6126
2024-06-03 02:08:50 [INFO]: Epoch 017 - training loss: 0.4645, validation loss: 0.5966
2024-06-03 02:08:54 [INFO]: Epoch 018 - training loss: 0.4552, validation loss: 0.6245
2024-06-03 02:08:58 [INFO]: Epoch 019 - training loss: 0.4628, validation loss: 0.5995
2024-06-03 02:09:02 [INFO]: Epoch 020 - training loss: 0.4472, validation loss: 0.5872
2024-06-03 02:09:07 [INFO]: Epoch 021 - training loss: 0.4418, validation loss: 0.5959
2024-06-03 02:09:11 [INFO]: Epoch 022 - training loss: 0.4356, validation loss: 0.5837
2024-06-03 02:09:14 [INFO]: Epoch 023 - training loss: 0.4308, validation loss: 0.5903
2024-06-03 02:09:18 [INFO]: Epoch 024 - training loss: 0.4300, validation loss: 0.5946
2024-06-03 02:09:22 [INFO]: Epoch 025 - training loss: 0.4250, validation loss: 0.5849
2024-06-03 02:09:26 [INFO]: Epoch 026 - training loss: 0.4178, validation loss: 0.5810
2024-06-03 02:09:30 [INFO]: Epoch 027 - training loss: 0.4257, validation loss: 0.5959
2024-06-03 02:09:34 [INFO]: Epoch 028 - training loss: 0.4153, validation loss: 0.5845
2024-06-03 02:09:38 [INFO]: Epoch 029 - training loss: 0.4125, validation loss: 0.5724
2024-06-03 02:09:42 [INFO]: Epoch 030 - training loss: 0.4064, validation loss: 0.5816
2024-06-03 02:09:46 [INFO]: Epoch 031 - training loss: 0.4024, validation loss: 0.5755
2024-06-03 02:09:50 [INFO]: Epoch 032 - training loss: 0.4059, validation loss: 0.5766
2024-06-03 02:09:54 [INFO]: Epoch 033 - training loss: 0.4014, validation loss: 0.5949
2024-06-03 02:09:58 [INFO]: Epoch 034 - training loss: 0.4002, validation loss: 0.5743
2024-06-03 02:10:02 [INFO]: Epoch 035 - training loss: 0.3968, validation loss: 0.5826
2024-06-03 02:10:06 [INFO]: Epoch 036 - training loss: 0.3931, validation loss: 0.5627
2024-06-03 02:10:10 [INFO]: Epoch 037 - training loss: 0.3895, validation loss: 0.5669
2024-06-03 02:10:14 [INFO]: Epoch 038 - training loss: 0.3856, validation loss: 0.5708
2024-06-03 02:10:18 [INFO]: Epoch 039 - training loss: 0.3838, validation loss: 0.5583
2024-06-03 02:10:22 [INFO]: Epoch 040 - training loss: 0.3791, validation loss: 0.5576
2024-06-03 02:10:26 [INFO]: Epoch 041 - training loss: 0.3831, validation loss: 0.5739
2024-06-03 02:10:30 [INFO]: Epoch 042 - training loss: 0.3785, validation loss: 0.5595
2024-06-03 02:10:34 [INFO]: Epoch 043 - training loss: 0.3767, validation loss: 0.5594
2024-06-03 02:10:39 [INFO]: Epoch 044 - training loss: 0.3746, validation loss: 0.5698
2024-06-03 02:10:43 [INFO]: Epoch 045 - training loss: 0.3717, validation loss: 0.5613
2024-06-03 02:10:47 [INFO]: Epoch 046 - training loss: 0.3752, validation loss: 0.5562
2024-06-03 02:10:51 [INFO]: Epoch 047 - training loss: 0.3711, validation loss: 0.5540
2024-06-03 02:10:55 [INFO]: Epoch 048 - training loss: 0.3657, validation loss: 0.5538
2024-06-03 02:10:59 [INFO]: Epoch 049 - training loss: 0.3643, validation loss: 0.5589
2024-06-03 02:11:03 [INFO]: Epoch 050 - training loss: 0.3626, validation loss: 0.5693
2024-06-03 02:11:07 [INFO]: Epoch 051 - training loss: 0.3667, validation loss: 0.5512
2024-06-03 02:11:11 [INFO]: Epoch 052 - training loss: 0.3670, validation loss: 0.5579
2024-06-03 02:11:15 [INFO]: Epoch 053 - training loss: 0.3613, validation loss: 0.5457
2024-06-03 02:11:19 [INFO]: Epoch 054 - training loss: 0.3585, validation loss: 0.5464
2024-06-03 02:11:23 [INFO]: Epoch 055 - training loss: 0.3551, validation loss: 0.5423
2024-06-03 02:11:27 [INFO]: Epoch 056 - training loss: 0.3578, validation loss: 0.5462
2024-06-03 02:11:30 [INFO]: Epoch 057 - training loss: 0.3538, validation loss: 0.5488
2024-06-03 02:11:34 [INFO]: Epoch 058 - training loss: 0.3536, validation loss: 0.5558
2024-06-03 02:11:38 [INFO]: Epoch 059 - training loss: 0.3544, validation loss: 0.5494
2024-06-03 02:11:42 [INFO]: Epoch 060 - training loss: 0.3513, validation loss: 0.5429
2024-06-03 02:11:46 [INFO]: Epoch 061 - training loss: 0.3487, validation loss: 0.5538
2024-06-03 02:11:50 [INFO]: Epoch 062 - training loss: 0.3476, validation loss: 0.5441
2024-06-03 02:11:53 [INFO]: Epoch 063 - training loss: 0.3478, validation loss: 0.5404
2024-06-03 02:11:57 [INFO]: Epoch 064 - training loss: 0.3440, validation loss: 0.5403
2024-06-03 02:12:01 [INFO]: Epoch 065 - training loss: 0.3443, validation loss: 0.5416
2024-06-03 02:12:05 [INFO]: Epoch 066 - training loss: 0.3445, validation loss: 0.5381
2024-06-03 02:12:09 [INFO]: Epoch 067 - training loss: 0.3440, validation loss: 0.5384
2024-06-03 02:12:13 [INFO]: Epoch 068 - training loss: 0.3477, validation loss: 0.5423
2024-06-03 02:12:17 [INFO]: Epoch 069 - training loss: 0.3385, validation loss: 0.5308
2024-06-03 02:12:21 [INFO]: Epoch 070 - training loss: 0.3401, validation loss: 0.5307
2024-06-03 02:12:24 [INFO]: Epoch 071 - training loss: 0.3407, validation loss: 0.5359
2024-06-03 02:12:28 [INFO]: Epoch 072 - training loss: 0.3399, validation loss: 0.5345
2024-06-03 02:12:33 [INFO]: Epoch 073 - training loss: 0.3409, validation loss: 0.5353
2024-06-03 02:12:37 [INFO]: Epoch 074 - training loss: 0.3407, validation loss: 0.5379
2024-06-03 02:12:41 [INFO]: Epoch 075 - training loss: 0.3445, validation loss: 0.5407
2024-06-03 02:12:45 [INFO]: Epoch 076 - training loss: 0.3421, validation loss: 0.5402
2024-06-03 02:12:49 [INFO]: Epoch 077 - training loss: 0.3389, validation loss: 0.5396
2024-06-03 02:12:53 [INFO]: Epoch 078 - training loss: 0.3346, validation loss: 0.5317
2024-06-03 02:12:57 [INFO]: Epoch 079 - training loss: 0.3329, validation loss: 0.5237
2024-06-03 02:13:01 [INFO]: Epoch 080 - training loss: 0.3318, validation loss: 0.5287
2024-06-03 02:13:05 [INFO]: Epoch 081 - training loss: 0.3325, validation loss: 0.5221
2024-06-03 02:13:09 [INFO]: Epoch 082 - training loss: 0.3311, validation loss: 0.5219
2024-06-03 02:13:13 [INFO]: Epoch 083 - training loss: 0.3270, validation loss: 0.5246
2024-06-03 02:13:17 [INFO]: Epoch 084 - training loss: 0.3275, validation loss: 0.5327
2024-06-03 02:13:21 [INFO]: Epoch 085 - training loss: 0.3294, validation loss: 0.5253
2024-06-03 02:13:25 [INFO]: Epoch 086 - training loss: 0.3271, validation loss: 0.5257
2024-06-03 02:13:29 [INFO]: Epoch 087 - training loss: 0.3241, validation loss: 0.5254
2024-06-03 02:13:33 [INFO]: Epoch 088 - training loss: 0.3311, validation loss: 0.5232
2024-06-03 02:13:37 [INFO]: Epoch 089 - training loss: 0.3237, validation loss: 0.5379
2024-06-03 02:13:41 [INFO]: Epoch 090 - training loss: 0.3297, validation loss: 0.5245
2024-06-03 02:13:45 [INFO]: Epoch 091 - training loss: 0.3224, validation loss: 0.5208
2024-06-03 02:13:49 [INFO]: Epoch 092 - training loss: 0.3224, validation loss: 0.5290
2024-06-03 02:13:53 [INFO]: Epoch 093 - training loss: 0.3238, validation loss: 0.5186
2024-06-03 02:13:57 [INFO]: Epoch 094 - training loss: 0.3220, validation loss: 0.5232
2024-06-03 02:14:01 [INFO]: Epoch 095 - training loss: 0.3221, validation loss: 0.5207
2024-06-03 02:14:05 [INFO]: Epoch 096 - training loss: 0.3195, validation loss: 0.5201
2024-06-03 02:14:09 [INFO]: Epoch 097 - training loss: 0.3196, validation loss: 0.5181
2024-06-03 02:14:13 [INFO]: Epoch 098 - training loss: 0.3182, validation loss: 0.5218
2024-06-03 02:14:17 [INFO]: Epoch 099 - training loss: 0.3178, validation loss: 0.5228
2024-06-03 02:14:21 [INFO]: Epoch 100 - training loss: 0.3157, validation loss: 0.5217
2024-06-03 02:14:21 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 02:14:21 [INFO]: Saved the model to results_point_rate05/PeMS/StemGNN_PeMS/round_0/20240603_T020741/StemGNN.pypots
2024-06-03 02:14:23 [INFO]: Successfully saved to results_point_rate05/PeMS/StemGNN_PeMS/round_0/imputation.pkl
2024-06-03 02:14:23 [INFO]: Round0 - StemGNN on PeMS: MAE=0.4114, MSE=0.7671, MRE=0.5105
2024-06-03 02:14:23 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 02:14:23 [INFO]: Using the given device: cuda:0
2024-06-03 02:14:23 [INFO]: Model files will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_1/20240603_T021423
2024-06-03 02:14:23 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_1/20240603_T021423/tensorboard
2024-06-03 02:14:23 [INFO]: StemGNN initialized with the given hyperparameters, the number of trainable parameters: 2,386,294
2024-06-03 02:14:27 [INFO]: Epoch 001 - training loss: 1.2316, validation loss: 0.9646
2024-06-03 02:14:32 [INFO]: Epoch 002 - training loss: 0.8891, validation loss: 0.9104
2024-06-03 02:14:36 [INFO]: Epoch 003 - training loss: 0.8063, validation loss: 0.8402
2024-06-03 02:14:40 [INFO]: Epoch 004 - training loss: 0.7423, validation loss: 0.8170
2024-06-03 02:14:44 [INFO]: Epoch 005 - training loss: 0.6663, validation loss: 0.7863
2024-06-03 02:14:47 [INFO]: Epoch 006 - training loss: 0.5969, validation loss: 0.7840
2024-06-03 02:14:51 [INFO]: Epoch 007 - training loss: 0.5769, validation loss: 0.7152
2024-06-03 02:14:55 [INFO]: Epoch 008 - training loss: 0.5570, validation loss: 0.6833
2024-06-03 02:14:59 [INFO]: Epoch 009 - training loss: 0.5364, validation loss: 0.6747
2024-06-03 02:15:03 [INFO]: Epoch 010 - training loss: 0.5225, validation loss: 0.6427
2024-06-03 02:15:07 [INFO]: Epoch 011 - training loss: 0.5162, validation loss: 0.6454
2024-06-03 02:15:11 [INFO]: Epoch 012 - training loss: 0.5068, validation loss: 0.6520
2024-06-03 02:15:15 [INFO]: Epoch 013 - training loss: 0.4977, validation loss: 0.6313
2024-06-03 02:15:19 [INFO]: Epoch 014 - training loss: 0.4985, validation loss: 0.6291
2024-06-03 02:15:22 [INFO]: Epoch 015 - training loss: 0.4854, validation loss: 0.6381
2024-06-03 02:15:26 [INFO]: Epoch 016 - training loss: 0.4886, validation loss: 0.6200
2024-06-03 02:15:30 [INFO]: Epoch 017 - training loss: 0.4758, validation loss: 0.6170
2024-06-03 02:15:34 [INFO]: Epoch 018 - training loss: 0.4683, validation loss: 0.6215
2024-06-03 02:15:38 [INFO]: Epoch 019 - training loss: 0.4584, validation loss: 0.6144
2024-06-03 02:15:42 [INFO]: Epoch 020 - training loss: 0.4602, validation loss: 0.6009
2024-06-03 02:15:46 [INFO]: Epoch 021 - training loss: 0.4513, validation loss: 0.6062
2024-06-03 02:15:50 [INFO]: Epoch 022 - training loss: 0.4421, validation loss: 0.5999
2024-06-03 02:15:54 [INFO]: Epoch 023 - training loss: 0.4400, validation loss: 0.5976
2024-06-03 02:15:58 [INFO]: Epoch 024 - training loss: 0.4344, validation loss: 0.5952
2024-06-03 02:16:02 [INFO]: Epoch 025 - training loss: 0.4306, validation loss: 0.5958
2024-06-03 02:16:06 [INFO]: Epoch 026 - training loss: 0.4321, validation loss: 0.5902
2024-06-03 02:16:10 [INFO]: Epoch 027 - training loss: 0.4295, validation loss: 0.6037
2024-06-03 02:16:14 [INFO]: Epoch 028 - training loss: 0.4238, validation loss: 0.5847
2024-06-03 02:16:18 [INFO]: Epoch 029 - training loss: 0.4141, validation loss: 0.5837
2024-06-03 02:16:22 [INFO]: Epoch 030 - training loss: 0.4152, validation loss: 0.6000
2024-06-03 02:16:26 [INFO]: Epoch 031 - training loss: 0.4174, validation loss: 0.5805
2024-06-03 02:16:30 [INFO]: Epoch 032 - training loss: 0.4139, validation loss: 0.5848
2024-06-03 02:16:34 [INFO]: Epoch 033 - training loss: 0.4075, validation loss: 0.5866
2024-06-03 02:16:38 [INFO]: Epoch 034 - training loss: 0.4047, validation loss: 0.5746
2024-06-03 02:16:42 [INFO]: Epoch 035 - training loss: 0.4050, validation loss: 0.5776
2024-06-03 02:16:46 [INFO]: Epoch 036 - training loss: 0.4012, validation loss: 0.5749
2024-06-03 02:16:50 [INFO]: Epoch 037 - training loss: 0.3988, validation loss: 0.5781
2024-06-03 02:16:54 [INFO]: Epoch 038 - training loss: 0.3965, validation loss: 0.5739
2024-06-03 02:16:58 [INFO]: Epoch 039 - training loss: 0.3936, validation loss: 0.5720
2024-06-03 02:17:02 [INFO]: Epoch 040 - training loss: 0.3936, validation loss: 0.5662
2024-06-03 02:17:06 [INFO]: Epoch 041 - training loss: 0.3942, validation loss: 0.5710
2024-06-03 02:17:10 [INFO]: Epoch 042 - training loss: 0.3883, validation loss: 0.5731
2024-06-03 02:17:14 [INFO]: Epoch 043 - training loss: 0.3883, validation loss: 0.5701
2024-06-03 02:17:18 [INFO]: Epoch 044 - training loss: 0.3931, validation loss: 0.5831
2024-06-03 02:17:22 [INFO]: Epoch 045 - training loss: 0.3870, validation loss: 0.5788
2024-06-03 02:17:26 [INFO]: Epoch 046 - training loss: 0.3843, validation loss: 0.5792
2024-06-03 02:17:30 [INFO]: Epoch 047 - training loss: 0.3824, validation loss: 0.5838
2024-06-03 02:17:34 [INFO]: Epoch 048 - training loss: 0.3796, validation loss: 0.5810
2024-06-03 02:17:38 [INFO]: Epoch 049 - training loss: 0.3798, validation loss: 0.5882
2024-06-03 02:17:42 [INFO]: Epoch 050 - training loss: 0.3813, validation loss: 0.5977
2024-06-03 02:17:42 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:17:42 [INFO]: Finished training. The best model is from epoch#40.
2024-06-03 02:17:42 [INFO]: Saved the model to results_point_rate05/PeMS/StemGNN_PeMS/round_1/20240603_T021423/StemGNN.pypots
2024-06-03 02:17:44 [INFO]: Successfully saved to results_point_rate05/PeMS/StemGNN_PeMS/round_1/imputation.pkl
2024-06-03 02:17:44 [INFO]: Round1 - StemGNN on PeMS: MAE=0.4525, MSE=0.9015, MRE=0.5615
2024-06-03 02:17:44 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 02:17:44 [INFO]: Using the given device: cuda:0
2024-06-03 02:17:44 [INFO]: Model files will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_2/20240603_T021744
2024-06-03 02:17:44 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_2/20240603_T021744/tensorboard
2024-06-03 02:17:44 [INFO]: StemGNN initialized with the given hyperparameters, the number of trainable parameters: 2,386,294
2024-06-03 02:17:49 [INFO]: Epoch 001 - training loss: 1.2326, validation loss: 0.9140
2024-06-03 02:17:52 [INFO]: Epoch 002 - training loss: 0.8859, validation loss: 0.9169
2024-06-03 02:17:56 [INFO]: Epoch 003 - training loss: 0.8154, validation loss: 0.8688
2024-06-03 02:18:00 [INFO]: Epoch 004 - training loss: 0.7278, validation loss: 0.8249
2024-06-03 02:18:04 [INFO]: Epoch 005 - training loss: 0.6747, validation loss: 0.8355
2024-06-03 02:18:08 [INFO]: Epoch 006 - training loss: 0.6494, validation loss: 0.7997
2024-06-03 02:18:12 [INFO]: Epoch 007 - training loss: 0.6283, validation loss: 0.7962
2024-06-03 02:18:15 [INFO]: Epoch 008 - training loss: 0.6119, validation loss: 0.7523
2024-06-03 02:18:19 [INFO]: Epoch 009 - training loss: 0.6181, validation loss: 0.7986
2024-06-03 02:18:23 [INFO]: Epoch 010 - training loss: 0.6071, validation loss: 0.7286
2024-06-03 02:18:26 [INFO]: Epoch 011 - training loss: 0.6019, validation loss: 0.7530
2024-06-03 02:18:30 [INFO]: Epoch 012 - training loss: 0.5881, validation loss: 0.7630
2024-06-03 02:18:34 [INFO]: Epoch 013 - training loss: 0.5874, validation loss: 0.7428
2024-06-03 02:18:37 [INFO]: Epoch 014 - training loss: 0.5924, validation loss: 0.7273
2024-06-03 02:18:41 [INFO]: Epoch 015 - training loss: 0.5805, validation loss: 0.7652
2024-06-03 02:18:45 [INFO]: Epoch 016 - training loss: 0.5673, validation loss: 0.7099
2024-06-03 02:18:48 [INFO]: Epoch 017 - training loss: 0.5674, validation loss: 0.7351
2024-06-03 02:18:50 [INFO]: Epoch 018 - training loss: 0.5714, validation loss: 0.6945
2024-06-03 02:18:53 [INFO]: Epoch 019 - training loss: 0.5584, validation loss: 0.6672
2024-06-03 02:18:56 [INFO]: Epoch 020 - training loss: 0.5512, validation loss: 0.6788
2024-06-03 02:18:58 [INFO]: Epoch 021 - training loss: 0.5385, validation loss: 0.6950
2024-06-03 02:19:01 [INFO]: Epoch 022 - training loss: 0.5362, validation loss: 0.6597
2024-06-03 02:19:03 [INFO]: Epoch 023 - training loss: 0.5309, validation loss: 0.6990
2024-06-03 02:19:06 [INFO]: Epoch 024 - training loss: 0.5182, validation loss: 0.6775
2024-06-03 02:19:08 [INFO]: Epoch 025 - training loss: 0.5189, validation loss: 0.6685
2024-06-03 02:19:11 [INFO]: Epoch 026 - training loss: 0.5095, validation loss: 0.6516
2024-06-03 02:19:13 [INFO]: Epoch 027 - training loss: 0.5099, validation loss: 0.6590
2024-06-03 02:19:17 [INFO]: Epoch 028 - training loss: 0.5087, validation loss: 0.6589
2024-06-03 02:19:20 [INFO]: Epoch 029 - training loss: 0.5155, validation loss: 0.6600
2024-06-03 02:19:23 [INFO]: Epoch 030 - training loss: 0.5253, validation loss: 0.6933
2024-06-03 02:19:25 [INFO]: Epoch 031 - training loss: 0.5477, validation loss: 0.6670
2024-06-03 02:19:28 [INFO]: Epoch 032 - training loss: 0.5760, validation loss: 0.6948
2024-06-03 02:19:30 [INFO]: Epoch 033 - training loss: 0.5510, validation loss: 0.6651
2024-06-03 02:19:33 [INFO]: Epoch 034 - training loss: 0.5287, validation loss: 0.6709
2024-06-03 02:19:35 [INFO]: Epoch 035 - training loss: 0.5231, validation loss: 0.7268
2024-06-03 02:19:38 [INFO]: Epoch 036 - training loss: 0.5226, validation loss: 0.6649
2024-06-03 02:19:38 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:19:38 [INFO]: Finished training. The best model is from epoch#26.
2024-06-03 02:19:38 [INFO]: Saved the model to results_point_rate05/PeMS/StemGNN_PeMS/round_2/20240603_T021744/StemGNN.pypots
2024-06-03 02:19:40 [INFO]: Successfully saved to results_point_rate05/PeMS/StemGNN_PeMS/round_2/imputation.pkl
2024-06-03 02:19:40 [INFO]: Round2 - StemGNN on PeMS: MAE=0.4724, MSE=0.9575, MRE=0.5862
2024-06-03 02:19:40 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 02:19:40 [INFO]: Using the given device: cuda:0
2024-06-03 02:19:40 [INFO]: Model files will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_3/20240603_T021940
2024-06-03 02:19:40 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_3/20240603_T021940/tensorboard
2024-06-03 02:19:40 [INFO]: StemGNN initialized with the given hyperparameters, the number of trainable parameters: 2,386,294
2024-06-03 02:19:42 [INFO]: Epoch 001 - training loss: 1.2830, validation loss: 0.9694
2024-06-03 02:19:45 [INFO]: Epoch 002 - training loss: 0.9380, validation loss: 0.9077
2024-06-03 02:19:48 [INFO]: Epoch 003 - training loss: 0.8204, validation loss: 0.8665
2024-06-03 02:19:50 [INFO]: Epoch 004 - training loss: 0.7778, validation loss: 0.8543
2024-06-03 02:19:53 [INFO]: Epoch 005 - training loss: 0.7433, validation loss: 0.8471
2024-06-03 02:19:56 [INFO]: Epoch 006 - training loss: 0.6860, validation loss: 0.8038
2024-06-03 02:20:00 [INFO]: Epoch 007 - training loss: 0.6311, validation loss: 0.7516
2024-06-03 02:20:04 [INFO]: Epoch 008 - training loss: 0.5958, validation loss: 0.7281
2024-06-03 02:20:07 [INFO]: Epoch 009 - training loss: 0.5716, validation loss: 0.6994
2024-06-03 02:20:11 [INFO]: Epoch 010 - training loss: 0.5533, validation loss: 0.6873
2024-06-03 02:20:15 [INFO]: Epoch 011 - training loss: 0.5610, validation loss: 0.6880
2024-06-03 02:20:18 [INFO]: Epoch 012 - training loss: 0.5453, validation loss: 0.6760
2024-06-03 02:20:22 [INFO]: Epoch 013 - training loss: 0.5334, validation loss: 0.6679
2024-06-03 02:20:26 [INFO]: Epoch 014 - training loss: 0.5363, validation loss: 0.6647
2024-06-03 02:20:29 [INFO]: Epoch 015 - training loss: 0.5214, validation loss: 0.6603
2024-06-03 02:20:33 [INFO]: Epoch 016 - training loss: 0.5158, validation loss: 0.6533
2024-06-03 02:20:37 [INFO]: Epoch 017 - training loss: 0.5115, validation loss: 0.6547
2024-06-03 02:20:41 [INFO]: Epoch 018 - training loss: 0.5037, validation loss: 0.6404
2024-06-03 02:20:44 [INFO]: Epoch 019 - training loss: 0.5009, validation loss: 0.6599
2024-06-03 02:20:48 [INFO]: Epoch 020 - training loss: 0.5042, validation loss: 0.6402
2024-06-03 02:20:51 [INFO]: Epoch 021 - training loss: 0.4970, validation loss: 0.6293
2024-06-03 02:20:55 [INFO]: Epoch 022 - training loss: 0.4924, validation loss: 0.6401
2024-06-03 02:20:59 [INFO]: Epoch 023 - training loss: 0.4890, validation loss: 0.6312
2024-06-03 02:21:02 [INFO]: Epoch 024 - training loss: 0.4829, validation loss: 0.6217
2024-06-03 02:21:06 [INFO]: Epoch 025 - training loss: 0.4745, validation loss: 0.6115
2024-06-03 02:21:09 [INFO]: Epoch 026 - training loss: 0.4700, validation loss: 0.6164
2024-06-03 02:21:13 [INFO]: Epoch 027 - training loss: 0.4667, validation loss: 0.6042
2024-06-03 02:21:17 [INFO]: Epoch 028 - training loss: 0.4651, validation loss: 0.6309
2024-06-03 02:21:21 [INFO]: Epoch 029 - training loss: 0.4645, validation loss: 0.6196
2024-06-03 02:21:24 [INFO]: Epoch 030 - training loss: 0.4624, validation loss: 0.6121
2024-06-03 02:21:28 [INFO]: Epoch 031 - training loss: 0.4534, validation loss: 0.5940
2024-06-03 02:21:32 [INFO]: Epoch 032 - training loss: 0.4471, validation loss: 0.5936
2024-06-03 02:21:35 [INFO]: Epoch 033 - training loss: 0.4438, validation loss: 0.6054
2024-06-03 02:21:39 [INFO]: Epoch 034 - training loss: 0.4422, validation loss: 0.5973
2024-06-03 02:21:43 [INFO]: Epoch 035 - training loss: 0.4423, validation loss: 0.5858
2024-06-03 02:21:46 [INFO]: Epoch 036 - training loss: 0.4354, validation loss: 0.5833
2024-06-03 02:21:50 [INFO]: Epoch 037 - training loss: 0.4355, validation loss: 0.5859
2024-06-03 02:21:53 [INFO]: Epoch 038 - training loss: 0.4287, validation loss: 0.5814
2024-06-03 02:21:57 [INFO]: Epoch 039 - training loss: 0.4268, validation loss: 0.5774
2024-06-03 02:22:00 [INFO]: Epoch 040 - training loss: 0.4275, validation loss: 0.5855
2024-06-03 02:22:04 [INFO]: Epoch 041 - training loss: 0.4206, validation loss: 0.5799
2024-06-03 02:22:08 [INFO]: Epoch 042 - training loss: 0.4150, validation loss: 0.5714
2024-06-03 02:22:11 [INFO]: Epoch 043 - training loss: 0.4132, validation loss: 0.5858
2024-06-03 02:22:15 [INFO]: Epoch 044 - training loss: 0.4149, validation loss: 0.5783
2024-06-03 02:22:19 [INFO]: Epoch 045 - training loss: 0.4072, validation loss: 0.5793
2024-06-03 02:22:22 [INFO]: Epoch 046 - training loss: 0.4103, validation loss: 0.5635
2024-06-03 02:22:26 [INFO]: Epoch 047 - training loss: 0.4107, validation loss: 0.5693
2024-06-03 02:22:29 [INFO]: Epoch 048 - training loss: 0.4074, validation loss: 0.5717
2024-06-03 02:22:33 [INFO]: Epoch 049 - training loss: 0.4006, validation loss: 0.5756
2024-06-03 02:22:37 [INFO]: Epoch 050 - training loss: 0.4082, validation loss: 0.5745
2024-06-03 02:22:40 [INFO]: Epoch 051 - training loss: 0.4165, validation loss: 0.5700
2024-06-03 02:22:44 [INFO]: Epoch 052 - training loss: 0.4085, validation loss: 0.5791
2024-06-03 02:22:48 [INFO]: Epoch 053 - training loss: 0.4037, validation loss: 0.5674
2024-06-03 02:22:52 [INFO]: Epoch 054 - training loss: 0.4072, validation loss: 0.5635
2024-06-03 02:22:55 [INFO]: Epoch 055 - training loss: 0.4026, validation loss: 0.5723
2024-06-03 02:22:59 [INFO]: Epoch 056 - training loss: 0.4014, validation loss: 0.5816
2024-06-03 02:22:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:22:59 [INFO]: Finished training. The best model is from epoch#46.
2024-06-03 02:22:59 [INFO]: Saved the model to results_point_rate05/PeMS/StemGNN_PeMS/round_3/20240603_T021940/StemGNN.pypots
2024-06-03 02:23:01 [INFO]: Successfully saved to results_point_rate05/PeMS/StemGNN_PeMS/round_3/imputation.pkl
2024-06-03 02:23:01 [INFO]: Round3 - StemGNN on PeMS: MAE=0.4579, MSE=0.8367, MRE=0.5683
2024-06-03 02:23:01 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 02:23:01 [INFO]: Using the given device: cuda:0
2024-06-03 02:23:01 [INFO]: Model files will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_4/20240603_T022301
2024-06-03 02:23:01 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/StemGNN_PeMS/round_4/20240603_T022301/tensorboard
2024-06-03 02:23:01 [INFO]: StemGNN initialized with the given hyperparameters, the number of trainable parameters: 2,386,294
2024-06-03 02:23:04 [INFO]: Epoch 001 - training loss: 1.1687, validation loss: 0.9551
2024-06-03 02:23:08 [INFO]: Epoch 002 - training loss: 0.8893, validation loss: 0.8790
2024-06-03 02:23:12 [INFO]: Epoch 003 - training loss: 0.7724, validation loss: 0.8398
2024-06-03 02:23:16 [INFO]: Epoch 004 - training loss: 0.6752, validation loss: 0.7711
2024-06-03 02:23:19 [INFO]: Epoch 005 - training loss: 0.6033, validation loss: 0.7399
2024-06-03 02:23:23 [INFO]: Epoch 006 - training loss: 0.5751, validation loss: 0.7274
2024-06-03 02:23:26 [INFO]: Epoch 007 - training loss: 0.5578, validation loss: 0.7045
2024-06-03 02:23:30 [INFO]: Epoch 008 - training loss: 0.5433, validation loss: 0.6788
2024-06-03 02:23:33 [INFO]: Epoch 009 - training loss: 0.5269, validation loss: 0.6510
2024-06-03 02:23:36 [INFO]: Epoch 010 - training loss: 0.5169, validation loss: 0.6474
2024-06-03 02:23:39 [INFO]: Epoch 011 - training loss: 0.5064, validation loss: 0.6366
2024-06-03 02:23:43 [INFO]: Epoch 012 - training loss: 0.5067, validation loss: 0.6532
2024-06-03 02:23:46 [INFO]: Epoch 013 - training loss: 0.4910, validation loss: 0.6332
2024-06-03 02:23:49 [INFO]: Epoch 014 - training loss: 0.4834, validation loss: 0.6317
2024-06-03 02:23:52 [INFO]: Epoch 015 - training loss: 0.4759, validation loss: 0.6229
2024-06-03 02:23:55 [INFO]: Epoch 016 - training loss: 0.4698, validation loss: 0.6117
2024-06-03 02:23:58 [INFO]: Epoch 017 - training loss: 0.4576, validation loss: 0.6253
2024-06-03 02:24:01 [INFO]: Epoch 018 - training loss: 0.4505, validation loss: 0.6194
2024-06-03 02:24:04 [INFO]: Epoch 019 - training loss: 0.4483, validation loss: 0.6077
2024-06-03 02:24:07 [INFO]: Epoch 020 - training loss: 0.4460, validation loss: 0.5943
2024-06-03 02:24:10 [INFO]: Epoch 021 - training loss: 0.4451, validation loss: 0.5974
2024-06-03 02:24:14 [INFO]: Epoch 022 - training loss: 0.4355, validation loss: 0.6085
2024-06-03 02:24:17 [INFO]: Epoch 023 - training loss: 0.4338, validation loss: 0.5948
2024-06-03 02:24:20 [INFO]: Epoch 024 - training loss: 0.4243, validation loss: 0.6007
2024-06-03 02:24:23 [INFO]: Epoch 025 - training loss: 0.4193, validation loss: 0.5978
2024-06-03 02:24:26 [INFO]: Epoch 026 - training loss: 0.4192, validation loss: 0.5934
2024-06-03 02:24:29 [INFO]: Epoch 027 - training loss: 0.4145, validation loss: 0.5897
2024-06-03 02:24:32 [INFO]: Epoch 028 - training loss: 0.4138, validation loss: 0.5958
2024-06-03 02:24:35 [INFO]: Epoch 029 - training loss: 0.4143, validation loss: 0.5801
2024-06-03 02:24:38 [INFO]: Epoch 030 - training loss: 0.4134, validation loss: 0.5834
2024-06-03 02:24:42 [INFO]: Epoch 031 - training loss: 0.4085, validation loss: 0.5818
2024-06-03 02:24:45 [INFO]: Epoch 032 - training loss: 0.4010, validation loss: 0.5845
2024-06-03 02:24:48 [INFO]: Epoch 033 - training loss: 0.3997, validation loss: 0.5788
2024-06-03 02:24:51 [INFO]: Epoch 034 - training loss: 0.4039, validation loss: 0.5887
2024-06-03 02:24:54 [INFO]: Epoch 035 - training loss: 0.3935, validation loss: 0.5836
2024-06-03 02:24:57 [INFO]: Epoch 036 - training loss: 0.3962, validation loss: 0.5888
2024-06-03 02:25:00 [INFO]: Epoch 037 - training loss: 0.3906, validation loss: 0.5912
2024-06-03 02:25:03 [INFO]: Epoch 038 - training loss: 0.3888, validation loss: 0.5845
2024-06-03 02:25:07 [INFO]: Epoch 039 - training loss: 0.3890, validation loss: 0.5827
2024-06-03 02:25:10 [INFO]: Epoch 040 - training loss: 0.3856, validation loss: 0.5760
2024-06-03 02:25:12 [INFO]: Epoch 041 - training loss: 0.3808, validation loss: 0.5750
2024-06-03 02:25:15 [INFO]: Epoch 042 - training loss: 0.3835, validation loss: 0.5835
2024-06-03 02:25:19 [INFO]: Epoch 043 - training loss: 0.3801, validation loss: 0.5649
2024-06-03 02:25:22 [INFO]: Epoch 044 - training loss: 0.3762, validation loss: 0.5663
2024-06-03 02:25:25 [INFO]: Epoch 045 - training loss: 0.3758, validation loss: 0.5718
2024-06-03 02:25:28 [INFO]: Epoch 046 - training loss: 0.3706, validation loss: 0.5831
2024-06-03 02:25:31 [INFO]: Epoch 047 - training loss: 0.3699, validation loss: 0.5699
2024-06-03 02:25:34 [INFO]: Epoch 048 - training loss: 0.3730, validation loss: 0.5678
2024-06-03 02:25:37 [INFO]: Epoch 049 - training loss: 0.3742, validation loss: 0.5691
2024-06-03 02:25:40 [INFO]: Epoch 050 - training loss: 0.3680, validation loss: 0.5698
2024-06-03 02:25:44 [INFO]: Epoch 051 - training loss: 0.3624, validation loss: 0.5714
2024-06-03 02:25:47 [INFO]: Epoch 052 - training loss: 0.3653, validation loss: 0.5623
2024-06-03 02:25:50 [INFO]: Epoch 053 - training loss: 0.3695, validation loss: 0.5608
2024-06-03 02:25:53 [INFO]: Epoch 054 - training loss: 0.3648, validation loss: 0.5690
2024-06-03 02:25:56 [INFO]: Epoch 055 - training loss: 0.3674, validation loss: 0.5576
2024-06-03 02:25:59 [INFO]: Epoch 056 - training loss: 0.3618, validation loss: 0.5600
2024-06-03 02:26:02 [INFO]: Epoch 057 - training loss: 0.3592, validation loss: 0.5604
2024-06-03 02:26:06 [INFO]: Epoch 058 - training loss: 0.3614, validation loss: 0.5500
2024-06-03 02:26:08 [INFO]: Epoch 059 - training loss: 0.3620, validation loss: 0.5620
2024-06-03 02:26:12 [INFO]: Epoch 060 - training loss: 0.3557, validation loss: 0.5597
2024-06-03 02:26:15 [INFO]: Epoch 061 - training loss: 0.3542, validation loss: 0.5579
2024-06-03 02:26:18 [INFO]: Epoch 062 - training loss: 0.3544, validation loss: 0.5580
2024-06-03 02:26:21 [INFO]: Epoch 063 - training loss: 0.3559, validation loss: 0.5555
2024-06-03 02:26:24 [INFO]: Epoch 064 - training loss: 0.3524, validation loss: 0.5730
2024-06-03 02:26:27 [INFO]: Epoch 065 - training loss: 0.3513, validation loss: 0.5471
2024-06-03 02:26:30 [INFO]: Epoch 066 - training loss: 0.3539, validation loss: 0.5705
2024-06-03 02:26:34 [INFO]: Epoch 067 - training loss: 0.3501, validation loss: 0.5509
2024-06-03 02:26:37 [INFO]: Epoch 068 - training loss: 0.3435, validation loss: 0.5592
2024-06-03 02:26:40 [INFO]: Epoch 069 - training loss: 0.3463, validation loss: 0.5569
2024-06-03 02:26:43 [INFO]: Epoch 070 - training loss: 0.3439, validation loss: 0.5522
2024-06-03 02:26:46 [INFO]: Epoch 071 - training loss: 0.3439, validation loss: 0.5574
2024-06-03 02:26:49 [INFO]: Epoch 072 - training loss: 0.3449, validation loss: 0.5559
2024-06-03 02:26:52 [INFO]: Epoch 073 - training loss: 0.3448, validation loss: 0.5487
2024-06-03 02:26:55 [INFO]: Epoch 074 - training loss: 0.3420, validation loss: 0.5572
2024-06-03 02:26:58 [INFO]: Epoch 075 - training loss: 0.3383, validation loss: 0.5753
2024-06-03 02:26:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:26:58 [INFO]: Finished training. The best model is from epoch#65.
2024-06-03 02:26:58 [INFO]: Saved the model to results_point_rate05/PeMS/StemGNN_PeMS/round_4/20240603_T022301/StemGNN.pypots
2024-06-03 02:27:00 [INFO]: Successfully saved to results_point_rate05/PeMS/StemGNN_PeMS/round_4/imputation.pkl
2024-06-03 02:27:00 [INFO]: Round4 - StemGNN on PeMS: MAE=0.4341, MSE=0.8452, MRE=0.5387
2024-06-03 02:27:00 [INFO]: Done! Final results:
Averaged StemGNN (2,386,294 params) on PeMS: MAE=0.4457 ± 0.021071859376449516, MSE=0.8616 ± 0.06420328862334465, MRE=0.5530 ± 0.026148390054357967, average inference time=0.27
