2024-06-03 00:45:08 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 00:45:08 [INFO]: Using the given device: cuda:0
2024-06-03 00:45:08 [INFO]: Model files will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_0/20240603_T004508
2024-06-03 00:45:08 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_0/20240603_T004508/tensorboard
2024-06-03 00:45:08 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 00:45:08 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 00:45:10 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 00:45:24 [INFO]: Epoch 001 - training loss: 1.0052, validation loss: 0.7302
2024-06-03 00:45:31 [INFO]: Epoch 002 - training loss: 0.6121, validation loss: 0.5802
2024-06-03 00:45:38 [INFO]: Epoch 003 - training loss: 0.5221, validation loss: 0.5227
2024-06-03 00:45:45 [INFO]: Epoch 004 - training loss: 0.4897, validation loss: 0.5162
2024-06-03 00:45:52 [INFO]: Epoch 005 - training loss: 0.4574, validation loss: 0.4989
2024-06-03 00:45:59 [INFO]: Epoch 006 - training loss: 0.4380, validation loss: 0.4949
2024-06-03 00:46:06 [INFO]: Epoch 007 - training loss: 0.4172, validation loss: 0.4946
2024-06-03 00:46:13 [INFO]: Epoch 008 - training loss: 0.3947, validation loss: 0.4941
2024-06-03 00:46:20 [INFO]: Epoch 009 - training loss: 0.3856, validation loss: 0.4739
2024-06-03 00:46:27 [INFO]: Epoch 010 - training loss: 0.3700, validation loss: 0.4678
2024-06-03 00:46:34 [INFO]: Epoch 011 - training loss: 0.3743, validation loss: 0.4709
2024-06-03 00:46:40 [INFO]: Epoch 012 - training loss: 0.3560, validation loss: 0.4592
2024-06-03 00:46:47 [INFO]: Epoch 013 - training loss: 0.3488, validation loss: 0.4580
2024-06-03 00:46:54 [INFO]: Epoch 014 - training loss: 0.3463, validation loss: 0.4455
2024-06-03 00:47:01 [INFO]: Epoch 015 - training loss: 0.3408, validation loss: 0.4397
2024-06-03 00:47:08 [INFO]: Epoch 016 - training loss: 0.3334, validation loss: 0.4369
2024-06-03 00:47:15 [INFO]: Epoch 017 - training loss: 0.3363, validation loss: 0.4340
2024-06-03 00:47:22 [INFO]: Epoch 018 - training loss: 0.3290, validation loss: 0.4406
2024-06-03 00:47:29 [INFO]: Epoch 019 - training loss: 0.3211, validation loss: 0.4348
2024-06-03 00:47:36 [INFO]: Epoch 020 - training loss: 0.3136, validation loss: 0.4280
2024-06-03 00:47:43 [INFO]: Epoch 021 - training loss: 0.3048, validation loss: 0.4284
2024-06-03 00:47:50 [INFO]: Epoch 022 - training loss: 0.3050, validation loss: 0.4233
2024-06-03 00:47:57 [INFO]: Epoch 023 - training loss: 0.2955, validation loss: 0.4211
2024-06-03 00:48:03 [INFO]: Epoch 024 - training loss: 0.2934, validation loss: 0.4112
2024-06-03 00:48:10 [INFO]: Epoch 025 - training loss: 0.2922, validation loss: 0.4149
2024-06-03 00:48:17 [INFO]: Epoch 026 - training loss: 0.2918, validation loss: 0.4151
2024-06-03 00:48:24 [INFO]: Epoch 027 - training loss: 0.2967, validation loss: 0.4028
2024-06-03 00:48:31 [INFO]: Epoch 028 - training loss: 0.2839, validation loss: 0.4225
2024-06-03 00:48:38 [INFO]: Epoch 029 - training loss: 0.2931, validation loss: 0.4109
2024-06-03 00:48:45 [INFO]: Epoch 030 - training loss: 0.2852, validation loss: 0.4044
2024-06-03 00:48:52 [INFO]: Epoch 031 - training loss: 0.2807, validation loss: 0.4028
2024-06-03 00:48:59 [INFO]: Epoch 032 - training loss: 0.2808, validation loss: 0.3918
2024-06-03 00:49:06 [INFO]: Epoch 033 - training loss: 0.2721, validation loss: 0.3965
2024-06-03 00:49:14 [INFO]: Epoch 034 - training loss: 0.2722, validation loss: 0.3912
2024-06-03 00:49:20 [INFO]: Epoch 035 - training loss: 0.2836, validation loss: 0.4028
2024-06-03 00:49:27 [INFO]: Epoch 036 - training loss: 0.2805, validation loss: 0.3964
2024-06-03 00:49:34 [INFO]: Epoch 037 - training loss: 0.2705, validation loss: 0.3921
2024-06-03 00:49:41 [INFO]: Epoch 038 - training loss: 0.2686, validation loss: 0.3943
2024-06-03 00:49:48 [INFO]: Epoch 039 - training loss: 0.2672, validation loss: 0.3920
2024-06-03 00:49:55 [INFO]: Epoch 040 - training loss: 0.2762, validation loss: 0.3904
2024-06-03 00:50:02 [INFO]: Epoch 041 - training loss: 0.2702, validation loss: 0.3895
2024-06-03 00:50:09 [INFO]: Epoch 042 - training loss: 0.2639, validation loss: 0.3904
2024-06-03 00:50:16 [INFO]: Epoch 043 - training loss: 0.2592, validation loss: 0.3928
2024-06-03 00:50:23 [INFO]: Epoch 044 - training loss: 0.2569, validation loss: 0.3843
2024-06-03 00:50:30 [INFO]: Epoch 045 - training loss: 0.2642, validation loss: 0.3819
2024-06-03 00:50:37 [INFO]: Epoch 046 - training loss: 0.2558, validation loss: 0.3828
2024-06-03 00:50:44 [INFO]: Epoch 047 - training loss: 0.2494, validation loss: 0.3864
2024-06-03 00:50:51 [INFO]: Epoch 048 - training loss: 0.2585, validation loss: 0.3882
2024-06-03 00:50:58 [INFO]: Epoch 049 - training loss: 0.2593, validation loss: 0.3795
2024-06-03 00:51:05 [INFO]: Epoch 050 - training loss: 0.2565, validation loss: 0.3816
2024-06-03 00:51:12 [INFO]: Epoch 051 - training loss: 0.2510, validation loss: 0.3839
2024-06-03 00:51:19 [INFO]: Epoch 052 - training loss: 0.2472, validation loss: 0.3763
2024-06-03 00:51:26 [INFO]: Epoch 053 - training loss: 0.2517, validation loss: 0.3710
2024-06-03 00:51:33 [INFO]: Epoch 054 - training loss: 0.2482, validation loss: 0.3758
2024-06-03 00:51:40 [INFO]: Epoch 055 - training loss: 0.2440, validation loss: 0.3773
2024-06-03 00:51:47 [INFO]: Epoch 056 - training loss: 0.2627, validation loss: 0.3728
2024-06-03 00:51:54 [INFO]: Epoch 057 - training loss: 0.2520, validation loss: 0.3707
2024-06-03 00:52:01 [INFO]: Epoch 058 - training loss: 0.2468, validation loss: 0.3708
2024-06-03 00:52:07 [INFO]: Epoch 059 - training loss: 0.2499, validation loss: 0.3627
2024-06-03 00:52:13 [INFO]: Epoch 060 - training loss: 0.2513, validation loss: 0.3751
2024-06-03 00:52:20 [INFO]: Epoch 061 - training loss: 0.2466, validation loss: 0.3615
2024-06-03 00:52:26 [INFO]: Epoch 062 - training loss: 0.2453, validation loss: 0.3632
2024-06-03 00:52:32 [INFO]: Epoch 063 - training loss: 0.2413, validation loss: 0.3667
2024-06-03 00:52:38 [INFO]: Epoch 064 - training loss: 0.2429, validation loss: 0.3664
2024-06-03 00:52:45 [INFO]: Epoch 065 - training loss: 0.2429, validation loss: 0.3641
2024-06-03 00:52:51 [INFO]: Epoch 066 - training loss: 0.2390, validation loss: 0.3631
2024-06-03 00:52:57 [INFO]: Epoch 067 - training loss: 0.2457, validation loss: 0.3688
2024-06-03 00:53:04 [INFO]: Epoch 068 - training loss: 0.2403, validation loss: 0.3618
2024-06-03 00:53:10 [INFO]: Epoch 069 - training loss: 0.2416, validation loss: 0.3616
2024-06-03 00:53:16 [INFO]: Epoch 070 - training loss: 0.2454, validation loss: 0.3605
2024-06-03 00:53:23 [INFO]: Epoch 071 - training loss: 0.2385, validation loss: 0.3619
2024-06-03 00:53:29 [INFO]: Epoch 072 - training loss: 0.2355, validation loss: 0.3621
2024-06-03 00:53:35 [INFO]: Epoch 073 - training loss: 0.2357, validation loss: 0.3550
2024-06-03 00:53:42 [INFO]: Epoch 074 - training loss: 0.2320, validation loss: 0.3656
2024-06-03 00:53:48 [INFO]: Epoch 075 - training loss: 0.2303, validation loss: 0.3564
2024-06-03 00:53:54 [INFO]: Epoch 076 - training loss: 0.2350, validation loss: 0.3575
2024-06-03 00:54:01 [INFO]: Epoch 077 - training loss: 0.2369, validation loss: 0.3585
2024-06-03 00:54:07 [INFO]: Epoch 078 - training loss: 0.2363, validation loss: 0.3561
2024-06-03 00:54:14 [INFO]: Epoch 079 - training loss: 0.2327, validation loss: 0.3720
2024-06-03 00:54:20 [INFO]: Epoch 080 - training loss: 0.2339, validation loss: 0.3566
2024-06-03 00:54:26 [INFO]: Epoch 081 - training loss: 0.2351, validation loss: 0.3512
2024-06-03 00:54:33 [INFO]: Epoch 082 - training loss: 0.2334, validation loss: 0.3555
2024-06-03 00:54:39 [INFO]: Epoch 083 - training loss: 0.2338, validation loss: 0.3522
2024-06-03 00:54:45 [INFO]: Epoch 084 - training loss: 0.2328, validation loss: 0.3570
2024-06-03 00:54:52 [INFO]: Epoch 085 - training loss: 0.2322, validation loss: 0.3526
2024-06-03 00:54:58 [INFO]: Epoch 086 - training loss: 0.2314, validation loss: 0.3549
2024-06-03 00:55:05 [INFO]: Epoch 087 - training loss: 0.2318, validation loss: 0.3584
2024-06-03 00:55:11 [INFO]: Epoch 088 - training loss: 0.2300, validation loss: 0.3588
2024-06-03 00:55:18 [INFO]: Epoch 089 - training loss: 0.2362, validation loss: 0.3590
2024-06-03 00:55:24 [INFO]: Epoch 090 - training loss: 0.2395, validation loss: 0.3588
2024-06-03 00:55:31 [INFO]: Epoch 091 - training loss: 0.2328, validation loss: 0.3577
2024-06-03 00:55:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:55:31 [INFO]: Finished training. The best model is from epoch#81.
2024-06-03 00:55:31 [INFO]: Saved the model to results_point_rate05/PeMS/iTransformer_PeMS/round_0/20240603_T004508/iTransformer.pypots
2024-06-03 00:55:34 [INFO]: Successfully saved to results_point_rate05/PeMS/iTransformer_PeMS/round_0/imputation.pkl
2024-06-03 00:55:34 [INFO]: Round0 - iTransformer on PeMS: MAE=0.2848, MSE=0.5134, MRE=0.3534
2024-06-03 00:55:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 00:55:34 [INFO]: Using the given device: cuda:0
2024-06-03 00:55:34 [INFO]: Model files will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_1/20240603_T005534
2024-06-03 00:55:34 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_1/20240603_T005534/tensorboard
2024-06-03 00:55:34 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 00:55:34 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 00:55:34 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 00:55:40 [INFO]: Epoch 001 - training loss: 1.0208, validation loss: 0.6827
2024-06-03 00:55:47 [INFO]: Epoch 002 - training loss: 0.6147, validation loss: 0.5949
2024-06-03 00:55:53 [INFO]: Epoch 003 - training loss: 0.5231, validation loss: 0.5371
2024-06-03 00:55:59 [INFO]: Epoch 004 - training loss: 0.4814, validation loss: 0.5231
2024-06-03 00:56:05 [INFO]: Epoch 005 - training loss: 0.4530, validation loss: 0.5149
2024-06-03 00:56:12 [INFO]: Epoch 006 - training loss: 0.4319, validation loss: 0.5077
2024-06-03 00:56:18 [INFO]: Epoch 007 - training loss: 0.4096, validation loss: 0.5087
2024-06-03 00:56:24 [INFO]: Epoch 008 - training loss: 0.3937, validation loss: 0.4840
2024-06-03 00:56:31 [INFO]: Epoch 009 - training loss: 0.3873, validation loss: 0.4948
2024-06-03 00:56:37 [INFO]: Epoch 010 - training loss: 0.3713, validation loss: 0.4946
2024-06-03 00:56:44 [INFO]: Epoch 011 - training loss: 0.3695, validation loss: 0.4680
2024-06-03 00:56:50 [INFO]: Epoch 012 - training loss: 0.3563, validation loss: 0.4618
2024-06-03 00:56:57 [INFO]: Epoch 013 - training loss: 0.3520, validation loss: 0.4522
2024-06-03 00:57:03 [INFO]: Epoch 014 - training loss: 0.3406, validation loss: 0.4572
2024-06-03 00:57:10 [INFO]: Epoch 015 - training loss: 0.3332, validation loss: 0.4482
2024-06-03 00:57:16 [INFO]: Epoch 016 - training loss: 0.3292, validation loss: 0.4473
2024-06-03 00:57:23 [INFO]: Epoch 017 - training loss: 0.3161, validation loss: 0.4394
2024-06-03 00:57:29 [INFO]: Epoch 018 - training loss: 0.3181, validation loss: 0.4305
2024-06-03 00:57:35 [INFO]: Epoch 019 - training loss: 0.3288, validation loss: 0.4315
2024-06-03 00:57:42 [INFO]: Epoch 020 - training loss: 0.3117, validation loss: 0.4392
2024-06-03 00:57:48 [INFO]: Epoch 021 - training loss: 0.3068, validation loss: 0.4283
2024-06-03 00:57:55 [INFO]: Epoch 022 - training loss: 0.3071, validation loss: 0.4258
2024-06-03 00:58:01 [INFO]: Epoch 023 - training loss: 0.3030, validation loss: 0.4256
2024-06-03 00:58:08 [INFO]: Epoch 024 - training loss: 0.2987, validation loss: 0.4168
2024-06-03 00:58:14 [INFO]: Epoch 025 - training loss: 0.3062, validation loss: 0.4294
2024-06-03 00:58:20 [INFO]: Epoch 026 - training loss: 0.2967, validation loss: 0.4174
2024-06-03 00:58:27 [INFO]: Epoch 027 - training loss: 0.2880, validation loss: 0.4132
2024-06-03 00:58:33 [INFO]: Epoch 028 - training loss: 0.2813, validation loss: 0.4086
2024-06-03 00:58:39 [INFO]: Epoch 029 - training loss: 0.2831, validation loss: 0.4222
2024-06-03 00:58:46 [INFO]: Epoch 030 - training loss: 0.2883, validation loss: 0.3994
2024-06-03 00:58:52 [INFO]: Epoch 031 - training loss: 0.2807, validation loss: 0.4075
2024-06-03 00:58:59 [INFO]: Epoch 032 - training loss: 0.2800, validation loss: 0.4053
2024-06-03 00:59:05 [INFO]: Epoch 033 - training loss: 0.2817, validation loss: 0.4012
2024-06-03 00:59:11 [INFO]: Epoch 034 - training loss: 0.2781, validation loss: 0.3989
2024-06-03 00:59:18 [INFO]: Epoch 035 - training loss: 0.2722, validation loss: 0.4023
2024-06-03 00:59:24 [INFO]: Epoch 036 - training loss: 0.2677, validation loss: 0.3917
2024-06-03 00:59:30 [INFO]: Epoch 037 - training loss: 0.2659, validation loss: 0.3973
2024-06-03 00:59:37 [INFO]: Epoch 038 - training loss: 0.2675, validation loss: 0.3944
2024-06-03 00:59:43 [INFO]: Epoch 039 - training loss: 0.2601, validation loss: 0.3997
2024-06-03 00:59:49 [INFO]: Epoch 040 - training loss: 0.2733, validation loss: 0.3932
2024-06-03 00:59:56 [INFO]: Epoch 041 - training loss: 0.2691, validation loss: 0.4099
2024-06-03 01:00:02 [INFO]: Epoch 042 - training loss: 0.2757, validation loss: 0.3921
2024-06-03 01:00:08 [INFO]: Epoch 043 - training loss: 0.2699, validation loss: 0.3936
2024-06-03 01:00:15 [INFO]: Epoch 044 - training loss: 0.2725, validation loss: 0.3864
2024-06-03 01:00:21 [INFO]: Epoch 045 - training loss: 0.2645, validation loss: 0.3925
2024-06-03 01:00:27 [INFO]: Epoch 046 - training loss: 0.2567, validation loss: 0.3829
2024-06-03 01:00:34 [INFO]: Epoch 047 - training loss: 0.2544, validation loss: 0.3836
2024-06-03 01:00:40 [INFO]: Epoch 048 - training loss: 0.2529, validation loss: 0.3874
2024-06-03 01:00:46 [INFO]: Epoch 049 - training loss: 0.2530, validation loss: 0.3860
2024-06-03 01:00:53 [INFO]: Epoch 050 - training loss: 0.2571, validation loss: 0.3838
2024-06-03 01:00:59 [INFO]: Epoch 051 - training loss: 0.2576, validation loss: 0.3807
2024-06-03 01:01:06 [INFO]: Epoch 052 - training loss: 0.2521, validation loss: 0.3770
2024-06-03 01:01:12 [INFO]: Epoch 053 - training loss: 0.2472, validation loss: 0.3785
2024-06-03 01:01:19 [INFO]: Epoch 054 - training loss: 0.2471, validation loss: 0.3764
2024-06-03 01:01:25 [INFO]: Epoch 055 - training loss: 0.2479, validation loss: 0.3726
2024-06-03 01:01:32 [INFO]: Epoch 056 - training loss: 0.2505, validation loss: 0.3790
2024-06-03 01:01:38 [INFO]: Epoch 057 - training loss: 0.2490, validation loss: 0.3720
2024-06-03 01:01:44 [INFO]: Epoch 058 - training loss: 0.2500, validation loss: 0.3705
2024-06-03 01:01:51 [INFO]: Epoch 059 - training loss: 0.2443, validation loss: 0.3742
2024-06-03 01:01:57 [INFO]: Epoch 060 - training loss: 0.2512, validation loss: 0.3752
2024-06-03 01:02:04 [INFO]: Epoch 061 - training loss: 0.2534, validation loss: 0.3727
2024-06-03 01:02:10 [INFO]: Epoch 062 - training loss: 0.2460, validation loss: 0.3792
2024-06-03 01:02:17 [INFO]: Epoch 063 - training loss: 0.2472, validation loss: 0.3769
2024-06-03 01:02:23 [INFO]: Epoch 064 - training loss: 0.2576, validation loss: 0.3727
2024-06-03 01:02:30 [INFO]: Epoch 065 - training loss: 0.2440, validation loss: 0.3809
2024-06-03 01:02:36 [INFO]: Epoch 066 - training loss: 0.2420, validation loss: 0.3702
2024-06-03 01:02:42 [INFO]: Epoch 067 - training loss: 0.2416, validation loss: 0.3731
2024-06-03 01:02:48 [INFO]: Epoch 068 - training loss: 0.2446, validation loss: 0.3697
2024-06-03 01:02:54 [INFO]: Epoch 069 - training loss: 0.2445, validation loss: 0.3711
2024-06-03 01:03:01 [INFO]: Epoch 070 - training loss: 0.2465, validation loss: 0.3727
2024-06-03 01:03:07 [INFO]: Epoch 071 - training loss: 0.2459, validation loss: 0.3698
2024-06-03 01:03:13 [INFO]: Epoch 072 - training loss: 0.2564, validation loss: 0.3742
2024-06-03 01:03:20 [INFO]: Epoch 073 - training loss: 0.2507, validation loss: 0.3701
2024-06-03 01:03:26 [INFO]: Epoch 074 - training loss: 0.2394, validation loss: 0.3685
2024-06-03 01:03:32 [INFO]: Epoch 075 - training loss: 0.2405, validation loss: 0.3659
2024-06-03 01:03:39 [INFO]: Epoch 076 - training loss: 0.2418, validation loss: 0.3660
2024-06-03 01:03:45 [INFO]: Epoch 077 - training loss: 0.2365, validation loss: 0.3627
2024-06-03 01:03:52 [INFO]: Epoch 078 - training loss: 0.2359, validation loss: 0.3668
2024-06-03 01:03:58 [INFO]: Epoch 079 - training loss: 0.2313, validation loss: 0.3624
2024-06-03 01:04:04 [INFO]: Epoch 080 - training loss: 0.2390, validation loss: 0.3717
2024-06-03 01:04:11 [INFO]: Epoch 081 - training loss: 0.2419, validation loss: 0.3628
2024-06-03 01:04:17 [INFO]: Epoch 082 - training loss: 0.2371, validation loss: 0.3576
2024-06-03 01:04:23 [INFO]: Epoch 083 - training loss: 0.2368, validation loss: 0.3694
2024-06-03 01:04:30 [INFO]: Epoch 084 - training loss: 0.2325, validation loss: 0.3587
2024-06-03 01:04:36 [INFO]: Epoch 085 - training loss: 0.2290, validation loss: 0.3572
2024-06-03 01:04:42 [INFO]: Epoch 086 - training loss: 0.2325, validation loss: 0.3623
2024-06-03 01:04:49 [INFO]: Epoch 087 - training loss: 0.2328, validation loss: 0.3705
2024-06-03 01:04:55 [INFO]: Epoch 088 - training loss: 0.2325, validation loss: 0.3623
2024-06-03 01:05:01 [INFO]: Epoch 089 - training loss: 0.2336, validation loss: 0.3602
2024-06-03 01:05:08 [INFO]: Epoch 090 - training loss: 0.2271, validation loss: 0.3679
2024-06-03 01:05:14 [INFO]: Epoch 091 - training loss: 0.2350, validation loss: 0.3607
2024-06-03 01:05:21 [INFO]: Epoch 092 - training loss: 0.2314, validation loss: 0.3613
2024-06-03 01:05:27 [INFO]: Epoch 093 - training loss: 0.2304, validation loss: 0.3677
2024-06-03 01:05:33 [INFO]: Epoch 094 - training loss: 0.2257, validation loss: 0.3637
2024-06-03 01:05:40 [INFO]: Epoch 095 - training loss: 0.2358, validation loss: 0.3713
2024-06-03 01:05:40 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 01:05:40 [INFO]: Finished training. The best model is from epoch#85.
2024-06-03 01:05:40 [INFO]: Saved the model to results_point_rate05/PeMS/iTransformer_PeMS/round_1/20240603_T005534/iTransformer.pypots
2024-06-03 01:05:43 [INFO]: Successfully saved to results_point_rate05/PeMS/iTransformer_PeMS/round_1/imputation.pkl
2024-06-03 01:05:43 [INFO]: Round1 - iTransformer on PeMS: MAE=0.3056, MSE=0.5578, MRE=0.3792
2024-06-03 01:05:43 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 01:05:43 [INFO]: Using the given device: cuda:0
2024-06-03 01:05:43 [INFO]: Model files will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_2/20240603_T010543
2024-06-03 01:05:43 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_2/20240603_T010543/tensorboard
2024-06-03 01:05:43 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 01:05:43 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 01:05:43 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 01:05:49 [INFO]: Epoch 001 - training loss: 1.0327, validation loss: 0.7423
2024-06-03 01:05:56 [INFO]: Epoch 002 - training loss: 0.6200, validation loss: 0.6002
2024-06-03 01:06:02 [INFO]: Epoch 003 - training loss: 0.5321, validation loss: 0.5414
2024-06-03 01:06:09 [INFO]: Epoch 004 - training loss: 0.4844, validation loss: 0.5082
2024-06-03 01:06:15 [INFO]: Epoch 005 - training loss: 0.4561, validation loss: 0.5014
2024-06-03 01:06:22 [INFO]: Epoch 006 - training loss: 0.4399, validation loss: 0.5022
2024-06-03 01:06:28 [INFO]: Epoch 007 - training loss: 0.4214, validation loss: 0.4971
2024-06-03 01:06:34 [INFO]: Epoch 008 - training loss: 0.4044, validation loss: 0.4954
2024-06-03 01:06:41 [INFO]: Epoch 009 - training loss: 0.3872, validation loss: 0.4734
2024-06-03 01:06:47 [INFO]: Epoch 010 - training loss: 0.3778, validation loss: 0.4747
2024-06-03 01:06:54 [INFO]: Epoch 011 - training loss: 0.3720, validation loss: 0.4685
2024-06-03 01:07:00 [INFO]: Epoch 012 - training loss: 0.3619, validation loss: 0.4731
2024-06-03 01:07:06 [INFO]: Epoch 013 - training loss: 0.3484, validation loss: 0.4755
2024-06-03 01:07:13 [INFO]: Epoch 014 - training loss: 0.3559, validation loss: 0.4512
2024-06-03 01:07:19 [INFO]: Epoch 015 - training loss: 0.3406, validation loss: 0.4571
2024-06-03 01:07:25 [INFO]: Epoch 016 - training loss: 0.3283, validation loss: 0.4375
2024-06-03 01:07:32 [INFO]: Epoch 017 - training loss: 0.3204, validation loss: 0.4393
2024-06-03 01:07:38 [INFO]: Epoch 018 - training loss: 0.3191, validation loss: 0.4457
2024-06-03 01:07:44 [INFO]: Epoch 019 - training loss: 0.3344, validation loss: 0.4340
2024-06-03 01:07:51 [INFO]: Epoch 020 - training loss: 0.3112, validation loss: 0.4230
2024-06-03 01:07:57 [INFO]: Epoch 021 - training loss: 0.3044, validation loss: 0.4231
2024-06-03 01:08:04 [INFO]: Epoch 022 - training loss: 0.3055, validation loss: 0.4256
2024-06-03 01:08:10 [INFO]: Epoch 023 - training loss: 0.2969, validation loss: 0.4201
2024-06-03 01:08:16 [INFO]: Epoch 024 - training loss: 0.2940, validation loss: 0.4236
2024-06-03 01:08:23 [INFO]: Epoch 025 - training loss: 0.2940, validation loss: 0.4140
2024-06-03 01:08:29 [INFO]: Epoch 026 - training loss: 0.2913, validation loss: 0.4109
2024-06-03 01:08:35 [INFO]: Epoch 027 - training loss: 0.2893, validation loss: 0.4166
2024-06-03 01:08:42 [INFO]: Epoch 028 - training loss: 0.2881, validation loss: 0.4082
2024-06-03 01:08:48 [INFO]: Epoch 029 - training loss: 0.2866, validation loss: 0.4019
2024-06-03 01:08:54 [INFO]: Epoch 030 - training loss: 0.2836, validation loss: 0.3961
2024-06-03 01:09:00 [INFO]: Epoch 031 - training loss: 0.2810, validation loss: 0.3969
2024-06-03 01:09:07 [INFO]: Epoch 032 - training loss: 0.2791, validation loss: 0.3990
2024-06-03 01:09:13 [INFO]: Epoch 033 - training loss: 0.2756, validation loss: 0.3960
2024-06-03 01:09:20 [INFO]: Epoch 034 - training loss: 0.2749, validation loss: 0.3947
2024-06-03 01:09:26 [INFO]: Epoch 035 - training loss: 0.2693, validation loss: 0.3949
2024-06-03 01:09:33 [INFO]: Epoch 036 - training loss: 0.2697, validation loss: 0.3958
2024-06-03 01:09:39 [INFO]: Epoch 037 - training loss: 0.2726, validation loss: 0.3950
2024-06-03 01:09:45 [INFO]: Epoch 038 - training loss: 0.2667, validation loss: 0.3966
2024-06-03 01:09:52 [INFO]: Epoch 039 - training loss: 0.2619, validation loss: 0.3947
2024-06-03 01:09:58 [INFO]: Epoch 040 - training loss: 0.2626, validation loss: 0.3921
2024-06-03 01:10:05 [INFO]: Epoch 041 - training loss: 0.2660, validation loss: 0.3901
2024-06-03 01:10:11 [INFO]: Epoch 042 - training loss: 0.2702, validation loss: 0.3901
2024-06-03 01:10:18 [INFO]: Epoch 043 - training loss: 0.2629, validation loss: 0.3906
2024-06-03 01:10:24 [INFO]: Epoch 044 - training loss: 0.2603, validation loss: 0.3835
2024-06-03 01:10:30 [INFO]: Epoch 045 - training loss: 0.2626, validation loss: 0.3893
2024-06-03 01:10:37 [INFO]: Epoch 046 - training loss: 0.2681, validation loss: 0.3853
2024-06-03 01:10:43 [INFO]: Epoch 047 - training loss: 0.2643, validation loss: 0.3888
2024-06-03 01:10:50 [INFO]: Epoch 048 - training loss: 0.2591, validation loss: 0.3900
2024-06-03 01:10:56 [INFO]: Epoch 049 - training loss: 0.2537, validation loss: 0.3806
2024-06-03 01:11:02 [INFO]: Epoch 050 - training loss: 0.2520, validation loss: 0.3825
2024-06-03 01:11:09 [INFO]: Epoch 051 - training loss: 0.2584, validation loss: 0.3731
2024-06-03 01:11:15 [INFO]: Epoch 052 - training loss: 0.2610, validation loss: 0.3823
2024-06-03 01:11:22 [INFO]: Epoch 053 - training loss: 0.2590, validation loss: 0.3821
2024-06-03 01:11:28 [INFO]: Epoch 054 - training loss: 0.2502, validation loss: 0.3755
2024-06-03 01:11:35 [INFO]: Epoch 055 - training loss: 0.2533, validation loss: 0.3792
2024-06-03 01:11:41 [INFO]: Epoch 056 - training loss: 0.2479, validation loss: 0.3776
2024-06-03 01:11:47 [INFO]: Epoch 057 - training loss: 0.2465, validation loss: 0.3733
2024-06-03 01:11:54 [INFO]: Epoch 058 - training loss: 0.2460, validation loss: 0.3738
2024-06-03 01:12:00 [INFO]: Epoch 059 - training loss: 0.2418, validation loss: 0.3727
2024-06-03 01:12:06 [INFO]: Epoch 060 - training loss: 0.2493, validation loss: 0.3722
2024-06-03 01:12:13 [INFO]: Epoch 061 - training loss: 0.2504, validation loss: 0.3701
2024-06-03 01:12:19 [INFO]: Epoch 062 - training loss: 0.2519, validation loss: 0.3728
2024-06-03 01:12:26 [INFO]: Epoch 063 - training loss: 0.2432, validation loss: 0.3726
2024-06-03 01:12:32 [INFO]: Epoch 064 - training loss: 0.2372, validation loss: 0.3779
2024-06-03 01:12:39 [INFO]: Epoch 065 - training loss: 0.2518, validation loss: 0.3769
2024-06-03 01:12:45 [INFO]: Epoch 066 - training loss: 0.2463, validation loss: 0.3786
2024-06-03 01:12:51 [INFO]: Epoch 067 - training loss: 0.2377, validation loss: 0.3750
2024-06-03 01:12:58 [INFO]: Epoch 068 - training loss: 0.2447, validation loss: 0.3734
2024-06-03 01:13:04 [INFO]: Epoch 069 - training loss: 0.2421, validation loss: 0.3723
2024-06-03 01:13:11 [INFO]: Epoch 070 - training loss: 0.2429, validation loss: 0.3763
2024-06-03 01:13:17 [INFO]: Epoch 071 - training loss: 0.2390, validation loss: 0.3723
2024-06-03 01:13:17 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 01:13:17 [INFO]: Finished training. The best model is from epoch#61.
2024-06-03 01:13:17 [INFO]: Saved the model to results_point_rate05/PeMS/iTransformer_PeMS/round_2/20240603_T010543/iTransformer.pypots
2024-06-03 01:13:20 [INFO]: Successfully saved to results_point_rate05/PeMS/iTransformer_PeMS/round_2/imputation.pkl
2024-06-03 01:13:20 [INFO]: Round2 - iTransformer on PeMS: MAE=0.2991, MSE=0.5543, MRE=0.3712
2024-06-03 01:13:20 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 01:13:20 [INFO]: Using the given device: cuda:0
2024-06-03 01:13:20 [INFO]: Model files will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_3/20240603_T011320
2024-06-03 01:13:20 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_3/20240603_T011320/tensorboard
2024-06-03 01:13:20 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 01:13:20 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 01:13:20 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 01:13:27 [INFO]: Epoch 001 - training loss: 1.0133, validation loss: 0.6623
2024-06-03 01:13:33 [INFO]: Epoch 002 - training loss: 0.6265, validation loss: 0.5997
2024-06-03 01:13:39 [INFO]: Epoch 003 - training loss: 0.5357, validation loss: 0.5524
2024-06-03 01:13:46 [INFO]: Epoch 004 - training loss: 0.4875, validation loss: 0.5259
2024-06-03 01:13:52 [INFO]: Epoch 005 - training loss: 0.4556, validation loss: 0.5211
2024-06-03 01:13:59 [INFO]: Epoch 006 - training loss: 0.4444, validation loss: 0.5294
2024-06-03 01:14:05 [INFO]: Epoch 007 - training loss: 0.4176, validation loss: 0.4993
2024-06-03 01:14:11 [INFO]: Epoch 008 - training loss: 0.4048, validation loss: 0.4999
2024-06-03 01:14:18 [INFO]: Epoch 009 - training loss: 0.3962, validation loss: 0.4951
2024-06-03 01:14:24 [INFO]: Epoch 010 - training loss: 0.3753, validation loss: 0.4886
2024-06-03 01:14:31 [INFO]: Epoch 011 - training loss: 0.3615, validation loss: 0.4664
2024-06-03 01:14:37 [INFO]: Epoch 012 - training loss: 0.3583, validation loss: 0.4577
2024-06-03 01:14:44 [INFO]: Epoch 013 - training loss: 0.3552, validation loss: 0.4596
2024-06-03 01:14:50 [INFO]: Epoch 014 - training loss: 0.3483, validation loss: 0.4474
2024-06-03 01:14:56 [INFO]: Epoch 015 - training loss: 0.3408, validation loss: 0.4577
2024-06-03 01:15:03 [INFO]: Epoch 016 - training loss: 0.3401, validation loss: 0.4403
2024-06-03 01:15:09 [INFO]: Epoch 017 - training loss: 0.3267, validation loss: 0.4498
2024-06-03 01:15:16 [INFO]: Epoch 018 - training loss: 0.3262, validation loss: 0.4414
2024-06-03 01:15:22 [INFO]: Epoch 019 - training loss: 0.3211, validation loss: 0.4396
2024-06-03 01:15:28 [INFO]: Epoch 020 - training loss: 0.3094, validation loss: 0.4390
2024-06-03 01:15:34 [INFO]: Epoch 021 - training loss: 0.3027, validation loss: 0.4233
2024-06-03 01:15:41 [INFO]: Epoch 022 - training loss: 0.3026, validation loss: 0.4248
2024-06-03 01:15:47 [INFO]: Epoch 023 - training loss: 0.2979, validation loss: 0.4249
2024-06-03 01:15:53 [INFO]: Epoch 024 - training loss: 0.3085, validation loss: 0.4233
2024-06-03 01:16:00 [INFO]: Epoch 025 - training loss: 0.3003, validation loss: 0.4244
2024-06-03 01:16:06 [INFO]: Epoch 026 - training loss: 0.3004, validation loss: 0.4197
2024-06-03 01:16:13 [INFO]: Epoch 027 - training loss: 0.2936, validation loss: 0.4125
2024-06-03 01:16:19 [INFO]: Epoch 028 - training loss: 0.2875, validation loss: 0.4112
2024-06-03 01:16:26 [INFO]: Epoch 029 - training loss: 0.2870, validation loss: 0.4100
2024-06-03 01:16:32 [INFO]: Epoch 030 - training loss: 0.2846, validation loss: 0.4110
2024-06-03 01:16:38 [INFO]: Epoch 031 - training loss: 0.2826, validation loss: 0.4097
2024-06-03 01:16:45 [INFO]: Epoch 032 - training loss: 0.2920, validation loss: 0.4106
2024-06-03 01:16:51 [INFO]: Epoch 033 - training loss: 0.2827, validation loss: 0.4073
2024-06-03 01:16:58 [INFO]: Epoch 034 - training loss: 0.2806, validation loss: 0.3962
2024-06-03 01:17:04 [INFO]: Epoch 035 - training loss: 0.2734, validation loss: 0.4038
2024-06-03 01:17:11 [INFO]: Epoch 036 - training loss: 0.2689, validation loss: 0.3991
2024-06-03 01:17:17 [INFO]: Epoch 037 - training loss: 0.2697, validation loss: 0.4062
2024-06-03 01:17:23 [INFO]: Epoch 038 - training loss: 0.2924, validation loss: 0.4003
2024-06-03 01:17:30 [INFO]: Epoch 039 - training loss: 0.2754, validation loss: 0.3996
2024-06-03 01:17:36 [INFO]: Epoch 040 - training loss: 0.2678, validation loss: 0.3961
2024-06-03 01:17:43 [INFO]: Epoch 041 - training loss: 0.2750, validation loss: 0.3943
2024-06-03 01:17:49 [INFO]: Epoch 042 - training loss: 0.2744, validation loss: 0.3941
2024-06-03 01:17:56 [INFO]: Epoch 043 - training loss: 0.2659, validation loss: 0.3894
2024-06-03 01:18:02 [INFO]: Epoch 044 - training loss: 0.2620, validation loss: 0.3903
2024-06-03 01:18:08 [INFO]: Epoch 045 - training loss: 0.2585, validation loss: 0.3876
2024-06-03 01:18:15 [INFO]: Epoch 046 - training loss: 0.2560, validation loss: 0.3926
2024-06-03 01:18:21 [INFO]: Epoch 047 - training loss: 0.2574, validation loss: 0.3997
2024-06-03 01:18:28 [INFO]: Epoch 048 - training loss: 0.2612, validation loss: 0.3872
2024-06-03 01:18:34 [INFO]: Epoch 049 - training loss: 0.2530, validation loss: 0.3879
2024-06-03 01:18:41 [INFO]: Epoch 050 - training loss: 0.2535, validation loss: 0.3829
2024-06-03 01:18:47 [INFO]: Epoch 051 - training loss: 0.2488, validation loss: 0.3784
2024-06-03 01:18:53 [INFO]: Epoch 052 - training loss: 0.2502, validation loss: 0.3825
2024-06-03 01:19:00 [INFO]: Epoch 053 - training loss: 0.2525, validation loss: 0.3781
2024-06-03 01:19:06 [INFO]: Epoch 054 - training loss: 0.2552, validation loss: 0.3842
2024-06-03 01:19:12 [INFO]: Epoch 055 - training loss: 0.2513, validation loss: 0.3788
2024-06-03 01:19:19 [INFO]: Epoch 056 - training loss: 0.2512, validation loss: 0.3804
2024-06-03 01:19:25 [INFO]: Epoch 057 - training loss: 0.2517, validation loss: 0.3837
2024-06-03 01:19:31 [INFO]: Epoch 058 - training loss: 0.2572, validation loss: 0.3757
2024-06-03 01:19:38 [INFO]: Epoch 059 - training loss: 0.2527, validation loss: 0.3761
2024-06-03 01:19:44 [INFO]: Epoch 060 - training loss: 0.2470, validation loss: 0.3793
2024-06-03 01:19:51 [INFO]: Epoch 061 - training loss: 0.2489, validation loss: 0.3821
2024-06-03 01:19:58 [INFO]: Epoch 062 - training loss: 0.2473, validation loss: 0.3819
2024-06-03 01:20:04 [INFO]: Epoch 063 - training loss: 0.2504, validation loss: 0.3906
2024-06-03 01:20:11 [INFO]: Epoch 064 - training loss: 0.2553, validation loss: 0.3808
2024-06-03 01:20:18 [INFO]: Epoch 065 - training loss: 0.2432, validation loss: 0.3793
2024-06-03 01:20:24 [INFO]: Epoch 066 - training loss: 0.2409, validation loss: 0.3750
2024-06-03 01:20:31 [INFO]: Epoch 067 - training loss: 0.2423, validation loss: 0.3741
2024-06-03 01:20:37 [INFO]: Epoch 068 - training loss: 0.2459, validation loss: 0.3753
2024-06-03 01:20:44 [INFO]: Epoch 069 - training loss: 0.2444, validation loss: 0.3773
2024-06-03 01:20:50 [INFO]: Epoch 070 - training loss: 0.2423, validation loss: 0.3712
2024-06-03 01:20:57 [INFO]: Epoch 071 - training loss: 0.2367, validation loss: 0.3733
2024-06-03 01:21:03 [INFO]: Epoch 072 - training loss: 0.2411, validation loss: 0.3764
2024-06-03 01:21:10 [INFO]: Epoch 073 - training loss: 0.2384, validation loss: 0.3678
2024-06-03 01:21:16 [INFO]: Epoch 074 - training loss: 0.2409, validation loss: 0.3744
2024-06-03 01:21:23 [INFO]: Epoch 075 - training loss: 0.2392, validation loss: 0.3712
2024-06-03 01:21:30 [INFO]: Epoch 076 - training loss: 0.2404, validation loss: 0.3672
2024-06-03 01:21:36 [INFO]: Epoch 077 - training loss: 0.2347, validation loss: 0.3774
2024-06-03 01:21:43 [INFO]: Epoch 078 - training loss: 0.2316, validation loss: 0.3663
2024-06-03 01:21:50 [INFO]: Epoch 079 - training loss: 0.2302, validation loss: 0.3650
2024-06-03 01:21:56 [INFO]: Epoch 080 - training loss: 0.2319, validation loss: 0.3658
2024-06-03 01:22:03 [INFO]: Epoch 081 - training loss: 0.2349, validation loss: 0.3622
2024-06-03 01:22:09 [INFO]: Epoch 082 - training loss: 0.2324, validation loss: 0.3676
2024-06-03 01:22:16 [INFO]: Epoch 083 - training loss: 0.2321, validation loss: 0.3699
2024-06-03 01:22:23 [INFO]: Epoch 084 - training loss: 0.2357, validation loss: 0.3658
2024-06-03 01:22:29 [INFO]: Epoch 085 - training loss: 0.2355, validation loss: 0.3653
2024-06-03 01:22:36 [INFO]: Epoch 086 - training loss: 0.2327, validation loss: 0.3611
2024-06-03 01:22:43 [INFO]: Epoch 087 - training loss: 0.2283, validation loss: 0.3640
2024-06-03 01:22:49 [INFO]: Epoch 088 - training loss: 0.2329, validation loss: 0.3670
2024-06-03 01:22:56 [INFO]: Epoch 089 - training loss: 0.2371, validation loss: 0.3607
2024-06-03 01:23:03 [INFO]: Epoch 090 - training loss: 0.2328, validation loss: 0.3758
2024-06-03 01:23:10 [INFO]: Epoch 091 - training loss: 0.2335, validation loss: 0.3650
2024-06-03 01:23:16 [INFO]: Epoch 092 - training loss: 0.2347, validation loss: 0.3658
2024-06-03 01:23:23 [INFO]: Epoch 093 - training loss: 0.2331, validation loss: 0.3609
2024-06-03 01:23:29 [INFO]: Epoch 094 - training loss: 0.2304, validation loss: 0.3638
2024-06-03 01:23:36 [INFO]: Epoch 095 - training loss: 0.2287, validation loss: 0.3667
2024-06-03 01:23:41 [INFO]: Epoch 096 - training loss: 0.2311, validation loss: 0.3747
2024-06-03 01:23:45 [INFO]: Epoch 097 - training loss: 0.2465, validation loss: 0.3774
2024-06-03 01:23:50 [INFO]: Epoch 098 - training loss: 0.2381, validation loss: 0.3619
2024-06-03 01:23:55 [INFO]: Epoch 099 - training loss: 0.2316, validation loss: 0.3625
2024-06-03 01:23:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 01:23:55 [INFO]: Finished training. The best model is from epoch#89.
2024-06-03 01:23:55 [INFO]: Saved the model to results_point_rate05/PeMS/iTransformer_PeMS/round_3/20240603_T011320/iTransformer.pypots
2024-06-03 01:23:57 [INFO]: Successfully saved to results_point_rate05/PeMS/iTransformer_PeMS/round_3/imputation.pkl
2024-06-03 01:23:57 [INFO]: Round3 - iTransformer on PeMS: MAE=0.2933, MSE=0.5411, MRE=0.3639
2024-06-03 01:23:57 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 01:23:57 [INFO]: Using the given device: cuda:0
2024-06-03 01:23:57 [INFO]: Model files will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_4/20240603_T012357
2024-06-03 01:23:57 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/iTransformer_PeMS/round_4/20240603_T012357/tensorboard
2024-06-03 01:23:57 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 01:23:57 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 01:23:57 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 01:24:02 [INFO]: Epoch 001 - training loss: 1.0153, validation loss: 0.7471
2024-06-03 01:24:07 [INFO]: Epoch 002 - training loss: 0.6350, validation loss: 0.6038
2024-06-03 01:24:12 [INFO]: Epoch 003 - training loss: 0.5463, validation loss: 0.5912
2024-06-03 01:24:16 [INFO]: Epoch 004 - training loss: 0.5081, validation loss: 0.5206
2024-06-03 01:24:21 [INFO]: Epoch 005 - training loss: 0.4689, validation loss: 0.5041
2024-06-03 01:24:26 [INFO]: Epoch 006 - training loss: 0.4413, validation loss: 0.4921
2024-06-03 01:24:30 [INFO]: Epoch 007 - training loss: 0.4221, validation loss: 0.4901
2024-06-03 01:24:35 [INFO]: Epoch 008 - training loss: 0.4027, validation loss: 0.4857
2024-06-03 01:24:40 [INFO]: Epoch 009 - training loss: 0.3899, validation loss: 0.4827
2024-06-03 01:24:45 [INFO]: Epoch 010 - training loss: 0.3716, validation loss: 0.4789
2024-06-03 01:24:49 [INFO]: Epoch 011 - training loss: 0.3655, validation loss: 0.4589
2024-06-03 01:24:54 [INFO]: Epoch 012 - training loss: 0.3555, validation loss: 0.4572
2024-06-03 01:24:59 [INFO]: Epoch 013 - training loss: 0.3534, validation loss: 0.4483
2024-06-03 01:25:03 [INFO]: Epoch 014 - training loss: 0.3385, validation loss: 0.4500
2024-06-03 01:25:08 [INFO]: Epoch 015 - training loss: 0.3389, validation loss: 0.4413
2024-06-03 01:25:13 [INFO]: Epoch 016 - training loss: 0.3320, validation loss: 0.4412
2024-06-03 01:25:18 [INFO]: Epoch 017 - training loss: 0.3290, validation loss: 0.4435
2024-06-03 01:25:22 [INFO]: Epoch 018 - training loss: 0.3209, validation loss: 0.4364
2024-06-03 01:25:27 [INFO]: Epoch 019 - training loss: 0.3166, validation loss: 0.4297
2024-06-03 01:25:32 [INFO]: Epoch 020 - training loss: 0.3071, validation loss: 0.4160
2024-06-03 01:25:36 [INFO]: Epoch 021 - training loss: 0.3126, validation loss: 0.4264
2024-06-03 01:25:41 [INFO]: Epoch 022 - training loss: 0.3093, validation loss: 0.4222
2024-06-03 01:25:46 [INFO]: Epoch 023 - training loss: 0.3090, validation loss: 0.4130
2024-06-03 01:25:50 [INFO]: Epoch 024 - training loss: 0.3080, validation loss: 0.4167
2024-06-03 01:25:55 [INFO]: Epoch 025 - training loss: 0.2945, validation loss: 0.4125
2024-06-03 01:25:59 [INFO]: Epoch 026 - training loss: 0.2912, validation loss: 0.4102
2024-06-03 01:26:04 [INFO]: Epoch 027 - training loss: 0.3012, validation loss: 0.4096
2024-06-03 01:26:09 [INFO]: Epoch 028 - training loss: 0.2867, validation loss: 0.4050
2024-06-03 01:26:14 [INFO]: Epoch 029 - training loss: 0.2851, validation loss: 0.3995
2024-06-03 01:26:18 [INFO]: Epoch 030 - training loss: 0.2821, validation loss: 0.4027
2024-06-03 01:26:23 [INFO]: Epoch 031 - training loss: 0.2790, validation loss: 0.3971
2024-06-03 01:26:28 [INFO]: Epoch 032 - training loss: 0.2773, validation loss: 0.4014
2024-06-03 01:26:33 [INFO]: Epoch 033 - training loss: 0.2975, validation loss: 0.3960
2024-06-03 01:26:38 [INFO]: Epoch 034 - training loss: 0.2860, validation loss: 0.4000
2024-06-03 01:26:42 [INFO]: Epoch 035 - training loss: 0.2775, validation loss: 0.3953
2024-06-03 01:26:47 [INFO]: Epoch 036 - training loss: 0.2710, validation loss: 0.3864
2024-06-03 01:26:52 [INFO]: Epoch 037 - training loss: 0.2728, validation loss: 0.3890
2024-06-03 01:26:56 [INFO]: Epoch 038 - training loss: 0.2735, validation loss: 0.3852
2024-06-03 01:27:01 [INFO]: Epoch 039 - training loss: 0.2698, validation loss: 0.3854
2024-06-03 01:27:06 [INFO]: Epoch 040 - training loss: 0.2650, validation loss: 0.3857
2024-06-03 01:27:10 [INFO]: Epoch 041 - training loss: 0.2684, validation loss: 0.3863
2024-06-03 01:27:15 [INFO]: Epoch 042 - training loss: 0.2742, validation loss: 0.3870
2024-06-03 01:27:20 [INFO]: Epoch 043 - training loss: 0.2645, validation loss: 0.3831
2024-06-03 01:27:25 [INFO]: Epoch 044 - training loss: 0.2546, validation loss: 0.3830
2024-06-03 01:27:29 [INFO]: Epoch 045 - training loss: 0.2700, validation loss: 0.3804
2024-06-03 01:27:34 [INFO]: Epoch 046 - training loss: 0.2615, validation loss: 0.3797
2024-06-03 01:27:39 [INFO]: Epoch 047 - training loss: 0.2637, validation loss: 0.3736
2024-06-03 01:27:44 [INFO]: Epoch 048 - training loss: 0.2564, validation loss: 0.3764
2024-06-03 01:27:48 [INFO]: Epoch 049 - training loss: 0.2562, validation loss: 0.3725
2024-06-03 01:27:53 [INFO]: Epoch 050 - training loss: 0.2552, validation loss: 0.3774
2024-06-03 01:27:58 [INFO]: Epoch 051 - training loss: 0.2555, validation loss: 0.3737
2024-06-03 01:28:02 [INFO]: Epoch 052 - training loss: 0.2568, validation loss: 0.3748
2024-06-03 01:28:07 [INFO]: Epoch 053 - training loss: 0.2517, validation loss: 0.3742
2024-06-03 01:28:12 [INFO]: Epoch 054 - training loss: 0.2436, validation loss: 0.3723
2024-06-03 01:28:17 [INFO]: Epoch 055 - training loss: 0.2528, validation loss: 0.3719
2024-06-03 01:28:21 [INFO]: Epoch 056 - training loss: 0.2536, validation loss: 0.3726
2024-06-03 01:28:26 [INFO]: Epoch 057 - training loss: 0.2485, validation loss: 0.3713
2024-06-03 01:28:30 [INFO]: Epoch 058 - training loss: 0.2422, validation loss: 0.3705
2024-06-03 01:28:35 [INFO]: Epoch 059 - training loss: 0.2637, validation loss: 0.3719
2024-06-03 01:28:39 [INFO]: Epoch 060 - training loss: 0.2656, validation loss: 0.3718
2024-06-03 01:28:43 [INFO]: Epoch 061 - training loss: 0.2531, validation loss: 0.3708
2024-06-03 01:28:48 [INFO]: Epoch 062 - training loss: 0.2474, validation loss: 0.3714
2024-06-03 01:28:52 [INFO]: Epoch 063 - training loss: 0.2439, validation loss: 0.3666
2024-06-03 01:28:57 [INFO]: Epoch 064 - training loss: 0.2422, validation loss: 0.3671
2024-06-03 01:29:02 [INFO]: Epoch 065 - training loss: 0.2456, validation loss: 0.3735
2024-06-03 01:29:06 [INFO]: Epoch 066 - training loss: 0.2499, validation loss: 0.3722
2024-06-03 01:29:11 [INFO]: Epoch 067 - training loss: 0.2489, validation loss: 0.3655
2024-06-03 01:29:16 [INFO]: Epoch 068 - training loss: 0.2407, validation loss: 0.3589
2024-06-03 01:29:20 [INFO]: Epoch 069 - training loss: 0.2392, validation loss: 0.3589
2024-06-03 01:29:25 [INFO]: Epoch 070 - training loss: 0.2360, validation loss: 0.3665
2024-06-03 01:29:30 [INFO]: Epoch 071 - training loss: 0.2384, validation loss: 0.3562
2024-06-03 01:29:35 [INFO]: Epoch 072 - training loss: 0.2339, validation loss: 0.3691
2024-06-03 01:29:39 [INFO]: Epoch 073 - training loss: 0.2373, validation loss: 0.3676
2024-06-03 01:29:44 [INFO]: Epoch 074 - training loss: 0.2496, validation loss: 0.3715
2024-06-03 01:29:49 [INFO]: Epoch 075 - training loss: 0.2435, validation loss: 0.3645
2024-06-03 01:29:54 [INFO]: Epoch 076 - training loss: 0.2363, validation loss: 0.3619
2024-06-03 01:29:58 [INFO]: Epoch 077 - training loss: 0.2356, validation loss: 0.3585
2024-06-03 01:30:03 [INFO]: Epoch 078 - training loss: 0.2384, validation loss: 0.3680
2024-06-03 01:30:08 [INFO]: Epoch 079 - training loss: 0.2328, validation loss: 0.3702
2024-06-03 01:30:12 [INFO]: Epoch 080 - training loss: 0.2337, validation loss: 0.3651
2024-06-03 01:30:17 [INFO]: Epoch 081 - training loss: 0.2322, validation loss: 0.3649
2024-06-03 01:30:17 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 01:30:17 [INFO]: Finished training. The best model is from epoch#71.
2024-06-03 01:30:17 [INFO]: Saved the model to results_point_rate05/PeMS/iTransformer_PeMS/round_4/20240603_T012357/iTransformer.pypots
2024-06-03 01:30:19 [INFO]: Successfully saved to results_point_rate05/PeMS/iTransformer_PeMS/round_4/imputation.pkl
2024-06-03 01:30:19 [INFO]: Round4 - iTransformer on PeMS: MAE=0.2914, MSE=0.5298, MRE=0.3616
2024-06-03 01:30:19 [INFO]: Done! Final results:
Averaged iTransformer (1,854,744 params) on PeMS: MAE=0.2948 ± 0.007051058018054937, MSE=0.5393 ± 0.01631850967494774, MRE=0.3659 ± 0.008749764890613772, average inference time=0.46
