2024-06-03 00:45:08 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 00:45:08 [INFO]: Using the given device: cuda:0
2024-06-03 00:45:09 [INFO]: Model files will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_0/20240603_T004508
2024-06-03 00:45:09 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_0/20240603_T004508/tensorboard
2024-06-03 00:45:10 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-03 00:45:23 [INFO]: Epoch 001 - training loss: 1.0378, validation loss: 0.6496
2024-06-03 00:45:27 [INFO]: Epoch 002 - training loss: 0.7632, validation loss: 0.5740
2024-06-03 00:45:30 [INFO]: Epoch 003 - training loss: 0.7060, validation loss: 0.5954
2024-06-03 00:45:33 [INFO]: Epoch 004 - training loss: 0.6847, validation loss: 0.5405
2024-06-03 00:45:37 [INFO]: Epoch 005 - training loss: 0.6602, validation loss: 0.5547
2024-06-03 00:45:40 [INFO]: Epoch 006 - training loss: 0.6406, validation loss: 0.5416
2024-06-03 00:45:44 [INFO]: Epoch 007 - training loss: 0.6276, validation loss: 0.5408
2024-06-03 00:45:47 [INFO]: Epoch 008 - training loss: 0.6200, validation loss: 0.5229
2024-06-03 00:45:50 [INFO]: Epoch 009 - training loss: 0.6100, validation loss: 0.5172
2024-06-03 00:45:53 [INFO]: Epoch 010 - training loss: 0.6008, validation loss: 0.5225
2024-06-03 00:45:57 [INFO]: Epoch 011 - training loss: 0.6048, validation loss: 0.5089
2024-06-03 00:46:00 [INFO]: Epoch 012 - training loss: 0.5971, validation loss: 0.5258
2024-06-03 00:46:03 [INFO]: Epoch 013 - training loss: 0.5931, validation loss: 0.5181
2024-06-03 00:46:06 [INFO]: Epoch 014 - training loss: 0.5877, validation loss: 0.5100
2024-06-03 00:46:09 [INFO]: Epoch 015 - training loss: 0.5851, validation loss: 0.5016
2024-06-03 00:46:13 [INFO]: Epoch 016 - training loss: 0.5866, validation loss: 0.5065
2024-06-03 00:46:16 [INFO]: Epoch 017 - training loss: 0.5881, validation loss: 0.4879
2024-06-03 00:46:19 [INFO]: Epoch 018 - training loss: 0.5834, validation loss: 0.5158
2024-06-03 00:46:22 [INFO]: Epoch 019 - training loss: 0.5851, validation loss: 0.5021
2024-06-03 00:46:25 [INFO]: Epoch 020 - training loss: 0.5785, validation loss: 0.5045
2024-06-03 00:46:28 [INFO]: Epoch 021 - training loss: 0.5837, validation loss: 0.4819
2024-06-03 00:46:31 [INFO]: Epoch 022 - training loss: 0.5807, validation loss: 0.4902
2024-06-03 00:46:34 [INFO]: Epoch 023 - training loss: 0.5758, validation loss: 0.4947
2024-06-03 00:46:37 [INFO]: Epoch 024 - training loss: 0.5749, validation loss: 0.4846
2024-06-03 00:46:40 [INFO]: Epoch 025 - training loss: 0.5707, validation loss: 0.5063
2024-06-03 00:46:44 [INFO]: Epoch 026 - training loss: 0.5724, validation loss: 0.5237
2024-06-03 00:46:47 [INFO]: Epoch 027 - training loss: 0.5734, validation loss: 0.5104
2024-06-03 00:46:50 [INFO]: Epoch 028 - training loss: 0.5716, validation loss: 0.5216
2024-06-03 00:46:54 [INFO]: Epoch 029 - training loss: 0.5729, validation loss: 0.4935
2024-06-03 00:46:57 [INFO]: Epoch 030 - training loss: 0.5708, validation loss: 0.4954
2024-06-03 00:47:00 [INFO]: Epoch 031 - training loss: 0.5713, validation loss: 0.5029
2024-06-03 00:47:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:47:00 [INFO]: Finished training. The best model is from epoch#21.
2024-06-03 00:47:00 [INFO]: Saved the model to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_0/20240603_T004508/NonstationaryTransformer.pypots
2024-06-03 00:47:01 [INFO]: Successfully saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_0/imputation.pkl
2024-06-03 00:47:01 [INFO]: Round0 - NonstationaryTransformer on PeMS: MAE=0.4068, MSE=0.7188, MRE=0.5048
2024-06-03 00:47:01 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 00:47:01 [INFO]: Using the given device: cuda:0
2024-06-03 00:47:01 [INFO]: Model files will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_1/20240603_T004701
2024-06-03 00:47:01 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_1/20240603_T004701/tensorboard
2024-06-03 00:47:02 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-03 00:47:05 [INFO]: Epoch 001 - training loss: 1.0534, validation loss: 0.6483
2024-06-03 00:47:08 [INFO]: Epoch 002 - training loss: 0.7566, validation loss: 0.5473
2024-06-03 00:47:11 [INFO]: Epoch 003 - training loss: 0.7096, validation loss: 0.5349
2024-06-03 00:47:14 [INFO]: Epoch 004 - training loss: 0.6798, validation loss: 0.5285
2024-06-03 00:47:18 [INFO]: Epoch 005 - training loss: 0.6578, validation loss: 0.5233
2024-06-03 00:47:21 [INFO]: Epoch 006 - training loss: 0.6468, validation loss: 0.5217
2024-06-03 00:47:24 [INFO]: Epoch 007 - training loss: 0.6288, validation loss: 0.5262
2024-06-03 00:47:27 [INFO]: Epoch 008 - training loss: 0.6236, validation loss: 0.5230
2024-06-03 00:47:31 [INFO]: Epoch 009 - training loss: 0.6147, validation loss: 0.4996
2024-06-03 00:47:34 [INFO]: Epoch 010 - training loss: 0.6054, validation loss: 0.5159
2024-06-03 00:47:37 [INFO]: Epoch 011 - training loss: 0.5982, validation loss: 0.5130
2024-06-03 00:47:41 [INFO]: Epoch 012 - training loss: 0.5927, validation loss: 0.4972
2024-06-03 00:47:44 [INFO]: Epoch 013 - training loss: 0.5920, validation loss: 0.5075
2024-06-03 00:47:47 [INFO]: Epoch 014 - training loss: 0.5860, validation loss: 0.4926
2024-06-03 00:47:50 [INFO]: Epoch 015 - training loss: 0.5865, validation loss: 0.4996
2024-06-03 00:47:54 [INFO]: Epoch 016 - training loss: 0.5864, validation loss: 0.5226
2024-06-03 00:47:57 [INFO]: Epoch 017 - training loss: 0.5860, validation loss: 0.5115
2024-06-03 00:48:00 [INFO]: Epoch 018 - training loss: 0.5863, validation loss: 0.4962
2024-06-03 00:48:03 [INFO]: Epoch 019 - training loss: 0.5808, validation loss: 0.4834
2024-06-03 00:48:06 [INFO]: Epoch 020 - training loss: 0.5779, validation loss: 0.5017
2024-06-03 00:48:09 [INFO]: Epoch 021 - training loss: 0.5775, validation loss: 0.4909
2024-06-03 00:48:12 [INFO]: Epoch 022 - training loss: 0.5771, validation loss: 0.5144
2024-06-03 00:48:15 [INFO]: Epoch 023 - training loss: 0.5821, validation loss: 0.4828
2024-06-03 00:48:18 [INFO]: Epoch 024 - training loss: 0.5763, validation loss: 0.4809
2024-06-03 00:48:21 [INFO]: Epoch 025 - training loss: 0.5690, validation loss: 0.5073
2024-06-03 00:48:25 [INFO]: Epoch 026 - training loss: 0.5726, validation loss: 0.4858
2024-06-03 00:48:28 [INFO]: Epoch 027 - training loss: 0.5679, validation loss: 0.4853
2024-06-03 00:48:31 [INFO]: Epoch 028 - training loss: 0.5706, validation loss: 0.4961
2024-06-03 00:48:34 [INFO]: Epoch 029 - training loss: 0.5737, validation loss: 0.4813
2024-06-03 00:48:37 [INFO]: Epoch 030 - training loss: 0.5730, validation loss: 0.4792
2024-06-03 00:48:41 [INFO]: Epoch 031 - training loss: 0.5638, validation loss: 0.4840
2024-06-03 00:48:44 [INFO]: Epoch 032 - training loss: 0.5622, validation loss: 0.4826
2024-06-03 00:48:48 [INFO]: Epoch 033 - training loss: 0.5681, validation loss: 0.4935
2024-06-03 00:48:51 [INFO]: Epoch 034 - training loss: 0.5737, validation loss: 0.4709
2024-06-03 00:48:54 [INFO]: Epoch 035 - training loss: 0.5614, validation loss: 0.5027
2024-06-03 00:48:57 [INFO]: Epoch 036 - training loss: 0.5591, validation loss: 0.4710
2024-06-03 00:49:00 [INFO]: Epoch 037 - training loss: 0.5674, validation loss: 0.4959
2024-06-03 00:49:04 [INFO]: Epoch 038 - training loss: 0.5656, validation loss: 0.4877
2024-06-03 00:49:07 [INFO]: Epoch 039 - training loss: 0.5636, validation loss: 0.5119
2024-06-03 00:49:10 [INFO]: Epoch 040 - training loss: 0.5631, validation loss: 0.4937
2024-06-03 00:49:13 [INFO]: Epoch 041 - training loss: 0.5656, validation loss: 0.4744
2024-06-03 00:49:16 [INFO]: Epoch 042 - training loss: 0.5627, validation loss: 0.4768
2024-06-03 00:49:20 [INFO]: Epoch 043 - training loss: 0.5589, validation loss: 0.4894
2024-06-03 00:49:22 [INFO]: Epoch 044 - training loss: 0.5627, validation loss: 0.4766
2024-06-03 00:49:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:49:22 [INFO]: Finished training. The best model is from epoch#34.
2024-06-03 00:49:22 [INFO]: Saved the model to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_1/20240603_T004701/NonstationaryTransformer.pypots
2024-06-03 00:49:24 [INFO]: Successfully saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_1/imputation.pkl
2024-06-03 00:49:24 [INFO]: Round1 - NonstationaryTransformer on PeMS: MAE=0.3925, MSE=0.6756, MRE=0.4871
2024-06-03 00:49:24 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 00:49:24 [INFO]: Using the given device: cuda:0
2024-06-03 00:49:24 [INFO]: Model files will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_2/20240603_T004924
2024-06-03 00:49:24 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_2/20240603_T004924/tensorboard
2024-06-03 00:49:24 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-03 00:49:27 [INFO]: Epoch 001 - training loss: 1.0363, validation loss: 0.6496
2024-06-03 00:49:30 [INFO]: Epoch 002 - training loss: 0.7586, validation loss: 0.5556
2024-06-03 00:49:33 [INFO]: Epoch 003 - training loss: 0.7169, validation loss: 0.5368
2024-06-03 00:49:36 [INFO]: Epoch 004 - training loss: 0.6848, validation loss: 0.5317
2024-06-03 00:49:39 [INFO]: Epoch 005 - training loss: 0.6645, validation loss: 0.5130
2024-06-03 00:49:43 [INFO]: Epoch 006 - training loss: 0.6407, validation loss: 0.5228
2024-06-03 00:49:46 [INFO]: Epoch 007 - training loss: 0.6353, validation loss: 0.5005
2024-06-03 00:49:49 [INFO]: Epoch 008 - training loss: 0.6219, validation loss: 0.5008
2024-06-03 00:49:53 [INFO]: Epoch 009 - training loss: 0.6091, validation loss: 0.4890
2024-06-03 00:49:56 [INFO]: Epoch 010 - training loss: 0.6052, validation loss: 0.4813
2024-06-03 00:49:59 [INFO]: Epoch 011 - training loss: 0.5986, validation loss: 0.4808
2024-06-03 00:50:02 [INFO]: Epoch 012 - training loss: 0.5979, validation loss: 0.4859
2024-06-03 00:50:06 [INFO]: Epoch 013 - training loss: 0.5895, validation loss: 0.4949
2024-06-03 00:50:09 [INFO]: Epoch 014 - training loss: 0.5981, validation loss: 0.4734
2024-06-03 00:50:12 [INFO]: Epoch 015 - training loss: 0.5841, validation loss: 0.4794
2024-06-03 00:50:15 [INFO]: Epoch 016 - training loss: 0.5843, validation loss: 0.4697
2024-06-03 00:50:19 [INFO]: Epoch 017 - training loss: 0.5879, validation loss: 0.4781
2024-06-03 00:50:22 [INFO]: Epoch 018 - training loss: 0.5855, validation loss: 0.4885
2024-06-03 00:50:26 [INFO]: Epoch 019 - training loss: 0.5806, validation loss: 0.4987
2024-06-03 00:50:29 [INFO]: Epoch 020 - training loss: 0.5781, validation loss: 0.4712
2024-06-03 00:50:32 [INFO]: Epoch 021 - training loss: 0.5711, validation loss: 0.4789
2024-06-03 00:50:35 [INFO]: Epoch 022 - training loss: 0.5759, validation loss: 0.4822
2024-06-03 00:50:38 [INFO]: Epoch 023 - training loss: 0.5710, validation loss: 0.4794
2024-06-03 00:50:42 [INFO]: Epoch 024 - training loss: 0.5713, validation loss: 0.4714
2024-06-03 00:50:45 [INFO]: Epoch 025 - training loss: 0.5759, validation loss: 0.4678
2024-06-03 00:50:48 [INFO]: Epoch 026 - training loss: 0.5682, validation loss: 0.4755
2024-06-03 00:50:51 [INFO]: Epoch 027 - training loss: 0.5710, validation loss: 0.4745
2024-06-03 00:50:54 [INFO]: Epoch 028 - training loss: 0.5679, validation loss: 0.4630
2024-06-03 00:50:58 [INFO]: Epoch 029 - training loss: 0.5667, validation loss: 0.4649
2024-06-03 00:51:01 [INFO]: Epoch 030 - training loss: 0.5667, validation loss: 0.4750
2024-06-03 00:51:04 [INFO]: Epoch 031 - training loss: 0.5741, validation loss: 0.5016
2024-06-03 00:51:08 [INFO]: Epoch 032 - training loss: 0.5697, validation loss: 0.4705
2024-06-03 00:51:11 [INFO]: Epoch 033 - training loss: 0.5709, validation loss: 0.4757
2024-06-03 00:51:14 [INFO]: Epoch 034 - training loss: 0.5722, validation loss: 0.4704
2024-06-03 00:51:17 [INFO]: Epoch 035 - training loss: 0.5655, validation loss: 0.4768
2024-06-03 00:51:21 [INFO]: Epoch 036 - training loss: 0.5687, validation loss: 0.4652
2024-06-03 00:51:24 [INFO]: Epoch 037 - training loss: 0.5662, validation loss: 0.4777
2024-06-03 00:51:27 [INFO]: Epoch 038 - training loss: 0.5648, validation loss: 0.4776
2024-06-03 00:51:27 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:51:27 [INFO]: Finished training. The best model is from epoch#28.
2024-06-03 00:51:27 [INFO]: Saved the model to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_2/20240603_T004924/NonstationaryTransformer.pypots
2024-06-03 00:51:29 [INFO]: Successfully saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_2/imputation.pkl
2024-06-03 00:51:29 [INFO]: Round2 - NonstationaryTransformer on PeMS: MAE=0.4087, MSE=0.6753, MRE=0.5072
2024-06-03 00:51:29 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 00:51:29 [INFO]: Using the given device: cuda:0
2024-06-03 00:51:29 [INFO]: Model files will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_3/20240603_T005129
2024-06-03 00:51:29 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_3/20240603_T005129/tensorboard
2024-06-03 00:51:29 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-03 00:51:32 [INFO]: Epoch 001 - training loss: 1.0510, validation loss: 0.6223
2024-06-03 00:51:36 [INFO]: Epoch 002 - training loss: 0.7589, validation loss: 0.5438
2024-06-03 00:51:39 [INFO]: Epoch 003 - training loss: 0.7109, validation loss: 0.5773
2024-06-03 00:51:42 [INFO]: Epoch 004 - training loss: 0.6821, validation loss: 0.5512
2024-06-03 00:51:45 [INFO]: Epoch 005 - training loss: 0.6591, validation loss: 0.5566
2024-06-03 00:51:48 [INFO]: Epoch 006 - training loss: 0.6405, validation loss: 0.5623
2024-06-03 00:51:52 [INFO]: Epoch 007 - training loss: 0.6413, validation loss: 0.5188
2024-06-03 00:51:55 [INFO]: Epoch 008 - training loss: 0.6189, validation loss: 0.5208
2024-06-03 00:51:58 [INFO]: Epoch 009 - training loss: 0.6136, validation loss: 0.5154
2024-06-03 00:52:01 [INFO]: Epoch 010 - training loss: 0.6075, validation loss: 0.5126
2024-06-03 00:52:04 [INFO]: Epoch 011 - training loss: 0.6025, validation loss: 0.4972
2024-06-03 00:52:07 [INFO]: Epoch 012 - training loss: 0.5947, validation loss: 0.4956
2024-06-03 00:52:09 [INFO]: Epoch 013 - training loss: 0.5940, validation loss: 0.4830
2024-06-03 00:52:12 [INFO]: Epoch 014 - training loss: 0.5950, validation loss: 0.4759
2024-06-03 00:52:15 [INFO]: Epoch 015 - training loss: 0.5877, validation loss: 0.4831
2024-06-03 00:52:17 [INFO]: Epoch 016 - training loss: 0.5876, validation loss: 0.4828
2024-06-03 00:52:20 [INFO]: Epoch 017 - training loss: 0.5897, validation loss: 0.4819
2024-06-03 00:52:23 [INFO]: Epoch 018 - training loss: 0.5906, validation loss: 0.4845
2024-06-03 00:52:25 [INFO]: Epoch 019 - training loss: 0.5785, validation loss: 0.4772
2024-06-03 00:52:28 [INFO]: Epoch 020 - training loss: 0.5849, validation loss: 0.4805
2024-06-03 00:52:31 [INFO]: Epoch 021 - training loss: 0.5783, validation loss: 0.4807
2024-06-03 00:52:34 [INFO]: Epoch 022 - training loss: 0.5774, validation loss: 0.4749
2024-06-03 00:52:36 [INFO]: Epoch 023 - training loss: 0.5801, validation loss: 0.4760
2024-06-03 00:52:39 [INFO]: Epoch 024 - training loss: 0.5768, validation loss: 0.4677
2024-06-03 00:52:42 [INFO]: Epoch 025 - training loss: 0.5699, validation loss: 0.4701
2024-06-03 00:52:45 [INFO]: Epoch 026 - training loss: 0.5728, validation loss: 0.4787
2024-06-03 00:52:47 [INFO]: Epoch 027 - training loss: 0.5733, validation loss: 0.4703
2024-06-03 00:52:50 [INFO]: Epoch 028 - training loss: 0.5663, validation loss: 0.4722
2024-06-03 00:52:53 [INFO]: Epoch 029 - training loss: 0.5737, validation loss: 0.4629
2024-06-03 00:52:56 [INFO]: Epoch 030 - training loss: 0.5745, validation loss: 0.4746
2024-06-03 00:52:59 [INFO]: Epoch 031 - training loss: 0.5705, validation loss: 0.4744
2024-06-03 00:53:02 [INFO]: Epoch 032 - training loss: 0.5691, validation loss: 0.4723
2024-06-03 00:53:05 [INFO]: Epoch 033 - training loss: 0.5690, validation loss: 0.4696
2024-06-03 00:53:07 [INFO]: Epoch 034 - training loss: 0.5714, validation loss: 0.4625
2024-06-03 00:53:10 [INFO]: Epoch 035 - training loss: 0.5669, validation loss: 0.4675
2024-06-03 00:53:13 [INFO]: Epoch 036 - training loss: 0.5698, validation loss: 0.4754
2024-06-03 00:53:16 [INFO]: Epoch 037 - training loss: 0.5629, validation loss: 0.4671
2024-06-03 00:53:19 [INFO]: Epoch 038 - training loss: 0.5632, validation loss: 0.4705
2024-06-03 00:53:21 [INFO]: Epoch 039 - training loss: 0.5616, validation loss: 0.4708
2024-06-03 00:53:24 [INFO]: Epoch 040 - training loss: 0.5579, validation loss: 0.4668
2024-06-03 00:53:27 [INFO]: Epoch 041 - training loss: 0.5619, validation loss: 0.4673
2024-06-03 00:53:30 [INFO]: Epoch 042 - training loss: 0.5578, validation loss: 0.4698
2024-06-03 00:53:33 [INFO]: Epoch 043 - training loss: 0.5661, validation loss: 0.4720
2024-06-03 00:53:36 [INFO]: Epoch 044 - training loss: 0.5594, validation loss: 0.4589
2024-06-03 00:53:38 [INFO]: Epoch 045 - training loss: 0.5559, validation loss: 0.4694
2024-06-03 00:53:41 [INFO]: Epoch 046 - training loss: 0.5635, validation loss: 0.4665
2024-06-03 00:53:44 [INFO]: Epoch 047 - training loss: 0.5592, validation loss: 0.4658
2024-06-03 00:53:47 [INFO]: Epoch 048 - training loss: 0.5570, validation loss: 0.4747
2024-06-03 00:53:50 [INFO]: Epoch 049 - training loss: 0.5619, validation loss: 0.4649
2024-06-03 00:53:52 [INFO]: Epoch 050 - training loss: 0.5582, validation loss: 0.4708
2024-06-03 00:53:55 [INFO]: Epoch 051 - training loss: 0.5594, validation loss: 0.4765
2024-06-03 00:53:58 [INFO]: Epoch 052 - training loss: 0.5640, validation loss: 0.4624
2024-06-03 00:54:01 [INFO]: Epoch 053 - training loss: 0.5607, validation loss: 0.4762
2024-06-03 00:54:03 [INFO]: Epoch 054 - training loss: 0.5596, validation loss: 0.4655
2024-06-03 00:54:03 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:54:03 [INFO]: Finished training. The best model is from epoch#44.
2024-06-03 00:54:03 [INFO]: Saved the model to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_3/20240603_T005129/NonstationaryTransformer.pypots
2024-06-03 00:54:04 [INFO]: Successfully saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_3/imputation.pkl
2024-06-03 00:54:04 [INFO]: Round3 - NonstationaryTransformer on PeMS: MAE=0.3773, MSE=0.6855, MRE=0.4682
2024-06-03 00:54:04 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 00:54:04 [INFO]: Using the given device: cuda:0
2024-06-03 00:54:04 [INFO]: Model files will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_4/20240603_T005404
2024-06-03 00:54:04 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_4/20240603_T005404/tensorboard
2024-06-03 00:54:05 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 346,318
2024-06-03 00:54:08 [INFO]: Epoch 001 - training loss: 1.0541, validation loss: 0.6463
2024-06-03 00:54:10 [INFO]: Epoch 002 - training loss: 0.7646, validation loss: 0.5592
2024-06-03 00:54:13 [INFO]: Epoch 003 - training loss: 0.7087, validation loss: 0.5329
2024-06-03 00:54:16 [INFO]: Epoch 004 - training loss: 0.6845, validation loss: 0.5262
2024-06-03 00:54:19 [INFO]: Epoch 005 - training loss: 0.6536, validation loss: 0.5182
2024-06-03 00:54:22 [INFO]: Epoch 006 - training loss: 0.6329, validation loss: 0.5187
2024-06-03 00:54:25 [INFO]: Epoch 007 - training loss: 0.6280, validation loss: 0.4966
2024-06-03 00:54:27 [INFO]: Epoch 008 - training loss: 0.6138, validation loss: 0.5002
2024-06-03 00:54:30 [INFO]: Epoch 009 - training loss: 0.6118, validation loss: 0.4937
2024-06-03 00:54:33 [INFO]: Epoch 010 - training loss: 0.6094, validation loss: 0.4752
2024-06-03 00:54:35 [INFO]: Epoch 011 - training loss: 0.6031, validation loss: 0.4898
2024-06-03 00:54:38 [INFO]: Epoch 012 - training loss: 0.6004, validation loss: 0.4812
2024-06-03 00:54:41 [INFO]: Epoch 013 - training loss: 0.5900, validation loss: 0.5106
2024-06-03 00:54:44 [INFO]: Epoch 014 - training loss: 0.5878, validation loss: 0.4730
2024-06-03 00:54:46 [INFO]: Epoch 015 - training loss: 0.5863, validation loss: 0.4944
2024-06-03 00:54:49 [INFO]: Epoch 016 - training loss: 0.5833, validation loss: 0.4731
2024-06-03 00:54:52 [INFO]: Epoch 017 - training loss: 0.5810, validation loss: 0.4599
2024-06-03 00:54:55 [INFO]: Epoch 018 - training loss: 0.5801, validation loss: 0.4720
2024-06-03 00:54:58 [INFO]: Epoch 019 - training loss: 0.5782, validation loss: 0.4672
2024-06-03 00:55:00 [INFO]: Epoch 020 - training loss: 0.5770, validation loss: 0.4667
2024-06-03 00:55:03 [INFO]: Epoch 021 - training loss: 0.5789, validation loss: 0.4740
2024-06-03 00:55:06 [INFO]: Epoch 022 - training loss: 0.5736, validation loss: 0.4733
2024-06-03 00:55:09 [INFO]: Epoch 023 - training loss: 0.5802, validation loss: 0.4580
2024-06-03 00:55:11 [INFO]: Epoch 024 - training loss: 0.5739, validation loss: 0.4791
2024-06-03 00:55:14 [INFO]: Epoch 025 - training loss: 0.5775, validation loss: 0.4595
2024-06-03 00:55:17 [INFO]: Epoch 026 - training loss: 0.5746, validation loss: 0.4796
2024-06-03 00:55:20 [INFO]: Epoch 027 - training loss: 0.5716, validation loss: 0.4573
2024-06-03 00:55:23 [INFO]: Epoch 028 - training loss: 0.5701, validation loss: 0.4711
2024-06-03 00:55:25 [INFO]: Epoch 029 - training loss: 0.5695, validation loss: 0.4762
2024-06-03 00:55:28 [INFO]: Epoch 030 - training loss: 0.5678, validation loss: 0.4791
2024-06-03 00:55:31 [INFO]: Epoch 031 - training loss: 0.5707, validation loss: 0.4512
2024-06-03 00:55:33 [INFO]: Epoch 032 - training loss: 0.5709, validation loss: 0.4716
2024-06-03 00:55:36 [INFO]: Epoch 033 - training loss: 0.5688, validation loss: 0.4569
2024-06-03 00:55:39 [INFO]: Epoch 034 - training loss: 0.5679, validation loss: 0.4607
2024-06-03 00:55:42 [INFO]: Epoch 035 - training loss: 0.5620, validation loss: 0.4604
2024-06-03 00:55:44 [INFO]: Epoch 036 - training loss: 0.5646, validation loss: 0.4632
2024-06-03 00:55:47 [INFO]: Epoch 037 - training loss: 0.5655, validation loss: 0.4632
2024-06-03 00:55:50 [INFO]: Epoch 038 - training loss: 0.5612, validation loss: 0.4587
2024-06-03 00:55:53 [INFO]: Epoch 039 - training loss: 0.5627, validation loss: 0.4770
2024-06-03 00:55:56 [INFO]: Epoch 040 - training loss: 0.5633, validation loss: 0.4545
2024-06-03 00:55:59 [INFO]: Epoch 041 - training loss: 0.5676, validation loss: 0.4634
2024-06-03 00:55:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:55:59 [INFO]: Finished training. The best model is from epoch#31.
2024-06-03 00:55:59 [INFO]: Saved the model to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_4/20240603_T005404/NonstationaryTransformer.pypots
2024-06-03 00:56:00 [INFO]: Successfully saved to results_point_rate05/PeMS/NonstationaryTransformer_PeMS/round_4/imputation.pkl
2024-06-03 00:56:00 [INFO]: Round4 - NonstationaryTransformer on PeMS: MAE=0.3823, MSE=0.6844, MRE=0.4744
2024-06-03 00:56:00 [INFO]: Done! Final results:
Averaged NonstationaryTransformer (346,318 params) on PeMS: MAE=0.3935 ± 0.012623637541866468, MSE=0.6879 ± 0.016016206643472433, MRE=0.4883 ± 0.015664863382605695, average inference time=0.24
