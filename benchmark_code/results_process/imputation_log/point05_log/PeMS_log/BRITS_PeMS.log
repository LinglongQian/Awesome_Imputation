2024-06-02 19:38:47 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:38:47 [INFO]: Using the given device: cuda:0
2024-06-02 19:38:47 [INFO]: Model files will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_0/20240602_T193847
2024-06-02 19:38:47 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_0/20240602_T193847/tensorboard
2024-06-02 19:38:49 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-02 19:39:55 [INFO]: Epoch 001 - training loss: 1.0303, validation loss: 0.6139
2024-06-02 19:41:00 [INFO]: Epoch 002 - training loss: 0.7026, validation loss: 0.5318
2024-06-02 19:42:07 [INFO]: Epoch 003 - training loss: 0.6151, validation loss: 0.5057
2024-06-02 19:43:17 [INFO]: Epoch 004 - training loss: 0.5786, validation loss: 0.4926
2024-06-02 19:44:19 [INFO]: Epoch 005 - training loss: 0.5521, validation loss: 0.4768
2024-06-02 19:45:25 [INFO]: Epoch 006 - training loss: 0.5365, validation loss: 0.4696
2024-06-02 19:46:25 [INFO]: Epoch 007 - training loss: 0.5172, validation loss: 0.4650
2024-06-02 19:47:28 [INFO]: Epoch 008 - training loss: 0.5065, validation loss: 0.4592
2024-06-02 19:48:28 [INFO]: Epoch 009 - training loss: 0.4931, validation loss: 0.4577
2024-06-02 19:49:24 [INFO]: Epoch 010 - training loss: 0.4829, validation loss: 0.4535
2024-06-02 19:50:19 [INFO]: Epoch 011 - training loss: 0.4787, validation loss: 0.4513
2024-06-02 19:51:24 [INFO]: Epoch 012 - training loss: 0.4700, validation loss: 0.4480
2024-06-02 19:52:18 [INFO]: Epoch 013 - training loss: 0.4622, validation loss: 0.4459
2024-06-02 19:53:09 [INFO]: Epoch 014 - training loss: 0.4538, validation loss: 0.4420
2024-06-02 19:53:59 [INFO]: Epoch 015 - training loss: 0.4473, validation loss: 0.4417
2024-06-02 19:54:49 [INFO]: Epoch 016 - training loss: 0.4441, validation loss: 0.4377
2024-06-02 19:55:38 [INFO]: Epoch 017 - training loss: 0.4359, validation loss: 0.4379
2024-06-02 19:56:28 [INFO]: Epoch 018 - training loss: 0.4340, validation loss: 0.4326
2024-06-02 19:57:15 [INFO]: Epoch 019 - training loss: 0.4243, validation loss: 0.4331
2024-06-02 19:58:01 [INFO]: Epoch 020 - training loss: 0.4180, validation loss: 0.4319
2024-06-02 19:58:52 [INFO]: Epoch 021 - training loss: 0.4189, validation loss: 0.4286
2024-06-02 19:59:40 [INFO]: Epoch 022 - training loss: 0.4138, validation loss: 0.4282
2024-06-02 20:00:27 [INFO]: Epoch 023 - training loss: 0.4067, validation loss: 0.4269
2024-06-02 20:01:12 [INFO]: Epoch 024 - training loss: 0.4032, validation loss: 0.4246
2024-06-02 20:02:01 [INFO]: Epoch 025 - training loss: 0.3988, validation loss: 0.4229
2024-06-02 20:02:44 [INFO]: Epoch 026 - training loss: 0.3920, validation loss: 0.4225
2024-06-02 20:03:25 [INFO]: Epoch 027 - training loss: 0.3927, validation loss: 0.4209
2024-06-02 20:04:02 [INFO]: Epoch 028 - training loss: 0.3876, validation loss: 0.4196
2024-06-02 20:04:39 [INFO]: Epoch 029 - training loss: 0.3860, validation loss: 0.4184
2024-06-02 20:05:21 [INFO]: Epoch 030 - training loss: 0.3807, validation loss: 0.4169
2024-06-02 20:06:03 [INFO]: Epoch 031 - training loss: 0.3793, validation loss: 0.4165
2024-06-02 20:06:44 [INFO]: Epoch 032 - training loss: 0.3722, validation loss: 0.4162
2024-06-02 20:07:26 [INFO]: Epoch 033 - training loss: 0.3734, validation loss: 0.4144
2024-06-02 20:08:05 [INFO]: Epoch 034 - training loss: 0.3688, validation loss: 0.4144
2024-06-02 20:08:35 [INFO]: Epoch 035 - training loss: 0.3670, validation loss: 0.4118
2024-06-02 20:08:51 [INFO]: Epoch 036 - training loss: 0.3629, validation loss: 0.4126
2024-06-02 20:09:10 [INFO]: Epoch 037 - training loss: 0.3604, validation loss: 0.4133
2024-06-02 20:09:23 [INFO]: Epoch 038 - training loss: 0.3556, validation loss: 0.4123
2024-06-02 20:09:44 [INFO]: Epoch 039 - training loss: 0.3540, validation loss: 0.4104
2024-06-02 20:09:59 [INFO]: Epoch 040 - training loss: 0.3551, validation loss: 0.4107
2024-06-02 20:10:19 [INFO]: Epoch 041 - training loss: 0.3487, validation loss: 0.4094
2024-06-02 20:10:34 [INFO]: Epoch 042 - training loss: 0.3499, validation loss: 0.4095
2024-06-02 20:10:51 [INFO]: Epoch 043 - training loss: 0.3467, validation loss: 0.4062
2024-06-02 20:11:09 [INFO]: Epoch 044 - training loss: 0.3458, validation loss: 0.4081
2024-06-02 20:11:24 [INFO]: Epoch 045 - training loss: 0.3414, validation loss: 0.4071
2024-06-02 20:11:44 [INFO]: Epoch 046 - training loss: 0.3412, validation loss: 0.4053
2024-06-02 20:11:58 [INFO]: Epoch 047 - training loss: 0.3376, validation loss: 0.4054
2024-06-02 20:12:19 [INFO]: Epoch 048 - training loss: 0.3376, validation loss: 0.4031
2024-06-02 20:12:34 [INFO]: Epoch 049 - training loss: 0.3355, validation loss: 0.4027
2024-06-02 20:12:51 [INFO]: Epoch 050 - training loss: 0.3341, validation loss: 0.4028
2024-06-02 20:13:09 [INFO]: Epoch 051 - training loss: 0.3315, validation loss: 0.4026
2024-06-02 20:13:24 [INFO]: Epoch 052 - training loss: 0.3318, validation loss: 0.4002
2024-06-02 20:13:44 [INFO]: Epoch 053 - training loss: 0.3272, validation loss: 0.4008
2024-06-02 20:13:59 [INFO]: Epoch 054 - training loss: 0.3276, validation loss: 0.4004
2024-06-02 20:14:19 [INFO]: Epoch 055 - training loss: 0.3254, validation loss: 0.4004
2024-06-02 20:14:34 [INFO]: Epoch 056 - training loss: 0.3254, validation loss: 0.3988
2024-06-02 20:14:52 [INFO]: Epoch 057 - training loss: 0.3220, validation loss: 0.3992
2024-06-02 20:15:09 [INFO]: Epoch 058 - training loss: 0.3209, validation loss: 0.3996
2024-06-02 20:15:24 [INFO]: Epoch 059 - training loss: 0.3180, validation loss: 0.3986
2024-06-02 20:15:45 [INFO]: Epoch 060 - training loss: 0.3162, validation loss: 0.3981
2024-06-02 20:15:59 [INFO]: Epoch 061 - training loss: 0.3158, validation loss: 0.3985
2024-06-02 20:16:20 [INFO]: Epoch 062 - training loss: 0.3131, validation loss: 0.3974
2024-06-02 20:16:35 [INFO]: Epoch 063 - training loss: 0.3138, validation loss: 0.3962
2024-06-02 20:16:52 [INFO]: Epoch 064 - training loss: 0.3121, validation loss: 0.3964
2024-06-02 20:17:10 [INFO]: Epoch 065 - training loss: 0.3112, validation loss: 0.3963
2024-06-02 20:17:25 [INFO]: Epoch 066 - training loss: 0.3082, validation loss: 0.3962
2024-06-02 20:17:45 [INFO]: Epoch 067 - training loss: 0.3066, validation loss: 0.3952
2024-06-02 20:18:00 [INFO]: Epoch 068 - training loss: 0.3030, validation loss: 0.3954
2024-06-02 20:18:20 [INFO]: Epoch 069 - training loss: 0.3078, validation loss: 0.3942
2024-06-02 20:18:35 [INFO]: Epoch 070 - training loss: 0.3031, validation loss: 0.3954
2024-06-02 20:18:53 [INFO]: Epoch 071 - training loss: 0.3013, validation loss: 0.3935
2024-06-02 20:19:10 [INFO]: Epoch 072 - training loss: 0.2996, validation loss: 0.3938
2024-06-02 20:19:25 [INFO]: Epoch 073 - training loss: 0.2981, validation loss: 0.3932
2024-06-02 20:19:45 [INFO]: Epoch 074 - training loss: 0.3014, validation loss: 0.3944
2024-06-02 20:19:59 [INFO]: Epoch 075 - training loss: 0.2973, validation loss: 0.3915
2024-06-02 20:20:20 [INFO]: Epoch 076 - training loss: 0.2959, validation loss: 0.3938
2024-06-02 20:20:35 [INFO]: Epoch 077 - training loss: 0.2947, validation loss: 0.3927
2024-06-02 20:20:53 [INFO]: Epoch 078 - training loss: 0.2931, validation loss: 0.3920
2024-06-02 20:21:10 [INFO]: Epoch 079 - training loss: 0.2925, validation loss: 0.3918
2024-06-02 20:21:26 [INFO]: Epoch 080 - training loss: 0.2901, validation loss: 0.3921
2024-06-02 20:21:45 [INFO]: Epoch 081 - training loss: 0.2913, validation loss: 0.3906
2024-06-02 20:22:00 [INFO]: Epoch 082 - training loss: 0.2858, validation loss: 0.3915
2024-06-02 20:22:21 [INFO]: Epoch 083 - training loss: 0.2865, validation loss: 0.3909
2024-06-02 20:22:35 [INFO]: Epoch 084 - training loss: 0.2884, validation loss: 0.3925
2024-06-02 20:22:53 [INFO]: Epoch 085 - training loss: 0.2863, validation loss: 0.3895
2024-06-02 20:23:11 [INFO]: Epoch 086 - training loss: 0.2836, validation loss: 0.3908
2024-06-02 20:23:26 [INFO]: Epoch 087 - training loss: 0.2845, validation loss: 0.3911
2024-06-02 20:23:47 [INFO]: Epoch 088 - training loss: 0.2839, validation loss: 0.3900
2024-06-02 20:24:01 [INFO]: Epoch 089 - training loss: 0.2822, validation loss: 0.3904
2024-06-02 20:24:22 [INFO]: Epoch 090 - training loss: 0.2808, validation loss: 0.3898
2024-06-02 20:24:37 [INFO]: Epoch 091 - training loss: 0.2810, validation loss: 0.3895
2024-06-02 20:24:54 [INFO]: Epoch 092 - training loss: 0.2773, validation loss: 0.3893
2024-06-02 20:25:12 [INFO]: Epoch 093 - training loss: 0.2775, validation loss: 0.3881
2024-06-02 20:25:26 [INFO]: Epoch 094 - training loss: 0.2798, validation loss: 0.3900
2024-06-02 20:25:46 [INFO]: Epoch 095 - training loss: 0.2759, validation loss: 0.3899
2024-06-02 20:26:00 [INFO]: Epoch 096 - training loss: 0.2770, validation loss: 0.3892
2024-06-02 20:26:22 [INFO]: Epoch 097 - training loss: 0.2755, validation loss: 0.3886
2024-06-02 20:26:36 [INFO]: Epoch 098 - training loss: 0.2768, validation loss: 0.3883
2024-06-02 20:26:54 [INFO]: Epoch 099 - training loss: 0.2745, validation loss: 0.3880
2024-06-02 20:27:12 [INFO]: Epoch 100 - training loss: 0.2719, validation loss: 0.3881
2024-06-02 20:27:12 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 20:27:12 [INFO]: Saved the model to results_point_rate05/PeMS/BRITS_PeMS/round_0/20240602_T193847/BRITS.pypots
2024-06-02 20:27:29 [INFO]: Successfully saved to results_point_rate05/PeMS/BRITS_PeMS/round_0/imputation.pkl
2024-06-02 20:27:29 [INFO]: Round0 - BRITS on PeMS: MAE=0.2877, MSE=0.5617, MRE=0.3570
2024-06-02 20:27:29 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 20:27:29 [INFO]: Using the given device: cuda:0
2024-06-02 20:27:29 [INFO]: Model files will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_1/20240602_T202729
2024-06-02 20:27:29 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_1/20240602_T202729/tensorboard
2024-06-02 20:27:30 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-02 20:27:51 [INFO]: Epoch 001 - training loss: 1.0274, validation loss: 0.6101
2024-06-02 20:28:06 [INFO]: Epoch 002 - training loss: 0.7028, validation loss: 0.5329
2024-06-02 20:28:24 [INFO]: Epoch 003 - training loss: 0.6225, validation loss: 0.5049
2024-06-02 20:28:41 [INFO]: Epoch 004 - training loss: 0.5820, validation loss: 0.4865
2024-06-02 20:28:56 [INFO]: Epoch 005 - training loss: 0.5594, validation loss: 0.4779
2024-06-02 20:29:16 [INFO]: Epoch 006 - training loss: 0.5349, validation loss: 0.4674
2024-06-02 20:29:30 [INFO]: Epoch 007 - training loss: 0.5208, validation loss: 0.4645
2024-06-02 20:29:51 [INFO]: Epoch 008 - training loss: 0.5059, validation loss: 0.4618
2024-06-02 20:30:06 [INFO]: Epoch 009 - training loss: 0.4955, validation loss: 0.4572
2024-06-02 20:30:24 [INFO]: Epoch 010 - training loss: 0.4804, validation loss: 0.4529
2024-06-02 20:30:42 [INFO]: Epoch 011 - training loss: 0.4776, validation loss: 0.4496
2024-06-02 20:30:57 [INFO]: Epoch 012 - training loss: 0.4685, validation loss: 0.4486
2024-06-02 20:31:17 [INFO]: Epoch 013 - training loss: 0.4621, validation loss: 0.4470
2024-06-02 20:31:31 [INFO]: Epoch 014 - training loss: 0.4564, validation loss: 0.4430
2024-06-02 20:31:52 [INFO]: Epoch 015 - training loss: 0.4476, validation loss: 0.4435
2024-06-02 20:32:07 [INFO]: Epoch 016 - training loss: 0.4461, validation loss: 0.4401
2024-06-02 20:32:25 [INFO]: Epoch 017 - training loss: 0.4368, validation loss: 0.4373
2024-06-02 20:32:42 [INFO]: Epoch 018 - training loss: 0.4325, validation loss: 0.4352
2024-06-02 20:32:58 [INFO]: Epoch 019 - training loss: 0.4276, validation loss: 0.4342
2024-06-02 20:33:18 [INFO]: Epoch 020 - training loss: 0.4190, validation loss: 0.4311
2024-06-02 20:33:32 [INFO]: Epoch 021 - training loss: 0.4132, validation loss: 0.4305
2024-06-02 20:33:53 [INFO]: Epoch 022 - training loss: 0.4121, validation loss: 0.4273
2024-06-02 20:34:08 [INFO]: Epoch 023 - training loss: 0.4083, validation loss: 0.4276
2024-06-02 20:34:26 [INFO]: Epoch 024 - training loss: 0.3986, validation loss: 0.4243
2024-06-02 20:34:44 [INFO]: Epoch 025 - training loss: 0.3989, validation loss: 0.4239
2024-06-02 20:34:58 [INFO]: Epoch 026 - training loss: 0.3932, validation loss: 0.4236
2024-06-02 20:35:19 [INFO]: Epoch 027 - training loss: 0.3887, validation loss: 0.4222
2024-06-02 20:35:33 [INFO]: Epoch 028 - training loss: 0.3903, validation loss: 0.4197
2024-06-02 20:35:54 [INFO]: Epoch 029 - training loss: 0.3844, validation loss: 0.4187
2024-06-02 20:36:09 [INFO]: Epoch 030 - training loss: 0.3770, validation loss: 0.4173
2024-06-02 20:36:26 [INFO]: Epoch 031 - training loss: 0.3780, validation loss: 0.4189
2024-06-02 20:36:44 [INFO]: Epoch 032 - training loss: 0.3752, validation loss: 0.4155
2024-06-02 20:36:59 [INFO]: Epoch 033 - training loss: 0.3727, validation loss: 0.4140
2024-06-02 20:37:19 [INFO]: Epoch 034 - training loss: 0.3662, validation loss: 0.4136
2024-06-02 20:37:34 [INFO]: Epoch 035 - training loss: 0.3656, validation loss: 0.4123
2024-06-02 20:37:54 [INFO]: Epoch 036 - training loss: 0.3636, validation loss: 0.4116
2024-06-02 20:38:09 [INFO]: Epoch 037 - training loss: 0.3622, validation loss: 0.4111
2024-06-02 20:38:27 [INFO]: Epoch 038 - training loss: 0.3560, validation loss: 0.4121
2024-06-02 20:38:44 [INFO]: Epoch 039 - training loss: 0.3546, validation loss: 0.4117
2024-06-02 20:38:59 [INFO]: Epoch 040 - training loss: 0.3541, validation loss: 0.4087
2024-06-02 20:39:19 [INFO]: Epoch 041 - training loss: 0.3518, validation loss: 0.4095
2024-06-02 20:39:34 [INFO]: Epoch 042 - training loss: 0.3459, validation loss: 0.4087
2024-06-02 20:39:55 [INFO]: Epoch 043 - training loss: 0.3451, validation loss: 0.4072
2024-06-02 20:40:10 [INFO]: Epoch 044 - training loss: 0.3459, validation loss: 0.4061
2024-06-02 20:40:30 [INFO]: Epoch 045 - training loss: 0.3427, validation loss: 0.4069
2024-06-02 20:40:52 [INFO]: Epoch 046 - training loss: 0.3417, validation loss: 0.4060
2024-06-02 20:41:14 [INFO]: Epoch 047 - training loss: 0.3405, validation loss: 0.4051
2024-06-02 20:41:35 [INFO]: Epoch 048 - training loss: 0.3368, validation loss: 0.4051
2024-06-02 20:41:57 [INFO]: Epoch 049 - training loss: 0.3334, validation loss: 0.4032
2024-06-02 20:42:19 [INFO]: Epoch 050 - training loss: 0.3333, validation loss: 0.4048
2024-06-02 20:42:40 [INFO]: Epoch 051 - training loss: 0.3316, validation loss: 0.4046
2024-06-02 20:43:02 [INFO]: Epoch 052 - training loss: 0.3307, validation loss: 0.4010
2024-06-02 20:43:23 [INFO]: Epoch 053 - training loss: 0.3301, validation loss: 0.4029
2024-06-02 20:43:45 [INFO]: Epoch 054 - training loss: 0.3264, validation loss: 0.4014
2024-06-02 20:44:07 [INFO]: Epoch 055 - training loss: 0.3246, validation loss: 0.4016
2024-06-02 20:44:28 [INFO]: Epoch 056 - training loss: 0.3255, validation loss: 0.4006
2024-06-02 20:44:50 [INFO]: Epoch 057 - training loss: 0.3208, validation loss: 0.3994
2024-06-02 20:45:12 [INFO]: Epoch 058 - training loss: 0.3185, validation loss: 0.4003
2024-06-02 20:45:32 [INFO]: Epoch 059 - training loss: 0.3187, validation loss: 0.3996
2024-06-02 20:45:53 [INFO]: Epoch 060 - training loss: 0.3152, validation loss: 0.3996
2024-06-02 20:46:15 [INFO]: Epoch 061 - training loss: 0.3134, validation loss: 0.3995
2024-06-02 20:46:37 [INFO]: Epoch 062 - training loss: 0.3134, validation loss: 0.3994
2024-06-02 20:46:58 [INFO]: Epoch 063 - training loss: 0.3122, validation loss: 0.3982
2024-06-02 20:47:20 [INFO]: Epoch 064 - training loss: 0.3114, validation loss: 0.3971
2024-06-02 20:47:41 [INFO]: Epoch 065 - training loss: 0.3091, validation loss: 0.3978
2024-06-02 20:48:03 [INFO]: Epoch 066 - training loss: 0.3104, validation loss: 0.3962
2024-06-02 20:48:25 [INFO]: Epoch 067 - training loss: 0.3057, validation loss: 0.3980
2024-06-02 20:48:46 [INFO]: Epoch 068 - training loss: 0.3061, validation loss: 0.3967
2024-06-02 20:49:08 [INFO]: Epoch 069 - training loss: 0.3064, validation loss: 0.3949
2024-06-02 20:49:30 [INFO]: Epoch 070 - training loss: 0.3038, validation loss: 0.3956
2024-06-02 20:49:51 [INFO]: Epoch 071 - training loss: 0.3004, validation loss: 0.3957
2024-06-02 20:50:13 [INFO]: Epoch 072 - training loss: 0.2994, validation loss: 0.3942
2024-06-02 20:50:35 [INFO]: Epoch 073 - training loss: 0.2983, validation loss: 0.3943
2024-06-02 20:50:56 [INFO]: Epoch 074 - training loss: 0.2949, validation loss: 0.3946
2024-06-02 20:51:18 [INFO]: Epoch 075 - training loss: 0.2980, validation loss: 0.3934
2024-06-02 20:51:39 [INFO]: Epoch 076 - training loss: 0.2956, validation loss: 0.3931
2024-06-02 20:52:01 [INFO]: Epoch 077 - training loss: 0.2931, validation loss: 0.3950
2024-06-02 20:52:23 [INFO]: Epoch 078 - training loss: 0.2925, validation loss: 0.3922
2024-06-02 20:52:44 [INFO]: Epoch 079 - training loss: 0.2918, validation loss: 0.3950
2024-06-02 20:53:06 [INFO]: Epoch 080 - training loss: 0.2876, validation loss: 0.3923
2024-06-02 20:53:28 [INFO]: Epoch 081 - training loss: 0.2941, validation loss: 0.3917
2024-06-02 20:53:49 [INFO]: Epoch 082 - training loss: 0.2898, validation loss: 0.3925
2024-06-02 20:54:11 [INFO]: Epoch 083 - training loss: 0.2907, validation loss: 0.3905
2024-06-02 20:54:32 [INFO]: Epoch 084 - training loss: 0.2890, validation loss: 0.3927
2024-06-02 20:54:54 [INFO]: Epoch 085 - training loss: 0.2859, validation loss: 0.3936
2024-06-02 20:55:16 [INFO]: Epoch 086 - training loss: 0.2842, validation loss: 0.3901
2024-06-02 20:55:38 [INFO]: Epoch 087 - training loss: 0.2814, validation loss: 0.3919
2024-06-02 20:55:59 [INFO]: Epoch 088 - training loss: 0.2814, validation loss: 0.3912
2024-06-02 20:56:21 [INFO]: Epoch 089 - training loss: 0.2803, validation loss: 0.3915
2024-06-02 20:56:42 [INFO]: Epoch 090 - training loss: 0.2812, validation loss: 0.3901
2024-06-02 20:57:04 [INFO]: Epoch 091 - training loss: 0.2795, validation loss: 0.3901
2024-06-02 20:57:26 [INFO]: Epoch 092 - training loss: 0.2767, validation loss: 0.3917
2024-06-02 20:57:47 [INFO]: Epoch 093 - training loss: 0.2795, validation loss: 0.3898
2024-06-02 20:58:09 [INFO]: Epoch 094 - training loss: 0.2791, validation loss: 0.3908
2024-06-02 20:58:31 [INFO]: Epoch 095 - training loss: 0.2757, validation loss: 0.3889
2024-06-02 20:58:52 [INFO]: Epoch 096 - training loss: 0.2770, validation loss: 0.3896
2024-06-02 20:59:14 [INFO]: Epoch 097 - training loss: 0.2765, validation loss: 0.3880
2024-06-02 20:59:36 [INFO]: Epoch 098 - training loss: 0.2726, validation loss: 0.3882
2024-06-02 20:59:57 [INFO]: Epoch 099 - training loss: 0.2749, validation loss: 0.3881
2024-06-02 21:00:19 [INFO]: Epoch 100 - training loss: 0.2747, validation loss: 0.3889
2024-06-02 21:00:19 [INFO]: Finished training. The best model is from epoch#97.
2024-06-02 21:00:19 [INFO]: Saved the model to results_point_rate05/PeMS/BRITS_PeMS/round_1/20240602_T202729/BRITS.pypots
2024-06-02 21:00:42 [INFO]: Successfully saved to results_point_rate05/PeMS/BRITS_PeMS/round_1/imputation.pkl
2024-06-02 21:00:42 [INFO]: Round1 - BRITS on PeMS: MAE=0.2882, MSE=0.5611, MRE=0.3576
2024-06-02 21:00:42 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 21:00:42 [INFO]: Using the given device: cuda:0
2024-06-02 21:00:42 [INFO]: Model files will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_2/20240602_T210042
2024-06-02 21:00:42 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_2/20240602_T210042/tensorboard
2024-06-02 21:00:42 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-02 21:01:04 [INFO]: Epoch 001 - training loss: 1.0309, validation loss: 0.6171
2024-06-02 21:01:26 [INFO]: Epoch 002 - training loss: 0.7016, validation loss: 0.5318
2024-06-02 21:01:48 [INFO]: Epoch 003 - training loss: 0.6229, validation loss: 0.5010
2024-06-02 21:02:09 [INFO]: Epoch 004 - training loss: 0.5771, validation loss: 0.4867
2024-06-02 21:02:31 [INFO]: Epoch 005 - training loss: 0.5565, validation loss: 0.4771
2024-06-02 21:02:53 [INFO]: Epoch 006 - training loss: 0.5334, validation loss: 0.4710
2024-06-02 21:03:14 [INFO]: Epoch 007 - training loss: 0.5197, validation loss: 0.4669
2024-06-02 21:03:36 [INFO]: Epoch 008 - training loss: 0.5119, validation loss: 0.4607
2024-06-02 21:03:57 [INFO]: Epoch 009 - training loss: 0.4934, validation loss: 0.4578
2024-06-02 21:04:19 [INFO]: Epoch 010 - training loss: 0.4868, validation loss: 0.4542
2024-06-02 21:04:41 [INFO]: Epoch 011 - training loss: 0.4768, validation loss: 0.4531
2024-06-02 21:04:53 [INFO]: Epoch 012 - training loss: 0.4682, validation loss: 0.4485
2024-06-02 21:05:13 [INFO]: Epoch 013 - training loss: 0.4629, validation loss: 0.4459
2024-06-02 21:05:28 [INFO]: Epoch 014 - training loss: 0.4547, validation loss: 0.4433
2024-06-02 21:05:46 [INFO]: Epoch 015 - training loss: 0.4470, validation loss: 0.4431
2024-06-02 21:06:03 [INFO]: Epoch 016 - training loss: 0.4464, validation loss: 0.4391
2024-06-02 21:06:18 [INFO]: Epoch 017 - training loss: 0.4366, validation loss: 0.4389
2024-06-02 21:06:39 [INFO]: Epoch 018 - training loss: 0.4301, validation loss: 0.4359
2024-06-02 21:06:53 [INFO]: Epoch 019 - training loss: 0.4256, validation loss: 0.4323
2024-06-02 21:07:14 [INFO]: Epoch 020 - training loss: 0.4204, validation loss: 0.4311
2024-06-02 21:07:29 [INFO]: Epoch 021 - training loss: 0.4175, validation loss: 0.4282
2024-06-02 21:07:47 [INFO]: Epoch 022 - training loss: 0.4117, validation loss: 0.4278
2024-06-02 21:08:04 [INFO]: Epoch 023 - training loss: 0.4096, validation loss: 0.4260
2024-06-02 21:08:19 [INFO]: Epoch 024 - training loss: 0.4059, validation loss: 0.4241
2024-06-02 21:08:39 [INFO]: Epoch 025 - training loss: 0.4009, validation loss: 0.4212
2024-06-02 21:08:54 [INFO]: Epoch 026 - training loss: 0.3949, validation loss: 0.4215
2024-06-02 21:09:15 [INFO]: Epoch 027 - training loss: 0.3877, validation loss: 0.4224
2024-06-02 21:09:30 [INFO]: Epoch 028 - training loss: 0.3897, validation loss: 0.4191
2024-06-02 21:09:47 [INFO]: Epoch 029 - training loss: 0.3816, validation loss: 0.4184
2024-06-02 21:10:05 [INFO]: Epoch 030 - training loss: 0.3783, validation loss: 0.4168
2024-06-02 21:10:20 [INFO]: Epoch 031 - training loss: 0.3767, validation loss: 0.4158
2024-06-02 21:10:40 [INFO]: Epoch 032 - training loss: 0.3746, validation loss: 0.4144
2024-06-02 21:10:55 [INFO]: Epoch 033 - training loss: 0.3708, validation loss: 0.4167
2024-06-02 21:11:15 [INFO]: Epoch 034 - training loss: 0.3683, validation loss: 0.4130
2024-06-02 21:11:30 [INFO]: Epoch 035 - training loss: 0.3659, validation loss: 0.4116
2024-06-02 21:11:48 [INFO]: Epoch 036 - training loss: 0.3620, validation loss: 0.4119
2024-06-02 21:12:05 [INFO]: Epoch 037 - training loss: 0.3627, validation loss: 0.4091
2024-06-02 21:12:20 [INFO]: Epoch 038 - training loss: 0.3586, validation loss: 0.4087
2024-06-02 21:12:40 [INFO]: Epoch 039 - training loss: 0.3575, validation loss: 0.4085
2024-06-02 21:12:54 [INFO]: Epoch 040 - training loss: 0.3545, validation loss: 0.4105
2024-06-02 21:13:15 [INFO]: Epoch 041 - training loss: 0.3548, validation loss: 0.4076
2024-06-02 21:13:30 [INFO]: Epoch 042 - training loss: 0.3485, validation loss: 0.4063
2024-06-02 21:13:48 [INFO]: Epoch 043 - training loss: 0.3449, validation loss: 0.4055
2024-06-02 21:14:05 [INFO]: Epoch 044 - training loss: 0.3450, validation loss: 0.4054
2024-06-02 21:14:21 [INFO]: Epoch 045 - training loss: 0.3400, validation loss: 0.4057
2024-06-02 21:14:41 [INFO]: Epoch 046 - training loss: 0.3389, validation loss: 0.4056
2024-06-02 21:14:55 [INFO]: Epoch 047 - training loss: 0.3380, validation loss: 0.4032
2024-06-02 21:15:16 [INFO]: Epoch 048 - training loss: 0.3398, validation loss: 0.4045
2024-06-02 21:15:31 [INFO]: Epoch 049 - training loss: 0.3356, validation loss: 0.4023
2024-06-02 21:15:49 [INFO]: Epoch 050 - training loss: 0.3345, validation loss: 0.4036
2024-06-02 21:16:06 [INFO]: Epoch 051 - training loss: 0.3327, validation loss: 0.4026
2024-06-02 21:16:21 [INFO]: Epoch 052 - training loss: 0.3292, validation loss: 0.4018
2024-06-02 21:16:41 [INFO]: Epoch 053 - training loss: 0.3308, validation loss: 0.3999
2024-06-02 21:16:56 [INFO]: Epoch 054 - training loss: 0.3253, validation loss: 0.4012
2024-06-02 21:17:17 [INFO]: Epoch 055 - training loss: 0.3233, validation loss: 0.3999
2024-06-02 21:17:32 [INFO]: Epoch 056 - training loss: 0.3222, validation loss: 0.4005
2024-06-02 21:17:50 [INFO]: Epoch 057 - training loss: 0.3217, validation loss: 0.3984
2024-06-02 21:18:07 [INFO]: Epoch 058 - training loss: 0.3204, validation loss: 0.3988
2024-06-02 21:18:23 [INFO]: Epoch 059 - training loss: 0.3145, validation loss: 0.3986
2024-06-02 21:18:43 [INFO]: Epoch 060 - training loss: 0.3190, validation loss: 0.4001
2024-06-02 21:18:58 [INFO]: Epoch 061 - training loss: 0.3149, validation loss: 0.3959
2024-06-02 21:19:18 [INFO]: Epoch 062 - training loss: 0.3150, validation loss: 0.3981
2024-06-02 21:19:33 [INFO]: Epoch 063 - training loss: 0.3140, validation loss: 0.3961
2024-06-02 21:19:51 [INFO]: Epoch 064 - training loss: 0.3109, validation loss: 0.3956
2024-06-02 21:20:08 [INFO]: Epoch 065 - training loss: 0.3081, validation loss: 0.3952
2024-06-02 21:20:23 [INFO]: Epoch 066 - training loss: 0.3050, validation loss: 0.3958
2024-06-02 21:20:44 [INFO]: Epoch 067 - training loss: 0.3059, validation loss: 0.3963
2024-06-02 21:20:58 [INFO]: Epoch 068 - training loss: 0.3048, validation loss: 0.3943
2024-06-02 21:21:19 [INFO]: Epoch 069 - training loss: 0.3034, validation loss: 0.3945
2024-06-02 21:21:34 [INFO]: Epoch 070 - training loss: 0.3038, validation loss: 0.3940
2024-06-02 21:21:51 [INFO]: Epoch 071 - training loss: 0.3026, validation loss: 0.3938
2024-06-02 21:22:09 [INFO]: Epoch 072 - training loss: 0.3033, validation loss: 0.3952
2024-06-02 21:22:24 [INFO]: Epoch 073 - training loss: 0.3003, validation loss: 0.3943
2024-06-02 21:22:44 [INFO]: Epoch 074 - training loss: 0.2986, validation loss: 0.3924
2024-06-02 21:22:59 [INFO]: Epoch 075 - training loss: 0.2965, validation loss: 0.3929
2024-06-02 21:23:20 [INFO]: Epoch 076 - training loss: 0.2941, validation loss: 0.3941
2024-06-02 21:23:35 [INFO]: Epoch 077 - training loss: 0.2978, validation loss: 0.3939
2024-06-02 21:23:52 [INFO]: Epoch 078 - training loss: 0.2956, validation loss: 0.3933
2024-06-02 21:24:10 [INFO]: Epoch 079 - training loss: 0.2959, validation loss: 0.3912
2024-06-02 21:24:24 [INFO]: Epoch 080 - training loss: 0.2929, validation loss: 0.3908
2024-06-02 21:24:44 [INFO]: Epoch 081 - training loss: 0.2907, validation loss: 0.3914
2024-06-02 21:24:59 [INFO]: Epoch 082 - training loss: 0.2884, validation loss: 0.3904
2024-06-02 21:25:20 [INFO]: Epoch 083 - training loss: 0.2884, validation loss: 0.3904
2024-06-02 21:25:34 [INFO]: Epoch 084 - training loss: 0.2859, validation loss: 0.3906
2024-06-02 21:25:53 [INFO]: Epoch 085 - training loss: 0.2844, validation loss: 0.3907
2024-06-02 21:26:09 [INFO]: Epoch 086 - training loss: 0.2872, validation loss: 0.3905
2024-06-02 21:26:25 [INFO]: Epoch 087 - training loss: 0.2847, validation loss: 0.3914
2024-06-02 21:26:45 [INFO]: Epoch 088 - training loss: 0.2838, validation loss: 0.3894
2024-06-02 21:26:59 [INFO]: Epoch 089 - training loss: 0.2851, validation loss: 0.3895
2024-06-02 21:27:20 [INFO]: Epoch 090 - training loss: 0.2823, validation loss: 0.3888
2024-06-02 21:27:35 [INFO]: Epoch 091 - training loss: 0.2811, validation loss: 0.3884
2024-06-02 21:27:53 [INFO]: Epoch 092 - training loss: 0.2788, validation loss: 0.3885
2024-06-02 21:28:11 [INFO]: Epoch 093 - training loss: 0.2774, validation loss: 0.3890
2024-06-02 21:28:26 [INFO]: Epoch 094 - training loss: 0.2782, validation loss: 0.3883
2024-06-02 21:28:46 [INFO]: Epoch 095 - training loss: 0.2761, validation loss: 0.3888
2024-06-02 21:29:00 [INFO]: Epoch 096 - training loss: 0.2760, validation loss: 0.3890
2024-06-02 21:29:21 [INFO]: Epoch 097 - training loss: 0.2767, validation loss: 0.3876
2024-06-02 21:29:37 [INFO]: Epoch 098 - training loss: 0.2769, validation loss: 0.3878
2024-06-02 21:29:54 [INFO]: Epoch 099 - training loss: 0.2743, validation loss: 0.3877
2024-06-02 21:30:12 [INFO]: Epoch 100 - training loss: 0.2740, validation loss: 0.3875
2024-06-02 21:30:12 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 21:30:12 [INFO]: Saved the model to results_point_rate05/PeMS/BRITS_PeMS/round_2/20240602_T210042/BRITS.pypots
2024-06-02 21:30:29 [INFO]: Successfully saved to results_point_rate05/PeMS/BRITS_PeMS/round_2/imputation.pkl
2024-06-02 21:30:29 [INFO]: Round2 - BRITS on PeMS: MAE=0.2870, MSE=0.5588, MRE=0.3562
2024-06-02 21:30:29 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 21:30:29 [INFO]: Using the given device: cuda:0
2024-06-02 21:30:29 [INFO]: Model files will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_3/20240602_T213029
2024-06-02 21:30:29 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_3/20240602_T213029/tensorboard
2024-06-02 21:30:29 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-02 21:30:51 [INFO]: Epoch 001 - training loss: 1.0355, validation loss: 0.6181
2024-06-02 21:31:05 [INFO]: Epoch 002 - training loss: 0.7050, validation loss: 0.5349
2024-06-02 21:31:24 [INFO]: Epoch 003 - training loss: 0.6256, validation loss: 0.5028
2024-06-02 21:31:41 [INFO]: Epoch 004 - training loss: 0.5791, validation loss: 0.4843
2024-06-02 21:31:56 [INFO]: Epoch 005 - training loss: 0.5550, validation loss: 0.4751
2024-06-02 21:32:16 [INFO]: Epoch 006 - training loss: 0.5343, validation loss: 0.4687
2024-06-02 21:32:30 [INFO]: Epoch 007 - training loss: 0.5193, validation loss: 0.4631
2024-06-02 21:32:52 [INFO]: Epoch 008 - training loss: 0.5075, validation loss: 0.4590
2024-06-02 21:33:06 [INFO]: Epoch 009 - training loss: 0.4999, validation loss: 0.4552
2024-06-02 21:33:24 [INFO]: Epoch 010 - training loss: 0.4884, validation loss: 0.4512
2024-06-02 21:33:41 [INFO]: Epoch 011 - training loss: 0.4769, validation loss: 0.4530
2024-06-02 21:33:57 [INFO]: Epoch 012 - training loss: 0.4689, validation loss: 0.4471
2024-06-02 21:34:16 [INFO]: Epoch 013 - training loss: 0.4628, validation loss: 0.4452
2024-06-02 21:34:31 [INFO]: Epoch 014 - training loss: 0.4558, validation loss: 0.4432
2024-06-02 21:34:52 [INFO]: Epoch 015 - training loss: 0.4474, validation loss: 0.4407
2024-06-02 21:35:07 [INFO]: Epoch 016 - training loss: 0.4422, validation loss: 0.4363
2024-06-02 21:35:25 [INFO]: Epoch 017 - training loss: 0.4364, validation loss: 0.4357
2024-06-02 21:35:42 [INFO]: Epoch 018 - training loss: 0.4304, validation loss: 0.4327
2024-06-02 21:35:57 [INFO]: Epoch 019 - training loss: 0.4294, validation loss: 0.4305
2024-06-02 21:36:17 [INFO]: Epoch 020 - training loss: 0.4189, validation loss: 0.4299
2024-06-02 21:36:31 [INFO]: Epoch 021 - training loss: 0.4146, validation loss: 0.4275
2024-06-02 21:36:52 [INFO]: Epoch 022 - training loss: 0.4143, validation loss: 0.4269
2024-06-02 21:37:07 [INFO]: Epoch 023 - training loss: 0.4058, validation loss: 0.4272
2024-06-02 21:37:25 [INFO]: Epoch 024 - training loss: 0.4027, validation loss: 0.4242
2024-06-02 21:37:42 [INFO]: Epoch 025 - training loss: 0.3977, validation loss: 0.4213
2024-06-02 21:37:58 [INFO]: Epoch 026 - training loss: 0.3983, validation loss: 0.4201
2024-06-02 21:38:17 [INFO]: Epoch 027 - training loss: 0.3912, validation loss: 0.4203
2024-06-02 21:38:31 [INFO]: Epoch 028 - training loss: 0.3878, validation loss: 0.4184
2024-06-02 21:38:53 [INFO]: Epoch 029 - training loss: 0.3835, validation loss: 0.4185
2024-06-02 21:39:07 [INFO]: Epoch 030 - training loss: 0.3827, validation loss: 0.4164
2024-06-02 21:39:26 [INFO]: Epoch 031 - training loss: 0.3758, validation loss: 0.4163
2024-06-02 21:39:43 [INFO]: Epoch 032 - training loss: 0.3771, validation loss: 0.4151
2024-06-02 21:39:59 [INFO]: Epoch 033 - training loss: 0.3722, validation loss: 0.4144
2024-06-02 21:40:18 [INFO]: Epoch 034 - training loss: 0.3697, validation loss: 0.4130
2024-06-02 21:40:32 [INFO]: Epoch 035 - training loss: 0.3681, validation loss: 0.4116
2024-06-02 21:40:53 [INFO]: Epoch 036 - training loss: 0.3622, validation loss: 0.4107
2024-06-02 21:41:08 [INFO]: Epoch 037 - training loss: 0.3622, validation loss: 0.4113
2024-06-02 21:41:27 [INFO]: Epoch 038 - training loss: 0.3601, validation loss: 0.4106
2024-06-02 21:41:44 [INFO]: Epoch 039 - training loss: 0.3571, validation loss: 0.4101
2024-06-02 21:41:59 [INFO]: Epoch 040 - training loss: 0.3551, validation loss: 0.4076
2024-06-02 21:42:19 [INFO]: Epoch 041 - training loss: 0.3549, validation loss: 0.4076
2024-06-02 21:42:33 [INFO]: Epoch 042 - training loss: 0.3505, validation loss: 0.4087
2024-06-02 21:42:54 [INFO]: Epoch 043 - training loss: 0.3477, validation loss: 0.4077
2024-06-02 21:43:09 [INFO]: Epoch 044 - training loss: 0.3474, validation loss: 0.4060
2024-06-02 21:43:27 [INFO]: Epoch 045 - training loss: 0.3410, validation loss: 0.4057
2024-06-02 21:43:44 [INFO]: Epoch 046 - training loss: 0.3389, validation loss: 0.4053
2024-06-02 21:44:00 [INFO]: Epoch 047 - training loss: 0.3387, validation loss: 0.4044
2024-06-02 21:44:18 [INFO]: Epoch 048 - training loss: 0.3397, validation loss: 0.4034
2024-06-02 21:44:32 [INFO]: Epoch 049 - training loss: 0.3358, validation loss: 0.4047
2024-06-02 21:44:53 [INFO]: Epoch 050 - training loss: 0.3324, validation loss: 0.4026
2024-06-02 21:45:08 [INFO]: Epoch 051 - training loss: 0.3318, validation loss: 0.4017
2024-06-02 21:45:28 [INFO]: Epoch 052 - training loss: 0.3299, validation loss: 0.4034
2024-06-02 21:45:43 [INFO]: Epoch 053 - training loss: 0.3301, validation loss: 0.4015
2024-06-02 21:46:00 [INFO]: Epoch 054 - training loss: 0.3285, validation loss: 0.3993
2024-06-02 21:46:19 [INFO]: Epoch 055 - training loss: 0.3260, validation loss: 0.4012
2024-06-02 21:46:33 [INFO]: Epoch 056 - training loss: 0.3239, validation loss: 0.4005
2024-06-02 21:46:54 [INFO]: Epoch 057 - training loss: 0.3218, validation loss: 0.3993
2024-06-02 21:47:09 [INFO]: Epoch 058 - training loss: 0.3170, validation loss: 0.4002
2024-06-02 21:47:28 [INFO]: Epoch 059 - training loss: 0.3195, validation loss: 0.3995
2024-06-02 21:47:45 [INFO]: Epoch 060 - training loss: 0.3161, validation loss: 0.3990
2024-06-02 21:48:01 [INFO]: Epoch 061 - training loss: 0.3166, validation loss: 0.3991
2024-06-02 21:48:20 [INFO]: Epoch 062 - training loss: 0.3140, validation loss: 0.3978
2024-06-02 21:48:34 [INFO]: Epoch 063 - training loss: 0.3137, validation loss: 0.3985
2024-06-02 21:48:55 [INFO]: Epoch 064 - training loss: 0.3125, validation loss: 0.3962
2024-06-02 21:49:10 [INFO]: Epoch 065 - training loss: 0.3116, validation loss: 0.3970
2024-06-02 21:49:29 [INFO]: Epoch 066 - training loss: 0.3092, validation loss: 0.3964
2024-06-02 21:49:45 [INFO]: Epoch 067 - training loss: 0.3072, validation loss: 0.3958
2024-06-02 21:50:02 [INFO]: Epoch 068 - training loss: 0.3062, validation loss: 0.3959
2024-06-02 21:50:20 [INFO]: Epoch 069 - training loss: 0.3041, validation loss: 0.3968
2024-06-02 21:50:35 [INFO]: Epoch 070 - training loss: 0.3043, validation loss: 0.3945
2024-06-02 21:50:56 [INFO]: Epoch 071 - training loss: 0.3033, validation loss: 0.3937
2024-06-02 21:51:10 [INFO]: Epoch 072 - training loss: 0.3035, validation loss: 0.3949
2024-06-02 21:51:30 [INFO]: Epoch 073 - training loss: 0.3015, validation loss: 0.3937
2024-06-02 21:51:46 [INFO]: Epoch 074 - training loss: 0.2994, validation loss: 0.3935
2024-06-02 21:52:03 [INFO]: Epoch 075 - training loss: 0.2960, validation loss: 0.3927
2024-06-02 21:52:21 [INFO]: Epoch 076 - training loss: 0.2965, validation loss: 0.3926
2024-06-02 21:52:35 [INFO]: Epoch 077 - training loss: 0.2940, validation loss: 0.3920
2024-06-02 21:52:57 [INFO]: Epoch 078 - training loss: 0.2945, validation loss: 0.3921
2024-06-02 21:53:11 [INFO]: Epoch 079 - training loss: 0.2904, validation loss: 0.3936
2024-06-02 21:53:30 [INFO]: Epoch 080 - training loss: 0.2913, validation loss: 0.3919
2024-06-02 21:53:46 [INFO]: Epoch 081 - training loss: 0.2898, validation loss: 0.3921
2024-06-02 21:54:03 [INFO]: Epoch 082 - training loss: 0.2901, validation loss: 0.3899
2024-06-02 21:54:21 [INFO]: Epoch 083 - training loss: 0.2879, validation loss: 0.3918
2024-06-02 21:54:35 [INFO]: Epoch 084 - training loss: 0.2848, validation loss: 0.3919
2024-06-02 21:54:56 [INFO]: Epoch 085 - training loss: 0.2870, validation loss: 0.3910
2024-06-02 21:55:17 [INFO]: Epoch 086 - training loss: 0.2856, validation loss: 0.3911
2024-06-02 21:55:39 [INFO]: Epoch 087 - training loss: 0.2831, validation loss: 0.3906
2024-06-02 21:56:01 [INFO]: Epoch 088 - training loss: 0.2819, validation loss: 0.3904
2024-06-02 21:56:22 [INFO]: Epoch 089 - training loss: 0.2828, validation loss: 0.3900
2024-06-02 21:56:44 [INFO]: Epoch 090 - training loss: 0.2806, validation loss: 0.3906
2024-06-02 21:57:06 [INFO]: Epoch 091 - training loss: 0.2793, validation loss: 0.3892
2024-06-02 21:57:27 [INFO]: Epoch 092 - training loss: 0.2810, validation loss: 0.3897
2024-06-02 21:57:49 [INFO]: Epoch 093 - training loss: 0.2757, validation loss: 0.3890
2024-06-02 21:58:11 [INFO]: Epoch 094 - training loss: 0.2790, validation loss: 0.3889
2024-06-02 21:58:32 [INFO]: Epoch 095 - training loss: 0.2742, validation loss: 0.3886
2024-06-02 21:58:54 [INFO]: Epoch 096 - training loss: 0.2755, validation loss: 0.3896
2024-06-02 21:59:16 [INFO]: Epoch 097 - training loss: 0.2738, validation loss: 0.3884
2024-06-02 21:59:37 [INFO]: Epoch 098 - training loss: 0.2727, validation loss: 0.3891
2024-06-02 21:59:58 [INFO]: Epoch 099 - training loss: 0.2748, validation loss: 0.3903
2024-06-02 22:00:19 [INFO]: Epoch 100 - training loss: 0.2704, validation loss: 0.3874
2024-06-02 22:00:19 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 22:00:19 [INFO]: Saved the model to results_point_rate05/PeMS/BRITS_PeMS/round_3/20240602_T213029/BRITS.pypots
2024-06-02 22:00:46 [INFO]: Successfully saved to results_point_rate05/PeMS/BRITS_PeMS/round_3/imputation.pkl
2024-06-02 22:00:46 [INFO]: Round3 - BRITS on PeMS: MAE=0.2866, MSE=0.5602, MRE=0.3557
2024-06-02 22:00:46 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 22:00:46 [INFO]: Using the given device: cuda:0
2024-06-02 22:00:46 [INFO]: Model files will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_4/20240602_T220046
2024-06-02 22:00:46 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/BRITS_PeMS/round_4/20240602_T220046/tensorboard
2024-06-02 22:00:46 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-02 22:01:08 [INFO]: Epoch 001 - training loss: 1.0261, validation loss: 0.6092
2024-06-02 22:01:30 [INFO]: Epoch 002 - training loss: 0.7050, validation loss: 0.5310
2024-06-02 22:01:52 [INFO]: Epoch 003 - training loss: 0.6288, validation loss: 0.5056
2024-06-02 22:02:13 [INFO]: Epoch 004 - training loss: 0.5801, validation loss: 0.4868
2024-06-02 22:02:35 [INFO]: Epoch 005 - training loss: 0.5485, validation loss: 0.4778
2024-06-02 22:02:57 [INFO]: Epoch 006 - training loss: 0.5367, validation loss: 0.4699
2024-06-02 22:03:18 [INFO]: Epoch 007 - training loss: 0.5152, validation loss: 0.4655
2024-06-02 22:03:40 [INFO]: Epoch 008 - training loss: 0.5072, validation loss: 0.4608
2024-06-02 22:04:02 [INFO]: Epoch 009 - training loss: 0.4950, validation loss: 0.4554
2024-06-02 22:04:23 [INFO]: Epoch 010 - training loss: 0.4856, validation loss: 0.4516
2024-06-02 22:04:45 [INFO]: Epoch 011 - training loss: 0.4773, validation loss: 0.4494
2024-06-02 22:05:07 [INFO]: Epoch 012 - training loss: 0.4689, validation loss: 0.4467
2024-06-02 22:05:28 [INFO]: Epoch 013 - training loss: 0.4641, validation loss: 0.4439
2024-06-02 22:05:50 [INFO]: Epoch 014 - training loss: 0.4550, validation loss: 0.4427
2024-06-02 22:06:12 [INFO]: Epoch 015 - training loss: 0.4474, validation loss: 0.4434
2024-06-02 22:06:33 [INFO]: Epoch 016 - training loss: 0.4395, validation loss: 0.4389
2024-06-02 22:06:55 [INFO]: Epoch 017 - training loss: 0.4364, validation loss: 0.4354
2024-06-02 22:07:17 [INFO]: Epoch 018 - training loss: 0.4300, validation loss: 0.4346
2024-06-02 22:07:38 [INFO]: Epoch 019 - training loss: 0.4242, validation loss: 0.4339
2024-06-02 22:08:00 [INFO]: Epoch 020 - training loss: 0.4194, validation loss: 0.4296
2024-06-02 22:08:22 [INFO]: Epoch 021 - training loss: 0.4174, validation loss: 0.4303
2024-06-02 22:08:43 [INFO]: Epoch 022 - training loss: 0.4111, validation loss: 0.4270
2024-06-02 22:09:05 [INFO]: Epoch 023 - training loss: 0.4074, validation loss: 0.4259
2024-06-02 22:09:27 [INFO]: Epoch 024 - training loss: 0.4029, validation loss: 0.4253
2024-06-02 22:09:48 [INFO]: Epoch 025 - training loss: 0.3996, validation loss: 0.4231
2024-06-02 22:10:10 [INFO]: Epoch 026 - training loss: 0.3960, validation loss: 0.4237
2024-06-02 22:10:32 [INFO]: Epoch 027 - training loss: 0.3929, validation loss: 0.4216
2024-06-02 22:10:53 [INFO]: Epoch 028 - training loss: 0.3859, validation loss: 0.4196
2024-06-02 22:11:15 [INFO]: Epoch 029 - training loss: 0.3848, validation loss: 0.4185
2024-06-02 22:11:37 [INFO]: Epoch 030 - training loss: 0.3816, validation loss: 0.4191
2024-06-02 22:11:58 [INFO]: Epoch 031 - training loss: 0.3786, validation loss: 0.4167
2024-06-02 22:12:20 [INFO]: Epoch 032 - training loss: 0.3730, validation loss: 0.4152
2024-06-02 22:12:42 [INFO]: Epoch 033 - training loss: 0.3702, validation loss: 0.4156
2024-06-02 22:13:03 [INFO]: Epoch 034 - training loss: 0.3678, validation loss: 0.4137
2024-06-02 22:13:25 [INFO]: Epoch 035 - training loss: 0.3648, validation loss: 0.4135
2024-06-02 22:13:47 [INFO]: Epoch 036 - training loss: 0.3630, validation loss: 0.4123
2024-06-02 22:14:08 [INFO]: Epoch 037 - training loss: 0.3623, validation loss: 0.4125
2024-06-02 22:14:30 [INFO]: Epoch 038 - training loss: 0.3598, validation loss: 0.4112
2024-06-02 22:14:52 [INFO]: Epoch 039 - training loss: 0.3535, validation loss: 0.4095
2024-06-02 22:15:13 [INFO]: Epoch 040 - training loss: 0.3511, validation loss: 0.4127
2024-06-02 22:15:32 [INFO]: Epoch 041 - training loss: 0.3532, validation loss: 0.4094
2024-06-02 22:15:53 [INFO]: Epoch 042 - training loss: 0.3490, validation loss: 0.4082
2024-06-02 22:16:15 [INFO]: Epoch 043 - training loss: 0.3443, validation loss: 0.4088
2024-06-02 22:16:37 [INFO]: Epoch 044 - training loss: 0.3450, validation loss: 0.4074
2024-06-02 22:16:59 [INFO]: Epoch 045 - training loss: 0.3395, validation loss: 0.4065
2024-06-02 22:17:20 [INFO]: Epoch 046 - training loss: 0.3393, validation loss: 0.4060
2024-06-02 22:17:42 [INFO]: Epoch 047 - training loss: 0.3377, validation loss: 0.4049
2024-06-02 22:18:04 [INFO]: Epoch 048 - training loss: 0.3341, validation loss: 0.4043
2024-06-02 22:18:25 [INFO]: Epoch 049 - training loss: 0.3349, validation loss: 0.4050
2024-06-02 22:18:47 [INFO]: Epoch 050 - training loss: 0.3341, validation loss: 0.4039
2024-06-02 22:19:08 [INFO]: Epoch 051 - training loss: 0.3325, validation loss: 0.4038
2024-06-02 22:19:23 [INFO]: Epoch 052 - training loss: 0.3280, validation loss: 0.4014
2024-06-02 22:19:41 [INFO]: Epoch 053 - training loss: 0.3276, validation loss: 0.4035
2024-06-02 22:19:55 [INFO]: Epoch 054 - training loss: 0.3245, validation loss: 0.4023
2024-06-02 22:20:16 [INFO]: Epoch 055 - training loss: 0.3218, validation loss: 0.4011
2024-06-02 22:20:30 [INFO]: Epoch 056 - training loss: 0.3252, validation loss: 0.4008
2024-06-02 22:20:50 [INFO]: Epoch 057 - training loss: 0.3229, validation loss: 0.4009
2024-06-02 22:21:06 [INFO]: Epoch 058 - training loss: 0.3218, validation loss: 0.3999
2024-06-02 22:21:23 [INFO]: Epoch 059 - training loss: 0.3167, validation loss: 0.4002
2024-06-02 22:21:41 [INFO]: Epoch 060 - training loss: 0.3173, validation loss: 0.3994
2024-06-02 22:21:55 [INFO]: Epoch 061 - training loss: 0.3126, validation loss: 0.3997
2024-06-02 22:22:15 [INFO]: Epoch 062 - training loss: 0.3147, validation loss: 0.3990
2024-06-02 22:22:30 [INFO]: Epoch 063 - training loss: 0.3120, validation loss: 0.3994
2024-06-02 22:22:50 [INFO]: Epoch 064 - training loss: 0.3097, validation loss: 0.3979
2024-06-02 22:23:05 [INFO]: Epoch 065 - training loss: 0.3086, validation loss: 0.3973
2024-06-02 22:23:22 [INFO]: Epoch 066 - training loss: 0.3061, validation loss: 0.3985
2024-06-02 22:23:39 [INFO]: Epoch 067 - training loss: 0.3067, validation loss: 0.3975
2024-06-02 22:23:55 [INFO]: Epoch 068 - training loss: 0.3048, validation loss: 0.3981
2024-06-02 22:24:14 [INFO]: Epoch 069 - training loss: 0.3027, validation loss: 0.3961
2024-06-02 22:24:28 [INFO]: Epoch 070 - training loss: 0.3017, validation loss: 0.3952
2024-06-02 22:24:49 [INFO]: Epoch 071 - training loss: 0.2995, validation loss: 0.3967
2024-06-02 22:25:03 [INFO]: Epoch 072 - training loss: 0.2994, validation loss: 0.3969
2024-06-02 22:25:22 [INFO]: Epoch 073 - training loss: 0.2967, validation loss: 0.3959
2024-06-02 22:25:38 [INFO]: Epoch 074 - training loss: 0.2952, validation loss: 0.3955
2024-06-02 22:25:55 [INFO]: Epoch 075 - training loss: 0.2977, validation loss: 0.3966
2024-06-02 22:26:13 [INFO]: Epoch 076 - training loss: 0.2977, validation loss: 0.3950
2024-06-02 22:26:28 [INFO]: Epoch 077 - training loss: 0.2936, validation loss: 0.3934
2024-06-02 22:26:49 [INFO]: Epoch 078 - training loss: 0.2945, validation loss: 0.3928
2024-06-02 22:27:03 [INFO]: Epoch 079 - training loss: 0.2943, validation loss: 0.3932
2024-06-02 22:27:23 [INFO]: Epoch 080 - training loss: 0.2908, validation loss: 0.3942
2024-06-02 22:27:38 [INFO]: Epoch 081 - training loss: 0.2896, validation loss: 0.3939
2024-06-02 22:27:55 [INFO]: Epoch 082 - training loss: 0.2866, validation loss: 0.3935
2024-06-02 22:28:13 [INFO]: Epoch 083 - training loss: 0.2861, validation loss: 0.3926
2024-06-02 22:28:28 [INFO]: Epoch 084 - training loss: 0.2874, validation loss: 0.3920
2024-06-02 22:28:49 [INFO]: Epoch 085 - training loss: 0.2859, validation loss: 0.3921
2024-06-02 22:29:04 [INFO]: Epoch 086 - training loss: 0.2871, validation loss: 0.3911
2024-06-02 22:29:23 [INFO]: Epoch 087 - training loss: 0.2842, validation loss: 0.3912
2024-06-02 22:29:39 [INFO]: Epoch 088 - training loss: 0.2828, validation loss: 0.3910
2024-06-02 22:29:55 [INFO]: Epoch 089 - training loss: 0.2820, validation loss: 0.3916
2024-06-02 22:30:13 [INFO]: Epoch 090 - training loss: 0.2824, validation loss: 0.3903
2024-06-02 22:30:28 [INFO]: Epoch 091 - training loss: 0.2789, validation loss: 0.3914
2024-06-02 22:30:48 [INFO]: Epoch 092 - training loss: 0.2775, validation loss: 0.3916
2024-06-02 22:31:02 [INFO]: Epoch 093 - training loss: 0.2792, validation loss: 0.3911
2024-06-02 22:31:23 [INFO]: Epoch 094 - training loss: 0.2787, validation loss: 0.3911
2024-06-02 22:31:38 [INFO]: Epoch 095 - training loss: 0.2748, validation loss: 0.3902
2024-06-02 22:31:55 [INFO]: Epoch 096 - training loss: 0.2727, validation loss: 0.3908
2024-06-02 22:32:13 [INFO]: Epoch 097 - training loss: 0.2715, validation loss: 0.3899
2024-06-02 22:32:28 [INFO]: Epoch 098 - training loss: 0.2766, validation loss: 0.3902
2024-06-02 22:32:48 [INFO]: Epoch 099 - training loss: 0.2718, validation loss: 0.3904
2024-06-02 22:33:02 [INFO]: Epoch 100 - training loss: 0.2715, validation loss: 0.3890
2024-06-02 22:33:02 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 22:33:03 [INFO]: Saved the model to results_point_rate05/PeMS/BRITS_PeMS/round_4/20240602_T220046/BRITS.pypots
2024-06-02 22:33:25 [INFO]: Successfully saved to results_point_rate05/PeMS/BRITS_PeMS/round_4/imputation.pkl
2024-06-02 22:33:25 [INFO]: Round4 - BRITS on PeMS: MAE=0.2881, MSE=0.5639, MRE=0.3574
2024-06-02 22:33:25 [INFO]: Done! Final results:
Averaged BRITS (32,012,048 params) on PeMS: MAE=0.2875 ± 0.0006006881985270687, MSE=0.5611 ± 0.0016818054329953039, MRE=0.3568 ± 0.0007454031006722823, average inference time=5.44
