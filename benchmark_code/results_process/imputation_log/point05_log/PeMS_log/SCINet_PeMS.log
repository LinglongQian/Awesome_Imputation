2024-06-03 02:07:41 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 02:07:41 [INFO]: Using the given device: cuda:0
2024-06-03 02:07:41 [INFO]: Model files will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_0/20240603_T020741
2024-06-03 02:07:41 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_0/20240603_T020741/tensorboard
2024-06-03 02:08:02 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-03 02:08:15 [INFO]: Epoch 001 - training loss: 1.6980, validation loss: 1.3097
2024-06-03 02:08:24 [INFO]: Epoch 002 - training loss: 1.1955, validation loss: 1.0674
2024-06-03 02:08:32 [INFO]: Epoch 003 - training loss: 1.0436, validation loss: 0.9659
2024-06-03 02:08:40 [INFO]: Epoch 004 - training loss: 0.9632, validation loss: 0.9342
2024-06-03 02:08:49 [INFO]: Epoch 005 - training loss: 0.8932, validation loss: 0.8860
2024-06-03 02:08:58 [INFO]: Epoch 006 - training loss: 0.8388, validation loss: 0.8471
2024-06-03 02:09:06 [INFO]: Epoch 007 - training loss: 0.7877, validation loss: 0.7845
2024-06-03 02:09:15 [INFO]: Epoch 008 - training loss: 0.7370, validation loss: 0.7175
2024-06-03 02:09:23 [INFO]: Epoch 009 - training loss: 0.6920, validation loss: 0.6499
2024-06-03 02:09:32 [INFO]: Epoch 010 - training loss: 0.6572, validation loss: 0.5837
2024-06-03 02:09:40 [INFO]: Epoch 011 - training loss: 0.6326, validation loss: 0.5501
2024-06-03 02:09:49 [INFO]: Epoch 012 - training loss: 0.6149, validation loss: 0.5286
2024-06-03 02:09:57 [INFO]: Epoch 013 - training loss: 0.5974, validation loss: 0.5077
2024-06-03 02:10:05 [INFO]: Epoch 014 - training loss: 0.5895, validation loss: 0.4970
2024-06-03 02:10:13 [INFO]: Epoch 015 - training loss: 0.5811, validation loss: 0.4963
2024-06-03 02:10:22 [INFO]: Epoch 016 - training loss: 0.5718, validation loss: 0.4890
2024-06-03 02:10:30 [INFO]: Epoch 017 - training loss: 0.5677, validation loss: 0.4888
2024-06-03 02:10:39 [INFO]: Epoch 018 - training loss: 0.5662, validation loss: 0.4843
2024-06-03 02:10:47 [INFO]: Epoch 019 - training loss: 0.5557, validation loss: 0.4789
2024-06-03 02:10:55 [INFO]: Epoch 020 - training loss: 0.5557, validation loss: 0.4733
2024-06-03 02:11:04 [INFO]: Epoch 021 - training loss: 0.5554, validation loss: 0.4739
2024-06-03 02:11:12 [INFO]: Epoch 022 - training loss: 0.5500, validation loss: 0.4749
2024-06-03 02:11:21 [INFO]: Epoch 023 - training loss: 0.5486, validation loss: 0.4671
2024-06-03 02:11:29 [INFO]: Epoch 024 - training loss: 0.5481, validation loss: 0.4673
2024-06-03 02:11:37 [INFO]: Epoch 025 - training loss: 0.5441, validation loss: 0.4670
2024-06-03 02:11:45 [INFO]: Epoch 026 - training loss: 0.5444, validation loss: 0.4638
2024-06-03 02:11:53 [INFO]: Epoch 027 - training loss: 0.5406, validation loss: 0.4569
2024-06-03 02:12:02 [INFO]: Epoch 028 - training loss: 0.5426, validation loss: 0.4593
2024-06-03 02:12:11 [INFO]: Epoch 029 - training loss: 0.5446, validation loss: 0.4594
2024-06-03 02:12:19 [INFO]: Epoch 030 - training loss: 0.5393, validation loss: 0.4614
2024-06-03 02:12:28 [INFO]: Epoch 031 - training loss: 0.5375, validation loss: 0.4566
2024-06-03 02:12:36 [INFO]: Epoch 032 - training loss: 0.5373, validation loss: 0.4551
2024-06-03 02:12:45 [INFO]: Epoch 033 - training loss: 0.5345, validation loss: 0.4561
2024-06-03 02:12:53 [INFO]: Epoch 034 - training loss: 0.5373, validation loss: 0.4588
2024-06-03 02:13:02 [INFO]: Epoch 035 - training loss: 0.5352, validation loss: 0.4476
2024-06-03 02:13:10 [INFO]: Epoch 036 - training loss: 0.5361, validation loss: 0.4526
2024-06-03 02:13:18 [INFO]: Epoch 037 - training loss: 0.5339, validation loss: 0.4541
2024-06-03 02:13:27 [INFO]: Epoch 038 - training loss: 0.5301, validation loss: 0.4461
2024-06-03 02:13:35 [INFO]: Epoch 039 - training loss: 0.5333, validation loss: 0.4527
2024-06-03 02:13:44 [INFO]: Epoch 040 - training loss: 0.5308, validation loss: 0.4532
2024-06-03 02:13:53 [INFO]: Epoch 041 - training loss: 0.5304, validation loss: 0.4467
2024-06-03 02:14:02 [INFO]: Epoch 042 - training loss: 0.5265, validation loss: 0.4435
2024-06-03 02:14:10 [INFO]: Epoch 043 - training loss: 0.5306, validation loss: 0.4515
2024-06-03 02:14:19 [INFO]: Epoch 044 - training loss: 0.5301, validation loss: 0.4450
2024-06-03 02:14:27 [INFO]: Epoch 045 - training loss: 0.5244, validation loss: 0.4451
2024-06-03 02:14:36 [INFO]: Epoch 046 - training loss: 0.5306, validation loss: 0.4473
2024-06-03 02:14:44 [INFO]: Epoch 047 - training loss: 0.5285, validation loss: 0.4420
2024-06-03 02:14:53 [INFO]: Epoch 048 - training loss: 0.5263, validation loss: 0.4423
2024-06-03 02:15:01 [INFO]: Epoch 049 - training loss: 0.5275, validation loss: 0.4483
2024-06-03 02:15:10 [INFO]: Epoch 050 - training loss: 0.5250, validation loss: 0.4399
2024-06-03 02:15:17 [INFO]: Epoch 051 - training loss: 0.5245, validation loss: 0.4460
2024-06-03 02:15:25 [INFO]: Epoch 052 - training loss: 0.5249, validation loss: 0.4449
2024-06-03 02:15:33 [INFO]: Epoch 053 - training loss: 0.5247, validation loss: 0.4454
2024-06-03 02:15:42 [INFO]: Epoch 054 - training loss: 0.5256, validation loss: 0.4430
2024-06-03 02:15:50 [INFO]: Epoch 055 - training loss: 0.5238, validation loss: 0.4419
2024-06-03 02:15:59 [INFO]: Epoch 056 - training loss: 0.5255, validation loss: 0.4425
2024-06-03 02:16:07 [INFO]: Epoch 057 - training loss: 0.5240, validation loss: 0.4387
2024-06-03 02:16:15 [INFO]: Epoch 058 - training loss: 0.5282, validation loss: 0.4426
2024-06-03 02:16:24 [INFO]: Epoch 059 - training loss: 0.5242, validation loss: 0.4424
2024-06-03 02:16:32 [INFO]: Epoch 060 - training loss: 0.5248, validation loss: 0.4360
2024-06-03 02:16:41 [INFO]: Epoch 061 - training loss: 0.5233, validation loss: 0.4460
2024-06-03 02:16:49 [INFO]: Epoch 062 - training loss: 0.5229, validation loss: 0.4391
2024-06-03 02:16:58 [INFO]: Epoch 063 - training loss: 0.5258, validation loss: 0.4385
2024-06-03 02:17:07 [INFO]: Epoch 064 - training loss: 0.5240, validation loss: 0.4437
2024-06-03 02:17:15 [INFO]: Epoch 065 - training loss: 0.5190, validation loss: 0.4383
2024-06-03 02:17:24 [INFO]: Epoch 066 - training loss: 0.5196, validation loss: 0.4319
2024-06-03 02:17:32 [INFO]: Epoch 067 - training loss: 0.5241, validation loss: 0.4460
2024-06-03 02:17:40 [INFO]: Epoch 068 - training loss: 0.5215, validation loss: 0.4327
2024-06-03 02:17:49 [INFO]: Epoch 069 - training loss: 0.5234, validation loss: 0.4357
2024-06-03 02:17:57 [INFO]: Epoch 070 - training loss: 0.5213, validation loss: 0.4421
2024-06-03 02:18:05 [INFO]: Epoch 071 - training loss: 0.5169, validation loss: 0.4378
2024-06-03 02:18:13 [INFO]: Epoch 072 - training loss: 0.5212, validation loss: 0.4358
2024-06-03 02:18:21 [INFO]: Epoch 073 - training loss: 0.5192, validation loss: 0.4325
2024-06-03 02:18:29 [INFO]: Epoch 074 - training loss: 0.5198, validation loss: 0.4432
2024-06-03 02:18:38 [INFO]: Epoch 075 - training loss: 0.5186, validation loss: 0.4345
2024-06-03 02:18:46 [INFO]: Epoch 076 - training loss: 0.5192, validation loss: 0.4387
2024-06-03 02:18:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:18:46 [INFO]: Finished training. The best model is from epoch#66.
2024-06-03 02:19:15 [INFO]: Saved the model to results_point_rate05/PeMS/SCINet_PeMS/round_0/20240603_T020741/SCINet.pypots
2024-06-03 02:19:19 [INFO]: Successfully saved to results_point_rate05/PeMS/SCINet_PeMS/round_0/imputation.pkl
2024-06-03 02:19:19 [INFO]: Round0 - SCINet on PeMS: MAE=0.3919, MSE=0.6202, MRE=0.4863
2024-06-03 02:19:19 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 02:19:19 [INFO]: Using the given device: cuda:0
2024-06-03 02:19:19 [INFO]: Model files will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_1/20240603_T021919
2024-06-03 02:19:19 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_1/20240603_T021919/tensorboard
2024-06-03 02:19:54 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-03 02:20:03 [INFO]: Epoch 001 - training loss: 1.6035, validation loss: 1.0962
2024-06-03 02:20:11 [INFO]: Epoch 002 - training loss: 1.1161, validation loss: 0.9885
2024-06-03 02:20:20 [INFO]: Epoch 003 - training loss: 0.9834, validation loss: 0.9729
2024-06-03 02:20:28 [INFO]: Epoch 004 - training loss: 0.9175, validation loss: 0.9390
2024-06-03 02:20:36 [INFO]: Epoch 005 - training loss: 0.8623, validation loss: 0.9008
2024-06-03 02:20:45 [INFO]: Epoch 006 - training loss: 0.8186, validation loss: 0.8926
2024-06-03 02:20:52 [INFO]: Epoch 007 - training loss: 0.7830, validation loss: 0.8647
2024-06-03 02:21:00 [INFO]: Epoch 008 - training loss: 0.7478, validation loss: 0.8469
2024-06-03 02:21:08 [INFO]: Epoch 009 - training loss: 0.7192, validation loss: 0.8501
2024-06-03 02:21:16 [INFO]: Epoch 010 - training loss: 0.7029, validation loss: 0.8117
2024-06-03 02:21:25 [INFO]: Epoch 011 - training loss: 0.6856, validation loss: 0.8060
2024-06-03 02:21:33 [INFO]: Epoch 012 - training loss: 0.6735, validation loss: 0.7858
2024-06-03 02:21:41 [INFO]: Epoch 013 - training loss: 0.6606, validation loss: 0.7821
2024-06-03 02:21:50 [INFO]: Epoch 014 - training loss: 0.6568, validation loss: 0.7651
2024-06-03 02:21:58 [INFO]: Epoch 015 - training loss: 0.6473, validation loss: 0.7740
2024-06-03 02:22:06 [INFO]: Epoch 016 - training loss: 0.6410, validation loss: 0.7728
2024-06-03 02:22:14 [INFO]: Epoch 017 - training loss: 0.6333, validation loss: 0.7400
2024-06-03 02:22:22 [INFO]: Epoch 018 - training loss: 0.6346, validation loss: 0.7125
2024-06-03 02:22:31 [INFO]: Epoch 019 - training loss: 0.6282, validation loss: 0.7243
2024-06-03 02:22:39 [INFO]: Epoch 020 - training loss: 0.6261, validation loss: 0.7143
2024-06-03 02:22:48 [INFO]: Epoch 021 - training loss: 0.6270, validation loss: 0.7048
2024-06-03 02:22:56 [INFO]: Epoch 022 - training loss: 0.6229, validation loss: 0.7017
2024-06-03 02:23:04 [INFO]: Epoch 023 - training loss: 0.6177, validation loss: 0.6969
2024-06-03 02:23:13 [INFO]: Epoch 024 - training loss: 0.6161, validation loss: 0.6770
2024-06-03 02:23:21 [INFO]: Epoch 025 - training loss: 0.6122, validation loss: 0.6574
2024-06-03 02:23:29 [INFO]: Epoch 026 - training loss: 0.6186, validation loss: 0.6525
2024-06-03 02:23:36 [INFO]: Epoch 027 - training loss: 0.6116, validation loss: 0.6292
2024-06-03 02:23:43 [INFO]: Epoch 028 - training loss: 0.5973, validation loss: 0.5897
2024-06-03 02:23:50 [INFO]: Epoch 029 - training loss: 0.5920, validation loss: 0.5643
2024-06-03 02:23:57 [INFO]: Epoch 030 - training loss: 0.5861, validation loss: 0.5391
2024-06-03 02:24:04 [INFO]: Epoch 031 - training loss: 0.5748, validation loss: 0.5290
2024-06-03 02:24:12 [INFO]: Epoch 032 - training loss: 0.5718, validation loss: 0.5159
2024-06-03 02:24:19 [INFO]: Epoch 033 - training loss: 0.5666, validation loss: 0.5004
2024-06-03 02:24:26 [INFO]: Epoch 034 - training loss: 0.5605, validation loss: 0.4936
2024-06-03 02:24:33 [INFO]: Epoch 035 - training loss: 0.5584, validation loss: 0.4840
2024-06-03 02:24:40 [INFO]: Epoch 036 - training loss: 0.5543, validation loss: 0.4869
2024-06-03 02:24:47 [INFO]: Epoch 037 - training loss: 0.5505, validation loss: 0.4737
2024-06-03 02:24:54 [INFO]: Epoch 038 - training loss: 0.5458, validation loss: 0.4770
2024-06-03 02:25:01 [INFO]: Epoch 039 - training loss: 0.5442, validation loss: 0.4693
2024-06-03 02:25:08 [INFO]: Epoch 040 - training loss: 0.5386, validation loss: 0.4657
2024-06-03 02:25:16 [INFO]: Epoch 041 - training loss: 0.5416, validation loss: 0.4649
2024-06-03 02:25:23 [INFO]: Epoch 042 - training loss: 0.5365, validation loss: 0.4611
2024-06-03 02:25:30 [INFO]: Epoch 043 - training loss: 0.5376, validation loss: 0.4545
2024-06-03 02:25:37 [INFO]: Epoch 044 - training loss: 0.5347, validation loss: 0.4554
2024-06-03 02:25:44 [INFO]: Epoch 045 - training loss: 0.5310, validation loss: 0.4558
2024-06-03 02:25:51 [INFO]: Epoch 046 - training loss: 0.5319, validation loss: 0.4562
2024-06-03 02:25:59 [INFO]: Epoch 047 - training loss: 0.5306, validation loss: 0.4571
2024-06-03 02:26:06 [INFO]: Epoch 048 - training loss: 0.5273, validation loss: 0.4506
2024-06-03 02:26:13 [INFO]: Epoch 049 - training loss: 0.5286, validation loss: 0.4497
2024-06-03 02:26:20 [INFO]: Epoch 050 - training loss: 0.5273, validation loss: 0.4484
2024-06-03 02:26:27 [INFO]: Epoch 051 - training loss: 0.5261, validation loss: 0.4502
2024-06-03 02:26:34 [INFO]: Epoch 052 - training loss: 0.5229, validation loss: 0.4484
2024-06-03 02:26:42 [INFO]: Epoch 053 - training loss: 0.5231, validation loss: 0.4415
2024-06-03 02:26:49 [INFO]: Epoch 054 - training loss: 0.5240, validation loss: 0.4429
2024-06-03 02:26:56 [INFO]: Epoch 055 - training loss: 0.5165, validation loss: 0.4422
2024-06-03 02:27:03 [INFO]: Epoch 056 - training loss: 0.5246, validation loss: 0.4407
2024-06-03 02:27:09 [INFO]: Epoch 057 - training loss: 0.5198, validation loss: 0.4393
2024-06-03 02:27:15 [INFO]: Epoch 058 - training loss: 0.5232, validation loss: 0.4386
2024-06-03 02:27:21 [INFO]: Epoch 059 - training loss: 0.5163, validation loss: 0.4364
2024-06-03 02:27:27 [INFO]: Epoch 060 - training loss: 0.5192, validation loss: 0.4398
2024-06-03 02:27:33 [INFO]: Epoch 061 - training loss: 0.5204, validation loss: 0.4380
2024-06-03 02:27:39 [INFO]: Epoch 062 - training loss: 0.5176, validation loss: 0.4379
2024-06-03 02:27:46 [INFO]: Epoch 063 - training loss: 0.5168, validation loss: 0.4400
2024-06-03 02:27:52 [INFO]: Epoch 064 - training loss: 0.5166, validation loss: 0.4382
2024-06-03 02:27:58 [INFO]: Epoch 065 - training loss: 0.5210, validation loss: 0.4402
2024-06-03 02:28:04 [INFO]: Epoch 066 - training loss: 0.5207, validation loss: 0.4358
2024-06-03 02:28:09 [INFO]: Epoch 067 - training loss: 0.5192, validation loss: 0.4369
2024-06-03 02:28:15 [INFO]: Epoch 068 - training loss: 0.5170, validation loss: 0.4376
2024-06-03 02:28:21 [INFO]: Epoch 069 - training loss: 0.5153, validation loss: 0.4379
2024-06-03 02:28:27 [INFO]: Epoch 070 - training loss: 0.5159, validation loss: 0.4394
2024-06-03 02:28:32 [INFO]: Epoch 071 - training loss: 0.5148, validation loss: 0.4328
2024-06-03 02:28:38 [INFO]: Epoch 072 - training loss: 0.5167, validation loss: 0.4372
2024-06-03 02:28:43 [INFO]: Epoch 073 - training loss: 0.5149, validation loss: 0.4339
2024-06-03 02:28:49 [INFO]: Epoch 074 - training loss: 0.5182, validation loss: 0.4299
2024-06-03 02:28:54 [INFO]: Epoch 075 - training loss: 0.5138, validation loss: 0.4354
2024-06-03 02:28:59 [INFO]: Epoch 076 - training loss: 0.5152, validation loss: 0.4332
2024-06-03 02:29:05 [INFO]: Epoch 077 - training loss: 0.5149, validation loss: 0.4364
2024-06-03 02:29:11 [INFO]: Epoch 078 - training loss: 0.5118, validation loss: 0.4333
2024-06-03 02:29:16 [INFO]: Epoch 079 - training loss: 0.5120, validation loss: 0.4359
2024-06-03 02:29:22 [INFO]: Epoch 080 - training loss: 0.5162, validation loss: 0.4293
2024-06-03 02:29:27 [INFO]: Epoch 081 - training loss: 0.5169, validation loss: 0.4274
2024-06-03 02:29:33 [INFO]: Epoch 082 - training loss: 0.5196, validation loss: 0.4325
2024-06-03 02:29:38 [INFO]: Epoch 083 - training loss: 0.5143, validation loss: 0.4277
2024-06-03 02:29:44 [INFO]: Epoch 084 - training loss: 0.5197, validation loss: 0.4320
2024-06-03 02:29:49 [INFO]: Epoch 085 - training loss: 0.5113, validation loss: 0.4304
2024-06-03 02:29:55 [INFO]: Epoch 086 - training loss: 0.5149, validation loss: 0.4336
2024-06-03 02:30:00 [INFO]: Epoch 087 - training loss: 0.5122, validation loss: 0.4323
2024-06-03 02:30:06 [INFO]: Epoch 088 - training loss: 0.5161, validation loss: 0.4251
2024-06-03 02:30:11 [INFO]: Epoch 089 - training loss: 0.5167, validation loss: 0.4302
2024-06-03 02:30:17 [INFO]: Epoch 090 - training loss: 0.5141, validation loss: 0.4311
2024-06-03 02:30:22 [INFO]: Epoch 091 - training loss: 0.5115, validation loss: 0.4320
2024-06-03 02:30:28 [INFO]: Epoch 092 - training loss: 0.5146, validation loss: 0.4323
2024-06-03 02:30:33 [INFO]: Epoch 093 - training loss: 0.5122, validation loss: 0.4263
2024-06-03 02:30:38 [INFO]: Epoch 094 - training loss: 0.5133, validation loss: 0.4287
2024-06-03 02:30:44 [INFO]: Epoch 095 - training loss: 0.5158, validation loss: 0.4402
2024-06-03 02:30:49 [INFO]: Epoch 096 - training loss: 0.5114, validation loss: 0.4362
2024-06-03 02:30:55 [INFO]: Epoch 097 - training loss: 0.5134, validation loss: 0.4342
2024-06-03 02:31:01 [INFO]: Epoch 098 - training loss: 0.5116, validation loss: 0.4340
2024-06-03 02:31:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:31:01 [INFO]: Finished training. The best model is from epoch#88.
2024-06-03 02:31:13 [INFO]: Saved the model to results_point_rate05/PeMS/SCINet_PeMS/round_1/20240603_T021919/SCINet.pypots
2024-06-03 02:31:15 [INFO]: Successfully saved to results_point_rate05/PeMS/SCINet_PeMS/round_1/imputation.pkl
2024-06-03 02:31:15 [INFO]: Round1 - SCINet on PeMS: MAE=0.3926, MSE=0.6154, MRE=0.4871
2024-06-03 02:31:15 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 02:31:15 [INFO]: Using the given device: cuda:0
2024-06-03 02:31:16 [INFO]: Model files will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_2/20240603_T023115
2024-06-03 02:31:16 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_2/20240603_T023115/tensorboard
2024-06-03 02:31:29 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-03 02:31:35 [INFO]: Epoch 001 - training loss: 1.6927, validation loss: 1.1593
2024-06-03 02:31:41 [INFO]: Epoch 002 - training loss: 1.1821, validation loss: 1.0184
2024-06-03 02:31:46 [INFO]: Epoch 003 - training loss: 1.0127, validation loss: 0.9752
2024-06-03 02:31:52 [INFO]: Epoch 004 - training loss: 0.9241, validation loss: 0.9303
2024-06-03 02:31:57 [INFO]: Epoch 005 - training loss: 0.8676, validation loss: 0.8878
2024-06-03 02:32:02 [INFO]: Epoch 006 - training loss: 0.8196, validation loss: 0.8628
2024-06-03 02:32:08 [INFO]: Epoch 007 - training loss: 0.7813, validation loss: 0.8610
2024-06-03 02:32:13 [INFO]: Epoch 008 - training loss: 0.7470, validation loss: 0.8197
2024-06-03 02:32:18 [INFO]: Epoch 009 - training loss: 0.7186, validation loss: 0.7901
2024-06-03 02:32:24 [INFO]: Epoch 010 - training loss: 0.6923, validation loss: 0.7734
2024-06-03 02:32:29 [INFO]: Epoch 011 - training loss: 0.6769, validation loss: 0.7443
2024-06-03 02:32:35 [INFO]: Epoch 012 - training loss: 0.6602, validation loss: 0.7304
2024-06-03 02:32:40 [INFO]: Epoch 013 - training loss: 0.6415, validation loss: 0.7091
2024-06-03 02:32:46 [INFO]: Epoch 014 - training loss: 0.6277, validation loss: 0.7173
2024-06-03 02:32:51 [INFO]: Epoch 015 - training loss: 0.6284, validation loss: 0.6820
2024-06-03 02:32:57 [INFO]: Epoch 016 - training loss: 0.6240, validation loss: 0.6834
2024-06-03 02:33:02 [INFO]: Epoch 017 - training loss: 0.6153, validation loss: 0.6884
2024-06-03 02:33:08 [INFO]: Epoch 018 - training loss: 0.6138, validation loss: 0.6739
2024-06-03 02:33:13 [INFO]: Epoch 019 - training loss: 0.6059, validation loss: 0.6673
2024-06-03 02:33:19 [INFO]: Epoch 020 - training loss: 0.6038, validation loss: 0.6818
2024-06-03 02:33:24 [INFO]: Epoch 021 - training loss: 0.6014, validation loss: 0.6737
2024-06-03 02:33:30 [INFO]: Epoch 022 - training loss: 0.6014, validation loss: 0.6570
2024-06-03 02:33:35 [INFO]: Epoch 023 - training loss: 0.5987, validation loss: 0.6506
2024-06-03 02:33:41 [INFO]: Epoch 024 - training loss: 0.5929, validation loss: 0.6602
2024-06-03 02:33:46 [INFO]: Epoch 025 - training loss: 0.5954, validation loss: 0.6391
2024-06-03 02:33:52 [INFO]: Epoch 026 - training loss: 0.5915, validation loss: 0.6566
2024-06-03 02:33:57 [INFO]: Epoch 027 - training loss: 0.5878, validation loss: 0.6554
2024-06-03 02:34:03 [INFO]: Epoch 028 - training loss: 0.5872, validation loss: 0.6425
2024-06-03 02:34:08 [INFO]: Epoch 029 - training loss: 0.5856, validation loss: 0.6568
2024-06-03 02:34:14 [INFO]: Epoch 030 - training loss: 0.5847, validation loss: 0.6459
2024-06-03 02:34:19 [INFO]: Epoch 031 - training loss: 0.5849, validation loss: 0.6454
2024-06-03 02:34:25 [INFO]: Epoch 032 - training loss: 0.5899, validation loss: 0.6536
2024-06-03 02:34:30 [INFO]: Epoch 033 - training loss: 0.5846, validation loss: 0.6431
2024-06-03 02:34:36 [INFO]: Epoch 034 - training loss: 0.5843, validation loss: 0.6375
2024-06-03 02:34:41 [INFO]: Epoch 035 - training loss: 0.5853, validation loss: 0.6425
2024-06-03 02:34:47 [INFO]: Epoch 036 - training loss: 0.5817, validation loss: 0.6328
2024-06-03 02:34:52 [INFO]: Epoch 037 - training loss: 0.5835, validation loss: 0.6276
2024-06-03 02:34:57 [INFO]: Epoch 038 - training loss: 0.5824, validation loss: 0.6358
2024-06-03 02:35:01 [INFO]: Epoch 039 - training loss: 0.5784, validation loss: 0.6430
2024-06-03 02:35:05 [INFO]: Epoch 040 - training loss: 0.5815, validation loss: 0.6338
2024-06-03 02:35:09 [INFO]: Epoch 041 - training loss: 0.5773, validation loss: 0.6273
2024-06-03 02:35:13 [INFO]: Epoch 042 - training loss: 0.5794, validation loss: 0.6284
2024-06-03 02:35:18 [INFO]: Epoch 043 - training loss: 0.5776, validation loss: 0.6255
2024-06-03 02:35:22 [INFO]: Epoch 044 - training loss: 0.5783, validation loss: 0.6321
2024-06-03 02:35:26 [INFO]: Epoch 045 - training loss: 0.5748, validation loss: 0.6388
2024-06-03 02:35:30 [INFO]: Epoch 046 - training loss: 0.5772, validation loss: 0.6343
2024-06-03 02:35:34 [INFO]: Epoch 047 - training loss: 0.5773, validation loss: 0.6393
2024-06-03 02:35:38 [INFO]: Epoch 048 - training loss: 0.5801, validation loss: 0.6347
2024-06-03 02:35:42 [INFO]: Epoch 049 - training loss: 0.5750, validation loss: 0.6341
2024-06-03 02:35:46 [INFO]: Epoch 050 - training loss: 0.5767, validation loss: 0.6243
2024-06-03 02:35:50 [INFO]: Epoch 051 - training loss: 0.5747, validation loss: 0.6349
2024-06-03 02:35:55 [INFO]: Epoch 052 - training loss: 0.5724, validation loss: 0.6573
2024-06-03 02:35:59 [INFO]: Epoch 053 - training loss: 0.5764, validation loss: 0.6373
2024-06-03 02:36:03 [INFO]: Epoch 054 - training loss: 0.5806, validation loss: 0.6137
2024-06-03 02:36:07 [INFO]: Epoch 055 - training loss: 0.5797, validation loss: 0.6386
2024-06-03 02:36:11 [INFO]: Epoch 056 - training loss: 0.5778, validation loss: 0.6386
2024-06-03 02:36:15 [INFO]: Epoch 057 - training loss: 0.5752, validation loss: 0.6371
2024-06-03 02:36:19 [INFO]: Epoch 058 - training loss: 0.5720, validation loss: 0.6350
2024-06-03 02:36:23 [INFO]: Epoch 059 - training loss: 0.5726, validation loss: 0.6358
2024-06-03 02:36:27 [INFO]: Epoch 060 - training loss: 0.5744, validation loss: 0.6343
2024-06-03 02:36:32 [INFO]: Epoch 061 - training loss: 0.5746, validation loss: 0.6524
2024-06-03 02:36:36 [INFO]: Epoch 062 - training loss: 0.5758, validation loss: 0.6423
2024-06-03 02:36:40 [INFO]: Epoch 063 - training loss: 0.5735, validation loss: 0.6569
2024-06-03 02:36:44 [INFO]: Epoch 064 - training loss: 0.5742, validation loss: 0.6276
2024-06-03 02:36:44 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:36:44 [INFO]: Finished training. The best model is from epoch#54.
2024-06-03 02:36:52 [INFO]: Saved the model to results_point_rate05/PeMS/SCINet_PeMS/round_2/20240603_T023115/SCINet.pypots
2024-06-03 02:36:54 [INFO]: Successfully saved to results_point_rate05/PeMS/SCINet_PeMS/round_2/imputation.pkl
2024-06-03 02:36:54 [INFO]: Round2 - SCINet on PeMS: MAE=0.5204, MSE=0.9270, MRE=0.6457
2024-06-03 02:36:54 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 02:36:54 [INFO]: Using the given device: cuda:0
2024-06-03 02:36:54 [INFO]: Model files will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_3/20240603_T023654
2024-06-03 02:36:54 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_3/20240603_T023654/tensorboard
2024-06-03 02:37:03 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-03 02:37:07 [INFO]: Epoch 001 - training loss: 1.5906, validation loss: 1.0778
2024-06-03 02:37:11 [INFO]: Epoch 002 - training loss: 1.1055, validation loss: 0.9795
2024-06-03 02:37:15 [INFO]: Epoch 003 - training loss: 0.9642, validation loss: 0.9164
2024-06-03 02:37:19 [INFO]: Epoch 004 - training loss: 0.8915, validation loss: 0.8641
2024-06-03 02:37:23 [INFO]: Epoch 005 - training loss: 0.8456, validation loss: 0.8392
2024-06-03 02:37:27 [INFO]: Epoch 006 - training loss: 0.8026, validation loss: 0.8221
2024-06-03 02:37:31 [INFO]: Epoch 007 - training loss: 0.7681, validation loss: 0.8151
2024-06-03 02:37:35 [INFO]: Epoch 008 - training loss: 0.7479, validation loss: 0.7974
2024-06-03 02:37:40 [INFO]: Epoch 009 - training loss: 0.7255, validation loss: 0.7725
2024-06-03 02:37:44 [INFO]: Epoch 010 - training loss: 0.7051, validation loss: 0.7695
2024-06-03 02:37:48 [INFO]: Epoch 011 - training loss: 0.6853, validation loss: 0.7866
2024-06-03 02:37:52 [INFO]: Epoch 012 - training loss: 0.6758, validation loss: 0.7758
2024-06-03 02:37:56 [INFO]: Epoch 013 - training loss: 0.6664, validation loss: 0.7735
2024-06-03 02:38:00 [INFO]: Epoch 014 - training loss: 0.6575, validation loss: 0.7852
2024-06-03 02:38:04 [INFO]: Epoch 015 - training loss: 0.6500, validation loss: 0.7733
2024-06-03 02:38:08 [INFO]: Epoch 016 - training loss: 0.6511, validation loss: 0.7612
2024-06-03 02:38:12 [INFO]: Epoch 017 - training loss: 0.6462, validation loss: 0.7616
2024-06-03 02:38:16 [INFO]: Epoch 018 - training loss: 0.6420, validation loss: 0.7636
2024-06-03 02:38:20 [INFO]: Epoch 019 - training loss: 0.6370, validation loss: 0.7554
2024-06-03 02:38:25 [INFO]: Epoch 020 - training loss: 0.6406, validation loss: 0.7644
2024-06-03 02:38:29 [INFO]: Epoch 021 - training loss: 0.6366, validation loss: 0.7853
2024-06-03 02:38:33 [INFO]: Epoch 022 - training loss: 0.6341, validation loss: 0.7643
2024-06-03 02:38:37 [INFO]: Epoch 023 - training loss: 0.6298, validation loss: 0.7782
2024-06-03 02:38:41 [INFO]: Epoch 024 - training loss: 0.6316, validation loss: 0.7435
2024-06-03 02:38:45 [INFO]: Epoch 025 - training loss: 0.6318, validation loss: 0.7629
2024-06-03 02:38:49 [INFO]: Epoch 026 - training loss: 0.6297, validation loss: 0.7666
2024-06-03 02:38:53 [INFO]: Epoch 027 - training loss: 0.6314, validation loss: 0.7835
2024-06-03 02:38:57 [INFO]: Epoch 028 - training loss: 0.6247, validation loss: 0.7554
2024-06-03 02:39:01 [INFO]: Epoch 029 - training loss: 0.6195, validation loss: 0.7505
2024-06-03 02:39:05 [INFO]: Epoch 030 - training loss: 0.6209, validation loss: 0.7513
2024-06-03 02:39:10 [INFO]: Epoch 031 - training loss: 0.6180, validation loss: 0.7446
2024-06-03 02:39:14 [INFO]: Epoch 032 - training loss: 0.6129, validation loss: 0.7393
2024-06-03 02:39:18 [INFO]: Epoch 033 - training loss: 0.6131, validation loss: 0.7325
2024-06-03 02:39:22 [INFO]: Epoch 034 - training loss: 0.6157, validation loss: 0.7267
2024-06-03 02:39:26 [INFO]: Epoch 035 - training loss: 0.6193, validation loss: 0.7236
2024-06-03 02:39:30 [INFO]: Epoch 036 - training loss: 0.6159, validation loss: 0.7272
2024-06-03 02:39:34 [INFO]: Epoch 037 - training loss: 0.6173, validation loss: 0.7001
2024-06-03 02:39:38 [INFO]: Epoch 038 - training loss: 0.6157, validation loss: 0.7093
2024-06-03 02:39:42 [INFO]: Epoch 039 - training loss: 0.6124, validation loss: 0.7154
2024-06-03 02:39:47 [INFO]: Epoch 040 - training loss: 0.6059, validation loss: 0.7264
2024-06-03 02:39:51 [INFO]: Epoch 041 - training loss: 0.6054, validation loss: 0.7102
2024-06-03 02:39:55 [INFO]: Epoch 042 - training loss: 0.6112, validation loss: 0.6984
2024-06-03 02:39:59 [INFO]: Epoch 043 - training loss: 0.6102, validation loss: 0.6936
2024-06-03 02:40:03 [INFO]: Epoch 044 - training loss: 0.6062, validation loss: 0.7245
2024-06-03 02:40:07 [INFO]: Epoch 045 - training loss: 0.6068, validation loss: 0.6964
2024-06-03 02:40:11 [INFO]: Epoch 046 - training loss: 0.6063, validation loss: 0.7017
2024-06-03 02:40:15 [INFO]: Epoch 047 - training loss: 0.6057, validation loss: 0.6840
2024-06-03 02:40:19 [INFO]: Epoch 048 - training loss: 0.6059, validation loss: 0.6930
2024-06-03 02:40:23 [INFO]: Epoch 049 - training loss: 0.6023, validation loss: 0.6880
2024-06-03 02:40:28 [INFO]: Epoch 050 - training loss: 0.6049, validation loss: 0.6804
2024-06-03 02:40:32 [INFO]: Epoch 051 - training loss: 0.6010, validation loss: 0.6715
2024-06-03 02:40:36 [INFO]: Epoch 052 - training loss: 0.6076, validation loss: 0.6779
2024-06-03 02:40:40 [INFO]: Epoch 053 - training loss: 0.6041, validation loss: 0.6694
2024-06-03 02:40:44 [INFO]: Epoch 054 - training loss: 0.5966, validation loss: 0.6937
2024-06-03 02:40:48 [INFO]: Epoch 055 - training loss: 0.6016, validation loss: 0.6810
2024-06-03 02:40:52 [INFO]: Epoch 056 - training loss: 0.6032, validation loss: 0.6865
2024-06-03 02:40:56 [INFO]: Epoch 057 - training loss: 0.5991, validation loss: 0.6747
2024-06-03 02:41:00 [INFO]: Epoch 058 - training loss: 0.5997, validation loss: 0.6919
2024-06-03 02:41:04 [INFO]: Epoch 059 - training loss: 0.6004, validation loss: 0.6757
2024-06-03 02:41:09 [INFO]: Epoch 060 - training loss: 0.5996, validation loss: 0.6715
2024-06-03 02:41:13 [INFO]: Epoch 061 - training loss: 0.5980, validation loss: 0.6983
2024-06-03 02:41:17 [INFO]: Epoch 062 - training loss: 0.6011, validation loss: 0.6767
2024-06-03 02:41:21 [INFO]: Epoch 063 - training loss: 0.5983, validation loss: 0.6686
2024-06-03 02:41:25 [INFO]: Epoch 064 - training loss: 0.5973, validation loss: 0.6840
2024-06-03 02:41:29 [INFO]: Epoch 065 - training loss: 0.6037, validation loss: 0.6887
2024-06-03 02:41:33 [INFO]: Epoch 066 - training loss: 0.5982, validation loss: 0.6785
2024-06-03 02:41:37 [INFO]: Epoch 067 - training loss: 0.5983, validation loss: 0.6577
2024-06-03 02:41:41 [INFO]: Epoch 068 - training loss: 0.5996, validation loss: 0.6770
2024-06-03 02:41:45 [INFO]: Epoch 069 - training loss: 0.6054, validation loss: 0.6989
2024-06-03 02:41:50 [INFO]: Epoch 070 - training loss: 0.6016, validation loss: 0.6550
2024-06-03 02:41:54 [INFO]: Epoch 071 - training loss: 0.6009, validation loss: 0.6663
2024-06-03 02:41:58 [INFO]: Epoch 072 - training loss: 0.5986, validation loss: 0.6570
2024-06-03 02:42:02 [INFO]: Epoch 073 - training loss: 0.5943, validation loss: 0.6765
2024-06-03 02:42:06 [INFO]: Epoch 074 - training loss: 0.6018, validation loss: 0.6655
2024-06-03 02:42:10 [INFO]: Epoch 075 - training loss: 0.6034, validation loss: 0.6760
2024-06-03 02:42:14 [INFO]: Epoch 076 - training loss: 0.5951, validation loss: 0.6729
2024-06-03 02:42:18 [INFO]: Epoch 077 - training loss: 0.5959, validation loss: 0.6772
2024-06-03 02:42:23 [INFO]: Epoch 078 - training loss: 0.5960, validation loss: 0.6659
2024-06-03 02:42:27 [INFO]: Epoch 079 - training loss: 0.5986, validation loss: 0.6826
2024-06-03 02:42:31 [INFO]: Epoch 080 - training loss: 0.5974, validation loss: 0.6721
2024-06-03 02:42:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:42:31 [INFO]: Finished training. The best model is from epoch#70.
2024-06-03 02:42:39 [INFO]: Saved the model to results_point_rate05/PeMS/SCINet_PeMS/round_3/20240603_T023654/SCINet.pypots
2024-06-03 02:42:40 [INFO]: Successfully saved to results_point_rate05/PeMS/SCINet_PeMS/round_3/imputation.pkl
2024-06-03 02:42:40 [INFO]: Round3 - SCINet on PeMS: MAE=0.6033, MSE=1.0411, MRE=0.7487
2024-06-03 02:42:40 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 02:42:40 [INFO]: Using the given device: cuda:0
2024-06-03 02:42:40 [INFO]: Model files will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_4/20240603_T024240
2024-06-03 02:42:40 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/SCINet_PeMS/round_4/20240603_T024240/tensorboard
2024-06-03 02:42:49 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 1,143,027,230
2024-06-03 02:42:53 [INFO]: Epoch 001 - training loss: 1.6984, validation loss: 1.1483
2024-06-03 02:42:57 [INFO]: Epoch 002 - training loss: 1.1679, validation loss: 0.9941
2024-06-03 02:43:02 [INFO]: Epoch 003 - training loss: 0.9845, validation loss: 0.8995
2024-06-03 02:43:06 [INFO]: Epoch 004 - training loss: 0.9017, validation loss: 0.8478
2024-06-03 02:43:10 [INFO]: Epoch 005 - training loss: 0.8488, validation loss: 0.8258
2024-06-03 02:43:14 [INFO]: Epoch 006 - training loss: 0.7946, validation loss: 0.8015
2024-06-03 02:43:18 [INFO]: Epoch 007 - training loss: 0.7672, validation loss: 0.7563
2024-06-03 02:43:22 [INFO]: Epoch 008 - training loss: 0.7322, validation loss: 0.7267
2024-06-03 02:43:26 [INFO]: Epoch 009 - training loss: 0.7080, validation loss: 0.7093
2024-06-03 02:43:30 [INFO]: Epoch 010 - training loss: 0.6831, validation loss: 0.6843
2024-06-03 02:43:34 [INFO]: Epoch 011 - training loss: 0.6636, validation loss: 0.6871
2024-06-03 02:43:38 [INFO]: Epoch 012 - training loss: 0.6510, validation loss: 0.6948
2024-06-03 02:43:43 [INFO]: Epoch 013 - training loss: 0.6424, validation loss: 0.6545
2024-06-03 02:43:47 [INFO]: Epoch 014 - training loss: 0.6346, validation loss: 0.6469
2024-06-03 02:43:51 [INFO]: Epoch 015 - training loss: 0.6308, validation loss: 0.6627
2024-06-03 02:43:55 [INFO]: Epoch 016 - training loss: 0.6210, validation loss: 0.6379
2024-06-03 02:43:59 [INFO]: Epoch 017 - training loss: 0.6169, validation loss: 0.6453
2024-06-03 02:44:03 [INFO]: Epoch 018 - training loss: 0.6189, validation loss: 0.6780
2024-06-03 02:44:07 [INFO]: Epoch 019 - training loss: 0.6189, validation loss: 0.6125
2024-06-03 02:44:11 [INFO]: Epoch 020 - training loss: 0.6227, validation loss: 0.6521
2024-06-03 02:44:15 [INFO]: Epoch 021 - training loss: 0.6110, validation loss: 0.6640
2024-06-03 02:44:20 [INFO]: Epoch 022 - training loss: 0.6112, validation loss: 0.6767
2024-06-03 02:44:24 [INFO]: Epoch 023 - training loss: 0.6062, validation loss: 0.6427
2024-06-03 02:44:28 [INFO]: Epoch 024 - training loss: 0.6080, validation loss: 0.6661
2024-06-03 02:44:32 [INFO]: Epoch 025 - training loss: 0.6063, validation loss: 0.6587
2024-06-03 02:44:36 [INFO]: Epoch 026 - training loss: 0.6053, validation loss: 0.6844
2024-06-03 02:44:40 [INFO]: Epoch 027 - training loss: 0.6026, validation loss: 0.6393
2024-06-03 02:44:44 [INFO]: Epoch 028 - training loss: 0.6053, validation loss: 0.6724
2024-06-03 02:44:48 [INFO]: Epoch 029 - training loss: 0.6017, validation loss: 0.6616
2024-06-03 02:44:48 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:44:48 [INFO]: Finished training. The best model is from epoch#19.
2024-06-03 02:44:56 [INFO]: Saved the model to results_point_rate05/PeMS/SCINet_PeMS/round_4/20240603_T024240/SCINet.pypots
2024-06-03 02:44:58 [INFO]: Successfully saved to results_point_rate05/PeMS/SCINet_PeMS/round_4/imputation.pkl
2024-06-03 02:44:58 [INFO]: Round4 - SCINet on PeMS: MAE=0.5924, MSE=1.0403, MRE=0.7351
2024-06-03 02:44:58 [INFO]: Done! Final results:
Averaged SCINet (1,143,027,230 params) on PeMS: MAE=0.5001 ± 0.09258537702423848, MSE=0.8488 ± 0.19313424366400836, MRE=0.6206 ± 0.1148905992826294, average inference time=0.41
