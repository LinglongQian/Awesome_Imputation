2024-06-02 19:38:46 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:38:46 [INFO]: Using the given device: cuda:0
2024-06-02 19:38:47 [INFO]: Model files will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_0/20240602_T193847
2024-06-02 19:38:47 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_0/20240602_T193847/tensorboard
2024-06-02 19:38:48 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-02 19:38:54 [INFO]: Epoch 001 - training loss: 1.1693, validation loss: 0.9056
2024-06-02 19:38:58 [INFO]: Epoch 002 - training loss: 0.7222, validation loss: 0.6447
2024-06-02 19:39:03 [INFO]: Epoch 003 - training loss: 0.5864, validation loss: 0.6375
2024-06-02 19:39:08 [INFO]: Epoch 004 - training loss: 0.5490, validation loss: 0.5726
2024-06-02 19:39:13 [INFO]: Epoch 005 - training loss: 0.5209, validation loss: 0.5408
2024-06-02 19:39:19 [INFO]: Epoch 006 - training loss: 0.4972, validation loss: 0.5365
2024-06-02 19:39:24 [INFO]: Epoch 007 - training loss: 0.4912, validation loss: 0.5380
2024-06-02 19:39:29 [INFO]: Epoch 008 - training loss: 0.4803, validation loss: 0.5189
2024-06-02 19:39:34 [INFO]: Epoch 009 - training loss: 0.4620, validation loss: 0.5123
2024-06-02 19:39:40 [INFO]: Epoch 010 - training loss: 0.4471, validation loss: 0.5059
2024-06-02 19:39:45 [INFO]: Epoch 011 - training loss: 0.4401, validation loss: 0.4957
2024-06-02 19:39:49 [INFO]: Epoch 012 - training loss: 0.4295, validation loss: 0.4961
2024-06-02 19:39:54 [INFO]: Epoch 013 - training loss: 0.4230, validation loss: 0.4841
2024-06-02 19:39:59 [INFO]: Epoch 014 - training loss: 0.4139, validation loss: 0.4823
2024-06-02 19:40:04 [INFO]: Epoch 015 - training loss: 0.4178, validation loss: 0.4840
2024-06-02 19:40:09 [INFO]: Epoch 016 - training loss: 0.4164, validation loss: 0.4750
2024-06-02 19:40:14 [INFO]: Epoch 017 - training loss: 0.4010, validation loss: 0.4669
2024-06-02 19:40:19 [INFO]: Epoch 018 - training loss: 0.3926, validation loss: 0.4644
2024-06-02 19:40:25 [INFO]: Epoch 019 - training loss: 0.3901, validation loss: 0.4651
2024-06-02 19:40:30 [INFO]: Epoch 020 - training loss: 0.3848, validation loss: 0.4673
2024-06-02 19:40:35 [INFO]: Epoch 021 - training loss: 0.3842, validation loss: 0.4585
2024-06-02 19:40:40 [INFO]: Epoch 022 - training loss: 0.3789, validation loss: 0.4565
2024-06-02 19:40:45 [INFO]: Epoch 023 - training loss: 0.3788, validation loss: 0.4541
2024-06-02 19:40:49 [INFO]: Epoch 024 - training loss: 0.3762, validation loss: 0.4548
2024-06-02 19:40:54 [INFO]: Epoch 025 - training loss: 0.3671, validation loss: 0.4529
2024-06-02 19:41:00 [INFO]: Epoch 026 - training loss: 0.3625, validation loss: 0.4509
2024-06-02 19:41:05 [INFO]: Epoch 027 - training loss: 0.3602, validation loss: 0.4489
2024-06-02 19:41:10 [INFO]: Epoch 028 - training loss: 0.3615, validation loss: 0.4449
2024-06-02 19:41:15 [INFO]: Epoch 029 - training loss: 0.3538, validation loss: 0.4491
2024-06-02 19:41:20 [INFO]: Epoch 030 - training loss: 0.3509, validation loss: 0.4418
2024-06-02 19:41:26 [INFO]: Epoch 031 - training loss: 0.3513, validation loss: 0.4493
2024-06-02 19:41:31 [INFO]: Epoch 032 - training loss: 0.3509, validation loss: 0.4425
2024-06-02 19:41:36 [INFO]: Epoch 033 - training loss: 0.3441, validation loss: 0.4437
2024-06-02 19:41:40 [INFO]: Epoch 034 - training loss: 0.3454, validation loss: 0.4426
2024-06-02 19:41:45 [INFO]: Epoch 035 - training loss: 0.3433, validation loss: 0.4444
2024-06-02 19:41:51 [INFO]: Epoch 036 - training loss: 0.3427, validation loss: 0.4374
2024-06-02 19:41:56 [INFO]: Epoch 037 - training loss: 0.3411, validation loss: 0.4355
2024-06-02 19:42:01 [INFO]: Epoch 038 - training loss: 0.3383, validation loss: 0.4361
2024-06-02 19:42:06 [INFO]: Epoch 039 - training loss: 0.3385, validation loss: 0.4347
2024-06-02 19:42:11 [INFO]: Epoch 040 - training loss: 0.3340, validation loss: 0.4430
2024-06-02 19:42:16 [INFO]: Epoch 041 - training loss: 0.3325, validation loss: 0.4309
2024-06-02 19:42:21 [INFO]: Epoch 042 - training loss: 0.3290, validation loss: 0.4345
2024-06-02 19:42:26 [INFO]: Epoch 043 - training loss: 0.3262, validation loss: 0.4354
2024-06-02 19:42:31 [INFO]: Epoch 044 - training loss: 0.3261, validation loss: 0.4337
2024-06-02 19:42:36 [INFO]: Epoch 045 - training loss: 0.3280, validation loss: 0.4308
2024-06-02 19:42:41 [INFO]: Epoch 046 - training loss: 0.3293, validation loss: 0.4339
2024-06-02 19:42:46 [INFO]: Epoch 047 - training loss: 0.3252, validation loss: 0.4320
2024-06-02 19:42:51 [INFO]: Epoch 048 - training loss: 0.3219, validation loss: 0.4371
2024-06-02 19:42:57 [INFO]: Epoch 049 - training loss: 0.3217, validation loss: 0.4294
2024-06-02 19:43:02 [INFO]: Epoch 050 - training loss: 0.3216, validation loss: 0.4358
2024-06-02 19:43:07 [INFO]: Epoch 051 - training loss: 0.3239, validation loss: 0.4322
2024-06-02 19:43:12 [INFO]: Epoch 052 - training loss: 0.3161, validation loss: 0.4346
2024-06-02 19:43:17 [INFO]: Epoch 053 - training loss: 0.3170, validation loss: 0.4321
2024-06-02 19:43:21 [INFO]: Epoch 054 - training loss: 0.3133, validation loss: 0.4284
2024-06-02 19:43:25 [INFO]: Epoch 055 - training loss: 0.3125, validation loss: 0.4336
2024-06-02 19:43:31 [INFO]: Epoch 056 - training loss: 0.3114, validation loss: 0.4305
2024-06-02 19:43:36 [INFO]: Epoch 057 - training loss: 0.3164, validation loss: 0.4345
2024-06-02 19:43:41 [INFO]: Epoch 058 - training loss: 0.3140, validation loss: 0.4259
2024-06-02 19:43:46 [INFO]: Epoch 059 - training loss: 0.3073, validation loss: 0.4295
2024-06-02 19:43:51 [INFO]: Epoch 060 - training loss: 0.3102, validation loss: 0.4290
2024-06-02 19:43:56 [INFO]: Epoch 061 - training loss: 0.3120, validation loss: 0.4287
2024-06-02 19:44:01 [INFO]: Epoch 062 - training loss: 0.3078, validation loss: 0.4317
2024-06-02 19:44:06 [INFO]: Epoch 063 - training loss: 0.3033, validation loss: 0.4271
2024-06-02 19:44:11 [INFO]: Epoch 064 - training loss: 0.3133, validation loss: 0.4282
2024-06-02 19:44:16 [INFO]: Epoch 065 - training loss: 0.3088, validation loss: 0.4228
2024-06-02 19:44:20 [INFO]: Epoch 066 - training loss: 0.3018, validation loss: 0.4221
2024-06-02 19:44:26 [INFO]: Epoch 067 - training loss: 0.3044, validation loss: 0.4247
2024-06-02 19:44:31 [INFO]: Epoch 068 - training loss: 0.2986, validation loss: 0.4258
2024-06-02 19:44:36 [INFO]: Epoch 069 - training loss: 0.2985, validation loss: 0.4198
2024-06-02 19:44:41 [INFO]: Epoch 070 - training loss: 0.2974, validation loss: 0.4233
2024-06-02 19:44:47 [INFO]: Epoch 071 - training loss: 0.3024, validation loss: 0.4263
2024-06-02 19:44:52 [INFO]: Epoch 072 - training loss: 0.3015, validation loss: 0.4232
2024-06-02 19:44:57 [INFO]: Epoch 073 - training loss: 0.2975, validation loss: 0.4244
2024-06-02 19:45:02 [INFO]: Epoch 074 - training loss: 0.2952, validation loss: 0.4243
2024-06-02 19:45:06 [INFO]: Epoch 075 - training loss: 0.2934, validation loss: 0.4214
2024-06-02 19:45:11 [INFO]: Epoch 076 - training loss: 0.2935, validation loss: 0.4181
2024-06-02 19:45:16 [INFO]: Epoch 077 - training loss: 0.2913, validation loss: 0.4170
2024-06-02 19:45:20 [INFO]: Epoch 078 - training loss: 0.2932, validation loss: 0.4248
2024-06-02 19:45:25 [INFO]: Epoch 079 - training loss: 0.2913, validation loss: 0.4244
2024-06-02 19:45:30 [INFO]: Epoch 080 - training loss: 0.2985, validation loss: 0.4253
2024-06-02 19:45:34 [INFO]: Epoch 081 - training loss: 0.2925, validation loss: 0.4194
2024-06-02 19:45:39 [INFO]: Epoch 082 - training loss: 0.2909, validation loss: 0.4193
2024-06-02 19:45:44 [INFO]: Epoch 083 - training loss: 0.2900, validation loss: 0.4204
2024-06-02 19:45:48 [INFO]: Epoch 084 - training loss: 0.2967, validation loss: 0.4231
2024-06-02 19:45:53 [INFO]: Epoch 085 - training loss: 0.2907, validation loss: 0.4247
2024-06-02 19:45:57 [INFO]: Epoch 086 - training loss: 0.2898, validation loss: 0.4198
2024-06-02 19:46:02 [INFO]: Epoch 087 - training loss: 0.2881, validation loss: 0.4230
2024-06-02 19:46:02 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:46:02 [INFO]: Finished training. The best model is from epoch#77.
2024-06-02 19:46:02 [INFO]: Saved the model to results_point_rate05/PeMS/Crossformer_PeMS/round_0/20240602_T193847/Crossformer.pypots
2024-06-02 19:46:04 [INFO]: Successfully saved to results_point_rate05/PeMS/Crossformer_PeMS/round_0/imputation.pkl
2024-06-02 19:46:04 [INFO]: Round0 - Crossformer on PeMS: MAE=0.3518, MSE=0.6046, MRE=0.4366
2024-06-02 19:46:04 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 19:46:04 [INFO]: Using the given device: cuda:0
2024-06-02 19:46:04 [INFO]: Model files will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_1/20240602_T194604
2024-06-02 19:46:04 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_1/20240602_T194604/tensorboard
2024-06-02 19:46:05 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-02 19:46:09 [INFO]: Epoch 001 - training loss: 1.2078, validation loss: 0.9563
2024-06-02 19:46:14 [INFO]: Epoch 002 - training loss: 0.7672, validation loss: 0.7479
2024-06-02 19:46:19 [INFO]: Epoch 003 - training loss: 0.6235, validation loss: 0.6709
2024-06-02 19:46:23 [INFO]: Epoch 004 - training loss: 0.5654, validation loss: 0.6354
2024-06-02 19:46:28 [INFO]: Epoch 005 - training loss: 0.5457, validation loss: 0.5808
2024-06-02 19:46:33 [INFO]: Epoch 006 - training loss: 0.5140, validation loss: 0.5619
2024-06-02 19:46:37 [INFO]: Epoch 007 - training loss: 0.4938, validation loss: 0.5428
2024-06-02 19:46:42 [INFO]: Epoch 008 - training loss: 0.4861, validation loss: 0.5468
2024-06-02 19:46:46 [INFO]: Epoch 009 - training loss: 0.4866, validation loss: 0.5255
2024-06-02 19:46:50 [INFO]: Epoch 010 - training loss: 0.4595, validation loss: 0.5131
2024-06-02 19:46:55 [INFO]: Epoch 011 - training loss: 0.4467, validation loss: 0.5030
2024-06-02 19:46:59 [INFO]: Epoch 012 - training loss: 0.4411, validation loss: 0.5027
2024-06-02 19:47:04 [INFO]: Epoch 013 - training loss: 0.4314, validation loss: 0.4979
2024-06-02 19:47:09 [INFO]: Epoch 014 - training loss: 0.4308, validation loss: 0.4917
2024-06-02 19:47:14 [INFO]: Epoch 015 - training loss: 0.4194, validation loss: 0.4903
2024-06-02 19:47:19 [INFO]: Epoch 016 - training loss: 0.4145, validation loss: 0.4848
2024-06-02 19:47:24 [INFO]: Epoch 017 - training loss: 0.4069, validation loss: 0.4765
2024-06-02 19:47:28 [INFO]: Epoch 018 - training loss: 0.4058, validation loss: 0.4790
2024-06-02 19:47:33 [INFO]: Epoch 019 - training loss: 0.3977, validation loss: 0.4726
2024-06-02 19:47:37 [INFO]: Epoch 020 - training loss: 0.3906, validation loss: 0.4660
2024-06-02 19:47:42 [INFO]: Epoch 021 - training loss: 0.3857, validation loss: 0.4675
2024-06-02 19:47:46 [INFO]: Epoch 022 - training loss: 0.3815, validation loss: 0.4632
2024-06-02 19:47:51 [INFO]: Epoch 023 - training loss: 0.3808, validation loss: 0.4630
2024-06-02 19:47:56 [INFO]: Epoch 024 - training loss: 0.3697, validation loss: 0.4537
2024-06-02 19:48:00 [INFO]: Epoch 025 - training loss: 0.3662, validation loss: 0.4608
2024-06-02 19:48:05 [INFO]: Epoch 026 - training loss: 0.3736, validation loss: 0.4593
2024-06-02 19:48:10 [INFO]: Epoch 027 - training loss: 0.3799, validation loss: 0.4613
2024-06-02 19:48:15 [INFO]: Epoch 028 - training loss: 0.3658, validation loss: 0.4521
2024-06-02 19:48:19 [INFO]: Epoch 029 - training loss: 0.3618, validation loss: 0.4492
2024-06-02 19:48:24 [INFO]: Epoch 030 - training loss: 0.3566, validation loss: 0.4489
2024-06-02 19:48:28 [INFO]: Epoch 031 - training loss: 0.3511, validation loss: 0.4446
2024-06-02 19:48:32 [INFO]: Epoch 032 - training loss: 0.3552, validation loss: 0.4527
2024-06-02 19:48:37 [INFO]: Epoch 033 - training loss: 0.3525, validation loss: 0.4447
2024-06-02 19:48:41 [INFO]: Epoch 034 - training loss: 0.3471, validation loss: 0.4433
2024-06-02 19:48:46 [INFO]: Epoch 035 - training loss: 0.3424, validation loss: 0.4425
2024-06-02 19:48:51 [INFO]: Epoch 036 - training loss: 0.3397, validation loss: 0.4432
2024-06-02 19:48:56 [INFO]: Epoch 037 - training loss: 0.3403, validation loss: 0.4434
2024-06-02 19:49:01 [INFO]: Epoch 038 - training loss: 0.3402, validation loss: 0.4376
2024-06-02 19:49:05 [INFO]: Epoch 039 - training loss: 0.3411, validation loss: 0.4374
2024-06-02 19:49:10 [INFO]: Epoch 040 - training loss: 0.3370, validation loss: 0.4441
2024-06-02 19:49:15 [INFO]: Epoch 041 - training loss: 0.3422, validation loss: 0.4422
2024-06-02 19:49:19 [INFO]: Epoch 042 - training loss: 0.3379, validation loss: 0.4483
2024-06-02 19:49:23 [INFO]: Epoch 043 - training loss: 0.3300, validation loss: 0.4383
2024-06-02 19:49:28 [INFO]: Epoch 044 - training loss: 0.3294, validation loss: 0.4352
2024-06-02 19:49:33 [INFO]: Epoch 045 - training loss: 0.3249, validation loss: 0.4391
2024-06-02 19:49:37 [INFO]: Epoch 046 - training loss: 0.3258, validation loss: 0.4340
2024-06-02 19:49:42 [INFO]: Epoch 047 - training loss: 0.3255, validation loss: 0.4365
2024-06-02 19:49:47 [INFO]: Epoch 048 - training loss: 0.3293, validation loss: 0.4372
2024-06-02 19:49:52 [INFO]: Epoch 049 - training loss: 0.3216, validation loss: 0.4354
2024-06-02 19:49:56 [INFO]: Epoch 050 - training loss: 0.3242, validation loss: 0.4327
2024-06-02 19:50:01 [INFO]: Epoch 051 - training loss: 0.3236, validation loss: 0.4325
2024-06-02 19:50:06 [INFO]: Epoch 052 - training loss: 0.3323, validation loss: 0.4384
2024-06-02 19:50:10 [INFO]: Epoch 053 - training loss: 0.3248, validation loss: 0.4345
2024-06-02 19:50:14 [INFO]: Epoch 054 - training loss: 0.3189, validation loss: 0.4315
2024-06-02 19:50:19 [INFO]: Epoch 055 - training loss: 0.3144, validation loss: 0.4309
2024-06-02 19:50:24 [INFO]: Epoch 056 - training loss: 0.3149, validation loss: 0.4272
2024-06-02 19:50:28 [INFO]: Epoch 057 - training loss: 0.3113, validation loss: 0.4275
2024-06-02 19:50:33 [INFO]: Epoch 058 - training loss: 0.3202, validation loss: 0.4308
2024-06-02 19:50:38 [INFO]: Epoch 059 - training loss: 0.3156, validation loss: 0.4313
2024-06-02 19:50:43 [INFO]: Epoch 060 - training loss: 0.3113, validation loss: 0.4291
2024-06-02 19:50:48 [INFO]: Epoch 061 - training loss: 0.3112, validation loss: 0.4283
2024-06-02 19:50:52 [INFO]: Epoch 062 - training loss: 0.3086, validation loss: 0.4290
2024-06-02 19:50:57 [INFO]: Epoch 063 - training loss: 0.3119, validation loss: 0.4313
2024-06-02 19:51:01 [INFO]: Epoch 064 - training loss: 0.3109, validation loss: 0.4259
2024-06-02 19:51:05 [INFO]: Epoch 065 - training loss: 0.3064, validation loss: 0.4307
2024-06-02 19:51:10 [INFO]: Epoch 066 - training loss: 0.3048, validation loss: 0.4280
2024-06-02 19:51:14 [INFO]: Epoch 067 - training loss: 0.3075, validation loss: 0.4244
2024-06-02 19:51:19 [INFO]: Epoch 068 - training loss: 0.3062, validation loss: 0.4273
2024-06-02 19:51:24 [INFO]: Epoch 069 - training loss: 0.3067, validation loss: 0.4279
2024-06-02 19:51:29 [INFO]: Epoch 070 - training loss: 0.3052, validation loss: 0.4284
2024-06-02 19:51:33 [INFO]: Epoch 071 - training loss: 0.2983, validation loss: 0.4264
2024-06-02 19:51:38 [INFO]: Epoch 072 - training loss: 0.2995, validation loss: 0.4224
2024-06-02 19:51:43 [INFO]: Epoch 073 - training loss: 0.2956, validation loss: 0.4226
2024-06-02 19:51:47 [INFO]: Epoch 074 - training loss: 0.2960, validation loss: 0.4244
2024-06-02 19:51:51 [INFO]: Epoch 075 - training loss: 0.2974, validation loss: 0.4235
2024-06-02 19:51:55 [INFO]: Epoch 076 - training loss: 0.2971, validation loss: 0.4237
2024-06-02 19:52:00 [INFO]: Epoch 077 - training loss: 0.3016, validation loss: 0.4316
2024-06-02 19:52:04 [INFO]: Epoch 078 - training loss: 0.3032, validation loss: 0.4193
2024-06-02 19:52:08 [INFO]: Epoch 079 - training loss: 0.3033, validation loss: 0.4259
2024-06-02 19:52:12 [INFO]: Epoch 080 - training loss: 0.2995, validation loss: 0.4226
2024-06-02 19:52:17 [INFO]: Epoch 081 - training loss: 0.2959, validation loss: 0.4231
2024-06-02 19:52:21 [INFO]: Epoch 082 - training loss: 0.2916, validation loss: 0.4273
2024-06-02 19:52:25 [INFO]: Epoch 083 - training loss: 0.2925, validation loss: 0.4197
2024-06-02 19:52:29 [INFO]: Epoch 084 - training loss: 0.2917, validation loss: 0.4247
2024-06-02 19:52:33 [INFO]: Epoch 085 - training loss: 0.2920, validation loss: 0.4201
2024-06-02 19:52:37 [INFO]: Epoch 086 - training loss: 0.2918, validation loss: 0.4217
2024-06-02 19:52:41 [INFO]: Epoch 087 - training loss: 0.2963, validation loss: 0.4250
2024-06-02 19:52:45 [INFO]: Epoch 088 - training loss: 0.2957, validation loss: 0.4274
2024-06-02 19:52:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:52:45 [INFO]: Finished training. The best model is from epoch#78.
2024-06-02 19:52:45 [INFO]: Saved the model to results_point_rate05/PeMS/Crossformer_PeMS/round_1/20240602_T194604/Crossformer.pypots
2024-06-02 19:52:46 [INFO]: Successfully saved to results_point_rate05/PeMS/Crossformer_PeMS/round_1/imputation.pkl
2024-06-02 19:52:46 [INFO]: Round1 - Crossformer on PeMS: MAE=0.3610, MSE=0.6055, MRE=0.4479
2024-06-02 19:52:46 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 19:52:46 [INFO]: Using the given device: cuda:0
2024-06-02 19:52:47 [INFO]: Model files will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_2/20240602_T195246
2024-06-02 19:52:47 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_2/20240602_T195246/tensorboard
2024-06-02 19:52:47 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-02 19:52:51 [INFO]: Epoch 001 - training loss: 1.1577, validation loss: 0.8130
2024-06-02 19:52:55 [INFO]: Epoch 002 - training loss: 0.7373, validation loss: 0.7263
2024-06-02 19:52:59 [INFO]: Epoch 003 - training loss: 0.6122, validation loss: 0.6322
2024-06-02 19:53:03 [INFO]: Epoch 004 - training loss: 0.5673, validation loss: 0.6109
2024-06-02 19:53:07 [INFO]: Epoch 005 - training loss: 0.5334, validation loss: 0.5944
2024-06-02 19:53:11 [INFO]: Epoch 006 - training loss: 0.5142, validation loss: 0.5561
2024-06-02 19:53:15 [INFO]: Epoch 007 - training loss: 0.4933, validation loss: 0.5472
2024-06-02 19:53:19 [INFO]: Epoch 008 - training loss: 0.4811, validation loss: 0.5181
2024-06-02 19:53:23 [INFO]: Epoch 009 - training loss: 0.4700, validation loss: 0.5100
2024-06-02 19:53:26 [INFO]: Epoch 010 - training loss: 0.4540, validation loss: 0.4991
2024-06-02 19:53:30 [INFO]: Epoch 011 - training loss: 0.4510, validation loss: 0.5000
2024-06-02 19:53:34 [INFO]: Epoch 012 - training loss: 0.4430, validation loss: 0.4966
2024-06-02 19:53:38 [INFO]: Epoch 013 - training loss: 0.4314, validation loss: 0.4873
2024-06-02 19:53:42 [INFO]: Epoch 014 - training loss: 0.4255, validation loss: 0.4809
2024-06-02 19:53:46 [INFO]: Epoch 015 - training loss: 0.4216, validation loss: 0.4753
2024-06-02 19:53:50 [INFO]: Epoch 016 - training loss: 0.4078, validation loss: 0.4746
2024-06-02 19:53:54 [INFO]: Epoch 017 - training loss: 0.4077, validation loss: 0.4766
2024-06-02 19:53:58 [INFO]: Epoch 018 - training loss: 0.4017, validation loss: 0.4623
2024-06-02 19:54:01 [INFO]: Epoch 019 - training loss: 0.3947, validation loss: 0.4627
2024-06-02 19:54:05 [INFO]: Epoch 020 - training loss: 0.3893, validation loss: 0.4576
2024-06-02 19:54:09 [INFO]: Epoch 021 - training loss: 0.3816, validation loss: 0.4540
2024-06-02 19:54:13 [INFO]: Epoch 022 - training loss: 0.3804, validation loss: 0.4564
2024-06-02 19:54:17 [INFO]: Epoch 023 - training loss: 0.3778, validation loss: 0.4537
2024-06-02 19:54:21 [INFO]: Epoch 024 - training loss: 0.3825, validation loss: 0.4565
2024-06-02 19:54:25 [INFO]: Epoch 025 - training loss: 0.3725, validation loss: 0.4487
2024-06-02 19:54:29 [INFO]: Epoch 026 - training loss: 0.3654, validation loss: 0.4456
2024-06-02 19:54:32 [INFO]: Epoch 027 - training loss: 0.3677, validation loss: 0.4464
2024-06-02 19:54:37 [INFO]: Epoch 028 - training loss: 0.3657, validation loss: 0.4516
2024-06-02 19:54:40 [INFO]: Epoch 029 - training loss: 0.3675, validation loss: 0.4542
2024-06-02 19:54:44 [INFO]: Epoch 030 - training loss: 0.3608, validation loss: 0.4468
2024-06-02 19:54:48 [INFO]: Epoch 031 - training loss: 0.3565, validation loss: 0.4387
2024-06-02 19:54:52 [INFO]: Epoch 032 - training loss: 0.3595, validation loss: 0.4399
2024-06-02 19:54:56 [INFO]: Epoch 033 - training loss: 0.3542, validation loss: 0.4414
2024-06-02 19:55:00 [INFO]: Epoch 034 - training loss: 0.3547, validation loss: 0.4383
2024-06-02 19:55:04 [INFO]: Epoch 035 - training loss: 0.3492, validation loss: 0.4362
2024-06-02 19:55:07 [INFO]: Epoch 036 - training loss: 0.3446, validation loss: 0.4367
2024-06-02 19:55:11 [INFO]: Epoch 037 - training loss: 0.3480, validation loss: 0.4525
2024-06-02 19:55:15 [INFO]: Epoch 038 - training loss: 0.3513, validation loss: 0.4428
2024-06-02 19:55:19 [INFO]: Epoch 039 - training loss: 0.3399, validation loss: 0.4321
2024-06-02 19:55:23 [INFO]: Epoch 040 - training loss: 0.3418, validation loss: 0.4376
2024-06-02 19:55:27 [INFO]: Epoch 041 - training loss: 0.3362, validation loss: 0.4355
2024-06-02 19:55:31 [INFO]: Epoch 042 - training loss: 0.3321, validation loss: 0.4363
2024-06-02 19:55:35 [INFO]: Epoch 043 - training loss: 0.3311, validation loss: 0.4343
2024-06-02 19:55:39 [INFO]: Epoch 044 - training loss: 0.3364, validation loss: 0.4319
2024-06-02 19:55:43 [INFO]: Epoch 045 - training loss: 0.3318, validation loss: 0.4336
2024-06-02 19:55:47 [INFO]: Epoch 046 - training loss: 0.3254, validation loss: 0.4365
2024-06-02 19:55:50 [INFO]: Epoch 047 - training loss: 0.3284, validation loss: 0.4327
2024-06-02 19:55:54 [INFO]: Epoch 048 - training loss: 0.3263, validation loss: 0.4285
2024-06-02 19:55:58 [INFO]: Epoch 049 - training loss: 0.3233, validation loss: 0.4290
2024-06-02 19:56:02 [INFO]: Epoch 050 - training loss: 0.3243, validation loss: 0.4299
2024-06-02 19:56:06 [INFO]: Epoch 051 - training loss: 0.3229, validation loss: 0.4299
2024-06-02 19:56:10 [INFO]: Epoch 052 - training loss: 0.3202, validation loss: 0.4326
2024-06-02 19:56:14 [INFO]: Epoch 053 - training loss: 0.3185, validation loss: 0.4265
2024-06-02 19:56:18 [INFO]: Epoch 054 - training loss: 0.3183, validation loss: 0.4306
2024-06-02 19:56:22 [INFO]: Epoch 055 - training loss: 0.3162, validation loss: 0.4327
2024-06-02 19:56:26 [INFO]: Epoch 056 - training loss: 0.3140, validation loss: 0.4264
2024-06-02 19:56:30 [INFO]: Epoch 057 - training loss: 0.3135, validation loss: 0.4264
2024-06-02 19:56:33 [INFO]: Epoch 058 - training loss: 0.3147, validation loss: 0.4265
2024-06-02 19:56:37 [INFO]: Epoch 059 - training loss: 0.3121, validation loss: 0.4303
2024-06-02 19:56:41 [INFO]: Epoch 060 - training loss: 0.3088, validation loss: 0.4228
2024-06-02 19:56:45 [INFO]: Epoch 061 - training loss: 0.3138, validation loss: 0.4268
2024-06-02 19:56:49 [INFO]: Epoch 062 - training loss: 0.3087, validation loss: 0.4250
2024-06-02 19:56:53 [INFO]: Epoch 063 - training loss: 0.3123, validation loss: 0.4243
2024-06-02 19:56:57 [INFO]: Epoch 064 - training loss: 0.3068, validation loss: 0.4261
2024-06-02 19:57:01 [INFO]: Epoch 065 - training loss: 0.3084, validation loss: 0.4251
2024-06-02 19:57:05 [INFO]: Epoch 066 - training loss: 0.3035, validation loss: 0.4201
2024-06-02 19:57:09 [INFO]: Epoch 067 - training loss: 0.3041, validation loss: 0.4211
2024-06-02 19:57:13 [INFO]: Epoch 068 - training loss: 0.3025, validation loss: 0.4279
2024-06-02 19:57:17 [INFO]: Epoch 069 - training loss: 0.3048, validation loss: 0.4213
2024-06-02 19:57:21 [INFO]: Epoch 070 - training loss: 0.3002, validation loss: 0.4237
2024-06-02 19:57:25 [INFO]: Epoch 071 - training loss: 0.3020, validation loss: 0.4170
2024-06-02 19:57:29 [INFO]: Epoch 072 - training loss: 0.2987, validation loss: 0.4239
2024-06-02 19:57:33 [INFO]: Epoch 073 - training loss: 0.3022, validation loss: 0.4242
2024-06-02 19:57:37 [INFO]: Epoch 074 - training loss: 0.3078, validation loss: 0.4232
2024-06-02 19:57:41 [INFO]: Epoch 075 - training loss: 0.3095, validation loss: 0.4247
2024-06-02 19:57:45 [INFO]: Epoch 076 - training loss: 0.3002, validation loss: 0.4278
2024-06-02 19:57:49 [INFO]: Epoch 077 - training loss: 0.2983, validation loss: 0.4223
2024-06-02 19:57:53 [INFO]: Epoch 078 - training loss: 0.2925, validation loss: 0.4259
2024-06-02 19:57:57 [INFO]: Epoch 079 - training loss: 0.2927, validation loss: 0.4229
2024-06-02 19:58:01 [INFO]: Epoch 080 - training loss: 0.3004, validation loss: 0.4277
2024-06-02 19:58:05 [INFO]: Epoch 081 - training loss: 0.2974, validation loss: 0.4187
2024-06-02 19:58:05 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 19:58:05 [INFO]: Finished training. The best model is from epoch#71.
2024-06-02 19:58:05 [INFO]: Saved the model to results_point_rate05/PeMS/Crossformer_PeMS/round_2/20240602_T195246/Crossformer.pypots
2024-06-02 19:58:06 [INFO]: Successfully saved to results_point_rate05/PeMS/Crossformer_PeMS/round_2/imputation.pkl
2024-06-02 19:58:06 [INFO]: Round2 - Crossformer on PeMS: MAE=0.3580, MSE=0.6020, MRE=0.4442
2024-06-02 19:58:06 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 19:58:06 [INFO]: Using the given device: cuda:0
2024-06-02 19:58:06 [INFO]: Model files will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_3/20240602_T195806
2024-06-02 19:58:06 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_3/20240602_T195806/tensorboard
2024-06-02 19:58:07 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-02 19:58:10 [INFO]: Epoch 001 - training loss: 1.1121, validation loss: 0.8215
2024-06-02 19:58:14 [INFO]: Epoch 002 - training loss: 0.7043, validation loss: 0.6459
2024-06-02 19:58:18 [INFO]: Epoch 003 - training loss: 0.6094, validation loss: 0.6095
2024-06-02 19:58:22 [INFO]: Epoch 004 - training loss: 0.5433, validation loss: 0.5567
2024-06-02 19:58:26 [INFO]: Epoch 005 - training loss: 0.5172, validation loss: 0.5555
2024-06-02 19:58:30 [INFO]: Epoch 006 - training loss: 0.5056, validation loss: 0.5408
2024-06-02 19:58:34 [INFO]: Epoch 007 - training loss: 0.4897, validation loss: 0.5364
2024-06-02 19:58:38 [INFO]: Epoch 008 - training loss: 0.4905, validation loss: 0.5183
2024-06-02 19:58:42 [INFO]: Epoch 009 - training loss: 0.4671, validation loss: 0.5052
2024-06-02 19:58:46 [INFO]: Epoch 010 - training loss: 0.4496, validation loss: 0.5001
2024-06-02 19:58:49 [INFO]: Epoch 011 - training loss: 0.4362, validation loss: 0.4929
2024-06-02 19:58:53 [INFO]: Epoch 012 - training loss: 0.4359, validation loss: 0.4916
2024-06-02 19:58:57 [INFO]: Epoch 013 - training loss: 0.4349, validation loss: 0.4890
2024-06-02 19:59:01 [INFO]: Epoch 014 - training loss: 0.4269, validation loss: 0.4887
2024-06-02 19:59:04 [INFO]: Epoch 015 - training loss: 0.4174, validation loss: 0.4809
2024-06-02 19:59:09 [INFO]: Epoch 016 - training loss: 0.4086, validation loss: 0.4742
2024-06-02 19:59:13 [INFO]: Epoch 017 - training loss: 0.4082, validation loss: 0.4754
2024-06-02 19:59:16 [INFO]: Epoch 018 - training loss: 0.3980, validation loss: 0.4660
2024-06-02 19:59:20 [INFO]: Epoch 019 - training loss: 0.3952, validation loss: 0.4656
2024-06-02 19:59:24 [INFO]: Epoch 020 - training loss: 0.3890, validation loss: 0.4608
2024-06-02 19:59:28 [INFO]: Epoch 021 - training loss: 0.3858, validation loss: 0.4543
2024-06-02 19:59:32 [INFO]: Epoch 022 - training loss: 0.3750, validation loss: 0.4510
2024-06-02 19:59:36 [INFO]: Epoch 023 - training loss: 0.3733, validation loss: 0.4553
2024-06-02 19:59:40 [INFO]: Epoch 024 - training loss: 0.3704, validation loss: 0.4480
2024-06-02 19:59:43 [INFO]: Epoch 025 - training loss: 0.3742, validation loss: 0.4488
2024-06-02 19:59:47 [INFO]: Epoch 026 - training loss: 0.3686, validation loss: 0.4510
2024-06-02 19:59:51 [INFO]: Epoch 027 - training loss: 0.3658, validation loss: 0.4441
2024-06-02 19:59:55 [INFO]: Epoch 028 - training loss: 0.3665, validation loss: 0.4489
2024-06-02 19:59:59 [INFO]: Epoch 029 - training loss: 0.3587, validation loss: 0.4437
2024-06-02 20:00:03 [INFO]: Epoch 030 - training loss: 0.3583, validation loss: 0.4422
2024-06-02 20:00:07 [INFO]: Epoch 031 - training loss: 0.3531, validation loss: 0.4415
2024-06-02 20:00:11 [INFO]: Epoch 032 - training loss: 0.3562, validation loss: 0.4403
2024-06-02 20:00:15 [INFO]: Epoch 033 - training loss: 0.3503, validation loss: 0.4388
2024-06-02 20:00:19 [INFO]: Epoch 034 - training loss: 0.3523, validation loss: 0.4400
2024-06-02 20:00:23 [INFO]: Epoch 035 - training loss: 0.3458, validation loss: 0.4353
2024-06-02 20:00:26 [INFO]: Epoch 036 - training loss: 0.3441, validation loss: 0.4369
2024-06-02 20:00:30 [INFO]: Epoch 037 - training loss: 0.3454, validation loss: 0.4346
2024-06-02 20:00:34 [INFO]: Epoch 038 - training loss: 0.3453, validation loss: 0.4382
2024-06-02 20:00:37 [INFO]: Epoch 039 - training loss: 0.3374, validation loss: 0.4327
2024-06-02 20:00:41 [INFO]: Epoch 040 - training loss: 0.3414, validation loss: 0.4319
2024-06-02 20:00:45 [INFO]: Epoch 041 - training loss: 0.3389, validation loss: 0.4330
2024-06-02 20:00:49 [INFO]: Epoch 042 - training loss: 0.3330, validation loss: 0.4357
2024-06-02 20:00:54 [INFO]: Epoch 043 - training loss: 0.3394, validation loss: 0.4309
2024-06-02 20:00:57 [INFO]: Epoch 044 - training loss: 0.3299, validation loss: 0.4333
2024-06-02 20:01:01 [INFO]: Epoch 045 - training loss: 0.3340, validation loss: 0.4288
2024-06-02 20:01:05 [INFO]: Epoch 046 - training loss: 0.3285, validation loss: 0.4290
2024-06-02 20:01:09 [INFO]: Epoch 047 - training loss: 0.3298, validation loss: 0.4300
2024-06-02 20:01:13 [INFO]: Epoch 048 - training loss: 0.3249, validation loss: 0.4299
2024-06-02 20:01:17 [INFO]: Epoch 049 - training loss: 0.3254, validation loss: 0.4312
2024-06-02 20:01:21 [INFO]: Epoch 050 - training loss: 0.3196, validation loss: 0.4321
2024-06-02 20:01:25 [INFO]: Epoch 051 - training loss: 0.3200, validation loss: 0.4280
2024-06-02 20:01:29 [INFO]: Epoch 052 - training loss: 0.3212, validation loss: 0.4262
2024-06-02 20:01:33 [INFO]: Epoch 053 - training loss: 0.3232, validation loss: 0.4254
2024-06-02 20:01:37 [INFO]: Epoch 054 - training loss: 0.3164, validation loss: 0.4258
2024-06-02 20:01:41 [INFO]: Epoch 055 - training loss: 0.3200, validation loss: 0.4245
2024-06-02 20:01:45 [INFO]: Epoch 056 - training loss: 0.3184, validation loss: 0.4248
2024-06-02 20:01:49 [INFO]: Epoch 057 - training loss: 0.3161, validation loss: 0.4256
2024-06-02 20:01:52 [INFO]: Epoch 058 - training loss: 0.3118, validation loss: 0.4274
2024-06-02 20:01:56 [INFO]: Epoch 059 - training loss: 0.3164, validation loss: 0.4242
2024-06-02 20:02:00 [INFO]: Epoch 060 - training loss: 0.3114, validation loss: 0.4240
2024-06-02 20:02:04 [INFO]: Epoch 061 - training loss: 0.3128, validation loss: 0.4227
2024-06-02 20:02:07 [INFO]: Epoch 062 - training loss: 0.3130, validation loss: 0.4251
2024-06-02 20:02:11 [INFO]: Epoch 063 - training loss: 0.3103, validation loss: 0.4240
2024-06-02 20:02:15 [INFO]: Epoch 064 - training loss: 0.3127, validation loss: 0.4239
2024-06-02 20:02:18 [INFO]: Epoch 065 - training loss: 0.3103, validation loss: 0.4215
2024-06-02 20:02:22 [INFO]: Epoch 066 - training loss: 0.3062, validation loss: 0.4208
2024-06-02 20:02:25 [INFO]: Epoch 067 - training loss: 0.2985, validation loss: 0.4228
2024-06-02 20:02:29 [INFO]: Epoch 068 - training loss: 0.3005, validation loss: 0.4211
2024-06-02 20:02:32 [INFO]: Epoch 069 - training loss: 0.3047, validation loss: 0.4190
2024-06-02 20:02:36 [INFO]: Epoch 070 - training loss: 0.3026, validation loss: 0.4191
2024-06-02 20:02:39 [INFO]: Epoch 071 - training loss: 0.3038, validation loss: 0.4197
2024-06-02 20:02:43 [INFO]: Epoch 072 - training loss: 0.2994, validation loss: 0.4206
2024-06-02 20:02:47 [INFO]: Epoch 073 - training loss: 0.2987, validation loss: 0.4204
2024-06-02 20:02:50 [INFO]: Epoch 074 - training loss: 0.2970, validation loss: 0.4193
2024-06-02 20:02:53 [INFO]: Epoch 075 - training loss: 0.2989, validation loss: 0.4197
2024-06-02 20:02:57 [INFO]: Epoch 076 - training loss: 0.2975, validation loss: 0.4210
2024-06-02 20:03:01 [INFO]: Epoch 077 - training loss: 0.2962, validation loss: 0.4206
2024-06-02 20:03:04 [INFO]: Epoch 078 - training loss: 0.2932, validation loss: 0.4191
2024-06-02 20:03:08 [INFO]: Epoch 079 - training loss: 0.2921, validation loss: 0.4175
2024-06-02 20:03:11 [INFO]: Epoch 080 - training loss: 0.2978, validation loss: 0.4168
2024-06-02 20:03:15 [INFO]: Epoch 081 - training loss: 0.2967, validation loss: 0.4202
2024-06-02 20:03:19 [INFO]: Epoch 082 - training loss: 0.2974, validation loss: 0.4196
2024-06-02 20:03:22 [INFO]: Epoch 083 - training loss: 0.2958, validation loss: 0.4178
2024-06-02 20:03:26 [INFO]: Epoch 084 - training loss: 0.2917, validation loss: 0.4182
2024-06-02 20:03:30 [INFO]: Epoch 085 - training loss: 0.2943, validation loss: 0.4191
2024-06-02 20:03:33 [INFO]: Epoch 086 - training loss: 0.2922, validation loss: 0.4198
2024-06-02 20:03:37 [INFO]: Epoch 087 - training loss: 0.2875, validation loss: 0.4159
2024-06-02 20:03:40 [INFO]: Epoch 088 - training loss: 0.2917, validation loss: 0.4187
2024-06-02 20:03:44 [INFO]: Epoch 089 - training loss: 0.2963, validation loss: 0.4146
2024-06-02 20:03:47 [INFO]: Epoch 090 - training loss: 0.2881, validation loss: 0.4161
2024-06-02 20:03:51 [INFO]: Epoch 091 - training loss: 0.2869, validation loss: 0.4175
2024-06-02 20:03:54 [INFO]: Epoch 092 - training loss: 0.2832, validation loss: 0.4184
2024-06-02 20:03:58 [INFO]: Epoch 093 - training loss: 0.2866, validation loss: 0.4144
2024-06-02 20:04:02 [INFO]: Epoch 094 - training loss: 0.2866, validation loss: 0.4168
2024-06-02 20:04:05 [INFO]: Epoch 095 - training loss: 0.2855, validation loss: 0.4156
2024-06-02 20:04:09 [INFO]: Epoch 096 - training loss: 0.2840, validation loss: 0.4165
2024-06-02 20:04:13 [INFO]: Epoch 097 - training loss: 0.2831, validation loss: 0.4164
2024-06-02 20:04:16 [INFO]: Epoch 098 - training loss: 0.2842, validation loss: 0.4142
2024-06-02 20:04:20 [INFO]: Epoch 099 - training loss: 0.2829, validation loss: 0.4153
2024-06-02 20:04:23 [INFO]: Epoch 100 - training loss: 0.2816, validation loss: 0.4183
2024-06-02 20:04:23 [INFO]: Finished training. The best model is from epoch#98.
2024-06-02 20:04:23 [INFO]: Saved the model to results_point_rate05/PeMS/Crossformer_PeMS/round_3/20240602_T195806/Crossformer.pypots
2024-06-02 20:04:25 [INFO]: Successfully saved to results_point_rate05/PeMS/Crossformer_PeMS/round_3/imputation.pkl
2024-06-02 20:04:25 [INFO]: Round3 - Crossformer on PeMS: MAE=0.3538, MSE=0.5994, MRE=0.4391
2024-06-02 20:04:25 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 20:04:25 [INFO]: Using the given device: cuda:0
2024-06-02 20:04:25 [INFO]: Model files will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_4/20240602_T200425
2024-06-02 20:04:25 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/Crossformer_PeMS/round_4/20240602_T200425/tensorboard
2024-06-02 20:04:25 [INFO]: Crossformer initialized with the given hyperparameters, the number of trainable parameters: 12,645,238
2024-06-02 20:04:28 [INFO]: Epoch 001 - training loss: 1.1734, validation loss: 0.8256
2024-06-02 20:04:32 [INFO]: Epoch 002 - training loss: 0.7171, validation loss: 0.6596
2024-06-02 20:04:36 [INFO]: Epoch 003 - training loss: 0.5982, validation loss: 0.6463
2024-06-02 20:04:39 [INFO]: Epoch 004 - training loss: 0.5595, validation loss: 0.6074
2024-06-02 20:04:43 [INFO]: Epoch 005 - training loss: 0.5240, validation loss: 0.5481
2024-06-02 20:04:46 [INFO]: Epoch 006 - training loss: 0.4984, validation loss: 0.5392
2024-06-02 20:04:50 [INFO]: Epoch 007 - training loss: 0.4817, validation loss: 0.5277
2024-06-02 20:04:54 [INFO]: Epoch 008 - training loss: 0.4719, validation loss: 0.5186
2024-06-02 20:04:57 [INFO]: Epoch 009 - training loss: 0.4739, validation loss: 0.5099
2024-06-02 20:05:01 [INFO]: Epoch 010 - training loss: 0.4755, validation loss: 0.5168
2024-06-02 20:05:04 [INFO]: Epoch 011 - training loss: 0.4579, validation loss: 0.4961
2024-06-02 20:05:08 [INFO]: Epoch 012 - training loss: 0.4357, validation loss: 0.4967
2024-06-02 20:05:11 [INFO]: Epoch 013 - training loss: 0.4252, validation loss: 0.4873
2024-06-02 20:05:15 [INFO]: Epoch 014 - training loss: 0.4190, validation loss: 0.4875
2024-06-02 20:05:18 [INFO]: Epoch 015 - training loss: 0.4117, validation loss: 0.4813
2024-06-02 20:05:22 [INFO]: Epoch 016 - training loss: 0.4165, validation loss: 0.4703
2024-06-02 20:05:26 [INFO]: Epoch 017 - training loss: 0.3991, validation loss: 0.4679
2024-06-02 20:05:29 [INFO]: Epoch 018 - training loss: 0.3950, validation loss: 0.4702
2024-06-02 20:05:33 [INFO]: Epoch 019 - training loss: 0.3897, validation loss: 0.4675
2024-06-02 20:05:37 [INFO]: Epoch 020 - training loss: 0.3867, validation loss: 0.4614
2024-06-02 20:05:40 [INFO]: Epoch 021 - training loss: 0.3762, validation loss: 0.4552
2024-06-02 20:05:44 [INFO]: Epoch 022 - training loss: 0.3742, validation loss: 0.4553
2024-06-02 20:05:47 [INFO]: Epoch 023 - training loss: 0.3731, validation loss: 0.4512
2024-06-02 20:05:51 [INFO]: Epoch 024 - training loss: 0.3731, validation loss: 0.4525
2024-06-02 20:05:54 [INFO]: Epoch 025 - training loss: 0.3702, validation loss: 0.4523
2024-06-02 20:05:58 [INFO]: Epoch 026 - training loss: 0.3608, validation loss: 0.4521
2024-06-02 20:06:01 [INFO]: Epoch 027 - training loss: 0.3652, validation loss: 0.4502
2024-06-02 20:06:04 [INFO]: Epoch 028 - training loss: 0.3582, validation loss: 0.4433
2024-06-02 20:06:08 [INFO]: Epoch 029 - training loss: 0.3586, validation loss: 0.4525
2024-06-02 20:06:12 [INFO]: Epoch 030 - training loss: 0.3582, validation loss: 0.4412
2024-06-02 20:06:15 [INFO]: Epoch 031 - training loss: 0.3512, validation loss: 0.4420
2024-06-02 20:06:19 [INFO]: Epoch 032 - training loss: 0.3482, validation loss: 0.4419
2024-06-02 20:06:22 [INFO]: Epoch 033 - training loss: 0.3469, validation loss: 0.4457
2024-06-02 20:06:26 [INFO]: Epoch 034 - training loss: 0.3487, validation loss: 0.4403
2024-06-02 20:06:30 [INFO]: Epoch 035 - training loss: 0.3470, validation loss: 0.4457
2024-06-02 20:06:33 [INFO]: Epoch 036 - training loss: 0.3438, validation loss: 0.4384
2024-06-02 20:06:37 [INFO]: Epoch 037 - training loss: 0.3421, validation loss: 0.4400
2024-06-02 20:06:40 [INFO]: Epoch 038 - training loss: 0.3384, validation loss: 0.4469
2024-06-02 20:06:44 [INFO]: Epoch 039 - training loss: 0.3400, validation loss: 0.4381
2024-06-02 20:06:47 [INFO]: Epoch 040 - training loss: 0.3318, validation loss: 0.4349
2024-06-02 20:06:51 [INFO]: Epoch 041 - training loss: 0.3299, validation loss: 0.4366
2024-06-02 20:06:54 [INFO]: Epoch 042 - training loss: 0.3290, validation loss: 0.4427
2024-06-02 20:06:58 [INFO]: Epoch 043 - training loss: 0.3272, validation loss: 0.4384
2024-06-02 20:07:02 [INFO]: Epoch 044 - training loss: 0.3280, validation loss: 0.4364
2024-06-02 20:07:05 [INFO]: Epoch 045 - training loss: 0.3280, validation loss: 0.4355
2024-06-02 20:07:09 [INFO]: Epoch 046 - training loss: 0.3235, validation loss: 0.4360
2024-06-02 20:07:12 [INFO]: Epoch 047 - training loss: 0.3277, validation loss: 0.4366
2024-06-02 20:07:16 [INFO]: Epoch 048 - training loss: 0.3239, validation loss: 0.4334
2024-06-02 20:07:20 [INFO]: Epoch 049 - training loss: 0.3213, validation loss: 0.4317
2024-06-02 20:07:23 [INFO]: Epoch 050 - training loss: 0.3239, validation loss: 0.4326
2024-06-02 20:07:27 [INFO]: Epoch 051 - training loss: 0.3203, validation loss: 0.4291
2024-06-02 20:07:30 [INFO]: Epoch 052 - training loss: 0.3128, validation loss: 0.4315
2024-06-02 20:07:33 [INFO]: Epoch 053 - training loss: 0.3163, validation loss: 0.4384
2024-06-02 20:07:37 [INFO]: Epoch 054 - training loss: 0.3154, validation loss: 0.4319
2024-06-02 20:07:40 [INFO]: Epoch 055 - training loss: 0.3124, validation loss: 0.4332
2024-06-02 20:07:44 [INFO]: Epoch 056 - training loss: 0.3152, validation loss: 0.4309
2024-06-02 20:07:48 [INFO]: Epoch 057 - training loss: 0.3130, validation loss: 0.4329
2024-06-02 20:07:51 [INFO]: Epoch 058 - training loss: 0.3104, validation loss: 0.4325
2024-06-02 20:07:54 [INFO]: Epoch 059 - training loss: 0.3096, validation loss: 0.4295
2024-06-02 20:07:58 [INFO]: Epoch 060 - training loss: 0.3105, validation loss: 0.4262
2024-06-02 20:08:01 [INFO]: Epoch 061 - training loss: 0.3117, validation loss: 0.4307
2024-06-02 20:08:04 [INFO]: Epoch 062 - training loss: 0.3057, validation loss: 0.4354
2024-06-02 20:08:07 [INFO]: Epoch 063 - training loss: 0.3077, validation loss: 0.4311
2024-06-02 20:08:11 [INFO]: Epoch 064 - training loss: 0.3050, validation loss: 0.4332
2024-06-02 20:08:14 [INFO]: Epoch 065 - training loss: 0.3059, validation loss: 0.4353
2024-06-02 20:08:17 [INFO]: Epoch 066 - training loss: 0.3049, validation loss: 0.4288
2024-06-02 20:08:20 [INFO]: Epoch 067 - training loss: 0.2992, validation loss: 0.4326
2024-06-02 20:08:23 [INFO]: Epoch 068 - training loss: 0.2975, validation loss: 0.4391
2024-06-02 20:08:26 [INFO]: Epoch 069 - training loss: 0.3013, validation loss: 0.4319
2024-06-02 20:08:30 [INFO]: Epoch 070 - training loss: 0.3031, validation loss: 0.4329
2024-06-02 20:08:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:08:30 [INFO]: Finished training. The best model is from epoch#60.
2024-06-02 20:08:30 [INFO]: Saved the model to results_point_rate05/PeMS/Crossformer_PeMS/round_4/20240602_T200425/Crossformer.pypots
2024-06-02 20:08:31 [INFO]: Successfully saved to results_point_rate05/PeMS/Crossformer_PeMS/round_4/imputation.pkl
2024-06-02 20:08:31 [INFO]: Round4 - Crossformer on PeMS: MAE=0.3587, MSE=0.6209, MRE=0.4451
2024-06-02 20:08:31 [INFO]: Done! Final results:
Averaged Crossformer (12,645,238 params) on PeMS: MAE=0.3567 ± 0.0033392252403151514, MSE=0.6065 ± 0.00752630141995578, MRE=0.4426 ± 0.004143695271652368, average inference time=0.32
