2024-06-02 19:38:47 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:38:47 [INFO]: Using the given device: cuda:0
2024-06-02 19:38:47 [INFO]: Model files will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_0/20240602_T193847
2024-06-02 19:38:47 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_0/20240602_T193847/tensorboard
2024-06-02 19:38:47 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 207,873
2024-06-02 19:39:46 [INFO]: Epoch 001 - training loss: 0.7834, validation loss: 0.4960
2024-06-02 19:40:38 [INFO]: Epoch 002 - training loss: 0.4330, validation loss: 0.3776
2024-06-02 19:41:31 [INFO]: Epoch 003 - training loss: 0.3451, validation loss: 0.3700
2024-06-02 19:42:23 [INFO]: Epoch 004 - training loss: 0.3268, validation loss: 0.3641
2024-06-02 19:43:16 [INFO]: Epoch 005 - training loss: 0.3148, validation loss: 0.3686
2024-06-02 19:44:08 [INFO]: Epoch 006 - training loss: 0.3396, validation loss: 0.3512
2024-06-02 19:45:00 [INFO]: Epoch 007 - training loss: 0.2702, validation loss: 0.3725
2024-06-02 19:45:52 [INFO]: Epoch 008 - training loss: 0.3020, validation loss: 0.3473
2024-06-02 19:46:42 [INFO]: Epoch 009 - training loss: 0.2780, validation loss: 0.3268
2024-06-02 19:47:33 [INFO]: Epoch 010 - training loss: 0.2692, validation loss: 0.3277
2024-06-02 19:48:24 [INFO]: Epoch 011 - training loss: 0.2666, validation loss: 0.3289
2024-06-02 19:49:15 [INFO]: Epoch 012 - training loss: 0.2438, validation loss: 0.3144
2024-06-02 19:50:06 [INFO]: Epoch 013 - training loss: 0.2395, validation loss: 0.3145
2024-06-02 19:50:57 [INFO]: Epoch 014 - training loss: 0.2301, validation loss: 0.2989
2024-06-02 19:51:48 [INFO]: Epoch 015 - training loss: 0.2511, validation loss: 0.2787
2024-06-02 19:52:35 [INFO]: Epoch 016 - training loss: 0.2176, validation loss: 0.2996
2024-06-02 19:53:22 [INFO]: Epoch 017 - training loss: 0.1847, validation loss: 0.2893
2024-06-02 19:54:09 [INFO]: Epoch 018 - training loss: 0.1843, validation loss: 0.2944
2024-06-02 19:54:56 [INFO]: Epoch 019 - training loss: 0.2047, validation loss: 0.2939
2024-06-02 19:55:44 [INFO]: Epoch 020 - training loss: 0.2022, validation loss: 0.2490
2024-06-02 19:56:31 [INFO]: Epoch 021 - training loss: 0.2071, validation loss: 0.2551
2024-06-02 19:57:18 [INFO]: Epoch 022 - training loss: 0.2309, validation loss: 0.2454
2024-06-02 19:58:05 [INFO]: Epoch 023 - training loss: 0.2323, validation loss: 0.2691
2024-06-02 19:58:52 [INFO]: Epoch 024 - training loss: 0.2029, validation loss: 0.2721
2024-06-02 19:59:39 [INFO]: Epoch 025 - training loss: 0.1976, validation loss: 0.2396
2024-06-02 20:00:26 [INFO]: Epoch 026 - training loss: 0.1847, validation loss: 0.2391
2024-06-02 20:01:14 [INFO]: Epoch 027 - training loss: 0.1880, validation loss: 0.2403
2024-06-02 20:02:01 [INFO]: Epoch 028 - training loss: 0.1839, validation loss: 0.2104
2024-06-02 20:02:47 [INFO]: Epoch 029 - training loss: 0.1829, validation loss: 0.2084
2024-06-02 20:03:33 [INFO]: Epoch 030 - training loss: 0.1806, validation loss: 0.2284
2024-06-02 20:04:20 [INFO]: Epoch 031 - training loss: 0.2044, validation loss: 0.2107
2024-06-02 20:05:06 [INFO]: Epoch 032 - training loss: 0.1725, validation loss: 0.1979
2024-06-02 20:05:52 [INFO]: Epoch 033 - training loss: 0.2021, validation loss: 0.2044
2024-06-02 20:06:38 [INFO]: Epoch 034 - training loss: 0.2168, validation loss: 0.2167
2024-06-02 20:07:24 [INFO]: Epoch 035 - training loss: 0.2037, validation loss: 0.2187
2024-06-02 20:08:11 [INFO]: Epoch 036 - training loss: 0.1587, validation loss: 0.2005
2024-06-02 20:08:48 [INFO]: Epoch 037 - training loss: 0.1847, validation loss: 0.1994
2024-06-02 20:09:18 [INFO]: Epoch 038 - training loss: 0.1674, validation loss: 0.1898
2024-06-02 20:09:48 [INFO]: Epoch 039 - training loss: 0.1808, validation loss: 0.1886
2024-06-02 20:10:18 [INFO]: Epoch 040 - training loss: 0.2009, validation loss: 0.2162
2024-06-02 20:10:48 [INFO]: Epoch 041 - training loss: 0.2110, validation loss: 0.1965
2024-06-02 20:11:19 [INFO]: Epoch 042 - training loss: 0.1848, validation loss: 0.1970
2024-06-02 20:11:49 [INFO]: Epoch 043 - training loss: 0.1811, validation loss: 0.2008
2024-06-02 20:12:19 [INFO]: Epoch 044 - training loss: 0.1668, validation loss: 0.2021
2024-06-02 20:12:49 [INFO]: Epoch 045 - training loss: 0.1621, validation loss: 0.1940
2024-06-02 20:13:19 [INFO]: Epoch 046 - training loss: 0.2104, validation loss: 0.1808
2024-06-02 20:13:49 [INFO]: Epoch 047 - training loss: 0.1504, validation loss: 0.1817
2024-06-02 20:14:19 [INFO]: Epoch 048 - training loss: 0.1699, validation loss: 0.1789
2024-06-02 20:14:50 [INFO]: Epoch 049 - training loss: 0.2075, validation loss: 0.1795
2024-06-02 20:15:20 [INFO]: Epoch 050 - training loss: 0.1355, validation loss: 0.1799
2024-06-02 20:15:50 [INFO]: Epoch 051 - training loss: 0.1747, validation loss: 0.1863
2024-06-02 20:16:20 [INFO]: Epoch 052 - training loss: 0.1849, validation loss: 0.1935
2024-06-02 20:16:50 [INFO]: Epoch 053 - training loss: 0.1791, validation loss: 0.1911
2024-06-02 20:17:20 [INFO]: Epoch 054 - training loss: 0.1473, validation loss: 0.1818
2024-06-02 20:17:50 [INFO]: Epoch 055 - training loss: 0.1743, validation loss: 0.1793
2024-06-02 20:18:20 [INFO]: Epoch 056 - training loss: 0.1646, validation loss: 0.1780
2024-06-02 20:18:51 [INFO]: Epoch 057 - training loss: 0.1631, validation loss: 0.1787
2024-06-02 20:19:21 [INFO]: Epoch 058 - training loss: 0.1658, validation loss: 0.1793
2024-06-02 20:19:51 [INFO]: Epoch 059 - training loss: 0.1757, validation loss: 0.1818
2024-06-02 20:20:21 [INFO]: Epoch 060 - training loss: 0.1577, validation loss: 0.1828
2024-06-02 20:20:51 [INFO]: Epoch 061 - training loss: 0.1725, validation loss: 0.1923
2024-06-02 20:21:21 [INFO]: Epoch 062 - training loss: 0.1589, validation loss: 0.1805
2024-06-02 20:21:51 [INFO]: Epoch 063 - training loss: 0.1532, validation loss: 0.1802
2024-06-02 20:22:21 [INFO]: Epoch 064 - training loss: 0.1881, validation loss: 0.1836
2024-06-02 20:22:52 [INFO]: Epoch 065 - training loss: 0.1645, validation loss: 0.1740
2024-06-02 20:23:22 [INFO]: Epoch 066 - training loss: 0.1416, validation loss: 0.1755
2024-06-02 20:23:52 [INFO]: Epoch 067 - training loss: 0.1542, validation loss: 0.1758
2024-06-02 20:24:22 [INFO]: Epoch 068 - training loss: 0.1659, validation loss: 0.1732
2024-06-02 20:24:52 [INFO]: Epoch 069 - training loss: 0.1638, validation loss: 0.1776
2024-06-02 20:25:22 [INFO]: Epoch 070 - training loss: 0.1643, validation loss: 0.1728
2024-06-02 20:25:52 [INFO]: Epoch 071 - training loss: 0.1446, validation loss: 0.1761
2024-06-02 20:26:22 [INFO]: Epoch 072 - training loss: 0.1752, validation loss: 0.1854
2024-06-02 20:26:53 [INFO]: Epoch 073 - training loss: 0.1576, validation loss: 0.1740
2024-06-02 20:27:22 [INFO]: Epoch 074 - training loss: 0.1638, validation loss: 0.1751
2024-06-02 20:27:52 [INFO]: Epoch 075 - training loss: 0.1873, validation loss: 0.1765
2024-06-02 20:28:22 [INFO]: Epoch 076 - training loss: 0.1856, validation loss: 0.1795
2024-06-02 20:28:52 [INFO]: Epoch 077 - training loss: 0.1538, validation loss: 0.1806
2024-06-02 20:29:22 [INFO]: Epoch 078 - training loss: 0.1753, validation loss: 0.1786
2024-06-02 20:29:52 [INFO]: Epoch 079 - training loss: 0.1677, validation loss: 0.1720
2024-06-02 20:30:22 [INFO]: Epoch 080 - training loss: 0.1464, validation loss: 0.1730
2024-06-02 20:30:53 [INFO]: Epoch 081 - training loss: 0.1552, validation loss: 0.1716
2024-06-02 20:31:23 [INFO]: Epoch 082 - training loss: 0.1491, validation loss: 0.1710
2024-06-02 20:31:53 [INFO]: Epoch 083 - training loss: 0.1714, validation loss: 0.1678
2024-06-02 20:32:23 [INFO]: Epoch 084 - training loss: 0.1451, validation loss: 0.1719
2024-06-02 20:32:53 [INFO]: Epoch 085 - training loss: 0.1796, validation loss: 0.1688
2024-06-02 20:33:23 [INFO]: Epoch 086 - training loss: 0.1563, validation loss: 0.1811
2024-06-02 20:33:53 [INFO]: Epoch 087 - training loss: 0.1754, validation loss: 0.1729
2024-06-02 20:34:23 [INFO]: Epoch 088 - training loss: 0.1360, validation loss: 0.1745
2024-06-02 20:34:53 [INFO]: Epoch 089 - training loss: 0.1489, validation loss: 0.1763
2024-06-02 20:35:24 [INFO]: Epoch 090 - training loss: 0.1464, validation loss: 0.1695
2024-06-02 20:35:54 [INFO]: Epoch 091 - training loss: 0.1344, validation loss: 0.1677
2024-06-02 20:36:24 [INFO]: Epoch 092 - training loss: 0.1409, validation loss: 0.1717
2024-06-02 20:36:54 [INFO]: Epoch 093 - training loss: 0.1627, validation loss: 0.1713
2024-06-02 20:37:24 [INFO]: Epoch 094 - training loss: 0.1528, validation loss: 0.1673
2024-06-02 20:37:54 [INFO]: Epoch 095 - training loss: 0.1390, validation loss: 0.1670
2024-06-02 20:38:24 [INFO]: Epoch 096 - training loss: 0.1411, validation loss: 0.1683
2024-06-02 20:38:54 [INFO]: Epoch 097 - training loss: 0.1633, validation loss: 0.1690
2024-06-02 20:39:24 [INFO]: Epoch 098 - training loss: 0.1345, validation loss: 0.1670
2024-06-02 20:39:55 [INFO]: Epoch 099 - training loss: 0.1544, validation loss: 0.1714
2024-06-02 20:40:25 [INFO]: Epoch 100 - training loss: 0.1464, validation loss: 0.1679
2024-06-02 20:40:25 [INFO]: Finished training. The best model is from epoch#95.
2024-06-02 20:40:25 [INFO]: Saved the model to results_point_rate05/PeMS/CSDI_PeMS/round_0/20240602_T193847/CSDI.pypots
2024-06-02 21:04:43 [INFO]: Successfully saved to results_point_rate05/PeMS/CSDI_PeMS/round_0/imputation.pkl
2024-06-02 21:04:43 [INFO]: Round0 - CSDI on PeMS: MAE=0.2433, MSE=0.6425, MRE=0.3019
2024-06-02 21:04:43 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 21:04:43 [INFO]: Using the given device: cuda:0
2024-06-02 21:04:43 [INFO]: Model files will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_1/20240602_T210443
2024-06-02 21:04:43 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_1/20240602_T210443/tensorboard
2024-06-02 21:04:43 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 207,873
2024-06-02 21:05:13 [INFO]: Epoch 001 - training loss: 0.7431, validation loss: 0.4524
2024-06-02 21:05:44 [INFO]: Epoch 002 - training loss: 0.4118, validation loss: 0.4179
2024-06-02 21:06:14 [INFO]: Epoch 003 - training loss: 0.3371, validation loss: 0.3879
2024-06-02 21:06:44 [INFO]: Epoch 004 - training loss: 0.3464, validation loss: 0.3707
2024-06-02 21:07:14 [INFO]: Epoch 005 - training loss: 0.3214, validation loss: 0.3502
2024-06-02 21:07:44 [INFO]: Epoch 006 - training loss: 0.2929, validation loss: 0.3426
2024-06-02 21:08:14 [INFO]: Epoch 007 - training loss: 0.3050, validation loss: 0.3441
2024-06-02 21:08:45 [INFO]: Epoch 008 - training loss: 0.2879, validation loss: 0.3209
2024-06-02 21:09:15 [INFO]: Epoch 009 - training loss: 0.2976, validation loss: 0.3794
2024-06-02 21:09:45 [INFO]: Epoch 010 - training loss: 0.2622, validation loss: 0.3361
2024-06-02 21:10:15 [INFO]: Epoch 011 - training loss: 0.2272, validation loss: 0.3224
2024-06-02 21:10:45 [INFO]: Epoch 012 - training loss: 0.2624, validation loss: 0.3051
2024-06-02 21:11:15 [INFO]: Epoch 013 - training loss: 0.2339, validation loss: 0.3174
2024-06-02 21:11:46 [INFO]: Epoch 014 - training loss: 0.2594, validation loss: 0.3058
2024-06-02 21:12:16 [INFO]: Epoch 015 - training loss: 0.2194, validation loss: 0.2805
2024-06-02 21:12:46 [INFO]: Epoch 016 - training loss: 0.2165, validation loss: 0.3052
2024-06-02 21:13:16 [INFO]: Epoch 017 - training loss: 0.1946, validation loss: 0.2670
2024-06-02 21:13:46 [INFO]: Epoch 018 - training loss: 0.2081, validation loss: 0.2642
2024-06-02 21:14:17 [INFO]: Epoch 019 - training loss: 0.2186, validation loss: 0.2695
2024-06-02 21:14:47 [INFO]: Epoch 020 - training loss: 0.2118, validation loss: 0.2569
2024-06-02 21:15:17 [INFO]: Epoch 021 - training loss: 0.2550, validation loss: 0.2761
2024-06-02 21:15:47 [INFO]: Epoch 022 - training loss: 0.1770, validation loss: 0.2691
2024-06-02 21:16:17 [INFO]: Epoch 023 - training loss: 0.2377, validation loss: 0.2714
2024-06-02 21:16:47 [INFO]: Epoch 024 - training loss: 0.2160, validation loss: 0.2971
2024-06-02 21:17:18 [INFO]: Epoch 025 - training loss: 0.2298, validation loss: 0.2726
2024-06-02 21:17:48 [INFO]: Epoch 026 - training loss: 0.2236, validation loss: 0.2875
2024-06-02 21:18:18 [INFO]: Epoch 027 - training loss: 0.1789, validation loss: 0.2562
2024-06-02 21:18:48 [INFO]: Epoch 028 - training loss: 0.1953, validation loss: 0.2469
2024-06-02 21:19:18 [INFO]: Epoch 029 - training loss: 0.1807, validation loss: 0.2440
2024-06-02 21:19:49 [INFO]: Epoch 030 - training loss: 0.2038, validation loss: 0.2386
2024-06-02 21:20:19 [INFO]: Epoch 031 - training loss: 0.1709, validation loss: 0.2290
2024-06-02 21:20:49 [INFO]: Epoch 032 - training loss: 0.2087, validation loss: 0.2451
2024-06-02 21:21:19 [INFO]: Epoch 033 - training loss: 0.1880, validation loss: 0.2298
2024-06-02 21:21:49 [INFO]: Epoch 034 - training loss: 0.1949, validation loss: 0.2168
2024-06-02 21:22:19 [INFO]: Epoch 035 - training loss: 0.1606, validation loss: 0.2097
2024-06-02 21:22:50 [INFO]: Epoch 036 - training loss: 0.1914, validation loss: 0.2061
2024-06-02 21:23:20 [INFO]: Epoch 037 - training loss: 0.1893, validation loss: 0.2100
2024-06-02 21:23:50 [INFO]: Epoch 038 - training loss: 0.1940, validation loss: 0.2153
2024-06-02 21:24:20 [INFO]: Epoch 039 - training loss: 0.1690, validation loss: 0.2193
2024-06-02 21:24:50 [INFO]: Epoch 040 - training loss: 0.2101, validation loss: 0.2104
2024-06-02 21:25:20 [INFO]: Epoch 041 - training loss: 0.2216, validation loss: 0.2557
2024-06-02 21:25:51 [INFO]: Epoch 042 - training loss: 0.1879, validation loss: 0.2353
2024-06-02 21:26:21 [INFO]: Epoch 043 - training loss: 0.2027, validation loss: 0.2077
2024-06-02 21:26:51 [INFO]: Epoch 044 - training loss: 0.1866, validation loss: 0.2201
2024-06-02 21:27:21 [INFO]: Epoch 045 - training loss: 0.1817, validation loss: 0.1996
2024-06-02 21:27:51 [INFO]: Epoch 046 - training loss: 0.1828, validation loss: 0.1965
2024-06-02 21:28:22 [INFO]: Epoch 047 - training loss: 0.1764, validation loss: 0.2050
2024-06-02 21:28:52 [INFO]: Epoch 048 - training loss: 0.1839, validation loss: 0.2017
2024-06-02 21:29:22 [INFO]: Epoch 049 - training loss: 0.1959, validation loss: 0.1948
2024-06-02 21:29:52 [INFO]: Epoch 050 - training loss: 0.1642, validation loss: 0.1954
2024-06-02 21:30:22 [INFO]: Epoch 051 - training loss: 0.1800, validation loss: 0.1937
2024-06-02 21:30:52 [INFO]: Epoch 052 - training loss: 0.1762, validation loss: 0.2023
2024-06-02 21:31:22 [INFO]: Epoch 053 - training loss: 0.1996, validation loss: 0.1987
2024-06-02 21:31:52 [INFO]: Epoch 054 - training loss: 0.1808, validation loss: 0.2033
2024-06-02 21:32:22 [INFO]: Epoch 055 - training loss: 0.1864, validation loss: 0.2009
2024-06-02 21:32:52 [INFO]: Epoch 056 - training loss: 0.1624, validation loss: 0.1970
2024-06-02 21:33:22 [INFO]: Epoch 057 - training loss: 0.1747, validation loss: 0.1932
2024-06-02 21:33:53 [INFO]: Epoch 058 - training loss: 0.1676, validation loss: 0.1863
2024-06-02 21:34:23 [INFO]: Epoch 059 - training loss: 0.1803, validation loss: 0.1849
2024-06-02 21:34:53 [INFO]: Epoch 060 - training loss: 0.1544, validation loss: 0.1826
2024-06-02 21:35:23 [INFO]: Epoch 061 - training loss: 0.1506, validation loss: 0.1834
2024-06-02 21:35:53 [INFO]: Epoch 062 - training loss: 0.1643, validation loss: 0.1843
2024-06-02 21:36:24 [INFO]: Epoch 063 - training loss: 0.1930, validation loss: 0.2091
2024-06-02 21:36:54 [INFO]: Epoch 064 - training loss: 0.1779, validation loss: 0.1848
2024-06-02 21:37:24 [INFO]: Epoch 065 - training loss: 0.1511, validation loss: 0.1896
2024-06-02 21:37:54 [INFO]: Epoch 066 - training loss: 0.1622, validation loss: 0.1857
2024-06-02 21:38:24 [INFO]: Epoch 067 - training loss: 0.1906, validation loss: 0.1830
2024-06-02 21:38:54 [INFO]: Epoch 068 - training loss: 0.1589, validation loss: 0.1834
2024-06-02 21:39:25 [INFO]: Epoch 069 - training loss: 0.1532, validation loss: 0.1822
2024-06-02 21:39:55 [INFO]: Epoch 070 - training loss: 0.1600, validation loss: 0.1808
2024-06-02 21:40:25 [INFO]: Epoch 071 - training loss: 0.1592, validation loss: 0.1786
2024-06-02 21:40:55 [INFO]: Epoch 072 - training loss: 0.1717, validation loss: 0.1797
2024-06-02 21:41:25 [INFO]: Epoch 073 - training loss: 0.1563, validation loss: 0.1813
2024-06-02 21:41:56 [INFO]: Epoch 074 - training loss: 0.1461, validation loss: 0.1757
2024-06-02 21:42:26 [INFO]: Epoch 075 - training loss: 0.1666, validation loss: 0.1825
2024-06-02 21:42:56 [INFO]: Epoch 076 - training loss: 0.1627, validation loss: 0.1759
2024-06-02 21:43:26 [INFO]: Epoch 077 - training loss: 0.1553, validation loss: 0.1857
2024-06-02 21:43:56 [INFO]: Epoch 078 - training loss: 0.1456, validation loss: 0.1779
2024-06-02 21:44:26 [INFO]: Epoch 079 - training loss: 0.1706, validation loss: 0.1789
2024-06-02 21:44:57 [INFO]: Epoch 080 - training loss: 0.1809, validation loss: 0.1751
2024-06-02 21:45:27 [INFO]: Epoch 081 - training loss: 0.1763, validation loss: 0.1769
2024-06-02 21:45:57 [INFO]: Epoch 082 - training loss: 0.1755, validation loss: 0.1750
2024-06-02 21:46:27 [INFO]: Epoch 083 - training loss: 0.1428, validation loss: 0.1740
2024-06-02 21:46:57 [INFO]: Epoch 084 - training loss: 0.1818, validation loss: 0.1764
2024-06-02 21:47:27 [INFO]: Epoch 085 - training loss: 0.1666, validation loss: 0.1789
2024-06-02 21:47:58 [INFO]: Epoch 086 - training loss: 0.1556, validation loss: 0.1822
2024-06-02 21:48:28 [INFO]: Epoch 087 - training loss: 0.1362, validation loss: 0.1759
2024-06-02 21:48:58 [INFO]: Epoch 088 - training loss: 0.1408, validation loss: 0.1813
2024-06-02 21:49:28 [INFO]: Epoch 089 - training loss: 0.1667, validation loss: 0.1887
2024-06-02 21:49:58 [INFO]: Epoch 090 - training loss: 0.1530, validation loss: 0.1768
2024-06-02 21:50:29 [INFO]: Epoch 091 - training loss: 0.1567, validation loss: 0.1785
2024-06-02 21:50:59 [INFO]: Epoch 092 - training loss: 0.1794, validation loss: 0.1733
2024-06-02 21:51:29 [INFO]: Epoch 093 - training loss: 0.1592, validation loss: 0.1745
2024-06-02 21:51:59 [INFO]: Epoch 094 - training loss: 0.1557, validation loss: 0.1779
2024-06-02 21:52:29 [INFO]: Epoch 095 - training loss: 0.1446, validation loss: 0.1761
2024-06-02 21:52:59 [INFO]: Epoch 096 - training loss: 0.1751, validation loss: 0.1723
2024-06-02 21:53:30 [INFO]: Epoch 097 - training loss: 0.1576, validation loss: 0.1769
2024-06-02 21:54:00 [INFO]: Epoch 098 - training loss: 0.1586, validation loss: 0.1727
2024-06-02 21:54:30 [INFO]: Epoch 099 - training loss: 0.1538, validation loss: 0.1727
2024-06-02 21:55:00 [INFO]: Epoch 100 - training loss: 0.1292, validation loss: 0.1728
2024-06-02 21:55:00 [INFO]: Finished training. The best model is from epoch#96.
2024-06-02 21:55:00 [INFO]: Saved the model to results_point_rate05/PeMS/CSDI_PeMS/round_1/20240602_T210443/CSDI.pypots
2024-06-02 22:19:19 [INFO]: Successfully saved to results_point_rate05/PeMS/CSDI_PeMS/round_1/imputation.pkl
2024-06-02 22:19:19 [INFO]: Round1 - CSDI on PeMS: MAE=0.2468, MSE=0.5430, MRE=0.3062
2024-06-02 22:19:19 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 22:19:19 [INFO]: Using the given device: cuda:0
2024-06-02 22:19:19 [INFO]: Model files will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_2/20240602_T221919
2024-06-02 22:19:19 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_2/20240602_T221919/tensorboard
2024-06-02 22:19:19 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 207,873
2024-06-02 22:19:50 [INFO]: Epoch 001 - training loss: 0.8112, validation loss: 0.5310
2024-06-02 22:20:20 [INFO]: Epoch 002 - training loss: 0.4637, validation loss: 0.4391
2024-06-02 22:20:50 [INFO]: Epoch 003 - training loss: 0.3747, validation loss: 0.3955
2024-06-02 22:21:20 [INFO]: Epoch 004 - training loss: 0.3315, validation loss: 0.3624
2024-06-02 22:21:50 [INFO]: Epoch 005 - training loss: 0.3542, validation loss: 0.3759
2024-06-02 22:22:20 [INFO]: Epoch 006 - training loss: 0.3127, validation loss: 0.3275
2024-06-02 22:22:50 [INFO]: Epoch 007 - training loss: 0.3067, validation loss: 0.3262
2024-06-02 22:23:21 [INFO]: Epoch 008 - training loss: 0.2957, validation loss: 0.3172
2024-06-02 22:23:51 [INFO]: Epoch 009 - training loss: 0.3012, validation loss: 0.3028
2024-06-02 22:24:21 [INFO]: Epoch 010 - training loss: 0.2779, validation loss: 0.3114
2024-06-02 22:24:51 [INFO]: Epoch 011 - training loss: 0.2831, validation loss: 0.2842
2024-06-02 22:25:21 [INFO]: Epoch 012 - training loss: 0.2754, validation loss: 0.2963
2024-06-02 22:25:51 [INFO]: Epoch 013 - training loss: 0.2228, validation loss: 0.2850
2024-06-02 22:26:21 [INFO]: Epoch 014 - training loss: 0.2617, validation loss: 0.2756
2024-06-02 22:26:52 [INFO]: Epoch 015 - training loss: 0.2423, validation loss: 0.2860
2024-06-02 22:27:22 [INFO]: Epoch 016 - training loss: 0.2511, validation loss: 0.3126
2024-06-02 22:27:52 [INFO]: Epoch 017 - training loss: 0.2154, validation loss: 0.2997
2024-06-02 22:28:22 [INFO]: Epoch 018 - training loss: 0.2098, validation loss: 0.2678
2024-06-02 22:28:52 [INFO]: Epoch 019 - training loss: 0.1884, validation loss: 0.2842
2024-06-02 22:29:22 [INFO]: Epoch 020 - training loss: 0.2013, validation loss: 0.2762
2024-06-02 22:29:52 [INFO]: Epoch 021 - training loss: 0.2104, validation loss: 0.2349
2024-06-02 22:30:23 [INFO]: Epoch 022 - training loss: 0.1943, validation loss: 0.2405
2024-06-02 22:30:53 [INFO]: Epoch 023 - training loss: 0.1982, validation loss: 0.2364
2024-06-02 22:31:23 [INFO]: Epoch 024 - training loss: 0.2474, validation loss: 0.2272
2024-06-02 22:31:53 [INFO]: Epoch 025 - training loss: 0.1762, validation loss: 0.2125
2024-06-02 22:32:23 [INFO]: Epoch 026 - training loss: 0.1925, validation loss: 0.2209
2024-06-02 22:32:53 [INFO]: Epoch 027 - training loss: 0.1797, validation loss: 0.2192
2024-06-02 22:33:23 [INFO]: Epoch 028 - training loss: 0.2082, validation loss: 0.2102
2024-06-02 22:33:47 [INFO]: Epoch 029 - training loss: 0.2366, validation loss: 0.2132
2024-06-02 22:34:11 [INFO]: Epoch 030 - training loss: 0.2293, validation loss: 0.2030
2024-06-02 22:34:35 [INFO]: Epoch 031 - training loss: 0.1758, validation loss: 0.2132
2024-06-02 22:35:00 [INFO]: Epoch 032 - training loss: 0.1952, validation loss: 0.2082
2024-06-02 22:35:24 [INFO]: Epoch 033 - training loss: 0.1860, validation loss: 0.2114
2024-06-02 22:35:48 [INFO]: Epoch 034 - training loss: 0.2126, validation loss: 0.2074
2024-06-02 22:36:12 [INFO]: Epoch 035 - training loss: 0.1780, validation loss: 0.2214
2024-06-02 22:36:36 [INFO]: Epoch 036 - training loss: 0.1865, validation loss: 0.2089
2024-06-02 22:37:00 [INFO]: Epoch 037 - training loss: 0.1726, validation loss: 0.2040
2024-06-02 22:37:24 [INFO]: Epoch 038 - training loss: 0.2009, validation loss: 0.2012
2024-06-02 22:37:48 [INFO]: Epoch 039 - training loss: 0.1640, validation loss: 0.1926
2024-06-02 22:38:12 [INFO]: Epoch 040 - training loss: 0.1762, validation loss: 0.1968
2024-06-02 22:38:36 [INFO]: Epoch 041 - training loss: 0.1914, validation loss: 0.1912
2024-06-02 22:39:00 [INFO]: Epoch 042 - training loss: 0.1794, validation loss: 0.1886
2024-06-02 22:39:24 [INFO]: Epoch 043 - training loss: 0.1483, validation loss: 0.1848
2024-06-02 22:39:48 [INFO]: Epoch 044 - training loss: 0.1871, validation loss: 0.1973
2024-06-02 22:40:13 [INFO]: Epoch 045 - training loss: 0.2003, validation loss: 0.2123
2024-06-02 22:40:37 [INFO]: Epoch 046 - training loss: 0.1483, validation loss: 0.1897
2024-06-02 22:41:01 [INFO]: Epoch 047 - training loss: 0.1914, validation loss: 0.1969
2024-06-02 22:41:25 [INFO]: Epoch 048 - training loss: 0.1529, validation loss: 0.1906
2024-06-02 22:41:49 [INFO]: Epoch 049 - training loss: 0.1729, validation loss: 0.1885
2024-06-02 22:42:13 [INFO]: Epoch 050 - training loss: 0.1691, validation loss: 0.1860
2024-06-02 22:42:37 [INFO]: Epoch 051 - training loss: 0.1818, validation loss: 0.1796
2024-06-02 22:43:01 [INFO]: Epoch 052 - training loss: 0.1613, validation loss: 0.1813
2024-06-02 22:43:25 [INFO]: Epoch 053 - training loss: 0.1397, validation loss: 0.1826
2024-06-02 22:43:49 [INFO]: Epoch 054 - training loss: 0.1786, validation loss: 0.1837
2024-06-02 22:44:13 [INFO]: Epoch 055 - training loss: 0.1857, validation loss: 0.1807
2024-06-02 22:44:37 [INFO]: Epoch 056 - training loss: 0.1687, validation loss: 0.1789
2024-06-02 22:45:01 [INFO]: Epoch 057 - training loss: 0.1540, validation loss: 0.1957
2024-06-02 22:45:25 [INFO]: Epoch 058 - training loss: 0.1602, validation loss: 0.1810
2024-06-02 22:45:50 [INFO]: Epoch 059 - training loss: 0.1817, validation loss: 0.1840
2024-06-02 22:46:14 [INFO]: Epoch 060 - training loss: 0.2102, validation loss: 0.1954
2024-06-02 22:46:38 [INFO]: Epoch 061 - training loss: 0.1900, validation loss: 0.1888
2024-06-02 22:47:02 [INFO]: Epoch 062 - training loss: 0.1874, validation loss: 0.2010
2024-06-02 22:47:26 [INFO]: Epoch 063 - training loss: 0.1738, validation loss: 0.1864
2024-06-02 22:47:50 [INFO]: Epoch 064 - training loss: 0.1802, validation loss: 0.1823
2024-06-02 22:48:14 [INFO]: Epoch 065 - training loss: 0.1513, validation loss: 0.1829
2024-06-02 22:48:38 [INFO]: Epoch 066 - training loss: 0.1625, validation loss: 0.1806
2024-06-02 22:48:38 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 22:48:38 [INFO]: Finished training. The best model is from epoch#56.
2024-06-02 22:48:38 [INFO]: Saved the model to results_point_rate05/PeMS/CSDI_PeMS/round_2/20240602_T221919/CSDI.pypots
2024-06-02 23:08:08 [INFO]: Successfully saved to results_point_rate05/PeMS/CSDI_PeMS/round_2/imputation.pkl
2024-06-02 23:08:08 [INFO]: Round2 - CSDI on PeMS: MAE=0.2870, MSE=0.5924, MRE=0.3561
2024-06-02 23:08:08 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 23:08:08 [INFO]: Using the given device: cuda:0
2024-06-02 23:08:08 [INFO]: Model files will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_3/20240602_T230808
2024-06-02 23:08:08 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_3/20240602_T230808/tensorboard
2024-06-02 23:08:08 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 207,873
2024-06-02 23:08:32 [INFO]: Epoch 001 - training loss: 0.7056, validation loss: 0.4424
2024-06-02 23:08:56 [INFO]: Epoch 002 - training loss: 0.4080, validation loss: 0.4139
2024-06-02 23:09:20 [INFO]: Epoch 003 - training loss: 0.3481, validation loss: 0.3570
2024-06-02 23:09:44 [INFO]: Epoch 004 - training loss: 0.3405, validation loss: 0.3675
2024-06-02 23:10:08 [INFO]: Epoch 005 - training loss: 0.2910, validation loss: 0.3420
2024-06-02 23:10:32 [INFO]: Epoch 006 - training loss: 0.3058, validation loss: 0.3318
2024-06-02 23:10:56 [INFO]: Epoch 007 - training loss: 0.2856, validation loss: 0.3253
2024-06-02 23:11:21 [INFO]: Epoch 008 - training loss: 0.2917, validation loss: 0.3170
2024-06-02 23:11:45 [INFO]: Epoch 009 - training loss: 0.2537, validation loss: 0.3257
2024-06-02 23:12:09 [INFO]: Epoch 010 - training loss: 0.2342, validation loss: 0.3100
2024-06-02 23:12:33 [INFO]: Epoch 011 - training loss: 0.2363, validation loss: 0.3106
2024-06-02 23:12:57 [INFO]: Epoch 012 - training loss: 0.2244, validation loss: 0.3354
2024-06-02 23:13:21 [INFO]: Epoch 013 - training loss: 0.2150, validation loss: 0.2947
2024-06-02 23:13:45 [INFO]: Epoch 014 - training loss: 0.2147, validation loss: 0.3568
2024-06-02 23:14:09 [INFO]: Epoch 015 - training loss: 0.2372, validation loss: 0.2686
2024-06-02 23:14:33 [INFO]: Epoch 016 - training loss: 0.2194, validation loss: 0.2737
2024-06-02 23:14:57 [INFO]: Epoch 017 - training loss: 0.2248, validation loss: 0.2946
2024-06-02 23:15:21 [INFO]: Epoch 018 - training loss: 0.2423, validation loss: 0.3046
2024-06-02 23:15:45 [INFO]: Epoch 019 - training loss: 0.2102, validation loss: 0.2800
2024-06-02 23:16:09 [INFO]: Epoch 020 - training loss: 0.2107, validation loss: 0.2556
2024-06-02 23:16:34 [INFO]: Epoch 021 - training loss: 0.1891, validation loss: 0.2486
2024-06-02 23:16:58 [INFO]: Epoch 022 - training loss: 0.1744, validation loss: 0.2444
2024-06-02 23:17:22 [INFO]: Epoch 023 - training loss: 0.2072, validation loss: 0.2282
2024-06-02 23:17:46 [INFO]: Epoch 024 - training loss: 0.1998, validation loss: 0.2646
2024-06-02 23:18:10 [INFO]: Epoch 025 - training loss: 0.1788, validation loss: 0.2442
2024-06-02 23:18:34 [INFO]: Epoch 026 - training loss: 0.1963, validation loss: 0.2264
2024-06-02 23:18:58 [INFO]: Epoch 027 - training loss: 0.1919, validation loss: 0.2429
2024-06-02 23:19:22 [INFO]: Epoch 028 - training loss: 0.1783, validation loss: 0.2270
2024-06-02 23:19:46 [INFO]: Epoch 029 - training loss: 0.1886, validation loss: 0.2167
2024-06-02 23:20:10 [INFO]: Epoch 030 - training loss: 0.1926, validation loss: 0.2214
2024-06-02 23:20:34 [INFO]: Epoch 031 - training loss: 0.2027, validation loss: 0.2240
2024-06-02 23:20:58 [INFO]: Epoch 032 - training loss: 0.1914, validation loss: 0.2165
2024-06-02 23:21:23 [INFO]: Epoch 033 - training loss: 0.2010, validation loss: 0.2051
2024-06-02 23:21:47 [INFO]: Epoch 034 - training loss: 0.1624, validation loss: 0.2228
2024-06-02 23:22:11 [INFO]: Epoch 035 - training loss: 0.1635, validation loss: 0.2037
2024-06-02 23:22:35 [INFO]: Epoch 036 - training loss: 0.2147, validation loss: 0.1929
2024-06-02 23:22:59 [INFO]: Epoch 037 - training loss: 0.1904, validation loss: 0.2006
2024-06-02 23:23:23 [INFO]: Epoch 038 - training loss: 0.1769, validation loss: 0.2075
2024-06-02 23:23:47 [INFO]: Epoch 039 - training loss: 0.1921, validation loss: 0.2017
2024-06-02 23:24:11 [INFO]: Epoch 040 - training loss: 0.1632, validation loss: 0.1912
2024-06-02 23:24:35 [INFO]: Epoch 041 - training loss: 0.1945, validation loss: 0.1952
2024-06-02 23:24:59 [INFO]: Epoch 042 - training loss: 0.1921, validation loss: 0.1940
2024-06-02 23:25:23 [INFO]: Epoch 043 - training loss: 0.1601, validation loss: 0.1858
2024-06-02 23:25:47 [INFO]: Epoch 044 - training loss: 0.1775, validation loss: 0.1934
2024-06-02 23:26:11 [INFO]: Epoch 045 - training loss: 0.1796, validation loss: 0.1872
2024-06-02 23:26:36 [INFO]: Epoch 046 - training loss: 0.1684, validation loss: 0.1801
2024-06-02 23:27:00 [INFO]: Epoch 047 - training loss: 0.1510, validation loss: 0.1808
2024-06-02 23:27:24 [INFO]: Epoch 048 - training loss: 0.1530, validation loss: 0.1947
2024-06-02 23:27:48 [INFO]: Epoch 049 - training loss: 0.1957, validation loss: 0.1914
2024-06-02 23:28:12 [INFO]: Epoch 050 - training loss: 0.1648, validation loss: 0.1857
2024-06-02 23:28:36 [INFO]: Epoch 051 - training loss: 0.1623, validation loss: 0.1817
2024-06-02 23:29:00 [INFO]: Epoch 052 - training loss: 0.1474, validation loss: 0.1803
2024-06-02 23:29:24 [INFO]: Epoch 053 - training loss: 0.1668, validation loss: 0.1807
2024-06-02 23:29:48 [INFO]: Epoch 054 - training loss: 0.1453, validation loss: 0.1809
2024-06-02 23:30:12 [INFO]: Epoch 055 - training loss: 0.1714, validation loss: 0.1832
2024-06-02 23:30:36 [INFO]: Epoch 056 - training loss: 0.1817, validation loss: 0.1788
2024-06-02 23:31:00 [INFO]: Epoch 057 - training loss: 0.1718, validation loss: 0.1877
2024-06-02 23:31:24 [INFO]: Epoch 058 - training loss: 0.1817, validation loss: 0.1964
2024-06-02 23:31:49 [INFO]: Epoch 059 - training loss: 0.1880, validation loss: 0.1800
2024-06-02 23:32:13 [INFO]: Epoch 060 - training loss: 0.1716, validation loss: 0.1789
2024-06-02 23:32:37 [INFO]: Epoch 061 - training loss: 0.1705, validation loss: 0.1743
2024-06-02 23:33:01 [INFO]: Epoch 062 - training loss: 0.2034, validation loss: 0.1821
2024-06-02 23:33:25 [INFO]: Epoch 063 - training loss: 0.1542, validation loss: 0.1877
2024-06-02 23:33:49 [INFO]: Epoch 064 - training loss: 0.2143, validation loss: 0.2109
2024-06-02 23:34:13 [INFO]: Epoch 065 - training loss: 0.1989, validation loss: 0.2076
2024-06-02 23:34:37 [INFO]: Epoch 066 - training loss: 0.2203, validation loss: 0.2134
2024-06-02 23:35:01 [INFO]: Epoch 067 - training loss: 0.1960, validation loss: 0.1822
2024-06-02 23:35:25 [INFO]: Epoch 068 - training loss: 0.1771, validation loss: 0.1747
2024-06-02 23:35:49 [INFO]: Epoch 069 - training loss: 0.1716, validation loss: 0.1766
2024-06-02 23:36:13 [INFO]: Epoch 070 - training loss: 0.1787, validation loss: 0.1759
2024-06-02 23:36:37 [INFO]: Epoch 071 - training loss: 0.1659, validation loss: 0.1846
2024-06-02 23:36:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 23:36:37 [INFO]: Finished training. The best model is from epoch#61.
2024-06-02 23:36:37 [INFO]: Saved the model to results_point_rate05/PeMS/CSDI_PeMS/round_3/20240602_T230808/CSDI.pypots
2024-06-02 23:56:08 [INFO]: Successfully saved to results_point_rate05/PeMS/CSDI_PeMS/round_3/imputation.pkl
2024-06-02 23:56:08 [INFO]: Round3 - CSDI on PeMS: MAE=0.3435, MSE=0.6705, MRE=0.4263
2024-06-02 23:56:08 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 23:56:08 [INFO]: Using the given device: cuda:0
2024-06-02 23:56:08 [INFO]: Model files will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_4/20240602_T235608
2024-06-02 23:56:08 [INFO]: Tensorboard file will be saved to results_point_rate05/PeMS/CSDI_PeMS/round_4/20240602_T235608/tensorboard
2024-06-02 23:56:08 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 207,873
2024-06-02 23:56:32 [INFO]: Epoch 001 - training loss: 0.7902, validation loss: 0.5309
2024-06-02 23:56:56 [INFO]: Epoch 002 - training loss: 0.4584, validation loss: 0.4016
2024-06-02 23:57:20 [INFO]: Epoch 003 - training loss: 0.3420, validation loss: 0.4109
2024-06-02 23:57:45 [INFO]: Epoch 004 - training loss: 0.3168, validation loss: 0.3481
2024-06-02 23:58:09 [INFO]: Epoch 005 - training loss: 0.3297, validation loss: 0.3431
2024-06-02 23:58:33 [INFO]: Epoch 006 - training loss: 0.3260, validation loss: 0.3625
2024-06-02 23:58:57 [INFO]: Epoch 007 - training loss: 0.2950, validation loss: 0.3626
2024-06-02 23:59:21 [INFO]: Epoch 008 - training loss: 0.2879, validation loss: 0.3585
2024-06-02 23:59:45 [INFO]: Epoch 009 - training loss: 0.2996, validation loss: 0.3349
2024-06-03 00:00:09 [INFO]: Epoch 010 - training loss: 0.2665, validation loss: 0.3365
2024-06-03 00:00:33 [INFO]: Epoch 011 - training loss: 0.2679, validation loss: 0.3572
2024-06-03 00:00:57 [INFO]: Epoch 012 - training loss: 0.2727, validation loss: 0.3255
2024-06-03 00:01:21 [INFO]: Epoch 013 - training loss: 0.2824, validation loss: 0.2914
2024-06-03 00:01:45 [INFO]: Epoch 014 - training loss: 0.2220, validation loss: 0.3650
2024-06-03 00:02:09 [INFO]: Epoch 015 - training loss: 0.2460, validation loss: 0.2827
2024-06-03 00:02:33 [INFO]: Epoch 016 - training loss: 0.2140, validation loss: 0.3125
2024-06-03 00:02:57 [INFO]: Epoch 017 - training loss: 0.2671, validation loss: 0.2894
2024-06-03 00:03:22 [INFO]: Epoch 018 - training loss: 0.2412, validation loss: 0.3061
2024-06-03 00:03:46 [INFO]: Epoch 019 - training loss: 0.2277, validation loss: 0.2713
2024-06-03 00:04:10 [INFO]: Epoch 020 - training loss: 0.2418, validation loss: 0.3249
2024-06-03 00:04:34 [INFO]: Epoch 021 - training loss: 0.2110, validation loss: 0.2490
2024-06-03 00:04:58 [INFO]: Epoch 022 - training loss: 0.2091, validation loss: 0.2896
2024-06-03 00:05:22 [INFO]: Epoch 023 - training loss: 0.2295, validation loss: 0.2708
2024-06-03 00:05:46 [INFO]: Epoch 024 - training loss: 0.2170, validation loss: 0.2866
2024-06-03 00:06:10 [INFO]: Epoch 025 - training loss: 0.2314, validation loss: 0.2616
2024-06-03 00:06:34 [INFO]: Epoch 026 - training loss: 0.2001, validation loss: 0.2571
2024-06-03 00:06:58 [INFO]: Epoch 027 - training loss: 0.1909, validation loss: 0.2417
2024-06-03 00:07:22 [INFO]: Epoch 028 - training loss: 0.1814, validation loss: 0.2392
2024-06-03 00:07:46 [INFO]: Epoch 029 - training loss: 0.1982, validation loss: 0.2319
2024-06-03 00:08:10 [INFO]: Epoch 030 - training loss: 0.1862, validation loss: 0.2528
2024-06-03 00:08:34 [INFO]: Epoch 031 - training loss: 0.1909, validation loss: 0.2449
2024-06-03 00:08:58 [INFO]: Epoch 032 - training loss: 0.2412, validation loss: 0.2231
2024-06-03 00:09:23 [INFO]: Epoch 033 - training loss: 0.2221, validation loss: 0.2178
2024-06-03 00:09:47 [INFO]: Epoch 034 - training loss: 0.1899, validation loss: 0.2366
2024-06-03 00:10:11 [INFO]: Epoch 035 - training loss: 0.1917, validation loss: 0.2432
2024-06-03 00:10:35 [INFO]: Epoch 036 - training loss: 0.2149, validation loss: 0.2250
2024-06-03 00:10:59 [INFO]: Epoch 037 - training loss: 0.1821, validation loss: 0.2056
2024-06-03 00:11:23 [INFO]: Epoch 038 - training loss: 0.1938, validation loss: 0.2068
2024-06-03 00:11:47 [INFO]: Epoch 039 - training loss: 0.1928, validation loss: 0.2180
2024-06-03 00:12:11 [INFO]: Epoch 040 - training loss: 0.1959, validation loss: 0.2162
2024-06-03 00:12:35 [INFO]: Epoch 041 - training loss: 0.1656, validation loss: 0.1997
2024-06-03 00:12:59 [INFO]: Epoch 042 - training loss: 0.1888, validation loss: 0.1983
2024-06-03 00:13:23 [INFO]: Epoch 043 - training loss: 0.1893, validation loss: 0.2047
2024-06-03 00:13:47 [INFO]: Epoch 044 - training loss: 0.1987, validation loss: 0.2010
2024-06-03 00:14:11 [INFO]: Epoch 045 - training loss: 0.1867, validation loss: 0.2005
2024-06-03 00:14:35 [INFO]: Epoch 046 - training loss: 0.1786, validation loss: 0.2145
2024-06-03 00:14:59 [INFO]: Epoch 047 - training loss: 0.1539, validation loss: 0.2041
2024-06-03 00:15:23 [INFO]: Epoch 048 - training loss: 0.1829, validation loss: 0.1952
2024-06-03 00:15:47 [INFO]: Epoch 049 - training loss: 0.1843, validation loss: 0.1954
2024-06-03 00:16:11 [INFO]: Epoch 050 - training loss: 0.1663, validation loss: 0.2058
2024-06-03 00:16:35 [INFO]: Epoch 051 - training loss: 0.1888, validation loss: 0.1948
2024-06-03 00:16:59 [INFO]: Epoch 052 - training loss: 0.1851, validation loss: 0.1880
2024-06-03 00:17:24 [INFO]: Epoch 053 - training loss: 0.1907, validation loss: 0.1891
2024-06-03 00:17:48 [INFO]: Epoch 054 - training loss: 0.1720, validation loss: 0.2006
2024-06-03 00:18:12 [INFO]: Epoch 055 - training loss: 0.1978, validation loss: 0.1949
2024-06-03 00:18:36 [INFO]: Epoch 056 - training loss: 0.1989, validation loss: 0.1892
2024-06-03 00:19:00 [INFO]: Epoch 057 - training loss: 0.1688, validation loss: 0.1946
2024-06-03 00:19:24 [INFO]: Epoch 058 - training loss: 0.1875, validation loss: 0.1896
2024-06-03 00:19:48 [INFO]: Epoch 059 - training loss: 0.1848, validation loss: 0.1848
2024-06-03 00:20:12 [INFO]: Epoch 060 - training loss: 0.1547, validation loss: 0.1867
2024-06-03 00:20:36 [INFO]: Epoch 061 - training loss: 0.1728, validation loss: 0.1944
2024-06-03 00:21:00 [INFO]: Epoch 062 - training loss: 0.1503, validation loss: 0.1961
2024-06-03 00:21:24 [INFO]: Epoch 063 - training loss: 0.1699, validation loss: 0.1960
2024-06-03 00:21:48 [INFO]: Epoch 064 - training loss: 0.1805, validation loss: 0.1989
2024-06-03 00:22:12 [INFO]: Epoch 065 - training loss: 0.1840, validation loss: 0.1896
2024-06-03 00:22:36 [INFO]: Epoch 066 - training loss: 0.1512, validation loss: 0.1885
2024-06-03 00:23:00 [INFO]: Epoch 067 - training loss: 0.1736, validation loss: 0.1899
2024-06-03 00:23:24 [INFO]: Epoch 068 - training loss: 0.1832, validation loss: 0.1981
2024-06-03 00:23:48 [INFO]: Epoch 069 - training loss: 0.1659, validation loss: 0.1866
2024-06-03 00:23:48 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:23:48 [INFO]: Finished training. The best model is from epoch#59.
2024-06-03 00:23:48 [INFO]: Saved the model to results_point_rate05/PeMS/CSDI_PeMS/round_4/20240602_T235608/CSDI.pypots
2024-06-03 00:43:18 [INFO]: Successfully saved to results_point_rate05/PeMS/CSDI_PeMS/round_4/imputation.pkl
2024-06-03 00:43:18 [INFO]: Round4 - CSDI on PeMS: MAE=0.3210, MSE=0.8087, MRE=0.3984
2024-06-03 00:43:18 [INFO]: Done! Final results:
Averaged CSDI (207,873 params) on PeMS: MAE=0.2883 ± 0.039683382960986995, MSE=0.6514 ± 0.08991322477992959, MRE=0.3578 ± 0.04924371209026117, average inference time=265.37
