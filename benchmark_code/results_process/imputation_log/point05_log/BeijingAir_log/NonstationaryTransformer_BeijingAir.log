2024-06-03 05:54:03 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 05:54:03 [INFO]: Using the given device: cuda:0
2024-06-03 05:54:03 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240603_T055403
2024-06-03 05:54:03 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240603_T055403/tensorboard
2024-06-03 05:54:05 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 05:54:19 [INFO]: Epoch 001 - training loss: 0.6573, validation loss: 0.3535
2024-06-03 05:54:25 [INFO]: Epoch 002 - training loss: 0.5702, validation loss: 0.3379
2024-06-03 05:54:31 [INFO]: Epoch 003 - training loss: 0.5531, validation loss: 0.3245
2024-06-03 05:54:37 [INFO]: Epoch 004 - training loss: 0.5453, validation loss: 0.3269
2024-06-03 05:54:43 [INFO]: Epoch 005 - training loss: 0.5387, validation loss: 0.3243
2024-06-03 05:54:49 [INFO]: Epoch 006 - training loss: 0.5352, validation loss: 0.3210
2024-06-03 05:54:54 [INFO]: Epoch 007 - training loss: 0.5274, validation loss: 0.3178
2024-06-03 05:55:01 [INFO]: Epoch 008 - training loss: 0.5243, validation loss: 0.3174
2024-06-03 05:55:07 [INFO]: Epoch 009 - training loss: 0.5204, validation loss: 0.3132
2024-06-03 05:55:12 [INFO]: Epoch 010 - training loss: 0.5174, validation loss: 0.3091
2024-06-03 05:55:18 [INFO]: Epoch 011 - training loss: 0.5137, validation loss: 0.3052
2024-06-03 05:55:24 [INFO]: Epoch 012 - training loss: 0.5107, validation loss: 0.3019
2024-06-03 05:55:30 [INFO]: Epoch 013 - training loss: 0.5075, validation loss: 0.3046
2024-06-03 05:55:35 [INFO]: Epoch 014 - training loss: 0.5050, validation loss: 0.3037
2024-06-03 05:55:41 [INFO]: Epoch 015 - training loss: 0.5046, validation loss: 0.3010
2024-06-03 05:55:47 [INFO]: Epoch 016 - training loss: 0.5013, validation loss: 0.3043
2024-06-03 05:55:52 [INFO]: Epoch 017 - training loss: 0.4995, validation loss: 0.3003
2024-06-03 05:55:58 [INFO]: Epoch 018 - training loss: 0.4963, validation loss: 0.2999
2024-06-03 05:56:04 [INFO]: Epoch 019 - training loss: 0.4952, validation loss: 0.2955
2024-06-03 05:56:09 [INFO]: Epoch 020 - training loss: 0.4941, validation loss: 0.3003
2024-06-03 05:56:15 [INFO]: Epoch 021 - training loss: 0.4927, validation loss: 0.2930
2024-06-03 05:56:21 [INFO]: Epoch 022 - training loss: 0.4892, validation loss: 0.2944
2024-06-03 05:56:27 [INFO]: Epoch 023 - training loss: 0.4887, validation loss: 0.2893
2024-06-03 05:56:33 [INFO]: Epoch 024 - training loss: 0.4869, validation loss: 0.2951
2024-06-03 05:56:39 [INFO]: Epoch 025 - training loss: 0.4858, validation loss: 0.2916
2024-06-03 05:56:45 [INFO]: Epoch 026 - training loss: 0.4835, validation loss: 0.2955
2024-06-03 05:56:50 [INFO]: Epoch 027 - training loss: 0.4819, validation loss: 0.2934
2024-06-03 05:56:56 [INFO]: Epoch 028 - training loss: 0.4807, validation loss: 0.2891
2024-06-03 05:57:02 [INFO]: Epoch 029 - training loss: 0.4795, validation loss: 0.2884
2024-06-03 05:57:08 [INFO]: Epoch 030 - training loss: 0.4766, validation loss: 0.2899
2024-06-03 05:57:14 [INFO]: Epoch 031 - training loss: 0.4775, validation loss: 0.2877
2024-06-03 05:57:20 [INFO]: Epoch 032 - training loss: 0.4761, validation loss: 0.2890
2024-06-03 05:57:25 [INFO]: Epoch 033 - training loss: 0.4747, validation loss: 0.2879
2024-06-03 05:57:32 [INFO]: Epoch 034 - training loss: 0.4717, validation loss: 0.2862
2024-06-03 05:57:38 [INFO]: Epoch 035 - training loss: 0.4713, validation loss: 0.2853
2024-06-03 05:57:43 [INFO]: Epoch 036 - training loss: 0.4708, validation loss: 0.2847
2024-06-03 05:57:49 [INFO]: Epoch 037 - training loss: 0.4694, validation loss: 0.2835
2024-06-03 05:57:55 [INFO]: Epoch 038 - training loss: 0.4680, validation loss: 0.2842
2024-06-03 05:58:01 [INFO]: Epoch 039 - training loss: 0.4677, validation loss: 0.2873
2024-06-03 05:58:07 [INFO]: Epoch 040 - training loss: 0.4656, validation loss: 0.2881
2024-06-03 05:58:13 [INFO]: Epoch 041 - training loss: 0.4661, validation loss: 0.2847
2024-06-03 05:58:18 [INFO]: Epoch 042 - training loss: 0.4637, validation loss: 0.2856
2024-06-03 05:58:24 [INFO]: Epoch 043 - training loss: 0.4635, validation loss: 0.2856
2024-06-03 05:58:30 [INFO]: Epoch 044 - training loss: 0.4624, validation loss: 0.2849
2024-06-03 05:58:36 [INFO]: Epoch 045 - training loss: 0.4626, validation loss: 0.2841
2024-06-03 05:58:42 [INFO]: Epoch 046 - training loss: 0.4610, validation loss: 0.2862
2024-06-03 05:58:47 [INFO]: Epoch 047 - training loss: 0.4619, validation loss: 0.2861
2024-06-03 05:58:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 05:58:47 [INFO]: Finished training. The best model is from epoch#37.
2024-06-03 05:58:48 [INFO]: Saved the model to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/20240603_T055403/NonstationaryTransformer.pypots
2024-06-03 05:58:50 [INFO]: Successfully saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_0/imputation.pkl
2024-06-03 05:58:50 [INFO]: Round0 - NonstationaryTransformer on BeijingAir: MAE=0.2456, MSE=0.2888, MRE=0.3344
2024-06-03 05:58:50 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 05:58:50 [INFO]: Using the given device: cuda:0
2024-06-03 05:58:50 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240603_T055850
2024-06-03 05:58:50 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240603_T055850/tensorboard
2024-06-03 05:58:50 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 05:58:56 [INFO]: Epoch 001 - training loss: 0.6527, validation loss: 0.3526
2024-06-03 05:59:02 [INFO]: Epoch 002 - training loss: 0.5717, validation loss: 0.3434
2024-06-03 05:59:07 [INFO]: Epoch 003 - training loss: 0.5549, validation loss: 0.3301
2024-06-03 05:59:13 [INFO]: Epoch 004 - training loss: 0.5449, validation loss: 0.3247
2024-06-03 05:59:19 [INFO]: Epoch 005 - training loss: 0.5401, validation loss: 0.3188
2024-06-03 05:59:25 [INFO]: Epoch 006 - training loss: 0.5333, validation loss: 0.3185
2024-06-03 05:59:31 [INFO]: Epoch 007 - training loss: 0.5275, validation loss: 0.3185
2024-06-03 05:59:37 [INFO]: Epoch 008 - training loss: 0.5234, validation loss: 0.3089
2024-06-03 05:59:43 [INFO]: Epoch 009 - training loss: 0.5206, validation loss: 0.3117
2024-06-03 05:59:48 [INFO]: Epoch 010 - training loss: 0.5173, validation loss: 0.3113
2024-06-03 05:59:54 [INFO]: Epoch 011 - training loss: 0.5140, validation loss: 0.3079
2024-06-03 06:00:00 [INFO]: Epoch 012 - training loss: 0.5096, validation loss: 0.3020
2024-06-03 06:00:05 [INFO]: Epoch 013 - training loss: 0.5072, validation loss: 0.3069
2024-06-03 06:00:11 [INFO]: Epoch 014 - training loss: 0.5033, validation loss: 0.3060
2024-06-03 06:00:17 [INFO]: Epoch 015 - training loss: 0.5007, validation loss: 0.3035
2024-06-03 06:00:23 [INFO]: Epoch 016 - training loss: 0.5026, validation loss: 0.3023
2024-06-03 06:00:28 [INFO]: Epoch 017 - training loss: 0.4970, validation loss: 0.3002
2024-06-03 06:00:34 [INFO]: Epoch 018 - training loss: 0.4956, validation loss: 0.3001
2024-06-03 06:00:40 [INFO]: Epoch 019 - training loss: 0.4929, validation loss: 0.2984
2024-06-03 06:00:46 [INFO]: Epoch 020 - training loss: 0.4924, validation loss: 0.2942
2024-06-03 06:00:51 [INFO]: Epoch 021 - training loss: 0.4884, validation loss: 0.2943
2024-06-03 06:00:57 [INFO]: Epoch 022 - training loss: 0.4874, validation loss: 0.2911
2024-06-03 06:01:03 [INFO]: Epoch 023 - training loss: 0.4866, validation loss: 0.2931
2024-06-03 06:01:09 [INFO]: Epoch 024 - training loss: 0.4845, validation loss: 0.2937
2024-06-03 06:01:15 [INFO]: Epoch 025 - training loss: 0.4819, validation loss: 0.2924
2024-06-03 06:01:20 [INFO]: Epoch 026 - training loss: 0.4824, validation loss: 0.2936
2024-06-03 06:01:26 [INFO]: Epoch 027 - training loss: 0.4803, validation loss: 0.2885
2024-06-03 06:01:32 [INFO]: Epoch 028 - training loss: 0.4792, validation loss: 0.2881
2024-06-03 06:01:38 [INFO]: Epoch 029 - training loss: 0.4765, validation loss: 0.2888
2024-06-03 06:01:44 [INFO]: Epoch 030 - training loss: 0.4766, validation loss: 0.2871
2024-06-03 06:01:50 [INFO]: Epoch 031 - training loss: 0.4759, validation loss: 0.2864
2024-06-03 06:01:55 [INFO]: Epoch 032 - training loss: 0.4754, validation loss: 0.2892
2024-06-03 06:02:01 [INFO]: Epoch 033 - training loss: 0.4714, validation loss: 0.2905
2024-06-03 06:02:07 [INFO]: Epoch 034 - training loss: 0.4716, validation loss: 0.2849
2024-06-03 06:02:13 [INFO]: Epoch 035 - training loss: 0.4716, validation loss: 0.2871
2024-06-03 06:02:18 [INFO]: Epoch 036 - training loss: 0.4711, validation loss: 0.2856
2024-06-03 06:02:24 [INFO]: Epoch 037 - training loss: 0.4673, validation loss: 0.2885
2024-06-03 06:02:30 [INFO]: Epoch 038 - training loss: 0.4680, validation loss: 0.2848
2024-06-03 06:02:35 [INFO]: Epoch 039 - training loss: 0.4666, validation loss: 0.2874
2024-06-03 06:02:41 [INFO]: Epoch 040 - training loss: 0.4659, validation loss: 0.2857
2024-06-03 06:02:47 [INFO]: Epoch 041 - training loss: 0.4638, validation loss: 0.2829
2024-06-03 06:02:53 [INFO]: Epoch 042 - training loss: 0.4627, validation loss: 0.2832
2024-06-03 06:02:59 [INFO]: Epoch 043 - training loss: 0.4625, validation loss: 0.2857
2024-06-03 06:03:05 [INFO]: Epoch 044 - training loss: 0.4623, validation loss: 0.2846
2024-06-03 06:03:10 [INFO]: Epoch 045 - training loss: 0.4610, validation loss: 0.2821
2024-06-03 06:03:16 [INFO]: Epoch 046 - training loss: 0.4622, validation loss: 0.2833
2024-06-03 06:03:21 [INFO]: Epoch 047 - training loss: 0.4600, validation loss: 0.2853
2024-06-03 06:03:27 [INFO]: Epoch 048 - training loss: 0.4617, validation loss: 0.2822
2024-06-03 06:03:33 [INFO]: Epoch 049 - training loss: 0.4575, validation loss: 0.2812
2024-06-03 06:03:38 [INFO]: Epoch 050 - training loss: 0.4586, validation loss: 0.2797
2024-06-03 06:03:44 [INFO]: Epoch 051 - training loss: 0.4569, validation loss: 0.2823
2024-06-03 06:03:50 [INFO]: Epoch 052 - training loss: 0.4573, validation loss: 0.2830
2024-06-03 06:03:56 [INFO]: Epoch 053 - training loss: 0.4582, validation loss: 0.2819
2024-06-03 06:04:01 [INFO]: Epoch 054 - training loss: 0.4541, validation loss: 0.2819
2024-06-03 06:04:07 [INFO]: Epoch 055 - training loss: 0.4538, validation loss: 0.2834
2024-06-03 06:04:13 [INFO]: Epoch 056 - training loss: 0.4539, validation loss: 0.2845
2024-06-03 06:04:19 [INFO]: Epoch 057 - training loss: 0.4529, validation loss: 0.2829
2024-06-03 06:04:24 [INFO]: Epoch 058 - training loss: 0.4531, validation loss: 0.2856
2024-06-03 06:04:29 [INFO]: Epoch 059 - training loss: 0.4511, validation loss: 0.2836
2024-06-03 06:04:36 [INFO]: Epoch 060 - training loss: 0.4531, validation loss: 0.2817
2024-06-03 06:04:36 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:04:36 [INFO]: Finished training. The best model is from epoch#50.
2024-06-03 06:04:36 [INFO]: Saved the model to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/20240603_T055850/NonstationaryTransformer.pypots
2024-06-03 06:04:38 [INFO]: Successfully saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_1/imputation.pkl
2024-06-03 06:04:38 [INFO]: Round1 - NonstationaryTransformer on BeijingAir: MAE=0.2425, MSE=0.2827, MRE=0.3301
2024-06-03 06:04:38 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 06:04:38 [INFO]: Using the given device: cuda:0
2024-06-03 06:04:38 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240603_T060438
2024-06-03 06:04:38 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240603_T060438/tensorboard
2024-06-03 06:04:38 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 06:04:44 [INFO]: Epoch 001 - training loss: 0.6527, validation loss: 0.3470
2024-06-03 06:04:50 [INFO]: Epoch 002 - training loss: 0.5682, validation loss: 0.3338
2024-06-03 06:04:56 [INFO]: Epoch 003 - training loss: 0.5534, validation loss: 0.3261
2024-06-03 06:05:02 [INFO]: Epoch 004 - training loss: 0.5470, validation loss: 0.3197
2024-06-03 06:05:07 [INFO]: Epoch 005 - training loss: 0.5385, validation loss: 0.3144
2024-06-03 06:05:13 [INFO]: Epoch 006 - training loss: 0.5350, validation loss: 0.3271
2024-06-03 06:05:19 [INFO]: Epoch 007 - training loss: 0.5312, validation loss: 0.3111
2024-06-03 06:05:25 [INFO]: Epoch 008 - training loss: 0.5250, validation loss: 0.3115
2024-06-03 06:05:30 [INFO]: Epoch 009 - training loss: 0.5213, validation loss: 0.3147
2024-06-03 06:05:35 [INFO]: Epoch 010 - training loss: 0.5173, validation loss: 0.3155
2024-06-03 06:05:40 [INFO]: Epoch 011 - training loss: 0.5134, validation loss: 0.3092
2024-06-03 06:05:45 [INFO]: Epoch 012 - training loss: 0.5128, validation loss: 0.3023
2024-06-03 06:05:50 [INFO]: Epoch 013 - training loss: 0.5083, validation loss: 0.3035
2024-06-03 06:05:55 [INFO]: Epoch 014 - training loss: 0.5057, validation loss: 0.3041
2024-06-03 06:06:00 [INFO]: Epoch 015 - training loss: 0.5037, validation loss: 0.3078
2024-06-03 06:06:05 [INFO]: Epoch 016 - training loss: 0.5033, validation loss: 0.3019
2024-06-03 06:06:11 [INFO]: Epoch 017 - training loss: 0.5001, validation loss: 0.3047
2024-06-03 06:06:16 [INFO]: Epoch 018 - training loss: 0.4974, validation loss: 0.2961
2024-06-03 06:06:21 [INFO]: Epoch 019 - training loss: 0.4957, validation loss: 0.2986
2024-06-03 06:06:26 [INFO]: Epoch 020 - training loss: 0.4951, validation loss: 0.3009
2024-06-03 06:06:31 [INFO]: Epoch 021 - training loss: 0.4916, validation loss: 0.3042
2024-06-03 06:06:36 [INFO]: Epoch 022 - training loss: 0.4909, validation loss: 0.2999
2024-06-03 06:06:41 [INFO]: Epoch 023 - training loss: 0.4879, validation loss: 0.2924
2024-06-03 06:06:46 [INFO]: Epoch 024 - training loss: 0.4881, validation loss: 0.2928
2024-06-03 06:06:51 [INFO]: Epoch 025 - training loss: 0.4859, validation loss: 0.2959
2024-06-03 06:06:56 [INFO]: Epoch 026 - training loss: 0.4828, validation loss: 0.2936
2024-06-03 06:07:02 [INFO]: Epoch 027 - training loss: 0.4804, validation loss: 0.2936
2024-06-03 06:07:07 [INFO]: Epoch 028 - training loss: 0.4816, validation loss: 0.2930
2024-06-03 06:07:11 [INFO]: Epoch 029 - training loss: 0.4795, validation loss: 0.2915
2024-06-03 06:07:16 [INFO]: Epoch 030 - training loss: 0.4761, validation loss: 0.2915
2024-06-03 06:07:22 [INFO]: Epoch 031 - training loss: 0.4764, validation loss: 0.2889
2024-06-03 06:07:27 [INFO]: Epoch 032 - training loss: 0.4751, validation loss: 0.2903
2024-06-03 06:07:32 [INFO]: Epoch 033 - training loss: 0.4740, validation loss: 0.2936
2024-06-03 06:07:37 [INFO]: Epoch 034 - training loss: 0.4748, validation loss: 0.2908
2024-06-03 06:07:42 [INFO]: Epoch 035 - training loss: 0.4721, validation loss: 0.2898
2024-06-03 06:07:46 [INFO]: Epoch 036 - training loss: 0.4710, validation loss: 0.2925
2024-06-03 06:07:52 [INFO]: Epoch 037 - training loss: 0.4708, validation loss: 0.2909
2024-06-03 06:07:57 [INFO]: Epoch 038 - training loss: 0.4692, validation loss: 0.2882
2024-06-03 06:08:01 [INFO]: Epoch 039 - training loss: 0.4699, validation loss: 0.2866
2024-06-03 06:08:06 [INFO]: Epoch 040 - training loss: 0.4675, validation loss: 0.2896
2024-06-03 06:08:11 [INFO]: Epoch 041 - training loss: 0.4662, validation loss: 0.2875
2024-06-03 06:08:16 [INFO]: Epoch 042 - training loss: 0.4655, validation loss: 0.2878
2024-06-03 06:08:21 [INFO]: Epoch 043 - training loss: 0.4654, validation loss: 0.2873
2024-06-03 06:08:26 [INFO]: Epoch 044 - training loss: 0.4634, validation loss: 0.2882
2024-06-03 06:08:31 [INFO]: Epoch 045 - training loss: 0.4628, validation loss: 0.2885
2024-06-03 06:08:36 [INFO]: Epoch 046 - training loss: 0.4626, validation loss: 0.2873
2024-06-03 06:08:41 [INFO]: Epoch 047 - training loss: 0.4614, validation loss: 0.2861
2024-06-03 06:08:46 [INFO]: Epoch 048 - training loss: 0.4606, validation loss: 0.2871
2024-06-03 06:08:51 [INFO]: Epoch 049 - training loss: 0.4590, validation loss: 0.2859
2024-06-03 06:08:56 [INFO]: Epoch 050 - training loss: 0.4576, validation loss: 0.2855
2024-06-03 06:09:01 [INFO]: Epoch 051 - training loss: 0.4586, validation loss: 0.2882
2024-06-03 06:09:06 [INFO]: Epoch 052 - training loss: 0.4567, validation loss: 0.2890
2024-06-03 06:09:11 [INFO]: Epoch 053 - training loss: 0.4541, validation loss: 0.2873
2024-06-03 06:09:15 [INFO]: Epoch 054 - training loss: 0.4572, validation loss: 0.2871
2024-06-03 06:09:20 [INFO]: Epoch 055 - training loss: 0.4540, validation loss: 0.2891
2024-06-03 06:09:25 [INFO]: Epoch 056 - training loss: 0.4562, validation loss: 0.2860
2024-06-03 06:09:30 [INFO]: Epoch 057 - training loss: 0.4539, validation loss: 0.2867
2024-06-03 06:09:35 [INFO]: Epoch 058 - training loss: 0.4523, validation loss: 0.2855
2024-06-03 06:09:40 [INFO]: Epoch 059 - training loss: 0.4528, validation loss: 0.2862
2024-06-03 06:09:45 [INFO]: Epoch 060 - training loss: 0.4528, validation loss: 0.2886
2024-06-03 06:09:50 [INFO]: Epoch 061 - training loss: 0.4511, validation loss: 0.2848
2024-06-03 06:09:56 [INFO]: Epoch 062 - training loss: 0.4525, validation loss: 0.2841
2024-06-03 06:10:00 [INFO]: Epoch 063 - training loss: 0.4525, validation loss: 0.2865
2024-06-03 06:10:05 [INFO]: Epoch 064 - training loss: 0.4488, validation loss: 0.2877
2024-06-03 06:10:11 [INFO]: Epoch 065 - training loss: 0.4505, validation loss: 0.2874
2024-06-03 06:10:16 [INFO]: Epoch 066 - training loss: 0.4496, validation loss: 0.2844
2024-06-03 06:10:21 [INFO]: Epoch 067 - training loss: 0.4479, validation loss: 0.2871
2024-06-03 06:10:26 [INFO]: Epoch 068 - training loss: 0.4462, validation loss: 0.2837
2024-06-03 06:10:31 [INFO]: Epoch 069 - training loss: 0.4468, validation loss: 0.2831
2024-06-03 06:10:36 [INFO]: Epoch 070 - training loss: 0.4462, validation loss: 0.2823
2024-06-03 06:10:41 [INFO]: Epoch 071 - training loss: 0.4455, validation loss: 0.2838
2024-06-03 06:10:46 [INFO]: Epoch 072 - training loss: 0.4440, validation loss: 0.2823
2024-06-03 06:10:51 [INFO]: Epoch 073 - training loss: 0.4446, validation loss: 0.2842
2024-06-03 06:10:56 [INFO]: Epoch 074 - training loss: 0.4456, validation loss: 0.2854
2024-06-03 06:11:01 [INFO]: Epoch 075 - training loss: 0.4428, validation loss: 0.2817
2024-06-03 06:11:06 [INFO]: Epoch 076 - training loss: 0.4438, validation loss: 0.2827
2024-06-03 06:11:11 [INFO]: Epoch 077 - training loss: 0.4423, validation loss: 0.2825
2024-06-03 06:11:16 [INFO]: Epoch 078 - training loss: 0.4419, validation loss: 0.2843
2024-06-03 06:11:20 [INFO]: Epoch 079 - training loss: 0.4416, validation loss: 0.2842
2024-06-03 06:11:26 [INFO]: Epoch 080 - training loss: 0.4435, validation loss: 0.2853
2024-06-03 06:11:31 [INFO]: Epoch 081 - training loss: 0.4410, validation loss: 0.2860
2024-06-03 06:11:36 [INFO]: Epoch 082 - training loss: 0.4400, validation loss: 0.2844
2024-06-03 06:11:41 [INFO]: Epoch 083 - training loss: 0.4402, validation loss: 0.2849
2024-06-03 06:11:46 [INFO]: Epoch 084 - training loss: 0.4398, validation loss: 0.2881
2024-06-03 06:11:51 [INFO]: Epoch 085 - training loss: 0.4387, validation loss: 0.2871
2024-06-03 06:11:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:11:51 [INFO]: Finished training. The best model is from epoch#75.
2024-06-03 06:11:51 [INFO]: Saved the model to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/20240603_T060438/NonstationaryTransformer.pypots
2024-06-03 06:11:53 [INFO]: Successfully saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_2/imputation.pkl
2024-06-03 06:11:53 [INFO]: Round2 - NonstationaryTransformer on BeijingAir: MAE=0.2464, MSE=0.3046, MRE=0.3355
2024-06-03 06:11:53 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 06:11:53 [INFO]: Using the given device: cuda:0
2024-06-03 06:11:53 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240603_T061153
2024-06-03 06:11:53 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240603_T061153/tensorboard
2024-06-03 06:11:53 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 06:11:58 [INFO]: Epoch 001 - training loss: 0.6528, validation loss: 0.3497
2024-06-03 06:12:03 [INFO]: Epoch 002 - training loss: 0.5682, validation loss: 0.3422
2024-06-03 06:12:09 [INFO]: Epoch 003 - training loss: 0.5521, validation loss: 0.3351
2024-06-03 06:12:13 [INFO]: Epoch 004 - training loss: 0.5460, validation loss: 0.3278
2024-06-03 06:12:18 [INFO]: Epoch 005 - training loss: 0.5392, validation loss: 0.3225
2024-06-03 06:12:24 [INFO]: Epoch 006 - training loss: 0.5338, validation loss: 0.3124
2024-06-03 06:12:29 [INFO]: Epoch 007 - training loss: 0.5301, validation loss: 0.3143
2024-06-03 06:12:33 [INFO]: Epoch 008 - training loss: 0.5248, validation loss: 0.3061
2024-06-03 06:12:38 [INFO]: Epoch 009 - training loss: 0.5204, validation loss: 0.3092
2024-06-03 06:12:43 [INFO]: Epoch 010 - training loss: 0.5179, validation loss: 0.3053
2024-06-03 06:12:48 [INFO]: Epoch 011 - training loss: 0.5138, validation loss: 0.3050
2024-06-03 06:12:53 [INFO]: Epoch 012 - training loss: 0.5101, validation loss: 0.3048
2024-06-03 06:12:58 [INFO]: Epoch 013 - training loss: 0.5091, validation loss: 0.3009
2024-06-03 06:13:03 [INFO]: Epoch 014 - training loss: 0.5068, validation loss: 0.3033
2024-06-03 06:13:08 [INFO]: Epoch 015 - training loss: 0.5050, validation loss: 0.2954
2024-06-03 06:13:14 [INFO]: Epoch 016 - training loss: 0.4999, validation loss: 0.2953
2024-06-03 06:13:19 [INFO]: Epoch 017 - training loss: 0.4983, validation loss: 0.2919
2024-06-03 06:13:24 [INFO]: Epoch 018 - training loss: 0.4964, validation loss: 0.2989
2024-06-03 06:13:29 [INFO]: Epoch 019 - training loss: 0.4946, validation loss: 0.2932
2024-06-03 06:13:34 [INFO]: Epoch 020 - training loss: 0.4934, validation loss: 0.2942
2024-06-03 06:13:39 [INFO]: Epoch 021 - training loss: 0.4928, validation loss: 0.2900
2024-06-03 06:13:44 [INFO]: Epoch 022 - training loss: 0.4906, validation loss: 0.2914
2024-06-03 06:13:49 [INFO]: Epoch 023 - training loss: 0.4874, validation loss: 0.2916
2024-06-03 06:13:54 [INFO]: Epoch 024 - training loss: 0.4855, validation loss: 0.2907
2024-06-03 06:13:59 [INFO]: Epoch 025 - training loss: 0.4843, validation loss: 0.2871
2024-06-03 06:14:04 [INFO]: Epoch 026 - training loss: 0.4825, validation loss: 0.2891
2024-06-03 06:14:09 [INFO]: Epoch 027 - training loss: 0.4846, validation loss: 0.2930
2024-06-03 06:14:14 [INFO]: Epoch 028 - training loss: 0.4789, validation loss: 0.2893
2024-06-03 06:14:19 [INFO]: Epoch 029 - training loss: 0.4781, validation loss: 0.2888
2024-06-03 06:14:24 [INFO]: Epoch 030 - training loss: 0.4780, validation loss: 0.2905
2024-06-03 06:14:29 [INFO]: Epoch 031 - training loss: 0.4760, validation loss: 0.2860
2024-06-03 06:14:34 [INFO]: Epoch 032 - training loss: 0.4756, validation loss: 0.2877
2024-06-03 06:14:39 [INFO]: Epoch 033 - training loss: 0.4744, validation loss: 0.2908
2024-06-03 06:14:44 [INFO]: Epoch 034 - training loss: 0.4728, validation loss: 0.2873
2024-06-03 06:14:49 [INFO]: Epoch 035 - training loss: 0.4716, validation loss: 0.2886
2024-06-03 06:14:54 [INFO]: Epoch 036 - training loss: 0.4692, validation loss: 0.2849
2024-06-03 06:14:59 [INFO]: Epoch 037 - training loss: 0.4687, validation loss: 0.2849
2024-06-03 06:15:04 [INFO]: Epoch 038 - training loss: 0.4672, validation loss: 0.2836
2024-06-03 06:15:09 [INFO]: Epoch 039 - training loss: 0.4667, validation loss: 0.2847
2024-06-03 06:15:14 [INFO]: Epoch 040 - training loss: 0.4649, validation loss: 0.2811
2024-06-03 06:15:20 [INFO]: Epoch 041 - training loss: 0.4656, validation loss: 0.2830
2024-06-03 06:15:25 [INFO]: Epoch 042 - training loss: 0.4649, validation loss: 0.2820
2024-06-03 06:15:30 [INFO]: Epoch 043 - training loss: 0.4639, validation loss: 0.2851
2024-06-03 06:15:35 [INFO]: Epoch 044 - training loss: 0.4631, validation loss: 0.2826
2024-06-03 06:15:40 [INFO]: Epoch 045 - training loss: 0.4617, validation loss: 0.2849
2024-06-03 06:15:45 [INFO]: Epoch 046 - training loss: 0.4610, validation loss: 0.2850
2024-06-03 06:15:50 [INFO]: Epoch 047 - training loss: 0.4611, validation loss: 0.2837
2024-06-03 06:15:54 [INFO]: Epoch 048 - training loss: 0.4588, validation loss: 0.2808
2024-06-03 06:15:59 [INFO]: Epoch 049 - training loss: 0.4580, validation loss: 0.2823
2024-06-03 06:16:04 [INFO]: Epoch 050 - training loss: 0.4576, validation loss: 0.2822
2024-06-03 06:16:09 [INFO]: Epoch 051 - training loss: 0.4581, validation loss: 0.2844
2024-06-03 06:16:14 [INFO]: Epoch 052 - training loss: 0.4573, validation loss: 0.2831
2024-06-03 06:16:19 [INFO]: Epoch 053 - training loss: 0.4548, validation loss: 0.2834
2024-06-03 06:16:24 [INFO]: Epoch 054 - training loss: 0.4553, validation loss: 0.2826
2024-06-03 06:16:29 [INFO]: Epoch 055 - training loss: 0.4547, validation loss: 0.2822
2024-06-03 06:16:34 [INFO]: Epoch 056 - training loss: 0.4535, validation loss: 0.2830
2024-06-03 06:16:39 [INFO]: Epoch 057 - training loss: 0.4523, validation loss: 0.2808
2024-06-03 06:16:45 [INFO]: Epoch 058 - training loss: 0.4526, validation loss: 0.2811
2024-06-03 06:16:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:16:45 [INFO]: Finished training. The best model is from epoch#48.
2024-06-03 06:16:45 [INFO]: Saved the model to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/20240603_T061153/NonstationaryTransformer.pypots
2024-06-03 06:16:46 [INFO]: Successfully saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_3/imputation.pkl
2024-06-03 06:16:46 [INFO]: Round3 - NonstationaryTransformer on BeijingAir: MAE=0.2436, MSE=0.2949, MRE=0.3316
2024-06-03 06:16:46 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 06:16:46 [INFO]: Using the given device: cuda:0
2024-06-03 06:16:47 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240603_T061646
2024-06-03 06:16:47 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240603_T061646/tensorboard
2024-06-03 06:16:47 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 6,978,068
2024-06-03 06:16:52 [INFO]: Epoch 001 - training loss: 0.6540, validation loss: 0.3535
2024-06-03 06:16:57 [INFO]: Epoch 002 - training loss: 0.5697, validation loss: 0.3423
2024-06-03 06:17:02 [INFO]: Epoch 003 - training loss: 0.5538, validation loss: 0.3265
2024-06-03 06:17:07 [INFO]: Epoch 004 - training loss: 0.5472, validation loss: 0.3255
2024-06-03 06:17:12 [INFO]: Epoch 005 - training loss: 0.5389, validation loss: 0.3186
2024-06-03 06:17:18 [INFO]: Epoch 006 - training loss: 0.5337, validation loss: 0.3153
2024-06-03 06:17:23 [INFO]: Epoch 007 - training loss: 0.5285, validation loss: 0.3105
2024-06-03 06:17:28 [INFO]: Epoch 008 - training loss: 0.5262, validation loss: 0.3082
2024-06-03 06:17:32 [INFO]: Epoch 009 - training loss: 0.5203, validation loss: 0.3147
2024-06-03 06:17:37 [INFO]: Epoch 010 - training loss: 0.5172, validation loss: 0.3089
2024-06-03 06:17:43 [INFO]: Epoch 011 - training loss: 0.5148, validation loss: 0.3061
2024-06-03 06:17:48 [INFO]: Epoch 012 - training loss: 0.5105, validation loss: 0.3064
2024-06-03 06:17:53 [INFO]: Epoch 013 - training loss: 0.5085, validation loss: 0.3040
2024-06-03 06:17:58 [INFO]: Epoch 014 - training loss: 0.5062, validation loss: 0.3047
2024-06-03 06:18:03 [INFO]: Epoch 015 - training loss: 0.5033, validation loss: 0.2988
2024-06-03 06:18:08 [INFO]: Epoch 016 - training loss: 0.5008, validation loss: 0.3010
2024-06-03 06:18:13 [INFO]: Epoch 017 - training loss: 0.5003, validation loss: 0.3005
2024-06-03 06:18:18 [INFO]: Epoch 018 - training loss: 0.4962, validation loss: 0.2962
2024-06-03 06:18:23 [INFO]: Epoch 019 - training loss: 0.4953, validation loss: 0.2960
2024-06-03 06:18:28 [INFO]: Epoch 020 - training loss: 0.4927, validation loss: 0.2980
2024-06-03 06:18:33 [INFO]: Epoch 021 - training loss: 0.4915, validation loss: 0.2965
2024-06-03 06:18:38 [INFO]: Epoch 022 - training loss: 0.4898, validation loss: 0.2954
2024-06-03 06:18:43 [INFO]: Epoch 023 - training loss: 0.4884, validation loss: 0.2932
2024-06-03 06:18:48 [INFO]: Epoch 024 - training loss: 0.4859, validation loss: 0.2962
2024-06-03 06:18:53 [INFO]: Epoch 025 - training loss: 0.4830, validation loss: 0.2957
2024-06-03 06:18:58 [INFO]: Epoch 026 - training loss: 0.4827, validation loss: 0.2910
2024-06-03 06:19:03 [INFO]: Epoch 027 - training loss: 0.4832, validation loss: 0.2933
2024-06-03 06:19:08 [INFO]: Epoch 028 - training loss: 0.4825, validation loss: 0.2907
2024-06-03 06:19:12 [INFO]: Epoch 029 - training loss: 0.4791, validation loss: 0.2916
2024-06-03 06:19:17 [INFO]: Epoch 030 - training loss: 0.4775, validation loss: 0.2896
2024-06-03 06:19:21 [INFO]: Epoch 031 - training loss: 0.4753, validation loss: 0.2916
2024-06-03 06:19:26 [INFO]: Epoch 032 - training loss: 0.4748, validation loss: 0.2966
2024-06-03 06:19:30 [INFO]: Epoch 033 - training loss: 0.4721, validation loss: 0.2899
2024-06-03 06:19:35 [INFO]: Epoch 034 - training loss: 0.4725, validation loss: 0.2886
2024-06-03 06:19:40 [INFO]: Epoch 035 - training loss: 0.4731, validation loss: 0.2876
2024-06-03 06:19:44 [INFO]: Epoch 036 - training loss: 0.4716, validation loss: 0.2877
2024-06-03 06:19:49 [INFO]: Epoch 037 - training loss: 0.4711, validation loss: 0.2909
2024-06-03 06:19:54 [INFO]: Epoch 038 - training loss: 0.4676, validation loss: 0.2858
2024-06-03 06:19:58 [INFO]: Epoch 039 - training loss: 0.4682, validation loss: 0.2896
2024-06-03 06:20:03 [INFO]: Epoch 040 - training loss: 0.4646, validation loss: 0.2889
2024-06-03 06:20:07 [INFO]: Epoch 041 - training loss: 0.4654, validation loss: 0.2864
2024-06-03 06:20:12 [INFO]: Epoch 042 - training loss: 0.4655, validation loss: 0.2897
2024-06-03 06:20:16 [INFO]: Epoch 043 - training loss: 0.4666, validation loss: 0.2850
2024-06-03 06:20:21 [INFO]: Epoch 044 - training loss: 0.4625, validation loss: 0.2869
2024-06-03 06:20:25 [INFO]: Epoch 045 - training loss: 0.4627, validation loss: 0.2890
2024-06-03 06:20:30 [INFO]: Epoch 046 - training loss: 0.4624, validation loss: 0.2891
2024-06-03 06:20:34 [INFO]: Epoch 047 - training loss: 0.4617, validation loss: 0.2872
2024-06-03 06:20:39 [INFO]: Epoch 048 - training loss: 0.4592, validation loss: 0.2852
2024-06-03 06:20:44 [INFO]: Epoch 049 - training loss: 0.4558, validation loss: 0.2882
2024-06-03 06:20:48 [INFO]: Epoch 050 - training loss: 0.4578, validation loss: 0.2867
2024-06-03 06:20:53 [INFO]: Epoch 051 - training loss: 0.4573, validation loss: 0.2868
2024-06-03 06:20:57 [INFO]: Epoch 052 - training loss: 0.4560, validation loss: 0.2854
2024-06-03 06:21:02 [INFO]: Epoch 053 - training loss: 0.4563, validation loss: 0.2882
2024-06-03 06:21:02 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:21:02 [INFO]: Finished training. The best model is from epoch#43.
2024-06-03 06:21:02 [INFO]: Saved the model to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/20240603_T061646/NonstationaryTransformer.pypots
2024-06-03 06:21:04 [INFO]: Successfully saved to results_point_rate05/BeijingAir/NonstationaryTransformer_BeijingAir/round_4/imputation.pkl
2024-06-03 06:21:04 [INFO]: Round4 - NonstationaryTransformer on BeijingAir: MAE=0.2457, MSE=0.2925, MRE=0.3345
2024-06-03 06:21:04 [INFO]: Done! Final results:
Averaged NonstationaryTransformer (6,978,068 params) on BeijingAir: MAE=0.2313 ± 0.0012229559297893256, MSE=0.2705 ± 0.00661509309314092, MRE=0.3065 ± 0.001620945554460641, average inference time=0.39