2024-06-03 11:16:53 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 11:16:53 [INFO]: Using the given device: cuda:0
2024-06-03 11:16:54 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_0/20240603_T111654
2024-06-03 11:16:54 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_0/20240603_T111654/tensorboard
2024-06-03 11:16:55 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 11:17:09 [INFO]: Epoch 001 - training loss: 1.4089, validation loss: 0.8274
2024-06-03 11:17:14 [INFO]: Epoch 002 - training loss: 0.9877, validation loss: 0.5336
2024-06-03 11:17:19 [INFO]: Epoch 003 - training loss: 0.7828, validation loss: 0.4345
2024-06-03 11:17:23 [INFO]: Epoch 004 - training loss: 0.6710, validation loss: 0.3737
2024-06-03 11:17:28 [INFO]: Epoch 005 - training loss: 0.6015, validation loss: 0.3412
2024-06-03 11:17:33 [INFO]: Epoch 006 - training loss: 0.5518, validation loss: 0.3187
2024-06-03 11:17:38 [INFO]: Epoch 007 - training loss: 0.5178, validation loss: 0.3065
2024-06-03 11:17:43 [INFO]: Epoch 008 - training loss: 0.4967, validation loss: 0.2961
2024-06-03 11:17:47 [INFO]: Epoch 009 - training loss: 0.4790, validation loss: 0.2887
2024-06-03 11:17:52 [INFO]: Epoch 010 - training loss: 0.4661, validation loss: 0.2850
2024-06-03 11:17:57 [INFO]: Epoch 011 - training loss: 0.4566, validation loss: 0.2767
2024-06-03 11:18:02 [INFO]: Epoch 012 - training loss: 0.4422, validation loss: 0.2675
2024-06-03 11:18:07 [INFO]: Epoch 013 - training loss: 0.4324, validation loss: 0.2681
2024-06-03 11:18:12 [INFO]: Epoch 014 - training loss: 0.4228, validation loss: 0.2661
2024-06-03 11:18:16 [INFO]: Epoch 015 - training loss: 0.4204, validation loss: 0.2607
2024-06-03 11:18:21 [INFO]: Epoch 016 - training loss: 0.4142, validation loss: 0.2562
2024-06-03 11:18:26 [INFO]: Epoch 017 - training loss: 0.4101, validation loss: 0.2485
2024-06-03 11:18:31 [INFO]: Epoch 018 - training loss: 0.4057, validation loss: 0.2534
2024-06-03 11:18:36 [INFO]: Epoch 019 - training loss: 0.4042, validation loss: 0.2522
2024-06-03 11:18:41 [INFO]: Epoch 020 - training loss: 0.4007, validation loss: 0.2481
2024-06-03 11:18:46 [INFO]: Epoch 021 - training loss: 0.3972, validation loss: 0.2435
2024-06-03 11:18:50 [INFO]: Epoch 022 - training loss: 0.3959, validation loss: 0.2457
2024-06-03 11:18:55 [INFO]: Epoch 023 - training loss: 0.3927, validation loss: 0.2413
2024-06-03 11:19:00 [INFO]: Epoch 024 - training loss: 0.3896, validation loss: 0.2420
2024-06-03 11:19:05 [INFO]: Epoch 025 - training loss: 0.3872, validation loss: 0.2399
2024-06-03 11:19:09 [INFO]: Epoch 026 - training loss: 0.3832, validation loss: 0.2373
2024-06-03 11:19:14 [INFO]: Epoch 027 - training loss: 0.3851, validation loss: 0.2401
2024-06-03 11:19:19 [INFO]: Epoch 028 - training loss: 0.3834, validation loss: 0.2388
2024-06-03 11:19:23 [INFO]: Epoch 029 - training loss: 0.3784, validation loss: 0.2356
2024-06-03 11:19:28 [INFO]: Epoch 030 - training loss: 0.3774, validation loss: 0.2371
2024-06-03 11:19:33 [INFO]: Epoch 031 - training loss: 0.3812, validation loss: 0.2361
2024-06-03 11:19:38 [INFO]: Epoch 032 - training loss: 0.3774, validation loss: 0.2371
2024-06-03 11:19:42 [INFO]: Epoch 033 - training loss: 0.3753, validation loss: 0.2366
2024-06-03 11:19:47 [INFO]: Epoch 034 - training loss: 0.3744, validation loss: 0.2376
2024-06-03 11:19:52 [INFO]: Epoch 035 - training loss: 0.3756, validation loss: 0.2384
2024-06-03 11:19:56 [INFO]: Epoch 036 - training loss: 0.3696, validation loss: 0.2341
2024-06-03 11:20:01 [INFO]: Epoch 037 - training loss: 0.3676, validation loss: 0.2324
2024-06-03 11:20:06 [INFO]: Epoch 038 - training loss: 0.3748, validation loss: 0.2359
2024-06-03 11:20:11 [INFO]: Epoch 039 - training loss: 0.3690, validation loss: 0.2344
2024-06-03 11:20:16 [INFO]: Epoch 040 - training loss: 0.3649, validation loss: 0.2314
2024-06-03 11:20:21 [INFO]: Epoch 041 - training loss: 0.3678, validation loss: 0.2334
2024-06-03 11:20:25 [INFO]: Epoch 042 - training loss: 0.3652, validation loss: 0.2295
2024-06-03 11:20:31 [INFO]: Epoch 043 - training loss: 0.3656, validation loss: 0.2313
2024-06-03 11:20:36 [INFO]: Epoch 044 - training loss: 0.3607, validation loss: 0.2332
2024-06-03 11:20:40 [INFO]: Epoch 045 - training loss: 0.3621, validation loss: 0.2317
2024-06-03 11:20:45 [INFO]: Epoch 046 - training loss: 0.3609, validation loss: 0.2303
2024-06-03 11:20:50 [INFO]: Epoch 047 - training loss: 0.3576, validation loss: 0.2319
2024-06-03 11:20:54 [INFO]: Epoch 048 - training loss: 0.3616, validation loss: 0.2361
2024-06-03 11:20:59 [INFO]: Epoch 049 - training loss: 0.3593, validation loss: 0.2316
2024-06-03 11:21:04 [INFO]: Epoch 050 - training loss: 0.3604, validation loss: 0.2314
2024-06-03 11:21:10 [INFO]: Epoch 051 - training loss: 0.3603, validation loss: 0.2327
2024-06-03 11:21:15 [INFO]: Epoch 052 - training loss: 0.3598, validation loss: 0.2357
2024-06-03 11:21:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:21:15 [INFO]: Finished training. The best model is from epoch#42.
2024-06-03 11:21:15 [INFO]: Saved the model to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_0/20240603_T111654/SCINet.pypots
2024-06-03 11:21:16 [INFO]: Successfully saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_0/imputation.pkl
2024-06-03 11:21:16 [INFO]: Round0 - SCINet on BeijingAir: MAE=0.2285, MSE=0.2228, MRE=0.3111
2024-06-03 11:21:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 11:21:16 [INFO]: Using the given device: cuda:0
2024-06-03 11:21:17 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_1/20240603_T112116
2024-06-03 11:21:17 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_1/20240603_T112116/tensorboard
2024-06-03 11:21:17 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 11:21:22 [INFO]: Epoch 001 - training loss: 1.3985, validation loss: 0.8002
2024-06-03 11:21:27 [INFO]: Epoch 002 - training loss: 0.9827, validation loss: 0.5377
2024-06-03 11:21:32 [INFO]: Epoch 003 - training loss: 0.7903, validation loss: 0.4350
2024-06-03 11:21:37 [INFO]: Epoch 004 - training loss: 0.6849, validation loss: 0.3806
2024-06-03 11:21:41 [INFO]: Epoch 005 - training loss: 0.6156, validation loss: 0.3459
2024-06-03 11:21:46 [INFO]: Epoch 006 - training loss: 0.5680, validation loss: 0.3340
2024-06-03 11:21:51 [INFO]: Epoch 007 - training loss: 0.5334, validation loss: 0.3181
2024-06-03 11:21:56 [INFO]: Epoch 008 - training loss: 0.5107, validation loss: 0.3141
2024-06-03 11:22:01 [INFO]: Epoch 009 - training loss: 0.4964, validation loss: 0.3015
2024-06-03 11:22:05 [INFO]: Epoch 010 - training loss: 0.4799, validation loss: 0.2941
2024-06-03 11:22:10 [INFO]: Epoch 011 - training loss: 0.4679, validation loss: 0.2834
2024-06-03 11:22:15 [INFO]: Epoch 012 - training loss: 0.4534, validation loss: 0.2823
2024-06-03 11:22:19 [INFO]: Epoch 013 - training loss: 0.4461, validation loss: 0.2776
2024-06-03 11:22:24 [INFO]: Epoch 014 - training loss: 0.4359, validation loss: 0.2755
2024-06-03 11:22:29 [INFO]: Epoch 015 - training loss: 0.4301, validation loss: 0.2673
2024-06-03 11:22:34 [INFO]: Epoch 016 - training loss: 0.4231, validation loss: 0.2698
2024-06-03 11:22:39 [INFO]: Epoch 017 - training loss: 0.4199, validation loss: 0.2651
2024-06-03 11:22:44 [INFO]: Epoch 018 - training loss: 0.4157, validation loss: 0.2627
2024-06-03 11:22:48 [INFO]: Epoch 019 - training loss: 0.4121, validation loss: 0.2539
2024-06-03 11:22:53 [INFO]: Epoch 020 - training loss: 0.4070, validation loss: 0.2559
2024-06-03 11:22:58 [INFO]: Epoch 021 - training loss: 0.4042, validation loss: 0.2534
2024-06-03 11:23:03 [INFO]: Epoch 022 - training loss: 0.4029, validation loss: 0.2502
2024-06-03 11:23:08 [INFO]: Epoch 023 - training loss: 0.3990, validation loss: 0.2465
2024-06-03 11:23:13 [INFO]: Epoch 024 - training loss: 0.4014, validation loss: 0.2546
2024-06-03 11:23:18 [INFO]: Epoch 025 - training loss: 0.3944, validation loss: 0.2455
2024-06-03 11:23:23 [INFO]: Epoch 026 - training loss: 0.3893, validation loss: 0.2422
2024-06-03 11:23:27 [INFO]: Epoch 027 - training loss: 0.3886, validation loss: 0.2471
2024-06-03 11:23:32 [INFO]: Epoch 028 - training loss: 0.3852, validation loss: 0.2493
2024-06-03 11:23:37 [INFO]: Epoch 029 - training loss: 0.3872, validation loss: 0.2426
2024-06-03 11:23:42 [INFO]: Epoch 030 - training loss: 0.3849, validation loss: 0.2432
2024-06-03 11:23:47 [INFO]: Epoch 031 - training loss: 0.3800, validation loss: 0.2406
2024-06-03 11:23:52 [INFO]: Epoch 032 - training loss: 0.3835, validation loss: 0.2443
2024-06-03 11:23:57 [INFO]: Epoch 033 - training loss: 0.3811, validation loss: 0.2390
2024-06-03 11:24:01 [INFO]: Epoch 034 - training loss: 0.3778, validation loss: 0.2404
2024-06-03 11:24:06 [INFO]: Epoch 035 - training loss: 0.3771, validation loss: 0.2364
2024-06-03 11:24:11 [INFO]: Epoch 036 - training loss: 0.3738, validation loss: 0.2384
2024-06-03 11:24:16 [INFO]: Epoch 037 - training loss: 0.3739, validation loss: 0.2356
2024-06-03 11:24:20 [INFO]: Epoch 038 - training loss: 0.3753, validation loss: 0.2376
2024-06-03 11:24:25 [INFO]: Epoch 039 - training loss: 0.3711, validation loss: 0.2362
2024-06-03 11:24:30 [INFO]: Epoch 040 - training loss: 0.3739, validation loss: 0.2389
2024-06-03 11:24:34 [INFO]: Epoch 041 - training loss: 0.3704, validation loss: 0.2339
2024-06-03 11:24:39 [INFO]: Epoch 042 - training loss: 0.3652, validation loss: 0.2381
2024-06-03 11:24:44 [INFO]: Epoch 043 - training loss: 0.3666, validation loss: 0.2384
2024-06-03 11:24:49 [INFO]: Epoch 044 - training loss: 0.3646, validation loss: 0.2335
2024-06-03 11:24:54 [INFO]: Epoch 045 - training loss: 0.3674, validation loss: 0.2332
2024-06-03 11:24:59 [INFO]: Epoch 046 - training loss: 0.3628, validation loss: 0.2333
2024-06-03 11:25:03 [INFO]: Epoch 047 - training loss: 0.3643, validation loss: 0.2303
2024-06-03 11:25:08 [INFO]: Epoch 048 - training loss: 0.3614, validation loss: 0.2360
2024-06-03 11:25:13 [INFO]: Epoch 049 - training loss: 0.3626, validation loss: 0.2351
2024-06-03 11:25:17 [INFO]: Epoch 050 - training loss: 0.3620, validation loss: 0.2358
2024-06-03 11:25:22 [INFO]: Epoch 051 - training loss: 0.3589, validation loss: 0.2322
2024-06-03 11:25:27 [INFO]: Epoch 052 - training loss: 0.3602, validation loss: 0.2330
2024-06-03 11:25:32 [INFO]: Epoch 053 - training loss: 0.3594, validation loss: 0.2329
2024-06-03 11:25:37 [INFO]: Epoch 054 - training loss: 0.3572, validation loss: 0.2314
2024-06-03 11:25:42 [INFO]: Epoch 055 - training loss: 0.3575, validation loss: 0.2320
2024-06-03 11:25:47 [INFO]: Epoch 056 - training loss: 0.3557, validation loss: 0.2368
2024-06-03 11:25:51 [INFO]: Epoch 057 - training loss: 0.3583, validation loss: 0.2357
2024-06-03 11:25:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:25:51 [INFO]: Finished training. The best model is from epoch#47.
2024-06-03 11:25:52 [INFO]: Saved the model to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_1/20240603_T112116/SCINet.pypots
2024-06-03 11:25:53 [INFO]: Successfully saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_1/imputation.pkl
2024-06-03 11:25:53 [INFO]: Round1 - SCINet on BeijingAir: MAE=0.2254, MSE=0.2291, MRE=0.3068
2024-06-03 11:25:53 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 11:25:53 [INFO]: Using the given device: cuda:0
2024-06-03 11:25:53 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_2/20240603_T112553
2024-06-03 11:25:53 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_2/20240603_T112553/tensorboard
2024-06-03 11:25:54 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 11:25:59 [INFO]: Epoch 001 - training loss: 1.3441, validation loss: 0.7864
2024-06-03 11:26:04 [INFO]: Epoch 002 - training loss: 0.9674, validation loss: 0.5827
2024-06-03 11:26:07 [INFO]: Epoch 003 - training loss: 0.8181, validation loss: 0.5130
2024-06-03 11:26:11 [INFO]: Epoch 004 - training loss: 0.7428, validation loss: 0.4866
2024-06-03 11:26:14 [INFO]: Epoch 005 - training loss: 0.6926, validation loss: 0.4516
2024-06-03 11:26:18 [INFO]: Epoch 006 - training loss: 0.6631, validation loss: 0.4342
2024-06-03 11:26:23 [INFO]: Epoch 007 - training loss: 0.6399, validation loss: 0.4216
2024-06-03 11:26:28 [INFO]: Epoch 008 - training loss: 0.6125, validation loss: 0.4226
2024-06-03 11:26:33 [INFO]: Epoch 009 - training loss: 0.6024, validation loss: 0.4062
2024-06-03 11:26:38 [INFO]: Epoch 010 - training loss: 0.5816, validation loss: 0.3953
2024-06-03 11:26:43 [INFO]: Epoch 011 - training loss: 0.5623, validation loss: 0.3894
2024-06-03 11:26:48 [INFO]: Epoch 012 - training loss: 0.5509, validation loss: 0.3848
2024-06-03 11:26:53 [INFO]: Epoch 013 - training loss: 0.5479, validation loss: 0.3831
2024-06-03 11:26:57 [INFO]: Epoch 014 - training loss: 0.5346, validation loss: 0.3786
2024-06-03 11:27:02 [INFO]: Epoch 015 - training loss: 0.5298, validation loss: 0.3769
2024-06-03 11:27:07 [INFO]: Epoch 016 - training loss: 0.5156, validation loss: 0.3686
2024-06-03 11:27:12 [INFO]: Epoch 017 - training loss: 0.5105, validation loss: 0.3580
2024-06-03 11:27:17 [INFO]: Epoch 018 - training loss: 0.4960, validation loss: 0.3573
2024-06-03 11:27:21 [INFO]: Epoch 019 - training loss: 0.4964, validation loss: 0.3560
2024-06-03 11:27:26 [INFO]: Epoch 020 - training loss: 0.4857, validation loss: 0.3508
2024-06-03 11:27:31 [INFO]: Epoch 021 - training loss: 0.4762, validation loss: 0.3454
2024-06-03 11:27:36 [INFO]: Epoch 022 - training loss: 0.4681, validation loss: 0.3470
2024-06-03 11:27:41 [INFO]: Epoch 023 - training loss: 0.4642, validation loss: 0.3360
2024-06-03 11:27:46 [INFO]: Epoch 024 - training loss: 0.4607, validation loss: 0.3372
2024-06-03 11:27:50 [INFO]: Epoch 025 - training loss: 0.4546, validation loss: 0.3365
2024-06-03 11:27:56 [INFO]: Epoch 026 - training loss: 0.4539, validation loss: 0.3344
2024-06-03 11:28:00 [INFO]: Epoch 027 - training loss: 0.4538, validation loss: 0.3381
2024-06-03 11:28:05 [INFO]: Epoch 028 - training loss: 0.4486, validation loss: 0.3304
2024-06-03 11:28:10 [INFO]: Epoch 029 - training loss: 0.4397, validation loss: 0.3317
2024-06-03 11:28:15 [INFO]: Epoch 030 - training loss: 0.4429, validation loss: 0.3296
2024-06-03 11:28:20 [INFO]: Epoch 031 - training loss: 0.4409, validation loss: 0.3258
2024-06-03 11:28:25 [INFO]: Epoch 032 - training loss: 0.4358, validation loss: 0.3316
2024-06-03 11:28:29 [INFO]: Epoch 033 - training loss: 0.4379, validation loss: 0.3270
2024-06-03 11:28:34 [INFO]: Epoch 034 - training loss: 0.4241, validation loss: 0.3248
2024-06-03 11:28:39 [INFO]: Epoch 035 - training loss: 0.4241, validation loss: 0.3235
2024-06-03 11:28:44 [INFO]: Epoch 036 - training loss: 0.4197, validation loss: 0.3209
2024-06-03 11:28:48 [INFO]: Epoch 037 - training loss: 0.4184, validation loss: 0.3146
2024-06-03 11:28:53 [INFO]: Epoch 038 - training loss: 0.4142, validation loss: 0.3168
2024-06-03 11:28:58 [INFO]: Epoch 039 - training loss: 0.4073, validation loss: 0.3115
2024-06-03 11:29:03 [INFO]: Epoch 040 - training loss: 0.4048, validation loss: 0.3097
2024-06-03 11:29:07 [INFO]: Epoch 041 - training loss: 0.4039, validation loss: 0.3103
2024-06-03 11:29:12 [INFO]: Epoch 042 - training loss: 0.4035, validation loss: 0.3065
2024-06-03 11:29:17 [INFO]: Epoch 043 - training loss: 0.4007, validation loss: 0.3106
2024-06-03 11:29:21 [INFO]: Epoch 044 - training loss: 0.4031, validation loss: 0.3096
2024-06-03 11:29:26 [INFO]: Epoch 045 - training loss: 0.3991, validation loss: 0.3120
2024-06-03 11:29:30 [INFO]: Epoch 046 - training loss: 0.3995, validation loss: 0.3109
2024-06-03 11:29:34 [INFO]: Epoch 047 - training loss: 0.3995, validation loss: 0.3049
2024-06-03 11:29:39 [INFO]: Epoch 048 - training loss: 0.3995, validation loss: 0.3061
2024-06-03 11:29:43 [INFO]: Epoch 049 - training loss: 0.3926, validation loss: 0.3105
2024-06-03 11:29:47 [INFO]: Epoch 050 - training loss: 0.3906, validation loss: 0.3123
2024-06-03 11:29:52 [INFO]: Epoch 051 - training loss: 0.4011, validation loss: 0.3055
2024-06-03 11:29:57 [INFO]: Epoch 052 - training loss: 0.3972, validation loss: 0.3102
2024-06-03 11:30:01 [INFO]: Epoch 053 - training loss: 0.3902, validation loss: 0.3056
2024-06-03 11:30:06 [INFO]: Epoch 054 - training loss: 0.3858, validation loss: 0.3044
2024-06-03 11:30:10 [INFO]: Epoch 055 - training loss: 0.3847, validation loss: 0.3015
2024-06-03 11:30:14 [INFO]: Epoch 056 - training loss: 0.3885, validation loss: 0.3096
2024-06-03 11:30:18 [INFO]: Epoch 057 - training loss: 0.3929, validation loss: 0.3016
2024-06-03 11:30:23 [INFO]: Epoch 058 - training loss: 0.3813, validation loss: 0.3033
2024-06-03 11:30:27 [INFO]: Epoch 059 - training loss: 0.3857, validation loss: 0.3063
2024-06-03 11:30:32 [INFO]: Epoch 060 - training loss: 0.3841, validation loss: 0.3033
2024-06-03 11:30:36 [INFO]: Epoch 061 - training loss: 0.3813, validation loss: 0.3133
2024-06-03 11:30:40 [INFO]: Epoch 062 - training loss: 0.3801, validation loss: 0.2992
2024-06-03 11:30:45 [INFO]: Epoch 063 - training loss: 0.3765, validation loss: 0.2989
2024-06-03 11:30:49 [INFO]: Epoch 064 - training loss: 0.3787, validation loss: 0.3047
2024-06-03 11:30:54 [INFO]: Epoch 065 - training loss: 0.3722, validation loss: 0.3020
2024-06-03 11:30:58 [INFO]: Epoch 066 - training loss: 0.3712, validation loss: 0.3006
2024-06-03 11:31:02 [INFO]: Epoch 067 - training loss: 0.3691, validation loss: 0.3004
2024-06-03 11:31:06 [INFO]: Epoch 068 - training loss: 0.3650, validation loss: 0.2966
2024-06-03 11:31:10 [INFO]: Epoch 069 - training loss: 0.3698, validation loss: 0.2975
2024-06-03 11:31:15 [INFO]: Epoch 070 - training loss: 0.3648, validation loss: 0.2953
2024-06-03 11:31:19 [INFO]: Epoch 071 - training loss: 0.3650, validation loss: 0.2962
2024-06-03 11:31:23 [INFO]: Epoch 072 - training loss: 0.3649, validation loss: 0.2953
2024-06-03 11:31:27 [INFO]: Epoch 073 - training loss: 0.3681, validation loss: 0.2944
2024-06-03 11:31:32 [INFO]: Epoch 074 - training loss: 0.3602, validation loss: 0.2966
2024-06-03 11:31:36 [INFO]: Epoch 075 - training loss: 0.3604, validation loss: 0.2967
2024-06-03 11:31:41 [INFO]: Epoch 076 - training loss: 0.3635, validation loss: 0.2972
2024-06-03 11:31:45 [INFO]: Epoch 077 - training loss: 0.3633, validation loss: 0.2939
2024-06-03 11:31:50 [INFO]: Epoch 078 - training loss: 0.3613, validation loss: 0.2935
2024-06-03 11:31:54 [INFO]: Epoch 079 - training loss: 0.3595, validation loss: 0.2957
2024-06-03 11:31:59 [INFO]: Epoch 080 - training loss: 0.3634, validation loss: 0.2949
2024-06-03 11:32:03 [INFO]: Epoch 081 - training loss: 0.3591, validation loss: 0.2952
2024-06-03 11:32:07 [INFO]: Epoch 082 - training loss: 0.3511, validation loss: 0.2923
2024-06-03 11:32:12 [INFO]: Epoch 083 - training loss: 0.3560, validation loss: 0.2966
2024-06-03 11:32:16 [INFO]: Epoch 084 - training loss: 0.3542, validation loss: 0.2920
2024-06-03 11:32:21 [INFO]: Epoch 085 - training loss: 0.3546, validation loss: 0.2921
2024-06-03 11:32:25 [INFO]: Epoch 086 - training loss: 0.3499, validation loss: 0.2895
2024-06-03 11:32:30 [INFO]: Epoch 087 - training loss: 0.3484, validation loss: 0.2937
2024-06-03 11:32:34 [INFO]: Epoch 088 - training loss: 0.3527, validation loss: 0.2951
2024-06-03 11:32:39 [INFO]: Epoch 089 - training loss: 0.3504, validation loss: 0.2915
2024-06-03 11:32:43 [INFO]: Epoch 090 - training loss: 0.3513, validation loss: 0.2931
2024-06-03 11:32:47 [INFO]: Epoch 091 - training loss: 0.3494, validation loss: 0.2918
2024-06-03 11:32:51 [INFO]: Epoch 092 - training loss: 0.3488, validation loss: 0.2922
2024-06-03 11:32:56 [INFO]: Epoch 093 - training loss: 0.3452, validation loss: 0.2905
2024-06-03 11:33:00 [INFO]: Epoch 094 - training loss: 0.3493, validation loss: 0.2930
2024-06-03 11:33:04 [INFO]: Epoch 095 - training loss: 0.3442, validation loss: 0.2945
2024-06-03 11:33:08 [INFO]: Epoch 096 - training loss: 0.3454, validation loss: 0.2893
2024-06-03 11:33:13 [INFO]: Epoch 097 - training loss: 0.3473, validation loss: 0.2932
2024-06-03 11:33:17 [INFO]: Epoch 098 - training loss: 0.3475, validation loss: 0.2936
2024-06-03 11:33:21 [INFO]: Epoch 099 - training loss: 0.3475, validation loss: 0.2945
2024-06-03 11:33:26 [INFO]: Epoch 100 - training loss: 0.3448, validation loss: 0.2948
2024-06-03 11:33:26 [INFO]: Finished training. The best model is from epoch#96.
2024-06-03 11:33:26 [INFO]: Saved the model to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_2/20240603_T112553/SCINet.pypots
2024-06-03 11:33:28 [INFO]: Successfully saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_2/imputation.pkl
2024-06-03 11:33:28 [INFO]: Round2 - SCINet on BeijingAir: MAE=0.2556, MSE=0.3111, MRE=0.3480
2024-06-03 11:33:28 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 11:33:28 [INFO]: Using the given device: cuda:0
2024-06-03 11:33:28 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_3/20240603_T113328
2024-06-03 11:33:28 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_3/20240603_T113328/tensorboard
2024-06-03 11:33:28 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 11:33:33 [INFO]: Epoch 001 - training loss: 1.3237, validation loss: 0.6995
2024-06-03 11:33:37 [INFO]: Epoch 002 - training loss: 0.9391, validation loss: 0.5310
2024-06-03 11:33:42 [INFO]: Epoch 003 - training loss: 0.7887, validation loss: 0.4346
2024-06-03 11:33:46 [INFO]: Epoch 004 - training loss: 0.6825, validation loss: 0.3738
2024-06-03 11:33:50 [INFO]: Epoch 005 - training loss: 0.6091, validation loss: 0.3414
2024-06-03 11:33:55 [INFO]: Epoch 006 - training loss: 0.5629, validation loss: 0.3288
2024-06-03 11:34:00 [INFO]: Epoch 007 - training loss: 0.5352, validation loss: 0.3172
2024-06-03 11:34:04 [INFO]: Epoch 008 - training loss: 0.5061, validation loss: 0.2995
2024-06-03 11:34:09 [INFO]: Epoch 009 - training loss: 0.4843, validation loss: 0.2936
2024-06-03 11:34:13 [INFO]: Epoch 010 - training loss: 0.4718, validation loss: 0.2855
2024-06-03 11:34:17 [INFO]: Epoch 011 - training loss: 0.4562, validation loss: 0.2819
2024-06-03 11:34:21 [INFO]: Epoch 012 - training loss: 0.4506, validation loss: 0.2701
2024-06-03 11:34:26 [INFO]: Epoch 013 - training loss: 0.4406, validation loss: 0.2727
2024-06-03 11:34:30 [INFO]: Epoch 014 - training loss: 0.4332, validation loss: 0.2669
2024-06-03 11:34:34 [INFO]: Epoch 015 - training loss: 0.4238, validation loss: 0.2627
2024-06-03 11:34:39 [INFO]: Epoch 016 - training loss: 0.4193, validation loss: 0.2586
2024-06-03 11:34:43 [INFO]: Epoch 017 - training loss: 0.4144, validation loss: 0.2548
2024-06-03 11:34:47 [INFO]: Epoch 018 - training loss: 0.4110, validation loss: 0.2554
2024-06-03 11:34:51 [INFO]: Epoch 019 - training loss: 0.4035, validation loss: 0.2506
2024-06-03 11:34:56 [INFO]: Epoch 020 - training loss: 0.4054, validation loss: 0.2510
2024-06-03 11:35:00 [INFO]: Epoch 021 - training loss: 0.4005, validation loss: 0.2547
2024-06-03 11:35:05 [INFO]: Epoch 022 - training loss: 0.3961, validation loss: 0.2432
2024-06-03 11:35:09 [INFO]: Epoch 023 - training loss: 0.3924, validation loss: 0.2456
2024-06-03 11:35:14 [INFO]: Epoch 024 - training loss: 0.3935, validation loss: 0.2396
2024-06-03 11:35:18 [INFO]: Epoch 025 - training loss: 0.3900, validation loss: 0.2441
2024-06-03 11:35:22 [INFO]: Epoch 026 - training loss: 0.3847, validation loss: 0.2392
2024-06-03 11:35:27 [INFO]: Epoch 027 - training loss: 0.3863, validation loss: 0.2399
2024-06-03 11:35:31 [INFO]: Epoch 028 - training loss: 0.3850, validation loss: 0.2493
2024-06-03 11:35:35 [INFO]: Epoch 029 - training loss: 0.3839, validation loss: 0.2370
2024-06-03 11:35:39 [INFO]: Epoch 030 - training loss: 0.3790, validation loss: 0.2363
2024-06-03 11:35:44 [INFO]: Epoch 031 - training loss: 0.3750, validation loss: 0.2335
2024-06-03 11:35:48 [INFO]: Epoch 032 - training loss: 0.3760, validation loss: 0.2386
2024-06-03 11:35:52 [INFO]: Epoch 033 - training loss: 0.3723, validation loss: 0.2360
2024-06-03 11:35:57 [INFO]: Epoch 034 - training loss: 0.3727, validation loss: 0.2375
2024-06-03 11:36:01 [INFO]: Epoch 035 - training loss: 0.3714, validation loss: 0.2337
2024-06-03 11:36:06 [INFO]: Epoch 036 - training loss: 0.3737, validation loss: 0.2382
2024-06-03 11:36:11 [INFO]: Epoch 037 - training loss: 0.3730, validation loss: 0.2362
2024-06-03 11:36:15 [INFO]: Epoch 038 - training loss: 0.3690, validation loss: 0.2321
2024-06-03 11:36:19 [INFO]: Epoch 039 - training loss: 0.3691, validation loss: 0.2373
2024-06-03 11:36:23 [INFO]: Epoch 040 - training loss: 0.3655, validation loss: 0.2295
2024-06-03 11:36:27 [INFO]: Epoch 041 - training loss: 0.3642, validation loss: 0.2350
2024-06-03 11:36:30 [INFO]: Epoch 042 - training loss: 0.3650, validation loss: 0.2330
2024-06-03 11:36:33 [INFO]: Epoch 043 - training loss: 0.3649, validation loss: 0.2310
2024-06-03 11:36:38 [INFO]: Epoch 044 - training loss: 0.3666, validation loss: 0.2327
2024-06-03 11:36:42 [INFO]: Epoch 045 - training loss: 0.3624, validation loss: 0.2294
2024-06-03 11:36:46 [INFO]: Epoch 046 - training loss: 0.3626, validation loss: 0.2309
2024-06-03 11:36:51 [INFO]: Epoch 047 - training loss: 0.3709, validation loss: 0.2326
2024-06-03 11:36:55 [INFO]: Epoch 048 - training loss: 0.3610, validation loss: 0.2303
2024-06-03 11:36:59 [INFO]: Epoch 049 - training loss: 0.3561, validation loss: 0.2279
2024-06-03 11:37:04 [INFO]: Epoch 050 - training loss: 0.3589, validation loss: 0.2315
2024-06-03 11:37:08 [INFO]: Epoch 051 - training loss: 0.3556, validation loss: 0.2357
2024-06-03 11:37:13 [INFO]: Epoch 052 - training loss: 0.3528, validation loss: 0.2314
2024-06-03 11:37:17 [INFO]: Epoch 053 - training loss: 0.3536, validation loss: 0.2330
2024-06-03 11:37:21 [INFO]: Epoch 054 - training loss: 0.3525, validation loss: 0.2338
2024-06-03 11:37:26 [INFO]: Epoch 055 - training loss: 0.3539, validation loss: 0.2321
2024-06-03 11:37:30 [INFO]: Epoch 056 - training loss: 0.3513, validation loss: 0.2326
2024-06-03 11:37:35 [INFO]: Epoch 057 - training loss: 0.3507, validation loss: 0.2352
2024-06-03 11:37:39 [INFO]: Epoch 058 - training loss: 0.3502, validation loss: 0.2289
2024-06-03 11:37:43 [INFO]: Epoch 059 - training loss: 0.3486, validation loss: 0.2308
2024-06-03 11:37:43 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:37:43 [INFO]: Finished training. The best model is from epoch#49.
2024-06-03 11:37:44 [INFO]: Saved the model to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_3/20240603_T113328/SCINet.pypots
2024-06-03 11:37:45 [INFO]: Successfully saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_3/imputation.pkl
2024-06-03 11:37:45 [INFO]: Round3 - SCINet on BeijingAir: MAE=0.2218, MSE=0.2210, MRE=0.3019
2024-06-03 11:37:45 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 11:37:45 [INFO]: Using the given device: cuda:0
2024-06-03 11:37:45 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_4/20240603_T113745
2024-06-03 11:37:45 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_4/20240603_T113745/tensorboard
2024-06-03 11:37:46 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 11:37:51 [INFO]: Epoch 001 - training loss: 1.2835, validation loss: 0.6275
2024-06-03 11:37:55 [INFO]: Epoch 002 - training loss: 0.8739, validation loss: 0.4641
2024-06-03 11:37:59 [INFO]: Epoch 003 - training loss: 0.7306, validation loss: 0.3995
2024-06-03 11:38:03 [INFO]: Epoch 004 - training loss: 0.6490, validation loss: 0.3652
2024-06-03 11:38:07 [INFO]: Epoch 005 - training loss: 0.5954, validation loss: 0.3405
2024-06-03 11:38:11 [INFO]: Epoch 006 - training loss: 0.5582, validation loss: 0.3260
2024-06-03 11:38:16 [INFO]: Epoch 007 - training loss: 0.5271, validation loss: 0.3147
2024-06-03 11:38:20 [INFO]: Epoch 008 - training loss: 0.5047, validation loss: 0.3053
2024-06-03 11:38:24 [INFO]: Epoch 009 - training loss: 0.4930, validation loss: 0.3013
2024-06-03 11:38:28 [INFO]: Epoch 010 - training loss: 0.4774, validation loss: 0.2889
2024-06-03 11:38:31 [INFO]: Epoch 011 - training loss: 0.4621, validation loss: 0.2812
2024-06-03 11:38:36 [INFO]: Epoch 012 - training loss: 0.4490, validation loss: 0.2744
2024-06-03 11:38:40 [INFO]: Epoch 013 - training loss: 0.4418, validation loss: 0.2712
2024-06-03 11:38:44 [INFO]: Epoch 014 - training loss: 0.4350, validation loss: 0.2725
2024-06-03 11:38:48 [INFO]: Epoch 015 - training loss: 0.4324, validation loss: 0.2661
2024-06-03 11:38:52 [INFO]: Epoch 016 - training loss: 0.4226, validation loss: 0.2618
2024-06-03 11:38:56 [INFO]: Epoch 017 - training loss: 0.4159, validation loss: 0.2590
2024-06-03 11:39:00 [INFO]: Epoch 018 - training loss: 0.4178, validation loss: 0.2629
2024-06-03 11:39:04 [INFO]: Epoch 019 - training loss: 0.4079, validation loss: 0.2510
2024-06-03 11:39:09 [INFO]: Epoch 020 - training loss: 0.4019, validation loss: 0.2514
2024-06-03 11:39:13 [INFO]: Epoch 021 - training loss: 0.4020, validation loss: 0.2487
2024-06-03 11:39:17 [INFO]: Epoch 022 - training loss: 0.4000, validation loss: 0.2462
2024-06-03 11:39:21 [INFO]: Epoch 023 - training loss: 0.3926, validation loss: 0.2465
2024-06-03 11:39:25 [INFO]: Epoch 024 - training loss: 0.3951, validation loss: 0.2458
2024-06-03 11:39:29 [INFO]: Epoch 025 - training loss: 0.3904, validation loss: 0.2470
2024-06-03 11:39:33 [INFO]: Epoch 026 - training loss: 0.3860, validation loss: 0.2421
2024-06-03 11:39:37 [INFO]: Epoch 027 - training loss: 0.3860, validation loss: 0.2437
2024-06-03 11:39:41 [INFO]: Epoch 028 - training loss: 0.3835, validation loss: 0.2418
2024-06-03 11:39:45 [INFO]: Epoch 029 - training loss: 0.3820, validation loss: 0.2405
2024-06-03 11:39:49 [INFO]: Epoch 030 - training loss: 0.3880, validation loss: 0.2434
2024-06-03 11:39:53 [INFO]: Epoch 031 - training loss: 0.3805, validation loss: 0.2396
2024-06-03 11:39:57 [INFO]: Epoch 032 - training loss: 0.3745, validation loss: 0.2356
2024-06-03 11:40:01 [INFO]: Epoch 033 - training loss: 0.3750, validation loss: 0.2387
2024-06-03 11:40:05 [INFO]: Epoch 034 - training loss: 0.3746, validation loss: 0.2365
2024-06-03 11:40:09 [INFO]: Epoch 035 - training loss: 0.3723, validation loss: 0.2363
2024-06-03 11:40:13 [INFO]: Epoch 036 - training loss: 0.3726, validation loss: 0.2375
2024-06-03 11:40:17 [INFO]: Epoch 037 - training loss: 0.3732, validation loss: 0.2334
2024-06-03 11:40:20 [INFO]: Epoch 038 - training loss: 0.3655, validation loss: 0.2358
2024-06-03 11:40:24 [INFO]: Epoch 039 - training loss: 0.3649, validation loss: 0.2365
2024-06-03 11:40:27 [INFO]: Epoch 040 - training loss: 0.3688, validation loss: 0.2394
2024-06-03 11:40:31 [INFO]: Epoch 041 - training loss: 0.3631, validation loss: 0.2315
2024-06-03 11:40:35 [INFO]: Epoch 042 - training loss: 0.3614, validation loss: 0.2327
2024-06-03 11:40:39 [INFO]: Epoch 043 - training loss: 0.3615, validation loss: 0.2326
2024-06-03 11:40:43 [INFO]: Epoch 044 - training loss: 0.3626, validation loss: 0.2345
2024-06-03 11:40:47 [INFO]: Epoch 045 - training loss: 0.3590, validation loss: 0.2359
2024-06-03 11:40:51 [INFO]: Epoch 046 - training loss: 0.3620, validation loss: 0.2304
2024-06-03 11:40:55 [INFO]: Epoch 047 - training loss: 0.3581, validation loss: 0.2323
2024-06-03 11:40:59 [INFO]: Epoch 048 - training loss: 0.3581, validation loss: 0.2336
2024-06-03 11:41:03 [INFO]: Epoch 049 - training loss: 0.3558, validation loss: 0.2321
2024-06-03 11:41:07 [INFO]: Epoch 050 - training loss: 0.3548, validation loss: 0.2374
2024-06-03 11:41:11 [INFO]: Epoch 051 - training loss: 0.3553, validation loss: 0.2314
2024-06-03 11:41:15 [INFO]: Epoch 052 - training loss: 0.3548, validation loss: 0.2346
2024-06-03 11:41:19 [INFO]: Epoch 053 - training loss: 0.3525, validation loss: 0.2323
2024-06-03 11:41:23 [INFO]: Epoch 054 - training loss: 0.3563, validation loss: 0.2312
2024-06-03 11:41:27 [INFO]: Epoch 055 - training loss: 0.3537, validation loss: 0.2359
2024-06-03 11:41:31 [INFO]: Epoch 056 - training loss: 0.3535, validation loss: 0.2349
2024-06-03 11:41:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 11:41:31 [INFO]: Finished training. The best model is from epoch#46.
2024-06-03 11:41:31 [INFO]: Saved the model to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_4/20240603_T113745/SCINet.pypots
2024-06-03 11:41:33 [INFO]: Successfully saved to results_point_rate05/BeijingAir/SCINet_BeijingAir/round_4/imputation.pkl
2024-06-03 11:41:33 [INFO]: Round4 - SCINet on BeijingAir: MAE=0.2295, MSE=0.2246, MRE=0.3124
2024-06-03 11:41:33 [INFO]: Done! Final results:
Averaged SCINet (26,833,140 params) on BeijingAir: MAE=0.2219 ± 0.012251746303065724, MSE=0.2303 ± 0.03587529314880369, MRE=0.2942 ± 0.016238862922685265, average inference time=0.31