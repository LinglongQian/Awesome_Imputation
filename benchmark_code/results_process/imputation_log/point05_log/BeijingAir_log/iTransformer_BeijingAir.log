2024-06-03 05:54:03 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 05:54:03 [INFO]: Using the given device: cuda:0
2024-06-03 05:54:03 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_0/20240603_T055403
2024-06-03 05:54:03 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_0/20240603_T055403/tensorboard
2024-06-03 05:54:04 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 8,286,232
2024-06-03 05:54:17 [INFO]: Epoch 001 - training loss: 0.8959, validation loss: 0.3715
2024-06-03 05:54:24 [INFO]: Epoch 002 - training loss: 0.5532, validation loss: 0.3350
2024-06-03 05:54:31 [INFO]: Epoch 003 - training loss: 0.5084, validation loss: 0.3366
2024-06-03 05:54:38 [INFO]: Epoch 004 - training loss: 0.4931, validation loss: 0.3102
2024-06-03 05:54:45 [INFO]: Epoch 005 - training loss: 0.4753, validation loss: 0.3072
2024-06-03 05:54:53 [INFO]: Epoch 006 - training loss: 0.4808, validation loss: 0.3028
2024-06-03 05:54:59 [INFO]: Epoch 007 - training loss: 0.4548, validation loss: 0.2941
2024-06-03 05:55:06 [INFO]: Epoch 008 - training loss: 0.4422, validation loss: 0.2876
2024-06-03 05:55:13 [INFO]: Epoch 009 - training loss: 0.4538, validation loss: 0.2926
2024-06-03 05:55:20 [INFO]: Epoch 010 - training loss: 0.4332, validation loss: 0.2834
2024-06-03 05:55:28 [INFO]: Epoch 011 - training loss: 0.4201, validation loss: 0.2791
2024-06-03 05:55:35 [INFO]: Epoch 012 - training loss: 0.4123, validation loss: 0.2706
2024-06-03 05:55:41 [INFO]: Epoch 013 - training loss: 0.4059, validation loss: 0.2698
2024-06-03 05:55:48 [INFO]: Epoch 014 - training loss: 0.3986, validation loss: 0.2659
2024-06-03 05:55:55 [INFO]: Epoch 015 - training loss: 0.3948, validation loss: 0.2632
2024-06-03 05:56:02 [INFO]: Epoch 016 - training loss: 0.3875, validation loss: 0.2603
2024-06-03 05:56:09 [INFO]: Epoch 017 - training loss: 0.3796, validation loss: 0.2587
2024-06-03 05:56:16 [INFO]: Epoch 018 - training loss: 0.3706, validation loss: 0.2555
2024-06-03 05:56:23 [INFO]: Epoch 019 - training loss: 0.3628, validation loss: 0.2546
2024-06-03 05:56:29 [INFO]: Epoch 020 - training loss: 0.3543, validation loss: 0.2531
2024-06-03 05:56:36 [INFO]: Epoch 021 - training loss: 0.3520, validation loss: 0.2484
2024-06-03 05:56:43 [INFO]: Epoch 022 - training loss: 0.3502, validation loss: 0.2467
2024-06-03 05:56:50 [INFO]: Epoch 023 - training loss: 0.3491, validation loss: 0.2492
2024-06-03 05:56:57 [INFO]: Epoch 024 - training loss: 0.3394, validation loss: 0.2465
2024-06-03 05:57:04 [INFO]: Epoch 025 - training loss: 0.3349, validation loss: 0.2429
2024-06-03 05:57:11 [INFO]: Epoch 026 - training loss: 0.3352, validation loss: 0.2458
2024-06-03 05:57:18 [INFO]: Epoch 027 - training loss: 0.3268, validation loss: 0.2436
2024-06-03 05:57:25 [INFO]: Epoch 028 - training loss: 0.3260, validation loss: 0.2399
2024-06-03 05:57:32 [INFO]: Epoch 029 - training loss: 0.3167, validation loss: 0.2404
2024-06-03 05:57:39 [INFO]: Epoch 030 - training loss: 0.3190, validation loss: 0.2394
2024-06-03 05:57:46 [INFO]: Epoch 031 - training loss: 0.3167, validation loss: 0.2372
2024-06-03 05:57:53 [INFO]: Epoch 032 - training loss: 0.3100, validation loss: 0.2383
2024-06-03 05:58:00 [INFO]: Epoch 033 - training loss: 0.3299, validation loss: 0.2490
2024-06-03 05:58:07 [INFO]: Epoch 034 - training loss: 0.3219, validation loss: 0.2371
2024-06-03 05:58:14 [INFO]: Epoch 035 - training loss: 0.3012, validation loss: 0.2343
2024-06-03 05:58:21 [INFO]: Epoch 036 - training loss: 0.2961, validation loss: 0.2362
2024-06-03 05:58:27 [INFO]: Epoch 037 - training loss: 0.3029, validation loss: 0.2373
2024-06-03 05:58:34 [INFO]: Epoch 038 - training loss: 0.3011, validation loss: 0.2383
2024-06-03 05:58:41 [INFO]: Epoch 039 - training loss: 0.2998, validation loss: 0.2359
2024-06-03 05:58:48 [INFO]: Epoch 040 - training loss: 0.2894, validation loss: 0.2332
2024-06-03 05:58:55 [INFO]: Epoch 041 - training loss: 0.2869, validation loss: 0.2311
2024-06-03 05:59:02 [INFO]: Epoch 042 - training loss: 0.2835, validation loss: 0.2328
2024-06-03 05:59:09 [INFO]: Epoch 043 - training loss: 0.2826, validation loss: 0.2315
2024-06-03 05:59:16 [INFO]: Epoch 044 - training loss: 0.2842, validation loss: 0.2323
2024-06-03 05:59:23 [INFO]: Epoch 045 - training loss: 0.2871, validation loss: 0.2329
2024-06-03 05:59:30 [INFO]: Epoch 046 - training loss: 0.2844, validation loss: 0.2319
2024-06-03 05:59:37 [INFO]: Epoch 047 - training loss: 0.2764, validation loss: 0.2303
2024-06-03 05:59:44 [INFO]: Epoch 048 - training loss: 0.2762, validation loss: 0.2304
2024-06-03 05:59:51 [INFO]: Epoch 049 - training loss: 0.2725, validation loss: 0.2278
2024-06-03 05:59:58 [INFO]: Epoch 050 - training loss: 0.2746, validation loss: 0.2263
2024-06-03 06:00:05 [INFO]: Epoch 051 - training loss: 0.2718, validation loss: 0.2267
2024-06-03 06:00:12 [INFO]: Epoch 052 - training loss: 0.2685, validation loss: 0.2318
2024-06-03 06:00:18 [INFO]: Epoch 053 - training loss: 0.2724, validation loss: 0.2294
2024-06-03 06:00:25 [INFO]: Epoch 054 - training loss: 0.2726, validation loss: 0.2287
2024-06-03 06:00:33 [INFO]: Epoch 055 - training loss: 0.2695, validation loss: 0.2258
2024-06-03 06:00:40 [INFO]: Epoch 056 - training loss: 0.2640, validation loss: 0.2250
2024-06-03 06:00:46 [INFO]: Epoch 057 - training loss: 0.2593, validation loss: 0.2258
2024-06-03 06:00:53 [INFO]: Epoch 058 - training loss: 0.2633, validation loss: 0.2259
2024-06-03 06:01:00 [INFO]: Epoch 059 - training loss: 0.2662, validation loss: 0.2252
2024-06-03 06:01:07 [INFO]: Epoch 060 - training loss: 0.2621, validation loss: 0.2225
2024-06-03 06:01:14 [INFO]: Epoch 061 - training loss: 0.2575, validation loss: 0.2258
2024-06-03 06:01:21 [INFO]: Epoch 062 - training loss: 0.2608, validation loss: 0.2257
2024-06-03 06:01:29 [INFO]: Epoch 063 - training loss: 0.2601, validation loss: 0.2275
2024-06-03 06:01:36 [INFO]: Epoch 064 - training loss: 0.2559, validation loss: 0.2231
2024-06-03 06:01:43 [INFO]: Epoch 065 - training loss: 0.2540, validation loss: 0.2241
2024-06-03 06:01:50 [INFO]: Epoch 066 - training loss: 0.2491, validation loss: 0.2231
2024-06-03 06:01:56 [INFO]: Epoch 067 - training loss: 0.2534, validation loss: 0.2224
2024-06-03 06:02:03 [INFO]: Epoch 068 - training loss: 0.2518, validation loss: 0.2260
2024-06-03 06:02:10 [INFO]: Epoch 069 - training loss: 0.2470, validation loss: 0.2205
2024-06-03 06:02:17 [INFO]: Epoch 070 - training loss: 0.2578, validation loss: 0.2239
2024-06-03 06:02:24 [INFO]: Epoch 071 - training loss: 0.2530, validation loss: 0.2238
2024-06-03 06:02:31 [INFO]: Epoch 072 - training loss: 0.2495, validation loss: 0.2236
2024-06-03 06:02:38 [INFO]: Epoch 073 - training loss: 0.2415, validation loss: 0.2253
2024-06-03 06:02:45 [INFO]: Epoch 074 - training loss: 0.2463, validation loss: 0.2221
2024-06-03 06:02:52 [INFO]: Epoch 075 - training loss: 0.2429, validation loss: 0.2197
2024-06-03 06:02:59 [INFO]: Epoch 076 - training loss: 0.2445, validation loss: 0.2212
2024-06-03 06:03:06 [INFO]: Epoch 077 - training loss: 0.2436, validation loss: 0.2217
2024-06-03 06:03:13 [INFO]: Epoch 078 - training loss: 0.2420, validation loss: 0.2244
2024-06-03 06:03:20 [INFO]: Epoch 079 - training loss: 0.2472, validation loss: 0.2223
2024-06-03 06:03:27 [INFO]: Epoch 080 - training loss: 0.2452, validation loss: 0.2215
2024-06-03 06:03:34 [INFO]: Epoch 081 - training loss: 0.2429, validation loss: 0.2198
2024-06-03 06:03:41 [INFO]: Epoch 082 - training loss: 0.2376, validation loss: 0.2195
2024-06-03 06:03:48 [INFO]: Epoch 083 - training loss: 0.2369, validation loss: 0.2187
2024-06-03 06:03:55 [INFO]: Epoch 084 - training loss: 0.2354, validation loss: 0.2193
2024-06-03 06:04:02 [INFO]: Epoch 085 - training loss: 0.2371, validation loss: 0.2229
2024-06-03 06:04:09 [INFO]: Epoch 086 - training loss: 0.2566, validation loss: 0.2219
2024-06-03 06:04:16 [INFO]: Epoch 087 - training loss: 0.2393, validation loss: 0.2190
2024-06-03 06:04:23 [INFO]: Epoch 088 - training loss: 0.2343, validation loss: 0.2209
2024-06-03 06:04:30 [INFO]: Epoch 089 - training loss: 0.2363, validation loss: 0.2189
2024-06-03 06:04:36 [INFO]: Epoch 090 - training loss: 0.2324, validation loss: 0.2211
2024-06-03 06:04:43 [INFO]: Epoch 091 - training loss: 0.2342, validation loss: 0.2187
2024-06-03 06:04:50 [INFO]: Epoch 092 - training loss: 0.2290, validation loss: 0.2179
2024-06-03 06:04:57 [INFO]: Epoch 093 - training loss: 0.2310, validation loss: 0.2190
2024-06-03 06:05:04 [INFO]: Epoch 094 - training loss: 0.2334, validation loss: 0.2180
2024-06-03 06:05:11 [INFO]: Epoch 095 - training loss: 0.2279, validation loss: 0.2178
2024-06-03 06:05:18 [INFO]: Epoch 096 - training loss: 0.2268, validation loss: 0.2168
2024-06-03 06:05:25 [INFO]: Epoch 097 - training loss: 0.2303, validation loss: 0.2182
2024-06-03 06:05:31 [INFO]: Epoch 098 - training loss: 0.2351, validation loss: 0.2175
2024-06-03 06:05:37 [INFO]: Epoch 099 - training loss: 0.2336, validation loss: 0.2177
2024-06-03 06:05:44 [INFO]: Epoch 100 - training loss: 0.2314, validation loss: 0.2173
2024-06-03 06:05:44 [INFO]: Finished training. The best model is from epoch#96.
2024-06-03 06:05:44 [INFO]: Saved the model to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_0/20240603_T055403/iTransformer.pypots
2024-06-03 06:05:46 [INFO]: Successfully saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_0/imputation.pkl
2024-06-03 06:05:46 [INFO]: Round0 - iTransformer on BeijingAir: MAE=0.1667, MSE=0.2401, MRE=0.2270
2024-06-03 06:05:46 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 06:05:46 [INFO]: Using the given device: cuda:0
2024-06-03 06:05:46 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_1/20240603_T060546
2024-06-03 06:05:46 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_1/20240603_T060546/tensorboard
2024-06-03 06:05:46 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 8,286,232
2024-06-03 06:05:52 [INFO]: Epoch 001 - training loss: 0.8968, validation loss: 0.3715
2024-06-03 06:05:58 [INFO]: Epoch 002 - training loss: 0.5489, validation loss: 0.3402
2024-06-03 06:06:04 [INFO]: Epoch 003 - training loss: 0.5114, validation loss: 0.3383
2024-06-03 06:06:10 [INFO]: Epoch 004 - training loss: 0.4871, validation loss: 0.3197
2024-06-03 06:06:16 [INFO]: Epoch 005 - training loss: 0.4741, validation loss: 0.3053
2024-06-03 06:06:22 [INFO]: Epoch 006 - training loss: 0.4681, validation loss: 0.2995
2024-06-03 06:06:29 [INFO]: Epoch 007 - training loss: 0.4523, validation loss: 0.2931
2024-06-03 06:06:35 [INFO]: Epoch 008 - training loss: 0.4528, validation loss: 0.2921
2024-06-03 06:06:41 [INFO]: Epoch 009 - training loss: 0.4347, validation loss: 0.2834
2024-06-03 06:06:47 [INFO]: Epoch 010 - training loss: 0.4267, validation loss: 0.2836
2024-06-03 06:06:53 [INFO]: Epoch 011 - training loss: 0.4215, validation loss: 0.2797
2024-06-03 06:06:59 [INFO]: Epoch 012 - training loss: 0.4134, validation loss: 0.2763
2024-06-03 06:07:06 [INFO]: Epoch 013 - training loss: 0.4106, validation loss: 0.2708
2024-06-03 06:07:12 [INFO]: Epoch 014 - training loss: 0.4011, validation loss: 0.2679
2024-06-03 06:07:18 [INFO]: Epoch 015 - training loss: 0.3960, validation loss: 0.2610
2024-06-03 06:07:24 [INFO]: Epoch 016 - training loss: 0.3813, validation loss: 0.2590
2024-06-03 06:07:30 [INFO]: Epoch 017 - training loss: 0.3772, validation loss: 0.2595
2024-06-03 06:07:36 [INFO]: Epoch 018 - training loss: 0.3709, validation loss: 0.2549
2024-06-03 06:07:43 [INFO]: Epoch 019 - training loss: 0.3599, validation loss: 0.2532
2024-06-03 06:07:49 [INFO]: Epoch 020 - training loss: 0.3586, validation loss: 0.2508
2024-06-03 06:07:55 [INFO]: Epoch 021 - training loss: 0.3466, validation loss: 0.2537
2024-06-03 06:08:02 [INFO]: Epoch 022 - training loss: 0.3487, validation loss: 0.2482
2024-06-03 06:08:08 [INFO]: Epoch 023 - training loss: 0.3497, validation loss: 0.2573
2024-06-03 06:08:14 [INFO]: Epoch 024 - training loss: 0.3542, validation loss: 0.2450
2024-06-03 06:08:20 [INFO]: Epoch 025 - training loss: 0.3324, validation loss: 0.2442
2024-06-03 06:08:27 [INFO]: Epoch 026 - training loss: 0.3297, validation loss: 0.2420
2024-06-03 06:08:33 [INFO]: Epoch 027 - training loss: 0.3241, validation loss: 0.2403
2024-06-03 06:08:39 [INFO]: Epoch 028 - training loss: 0.3242, validation loss: 0.2391
2024-06-03 06:08:45 [INFO]: Epoch 029 - training loss: 0.3146, validation loss: 0.2411
2024-06-03 06:08:51 [INFO]: Epoch 030 - training loss: 0.3136, validation loss: 0.2354
2024-06-03 06:08:58 [INFO]: Epoch 031 - training loss: 0.3104, validation loss: 0.2352
2024-06-03 06:09:04 [INFO]: Epoch 032 - training loss: 0.3105, validation loss: 0.2362
2024-06-03 06:09:10 [INFO]: Epoch 033 - training loss: 0.3057, validation loss: 0.2375
2024-06-03 06:09:16 [INFO]: Epoch 034 - training loss: 0.3130, validation loss: 0.2401
2024-06-03 06:09:22 [INFO]: Epoch 035 - training loss: 0.3046, validation loss: 0.2359
2024-06-03 06:09:28 [INFO]: Epoch 036 - training loss: 0.3032, validation loss: 0.2338
2024-06-03 06:09:35 [INFO]: Epoch 037 - training loss: 0.3051, validation loss: 0.2337
2024-06-03 06:09:41 [INFO]: Epoch 038 - training loss: 0.2922, validation loss: 0.2314
2024-06-03 06:09:47 [INFO]: Epoch 039 - training loss: 0.2914, validation loss: 0.2346
2024-06-03 06:09:53 [INFO]: Epoch 040 - training loss: 0.2903, validation loss: 0.2359
2024-06-03 06:09:59 [INFO]: Epoch 041 - training loss: 0.2889, validation loss: 0.2320
2024-06-03 06:10:06 [INFO]: Epoch 042 - training loss: 0.2824, validation loss: 0.2299
2024-06-03 06:10:12 [INFO]: Epoch 043 - training loss: 0.2811, validation loss: 0.2333
2024-06-03 06:10:18 [INFO]: Epoch 044 - training loss: 0.2786, validation loss: 0.2314
2024-06-03 06:10:24 [INFO]: Epoch 045 - training loss: 0.2790, validation loss: 0.2308
2024-06-03 06:10:30 [INFO]: Epoch 046 - training loss: 0.2817, validation loss: 0.2318
2024-06-03 06:10:37 [INFO]: Epoch 047 - training loss: 0.2754, validation loss: 0.2308
2024-06-03 06:10:43 [INFO]: Epoch 048 - training loss: 0.2733, validation loss: 0.2296
2024-06-03 06:10:49 [INFO]: Epoch 049 - training loss: 0.2689, validation loss: 0.2264
2024-06-03 06:10:55 [INFO]: Epoch 050 - training loss: 0.2684, validation loss: 0.2270
2024-06-03 06:11:02 [INFO]: Epoch 051 - training loss: 0.2693, validation loss: 0.2286
2024-06-03 06:11:08 [INFO]: Epoch 052 - training loss: 0.2696, validation loss: 0.2284
2024-06-03 06:11:14 [INFO]: Epoch 053 - training loss: 0.2721, validation loss: 0.2276
2024-06-03 06:11:20 [INFO]: Epoch 054 - training loss: 0.2645, validation loss: 0.2244
2024-06-03 06:11:27 [INFO]: Epoch 055 - training loss: 0.2608, validation loss: 0.2229
2024-06-03 06:11:33 [INFO]: Epoch 056 - training loss: 0.2617, validation loss: 0.2258
2024-06-03 06:11:39 [INFO]: Epoch 057 - training loss: 0.2654, validation loss: 0.2214
2024-06-03 06:11:45 [INFO]: Epoch 058 - training loss: 0.2608, validation loss: 0.2227
2024-06-03 06:11:51 [INFO]: Epoch 059 - training loss: 0.2596, validation loss: 0.2250
2024-06-03 06:11:58 [INFO]: Epoch 060 - training loss: 0.2594, validation loss: 0.2230
2024-06-03 06:12:04 [INFO]: Epoch 061 - training loss: 0.2587, validation loss: 0.2224
2024-06-03 06:12:10 [INFO]: Epoch 062 - training loss: 0.2559, validation loss: 0.2255
2024-06-03 06:12:17 [INFO]: Epoch 063 - training loss: 0.2570, validation loss: 0.2254
2024-06-03 06:12:23 [INFO]: Epoch 064 - training loss: 0.2539, validation loss: 0.2260
2024-06-03 06:12:29 [INFO]: Epoch 065 - training loss: 0.2485, validation loss: 0.2233
2024-06-03 06:12:35 [INFO]: Epoch 066 - training loss: 0.2495, validation loss: 0.2224
2024-06-03 06:12:41 [INFO]: Epoch 067 - training loss: 0.2536, validation loss: 0.2258
2024-06-03 06:12:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:12:41 [INFO]: Finished training. The best model is from epoch#57.
2024-06-03 06:12:41 [INFO]: Saved the model to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_1/20240603_T060546/iTransformer.pypots
2024-06-03 06:12:43 [INFO]: Successfully saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_1/imputation.pkl
2024-06-03 06:12:43 [INFO]: Round1 - iTransformer on BeijingAir: MAE=0.1757, MSE=0.2470, MRE=0.2392
2024-06-03 06:12:43 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 06:12:43 [INFO]: Using the given device: cuda:0
2024-06-03 06:12:43 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_2/20240603_T061243
2024-06-03 06:12:43 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_2/20240603_T061243/tensorboard
2024-06-03 06:12:44 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 8,286,232
2024-06-03 06:12:50 [INFO]: Epoch 001 - training loss: 0.8765, validation loss: 0.3707
2024-06-03 06:12:56 [INFO]: Epoch 002 - training loss: 0.5536, validation loss: 0.3609
2024-06-03 06:13:03 [INFO]: Epoch 003 - training loss: 0.5175, validation loss: 0.3293
2024-06-03 06:13:09 [INFO]: Epoch 004 - training loss: 0.4880, validation loss: 0.3192
2024-06-03 06:13:15 [INFO]: Epoch 005 - training loss: 0.4795, validation loss: 0.3105
2024-06-03 06:13:21 [INFO]: Epoch 006 - training loss: 0.4699, validation loss: 0.3028
2024-06-03 06:13:27 [INFO]: Epoch 007 - training loss: 0.4577, validation loss: 0.2987
2024-06-03 06:13:33 [INFO]: Epoch 008 - training loss: 0.4499, validation loss: 0.2908
2024-06-03 06:13:40 [INFO]: Epoch 009 - training loss: 0.4391, validation loss: 0.2877
2024-06-03 06:13:46 [INFO]: Epoch 010 - training loss: 0.4269, validation loss: 0.2781
2024-06-03 06:13:52 [INFO]: Epoch 011 - training loss: 0.4217, validation loss: 0.2755
2024-06-03 06:13:58 [INFO]: Epoch 012 - training loss: 0.4114, validation loss: 0.2713
2024-06-03 06:14:05 [INFO]: Epoch 013 - training loss: 0.3976, validation loss: 0.2641
2024-06-03 06:14:11 [INFO]: Epoch 014 - training loss: 0.3949, validation loss: 0.2716
2024-06-03 06:14:17 [INFO]: Epoch 015 - training loss: 0.3918, validation loss: 0.2624
2024-06-03 06:14:23 [INFO]: Epoch 016 - training loss: 0.3770, validation loss: 0.2593
2024-06-03 06:14:29 [INFO]: Epoch 017 - training loss: 0.3721, validation loss: 0.2591
2024-06-03 06:14:36 [INFO]: Epoch 018 - training loss: 0.3757, validation loss: 0.2531
2024-06-03 06:14:42 [INFO]: Epoch 019 - training loss: 0.3592, validation loss: 0.2505
2024-06-03 06:14:49 [INFO]: Epoch 020 - training loss: 0.3555, validation loss: 0.2518
2024-06-03 06:14:55 [INFO]: Epoch 021 - training loss: 0.3498, validation loss: 0.2506
2024-06-03 06:15:01 [INFO]: Epoch 022 - training loss: 0.3381, validation loss: 0.2456
2024-06-03 06:15:07 [INFO]: Epoch 023 - training loss: 0.3388, validation loss: 0.2460
2024-06-03 06:15:13 [INFO]: Epoch 024 - training loss: 0.3435, validation loss: 0.2488
2024-06-03 06:15:19 [INFO]: Epoch 025 - training loss: 0.3330, validation loss: 0.2440
2024-06-03 06:15:26 [INFO]: Epoch 026 - training loss: 0.3237, validation loss: 0.2438
2024-06-03 06:15:32 [INFO]: Epoch 027 - training loss: 0.3338, validation loss: 0.2470
2024-06-03 06:15:38 [INFO]: Epoch 028 - training loss: 0.3285, validation loss: 0.2418
2024-06-03 06:15:44 [INFO]: Epoch 029 - training loss: 0.3146, validation loss: 0.2417
2024-06-03 06:15:50 [INFO]: Epoch 030 - training loss: 0.3148, validation loss: 0.2389
2024-06-03 06:15:56 [INFO]: Epoch 031 - training loss: 0.3065, validation loss: 0.2388
2024-06-03 06:16:02 [INFO]: Epoch 032 - training loss: 0.3122, validation loss: 0.2393
2024-06-03 06:16:08 [INFO]: Epoch 033 - training loss: 0.3093, validation loss: 0.2418
2024-06-03 06:16:14 [INFO]: Epoch 034 - training loss: 0.3015, validation loss: 0.2401
2024-06-03 06:16:21 [INFO]: Epoch 035 - training loss: 0.2994, validation loss: 0.2366
2024-06-03 06:16:27 [INFO]: Epoch 036 - training loss: 0.2967, validation loss: 0.2371
2024-06-03 06:16:33 [INFO]: Epoch 037 - training loss: 0.2969, validation loss: 0.2344
2024-06-03 06:16:39 [INFO]: Epoch 038 - training loss: 0.2944, validation loss: 0.2385
2024-06-03 06:16:46 [INFO]: Epoch 039 - training loss: 0.2905, validation loss: 0.2358
2024-06-03 06:16:52 [INFO]: Epoch 040 - training loss: 0.2898, validation loss: 0.2349
2024-06-03 06:16:58 [INFO]: Epoch 041 - training loss: 0.2824, validation loss: 0.2350
2024-06-03 06:17:05 [INFO]: Epoch 042 - training loss: 0.2844, validation loss: 0.2314
2024-06-03 06:17:11 [INFO]: Epoch 043 - training loss: 0.2871, validation loss: 0.2322
2024-06-03 06:17:17 [INFO]: Epoch 044 - training loss: 0.2797, validation loss: 0.2324
2024-06-03 06:17:23 [INFO]: Epoch 045 - training loss: 0.2771, validation loss: 0.2318
2024-06-03 06:17:29 [INFO]: Epoch 046 - training loss: 0.2757, validation loss: 0.2313
2024-06-03 06:17:36 [INFO]: Epoch 047 - training loss: 0.2768, validation loss: 0.2336
2024-06-03 06:17:42 [INFO]: Epoch 048 - training loss: 0.2782, validation loss: 0.2345
2024-06-03 06:17:48 [INFO]: Epoch 049 - training loss: 0.2693, validation loss: 0.2290
2024-06-03 06:17:54 [INFO]: Epoch 050 - training loss: 0.2678, validation loss: 0.2276
2024-06-03 06:18:00 [INFO]: Epoch 051 - training loss: 0.2741, validation loss: 0.2319
2024-06-03 06:18:06 [INFO]: Epoch 052 - training loss: 0.2723, validation loss: 0.2300
2024-06-03 06:18:12 [INFO]: Epoch 053 - training loss: 0.2625, validation loss: 0.2270
2024-06-03 06:18:18 [INFO]: Epoch 054 - training loss: 0.2648, validation loss: 0.2281
2024-06-03 06:18:25 [INFO]: Epoch 055 - training loss: 0.2642, validation loss: 0.2283
2024-06-03 06:18:31 [INFO]: Epoch 056 - training loss: 0.2712, validation loss: 0.2314
2024-06-03 06:18:37 [INFO]: Epoch 057 - training loss: 0.2644, validation loss: 0.2255
2024-06-03 06:18:43 [INFO]: Epoch 058 - training loss: 0.2579, validation loss: 0.2254
2024-06-03 06:18:50 [INFO]: Epoch 059 - training loss: 0.2541, validation loss: 0.2239
2024-06-03 06:18:56 [INFO]: Epoch 060 - training loss: 0.2551, validation loss: 0.2265
2024-06-03 06:19:02 [INFO]: Epoch 061 - training loss: 0.2532, validation loss: 0.2235
2024-06-03 06:19:08 [INFO]: Epoch 062 - training loss: 0.2544, validation loss: 0.2249
2024-06-03 06:19:14 [INFO]: Epoch 063 - training loss: 0.2584, validation loss: 0.2248
2024-06-03 06:19:20 [INFO]: Epoch 064 - training loss: 0.2552, validation loss: 0.2252
2024-06-03 06:19:26 [INFO]: Epoch 065 - training loss: 0.2569, validation loss: 0.2278
2024-06-03 06:19:31 [INFO]: Epoch 066 - training loss: 0.2568, validation loss: 0.2240
2024-06-03 06:19:38 [INFO]: Epoch 067 - training loss: 0.2523, validation loss: 0.2228
2024-06-03 06:19:43 [INFO]: Epoch 068 - training loss: 0.2464, validation loss: 0.2259
2024-06-03 06:19:49 [INFO]: Epoch 069 - training loss: 0.2459, validation loss: 0.2254
2024-06-03 06:19:55 [INFO]: Epoch 070 - training loss: 0.2481, validation loss: 0.2245
2024-06-03 06:20:01 [INFO]: Epoch 071 - training loss: 0.2456, validation loss: 0.2253
2024-06-03 06:20:07 [INFO]: Epoch 072 - training loss: 0.2473, validation loss: 0.2230
2024-06-03 06:20:13 [INFO]: Epoch 073 - training loss: 0.2430, validation loss: 0.2245
2024-06-03 06:20:19 [INFO]: Epoch 074 - training loss: 0.2424, validation loss: 0.2229
2024-06-03 06:20:25 [INFO]: Epoch 075 - training loss: 0.2549, validation loss: 0.2236
2024-06-03 06:20:31 [INFO]: Epoch 076 - training loss: 0.2479, validation loss: 0.2215
2024-06-03 06:20:36 [INFO]: Epoch 077 - training loss: 0.2402, validation loss: 0.2217
2024-06-03 06:20:42 [INFO]: Epoch 078 - training loss: 0.2411, validation loss: 0.2232
2024-06-03 06:20:48 [INFO]: Epoch 079 - training loss: 0.2375, validation loss: 0.2220
2024-06-03 06:20:54 [INFO]: Epoch 080 - training loss: 0.2386, validation loss: 0.2196
2024-06-03 06:21:00 [INFO]: Epoch 081 - training loss: 0.2429, validation loss: 0.2263
2024-06-03 06:21:06 [INFO]: Epoch 082 - training loss: 0.2516, validation loss: 0.2226
2024-06-03 06:21:11 [INFO]: Epoch 083 - training loss: 0.2406, validation loss: 0.2237
2024-06-03 06:21:17 [INFO]: Epoch 084 - training loss: 0.2418, validation loss: 0.2209
2024-06-03 06:21:22 [INFO]: Epoch 085 - training loss: 0.2372, validation loss: 0.2200
2024-06-03 06:21:28 [INFO]: Epoch 086 - training loss: 0.2326, validation loss: 0.2222
2024-06-03 06:21:33 [INFO]: Epoch 087 - training loss: 0.2349, validation loss: 0.2207
2024-06-03 06:21:39 [INFO]: Epoch 088 - training loss: 0.2328, validation loss: 0.2203
2024-06-03 06:21:44 [INFO]: Epoch 089 - training loss: 0.2317, validation loss: 0.2203
2024-06-03 06:21:50 [INFO]: Epoch 090 - training loss: 0.2338, validation loss: 0.2206
2024-06-03 06:21:50 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:21:50 [INFO]: Finished training. The best model is from epoch#80.
2024-06-03 06:21:50 [INFO]: Saved the model to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_2/20240603_T061243/iTransformer.pypots
2024-06-03 06:21:52 [INFO]: Successfully saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_2/imputation.pkl
2024-06-03 06:21:52 [INFO]: Round2 - iTransformer on BeijingAir: MAE=0.1696, MSE=0.2385, MRE=0.2309
2024-06-03 06:21:52 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 06:21:52 [INFO]: Using the given device: cuda:0
2024-06-03 06:21:52 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_3/20240603_T062152
2024-06-03 06:21:52 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_3/20240603_T062152/tensorboard
2024-06-03 06:21:52 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 8,286,232
2024-06-03 06:21:57 [INFO]: Epoch 001 - training loss: 0.8553, validation loss: 0.3640
2024-06-03 06:22:03 [INFO]: Epoch 002 - training loss: 0.5443, validation loss: 0.3418
2024-06-03 06:22:08 [INFO]: Epoch 003 - training loss: 0.5068, validation loss: 0.3316
2024-06-03 06:22:14 [INFO]: Epoch 004 - training loss: 0.4861, validation loss: 0.3199
2024-06-03 06:22:19 [INFO]: Epoch 005 - training loss: 0.4805, validation loss: 0.3075
2024-06-03 06:22:25 [INFO]: Epoch 006 - training loss: 0.4677, validation loss: 0.3017
2024-06-03 06:22:30 [INFO]: Epoch 007 - training loss: 0.4489, validation loss: 0.2961
2024-06-03 06:22:35 [INFO]: Epoch 008 - training loss: 0.4457, validation loss: 0.2884
2024-06-03 06:22:40 [INFO]: Epoch 009 - training loss: 0.4394, validation loss: 0.2822
2024-06-03 06:22:46 [INFO]: Epoch 010 - training loss: 0.4261, validation loss: 0.2786
2024-06-03 06:22:51 [INFO]: Epoch 011 - training loss: 0.4307, validation loss: 0.2738
2024-06-03 06:22:57 [INFO]: Epoch 012 - training loss: 0.4113, validation loss: 0.2707
2024-06-03 06:23:02 [INFO]: Epoch 013 - training loss: 0.4068, validation loss: 0.2695
2024-06-03 06:23:08 [INFO]: Epoch 014 - training loss: 0.3974, validation loss: 0.2634
2024-06-03 06:23:13 [INFO]: Epoch 015 - training loss: 0.3930, validation loss: 0.2587
2024-06-03 06:23:18 [INFO]: Epoch 016 - training loss: 0.3791, validation loss: 0.2576
2024-06-03 06:23:24 [INFO]: Epoch 017 - training loss: 0.3743, validation loss: 0.2563
2024-06-03 06:23:29 [INFO]: Epoch 018 - training loss: 0.3648, validation loss: 0.2525
2024-06-03 06:23:35 [INFO]: Epoch 019 - training loss: 0.3600, validation loss: 0.2511
2024-06-03 06:23:40 [INFO]: Epoch 020 - training loss: 0.3592, validation loss: 0.2504
2024-06-03 06:23:46 [INFO]: Epoch 021 - training loss: 0.3500, validation loss: 0.2482
2024-06-03 06:23:52 [INFO]: Epoch 022 - training loss: 0.3476, validation loss: 0.2474
2024-06-03 06:23:57 [INFO]: Epoch 023 - training loss: 0.3471, validation loss: 0.2513
2024-06-03 06:24:02 [INFO]: Epoch 024 - training loss: 0.3396, validation loss: 0.2470
2024-06-03 06:24:08 [INFO]: Epoch 025 - training loss: 0.3370, validation loss: 0.2440
2024-06-03 06:24:13 [INFO]: Epoch 026 - training loss: 0.3300, validation loss: 0.2433
2024-06-03 06:24:19 [INFO]: Epoch 027 - training loss: 0.3262, validation loss: 0.2428
2024-06-03 06:24:24 [INFO]: Epoch 028 - training loss: 0.3214, validation loss: 0.2387
2024-06-03 06:24:30 [INFO]: Epoch 029 - training loss: 0.3206, validation loss: 0.2402
2024-06-03 06:24:35 [INFO]: Epoch 030 - training loss: 0.3201, validation loss: 0.2386
2024-06-03 06:24:40 [INFO]: Epoch 031 - training loss: 0.3119, validation loss: 0.2404
2024-06-03 06:24:46 [INFO]: Epoch 032 - training loss: 0.3121, validation loss: 0.2361
2024-06-03 06:24:51 [INFO]: Epoch 033 - training loss: 0.3127, validation loss: 0.2366
2024-06-03 06:24:57 [INFO]: Epoch 034 - training loss: 0.3076, validation loss: 0.2371
2024-06-03 06:25:02 [INFO]: Epoch 035 - training loss: 0.2965, validation loss: 0.2373
2024-06-03 06:25:08 [INFO]: Epoch 036 - training loss: 0.2997, validation loss: 0.2347
2024-06-03 06:25:13 [INFO]: Epoch 037 - training loss: 0.3005, validation loss: 0.2362
2024-06-03 06:25:19 [INFO]: Epoch 038 - training loss: 0.3036, validation loss: 0.2356
2024-06-03 06:25:24 [INFO]: Epoch 039 - training loss: 0.2937, validation loss: 0.2342
2024-06-03 06:25:30 [INFO]: Epoch 040 - training loss: 0.2853, validation loss: 0.2341
2024-06-03 06:25:35 [INFO]: Epoch 041 - training loss: 0.2875, validation loss: 0.2345
2024-06-03 06:25:41 [INFO]: Epoch 042 - training loss: 0.2875, validation loss: 0.2318
2024-06-03 06:25:47 [INFO]: Epoch 043 - training loss: 0.2876, validation loss: 0.2360
2024-06-03 06:25:52 [INFO]: Epoch 044 - training loss: 0.2828, validation loss: 0.2286
2024-06-03 06:25:58 [INFO]: Epoch 045 - training loss: 0.2757, validation loss: 0.2296
2024-06-03 06:26:03 [INFO]: Epoch 046 - training loss: 0.2759, validation loss: 0.2298
2024-06-03 06:26:08 [INFO]: Epoch 047 - training loss: 0.2788, validation loss: 0.2301
2024-06-03 06:26:14 [INFO]: Epoch 048 - training loss: 0.2771, validation loss: 0.2317
2024-06-03 06:26:19 [INFO]: Epoch 049 - training loss: 0.2752, validation loss: 0.2286
2024-06-03 06:26:25 [INFO]: Epoch 050 - training loss: 0.2698, validation loss: 0.2309
2024-06-03 06:26:30 [INFO]: Epoch 051 - training loss: 0.2673, validation loss: 0.2269
2024-06-03 06:26:36 [INFO]: Epoch 052 - training loss: 0.2695, validation loss: 0.2305
2024-06-03 06:26:41 [INFO]: Epoch 053 - training loss: 0.2674, validation loss: 0.2310
2024-06-03 06:26:47 [INFO]: Epoch 054 - training loss: 0.2674, validation loss: 0.2264
2024-06-03 06:26:52 [INFO]: Epoch 055 - training loss: 0.2671, validation loss: 0.2267
2024-06-03 06:26:58 [INFO]: Epoch 056 - training loss: 0.2625, validation loss: 0.2259
2024-06-03 06:27:03 [INFO]: Epoch 057 - training loss: 0.2596, validation loss: 0.2297
2024-06-03 06:27:08 [INFO]: Epoch 058 - training loss: 0.2646, validation loss: 0.2275
2024-06-03 06:27:14 [INFO]: Epoch 059 - training loss: 0.2603, validation loss: 0.2240
2024-06-03 06:27:19 [INFO]: Epoch 060 - training loss: 0.2580, validation loss: 0.2260
2024-06-03 06:27:25 [INFO]: Epoch 061 - training loss: 0.2593, validation loss: 0.2260
2024-06-03 06:27:30 [INFO]: Epoch 062 - training loss: 0.2563, validation loss: 0.2261
2024-06-03 06:27:35 [INFO]: Epoch 063 - training loss: 0.2538, validation loss: 0.2247
2024-06-03 06:27:39 [INFO]: Epoch 064 - training loss: 0.2529, validation loss: 0.2239
2024-06-03 06:27:44 [INFO]: Epoch 065 - training loss: 0.2539, validation loss: 0.2251
2024-06-03 06:27:48 [INFO]: Epoch 066 - training loss: 0.2487, validation loss: 0.2212
2024-06-03 06:27:53 [INFO]: Epoch 067 - training loss: 0.2524, validation loss: 0.2227
2024-06-03 06:27:57 [INFO]: Epoch 068 - training loss: 0.2509, validation loss: 0.2214
2024-06-03 06:28:02 [INFO]: Epoch 069 - training loss: 0.2512, validation loss: 0.2241
2024-06-03 06:28:06 [INFO]: Epoch 070 - training loss: 0.2506, validation loss: 0.2283
2024-06-03 06:28:10 [INFO]: Epoch 071 - training loss: 0.2483, validation loss: 0.2219
2024-06-03 06:28:15 [INFO]: Epoch 072 - training loss: 0.2486, validation loss: 0.2247
2024-06-03 06:28:19 [INFO]: Epoch 073 - training loss: 0.2525, validation loss: 0.2265
2024-06-03 06:28:24 [INFO]: Epoch 074 - training loss: 0.2482, validation loss: 0.2234
2024-06-03 06:28:28 [INFO]: Epoch 075 - training loss: 0.2446, validation loss: 0.2257
2024-06-03 06:28:32 [INFO]: Epoch 076 - training loss: 0.2461, validation loss: 0.2226
2024-06-03 06:28:32 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:28:32 [INFO]: Finished training. The best model is from epoch#66.
2024-06-03 06:28:33 [INFO]: Saved the model to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_3/20240603_T062152/iTransformer.pypots
2024-06-03 06:28:34 [INFO]: Successfully saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_3/imputation.pkl
2024-06-03 06:28:34 [INFO]: Round3 - iTransformer on BeijingAir: MAE=0.1734, MSE=0.2413, MRE=0.2361
2024-06-03 06:28:34 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 06:28:34 [INFO]: Using the given device: cuda:0
2024-06-03 06:28:34 [INFO]: Model files will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_4/20240603_T062834
2024-06-03 06:28:34 [INFO]: Tensorboard file will be saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_4/20240603_T062834/tensorboard
2024-06-03 06:28:34 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 8,286,232
2024-06-03 06:28:39 [INFO]: Epoch 001 - training loss: 0.8605, validation loss: 0.3687
2024-06-03 06:28:43 [INFO]: Epoch 002 - training loss: 0.5470, validation loss: 0.3350
2024-06-03 06:28:47 [INFO]: Epoch 003 - training loss: 0.5060, validation loss: 0.3307
2024-06-03 06:28:52 [INFO]: Epoch 004 - training loss: 0.4867, validation loss: 0.3137
2024-06-03 06:28:56 [INFO]: Epoch 005 - training loss: 0.4828, validation loss: 0.3078
2024-06-03 06:29:01 [INFO]: Epoch 006 - training loss: 0.4644, validation loss: 0.3059
2024-06-03 06:29:05 [INFO]: Epoch 007 - training loss: 0.4524, validation loss: 0.3003
2024-06-03 06:29:10 [INFO]: Epoch 008 - training loss: 0.4458, validation loss: 0.2900
2024-06-03 06:29:14 [INFO]: Epoch 009 - training loss: 0.4390, validation loss: 0.2814
2024-06-03 06:29:19 [INFO]: Epoch 010 - training loss: 0.4278, validation loss: 0.2790
2024-06-03 06:29:23 [INFO]: Epoch 011 - training loss: 0.4253, validation loss: 0.2802
2024-06-03 06:29:28 [INFO]: Epoch 012 - training loss: 0.4241, validation loss: 0.2765
2024-06-03 06:29:32 [INFO]: Epoch 013 - training loss: 0.4112, validation loss: 0.2691
2024-06-03 06:29:37 [INFO]: Epoch 014 - training loss: 0.3982, validation loss: 0.2670
2024-06-03 06:29:41 [INFO]: Epoch 015 - training loss: 0.3915, validation loss: 0.2641
2024-06-03 06:29:46 [INFO]: Epoch 016 - training loss: 0.3827, validation loss: 0.2578
2024-06-03 06:29:50 [INFO]: Epoch 017 - training loss: 0.3762, validation loss: 0.2583
2024-06-03 06:29:55 [INFO]: Epoch 018 - training loss: 0.3706, validation loss: 0.2536
2024-06-03 06:29:59 [INFO]: Epoch 019 - training loss: 0.3676, validation loss: 0.2512
2024-06-03 06:30:03 [INFO]: Epoch 020 - training loss: 0.3732, validation loss: 0.2557
2024-06-03 06:30:08 [INFO]: Epoch 021 - training loss: 0.3595, validation loss: 0.2489
2024-06-03 06:30:12 [INFO]: Epoch 022 - training loss: 0.3460, validation loss: 0.2473
2024-06-03 06:30:17 [INFO]: Epoch 023 - training loss: 0.3406, validation loss: 0.2436
2024-06-03 06:30:22 [INFO]: Epoch 024 - training loss: 0.3389, validation loss: 0.2534
2024-06-03 06:30:26 [INFO]: Epoch 025 - training loss: 0.3476, validation loss: 0.2461
2024-06-03 06:30:31 [INFO]: Epoch 026 - training loss: 0.3351, validation loss: 0.2447
2024-06-03 06:30:35 [INFO]: Epoch 027 - training loss: 0.3266, validation loss: 0.2424
2024-06-03 06:30:39 [INFO]: Epoch 028 - training loss: 0.3228, validation loss: 0.2415
2024-06-03 06:30:44 [INFO]: Epoch 029 - training loss: 0.3244, validation loss: 0.2393
2024-06-03 06:30:48 [INFO]: Epoch 030 - training loss: 0.3258, validation loss: 0.2387
2024-06-03 06:30:53 [INFO]: Epoch 031 - training loss: 0.3164, validation loss: 0.2397
2024-06-03 06:30:57 [INFO]: Epoch 032 - training loss: 0.3091, validation loss: 0.2384
2024-06-03 06:31:02 [INFO]: Epoch 033 - training loss: 0.3091, validation loss: 0.2350
2024-06-03 06:31:06 [INFO]: Epoch 034 - training loss: 0.3096, validation loss: 0.2364
2024-06-03 06:31:11 [INFO]: Epoch 035 - training loss: 0.3043, validation loss: 0.2362
2024-06-03 06:31:15 [INFO]: Epoch 036 - training loss: 0.3001, validation loss: 0.2358
2024-06-03 06:31:20 [INFO]: Epoch 037 - training loss: 0.2983, validation loss: 0.2371
2024-06-03 06:31:24 [INFO]: Epoch 038 - training loss: 0.2980, validation loss: 0.2329
2024-06-03 06:31:29 [INFO]: Epoch 039 - training loss: 0.2954, validation loss: 0.2370
2024-06-03 06:31:33 [INFO]: Epoch 040 - training loss: 0.2930, validation loss: 0.2343
2024-06-03 06:31:38 [INFO]: Epoch 041 - training loss: 0.2955, validation loss: 0.2336
2024-06-03 06:31:42 [INFO]: Epoch 042 - training loss: 0.2864, validation loss: 0.2307
2024-06-03 06:31:47 [INFO]: Epoch 043 - training loss: 0.2803, validation loss: 0.2297
2024-06-03 06:31:51 [INFO]: Epoch 044 - training loss: 0.2858, validation loss: 0.2350
2024-06-03 06:31:56 [INFO]: Epoch 045 - training loss: 0.2816, validation loss: 0.2304
2024-06-03 06:32:00 [INFO]: Epoch 046 - training loss: 0.2798, validation loss: 0.2293
2024-06-03 06:32:05 [INFO]: Epoch 047 - training loss: 0.2770, validation loss: 0.2294
2024-06-03 06:32:09 [INFO]: Epoch 048 - training loss: 0.2754, validation loss: 0.2282
2024-06-03 06:32:14 [INFO]: Epoch 049 - training loss: 0.2779, validation loss: 0.2312
2024-06-03 06:32:18 [INFO]: Epoch 050 - training loss: 0.2748, validation loss: 0.2306
2024-06-03 06:32:22 [INFO]: Epoch 051 - training loss: 0.2703, validation loss: 0.2277
2024-06-03 06:32:27 [INFO]: Epoch 052 - training loss: 0.2683, validation loss: 0.2264
2024-06-03 06:32:32 [INFO]: Epoch 053 - training loss: 0.2718, validation loss: 0.2261
2024-06-03 06:32:36 [INFO]: Epoch 054 - training loss: 0.2686, validation loss: 0.2258
2024-06-03 06:32:41 [INFO]: Epoch 055 - training loss: 0.2637, validation loss: 0.2246
2024-06-03 06:32:45 [INFO]: Epoch 056 - training loss: 0.2698, validation loss: 0.2245
2024-06-03 06:32:50 [INFO]: Epoch 057 - training loss: 0.2641, validation loss: 0.2244
2024-06-03 06:32:54 [INFO]: Epoch 058 - training loss: 0.2608, validation loss: 0.2243
2024-06-03 06:32:59 [INFO]: Epoch 059 - training loss: 0.2598, validation loss: 0.2279
2024-06-03 06:33:03 [INFO]: Epoch 060 - training loss: 0.2601, validation loss: 0.2236
2024-06-03 06:33:08 [INFO]: Epoch 061 - training loss: 0.2581, validation loss: 0.2255
2024-06-03 06:33:12 [INFO]: Epoch 062 - training loss: 0.2575, validation loss: 0.2253
2024-06-03 06:33:17 [INFO]: Epoch 063 - training loss: 0.2581, validation loss: 0.2233
2024-06-03 06:33:21 [INFO]: Epoch 064 - training loss: 0.2504, validation loss: 0.2221
2024-06-03 06:33:26 [INFO]: Epoch 065 - training loss: 0.2540, validation loss: 0.2254
2024-06-03 06:33:30 [INFO]: Epoch 066 - training loss: 0.2493, validation loss: 0.2256
2024-06-03 06:33:34 [INFO]: Epoch 067 - training loss: 0.2535, validation loss: 0.2217
2024-06-03 06:33:39 [INFO]: Epoch 068 - training loss: 0.2506, validation loss: 0.2215
2024-06-03 06:33:43 [INFO]: Epoch 069 - training loss: 0.2544, validation loss: 0.2223
2024-06-03 06:33:48 [INFO]: Epoch 070 - training loss: 0.2502, validation loss: 0.2233
2024-06-03 06:33:52 [INFO]: Epoch 071 - training loss: 0.2496, validation loss: 0.2203
2024-06-03 06:33:57 [INFO]: Epoch 072 - training loss: 0.2487, validation loss: 0.2207
2024-06-03 06:34:01 [INFO]: Epoch 073 - training loss: 0.2482, validation loss: 0.2201
2024-06-03 06:34:06 [INFO]: Epoch 074 - training loss: 0.2484, validation loss: 0.2263
2024-06-03 06:34:10 [INFO]: Epoch 075 - training loss: 0.2489, validation loss: 0.2219
2024-06-03 06:34:15 [INFO]: Epoch 076 - training loss: 0.2441, validation loss: 0.2193
2024-06-03 06:34:20 [INFO]: Epoch 077 - training loss: 0.2445, validation loss: 0.2205
2024-06-03 06:34:24 [INFO]: Epoch 078 - training loss: 0.2398, validation loss: 0.2217
2024-06-03 06:34:29 [INFO]: Epoch 079 - training loss: 0.2385, validation loss: 0.2196
2024-06-03 06:34:33 [INFO]: Epoch 080 - training loss: 0.2395, validation loss: 0.2196
2024-06-03 06:34:37 [INFO]: Epoch 081 - training loss: 0.2375, validation loss: 0.2197
2024-06-03 06:34:42 [INFO]: Epoch 082 - training loss: 0.2351, validation loss: 0.2224
2024-06-03 06:34:47 [INFO]: Epoch 083 - training loss: 0.2440, validation loss: 0.2212
2024-06-03 06:34:51 [INFO]: Epoch 084 - training loss: 0.2387, validation loss: 0.2213
2024-06-03 06:34:55 [INFO]: Epoch 085 - training loss: 0.2356, validation loss: 0.2198
2024-06-03 06:35:00 [INFO]: Epoch 086 - training loss: 0.2350, validation loss: 0.2194
2024-06-03 06:35:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 06:35:00 [INFO]: Finished training. The best model is from epoch#76.
2024-06-03 06:35:00 [INFO]: Saved the model to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_4/20240603_T062834/iTransformer.pypots
2024-06-03 06:35:02 [INFO]: Successfully saved to results_point_rate05/BeijingAir/iTransformer_BeijingAir/round_4/imputation.pkl
2024-06-03 06:35:02 [INFO]: Round4 - iTransformer on BeijingAir: MAE=0.1695, MSE=0.2380, MRE=0.2308
2024-06-03 06:35:02 [INFO]: Done! Final results:
Averaged iTransformer (8,286,232 params) on BeijingAir: MAE=0.1625 ± 0.0032011844195699007, MSE=0.2325 ± 0.0035534843650911656, MRE=0.2153 ± 0.004242953918056868, average inference time=0.34