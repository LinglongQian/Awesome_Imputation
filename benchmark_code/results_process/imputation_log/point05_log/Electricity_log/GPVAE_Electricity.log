2024-06-02 19:59:13 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:59:13 [INFO]: Using the given device: cuda:0
2024-06-02 19:59:13 [INFO]: Model files will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_0/20240602_T195913
2024-06-02 19:59:13 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_0/20240602_T195913/tensorboard
2024-06-02 19:59:14 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,825,022
2024-06-02 19:59:34 [INFO]: Epoch 001 - training loss: 213653.6169, validation loss: 3.3868
2024-06-02 19:59:52 [INFO]: Epoch 002 - training loss: 185314.4612, validation loss: 3.0567
2024-06-02 20:00:10 [INFO]: Epoch 003 - training loss: 183607.1661, validation loss: 2.9915
2024-06-02 20:00:28 [INFO]: Epoch 004 - training loss: 182639.7263, validation loss: 2.8800
2024-06-02 20:00:45 [INFO]: Epoch 005 - training loss: 182279.6134, validation loss: 2.8204
2024-06-02 20:01:03 [INFO]: Epoch 006 - training loss: 182080.3137, validation loss: 2.7616
2024-06-02 20:01:21 [INFO]: Epoch 007 - training loss: 181938.5289, validation loss: 2.7193
2024-06-02 20:01:39 [INFO]: Epoch 008 - training loss: 181866.0376, validation loss: 2.7236
2024-06-02 20:01:57 [INFO]: Epoch 009 - training loss: 181794.5197, validation loss: 2.7056
2024-06-02 20:02:15 [INFO]: Epoch 010 - training loss: 181753.5058, validation loss: 2.6874
2024-06-02 20:02:34 [INFO]: Epoch 011 - training loss: 181732.2355, validation loss: 2.6934
2024-06-02 20:02:52 [INFO]: Epoch 012 - training loss: 181693.3137, validation loss: 2.6766
2024-06-02 20:03:11 [INFO]: Epoch 013 - training loss: 181662.9479, validation loss: 2.6745
2024-06-02 20:03:29 [INFO]: Epoch 014 - training loss: 181641.8495, validation loss: 2.6722
2024-06-02 20:03:49 [INFO]: Epoch 015 - training loss: 181623.1753, validation loss: 2.6720
2024-06-02 20:04:08 [INFO]: Epoch 016 - training loss: 181592.9421, validation loss: 2.6701
2024-06-02 20:04:27 [INFO]: Epoch 017 - training loss: 181565.9421, validation loss: 2.6519
2024-06-02 20:04:46 [INFO]: Epoch 018 - training loss: 181533.1383, validation loss: 2.6529
2024-06-02 20:05:04 [INFO]: Epoch 019 - training loss: 181515.6198, validation loss: 2.6525
2024-06-02 20:05:23 [INFO]: Epoch 020 - training loss: 181495.1730, validation loss: 2.6504
2024-06-02 20:05:41 [INFO]: Epoch 021 - training loss: 181489.2865, validation loss: 2.6508
2024-06-02 20:06:00 [INFO]: Epoch 022 - training loss: 181475.8860, validation loss: 2.6586
2024-06-02 20:06:19 [INFO]: Epoch 023 - training loss: 181481.9491, validation loss: 2.6515
2024-06-02 20:06:37 [INFO]: Epoch 024 - training loss: 181472.2257, validation loss: 2.6444
2024-06-02 20:06:56 [INFO]: Epoch 025 - training loss: 181427.9369, validation loss: 2.6313
2024-06-02 20:07:15 [INFO]: Epoch 026 - training loss: 181432.9167, validation loss: 2.6368
2024-06-02 20:07:33 [INFO]: Epoch 027 - training loss: 181419.9161, validation loss: 2.6418
2024-06-02 20:07:52 [INFO]: Epoch 028 - training loss: 181395.2934, validation loss: 2.6298
2024-06-02 20:08:10 [INFO]: Epoch 029 - training loss: 181382.3600, validation loss: 2.6362
2024-06-02 20:08:28 [INFO]: Epoch 030 - training loss: 181379.6811, validation loss: 2.6425
2024-06-02 20:08:46 [INFO]: Epoch 031 - training loss: 181427.8247, validation loss: 2.6370
2024-06-02 20:09:05 [INFO]: Epoch 032 - training loss: 181363.6395, validation loss: 2.6268
2024-06-02 20:09:23 [INFO]: Epoch 033 - training loss: 181344.8484, validation loss: 2.6354
2024-06-02 20:09:41 [INFO]: Epoch 034 - training loss: 181340.1429, validation loss: 2.6269
2024-06-02 20:10:00 [INFO]: Epoch 035 - training loss: 181337.4514, validation loss: 2.6340
2024-06-02 20:10:19 [INFO]: Epoch 036 - training loss: 181315.3356, validation loss: 2.6323
2024-06-02 20:10:38 [INFO]: Epoch 037 - training loss: 181307.6968, validation loss: 2.6260
2024-06-02 20:10:57 [INFO]: Epoch 038 - training loss: 181308.3403, validation loss: 2.6433
2024-06-02 20:11:15 [INFO]: Epoch 039 - training loss: 181313.2934, validation loss: 2.6330
2024-06-02 20:11:34 [INFO]: Epoch 040 - training loss: 181288.3860, validation loss: 2.6179
2024-06-02 20:11:52 [INFO]: Epoch 041 - training loss: 181284.9670, validation loss: 2.6312
2024-06-02 20:12:11 [INFO]: Epoch 042 - training loss: 181278.2390, validation loss: 2.6325
2024-06-02 20:12:29 [INFO]: Epoch 043 - training loss: 181270.9161, validation loss: 2.6257
2024-06-02 20:12:48 [INFO]: Epoch 044 - training loss: 181254.7240, validation loss: 2.6199
2024-06-02 20:13:06 [INFO]: Epoch 045 - training loss: 181271.1638, validation loss: 2.6250
2024-06-02 20:13:25 [INFO]: Epoch 046 - training loss: 181274.5891, validation loss: 2.6233
2024-06-02 20:13:43 [INFO]: Epoch 047 - training loss: 181271.8872, validation loss: 2.6226
2024-06-02 20:14:02 [INFO]: Epoch 048 - training loss: 181253.2407, validation loss: 2.6223
2024-06-02 20:14:20 [INFO]: Epoch 049 - training loss: 181244.4815, validation loss: 2.6144
2024-06-02 20:14:39 [INFO]: Epoch 050 - training loss: 181231.5550, validation loss: 2.6268
2024-06-02 20:14:57 [INFO]: Epoch 051 - training loss: 181221.3050, validation loss: 2.6312
2024-06-02 20:15:15 [INFO]: Epoch 052 - training loss: 181215.0289, validation loss: 2.6087
2024-06-02 20:15:34 [INFO]: Epoch 053 - training loss: 181215.3519, validation loss: 2.6305
2024-06-02 20:15:52 [INFO]: Epoch 054 - training loss: 181227.4792, validation loss: 2.6337
2024-06-02 20:16:10 [INFO]: Epoch 055 - training loss: 181221.9821, validation loss: 2.6276
2024-06-02 20:16:28 [INFO]: Epoch 056 - training loss: 181207.1470, validation loss: 2.6241
2024-06-02 20:16:46 [INFO]: Epoch 057 - training loss: 181213.9907, validation loss: 2.6389
2024-06-02 20:17:05 [INFO]: Epoch 058 - training loss: 181213.4346, validation loss: 2.6324
2024-06-02 20:17:23 [INFO]: Epoch 059 - training loss: 181193.8877, validation loss: 2.6271
2024-06-02 20:17:41 [INFO]: Epoch 060 - training loss: 181193.9705, validation loss: 2.6235
2024-06-02 20:17:59 [INFO]: Epoch 061 - training loss: 181183.1777, validation loss: 2.6307
2024-06-02 20:18:19 [INFO]: Epoch 062 - training loss: 181179.7604, validation loss: 2.6135
2024-06-02 20:18:19 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:18:19 [INFO]: Finished training. The best model is from epoch#52.
2024-06-02 20:18:19 [INFO]: Saved the model to results_point_rate05/Electricity/GPVAE_Electricity/round_0/20240602_T195913/GPVAE.pypots
2024-06-02 20:18:54 [INFO]: Successfully saved to results_point_rate05/Electricity/GPVAE_Electricity/round_0/imputation.pkl
2024-06-02 20:18:54 [INFO]: Round0 - GPVAE on Electricity: MAE=1.0815, MSE=2.9987, MRE=0.5791
2024-06-02 20:18:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 20:18:54 [INFO]: Using the given device: cuda:0
2024-06-02 20:18:54 [INFO]: Model files will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_1/20240602_T201854
2024-06-02 20:18:54 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_1/20240602_T201854/tensorboard
2024-06-02 20:18:54 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,825,022
2024-06-02 20:19:11 [INFO]: Epoch 001 - training loss: 214674.9722, validation loss: 3.3066
2024-06-02 20:19:29 [INFO]: Epoch 002 - training loss: 185062.4039, validation loss: 3.1006
2024-06-02 20:19:47 [INFO]: Epoch 003 - training loss: 183257.7697, validation loss: 3.0244
2024-06-02 20:20:04 [INFO]: Epoch 004 - training loss: 182630.5434, validation loss: 2.8999
2024-06-02 20:20:22 [INFO]: Epoch 005 - training loss: 182287.8414, validation loss: 2.7827
2024-06-02 20:20:40 [INFO]: Epoch 006 - training loss: 182086.6181, validation loss: 2.7458
2024-06-02 20:20:58 [INFO]: Epoch 007 - training loss: 181935.6395, validation loss: 2.7043
2024-06-02 20:21:16 [INFO]: Epoch 008 - training loss: 181832.4126, validation loss: 2.6812
2024-06-02 20:21:34 [INFO]: Epoch 009 - training loss: 181763.7951, validation loss: 2.7047
2024-06-02 20:21:51 [INFO]: Epoch 010 - training loss: 181737.1939, validation loss: 2.6969
2024-06-02 20:22:07 [INFO]: Epoch 011 - training loss: 181692.9277, validation loss: 2.6913
2024-06-02 20:22:22 [INFO]: Epoch 012 - training loss: 181663.0162, validation loss: 2.6736
2024-06-02 20:22:37 [INFO]: Epoch 013 - training loss: 181695.9977, validation loss: 2.6578
2024-06-02 20:22:52 [INFO]: Epoch 014 - training loss: 181639.4022, validation loss: 2.6538
2024-06-02 20:23:06 [INFO]: Epoch 015 - training loss: 181599.0052, validation loss: 2.6461
2024-06-02 20:23:20 [INFO]: Epoch 016 - training loss: 181541.7332, validation loss: 2.6373
2024-06-02 20:23:35 [INFO]: Epoch 017 - training loss: 181508.8640, validation loss: 2.6403
2024-06-02 20:23:49 [INFO]: Epoch 018 - training loss: 181486.4103, validation loss: 2.6206
2024-06-02 20:24:03 [INFO]: Epoch 019 - training loss: 181488.2633, validation loss: 2.6437
2024-06-02 20:24:17 [INFO]: Epoch 020 - training loss: 181485.6759, validation loss: 2.6386
2024-06-02 20:24:30 [INFO]: Epoch 021 - training loss: 181498.2743, validation loss: 2.6233
2024-06-02 20:24:44 [INFO]: Epoch 022 - training loss: 181449.1800, validation loss: 2.6203
2024-06-02 20:24:58 [INFO]: Epoch 023 - training loss: 181412.1875, validation loss: 2.6266
2024-06-02 20:25:12 [INFO]: Epoch 024 - training loss: 181403.1059, validation loss: 2.6206
2024-06-02 20:25:26 [INFO]: Epoch 025 - training loss: 181410.2135, validation loss: 2.6271
2024-06-02 20:25:40 [INFO]: Epoch 026 - training loss: 181387.5174, validation loss: 2.6385
2024-06-02 20:25:54 [INFO]: Epoch 027 - training loss: 181367.5862, validation loss: 2.6245
2024-06-02 20:26:09 [INFO]: Epoch 028 - training loss: 181349.0689, validation loss: 2.6127
2024-06-02 20:26:23 [INFO]: Epoch 029 - training loss: 181339.7708, validation loss: 2.6154
2024-06-02 20:26:38 [INFO]: Epoch 030 - training loss: 181326.5764, validation loss: 2.6133
2024-06-02 20:26:52 [INFO]: Epoch 031 - training loss: 181318.5909, validation loss: 2.6176
2024-06-02 20:27:07 [INFO]: Epoch 032 - training loss: 181320.0098, validation loss: 2.6199
2024-06-02 20:27:21 [INFO]: Epoch 033 - training loss: 181319.6522, validation loss: 2.6186
2024-06-02 20:27:36 [INFO]: Epoch 034 - training loss: 181317.2193, validation loss: 2.6217
2024-06-02 20:27:51 [INFO]: Epoch 035 - training loss: 181306.9520, validation loss: 2.6188
2024-06-02 20:28:06 [INFO]: Epoch 036 - training loss: 181313.0839, validation loss: 2.6163
2024-06-02 20:28:20 [INFO]: Epoch 037 - training loss: 181291.8247, validation loss: 2.6034
2024-06-02 20:28:35 [INFO]: Epoch 038 - training loss: 181268.8900, validation loss: 2.6059
2024-06-02 20:28:50 [INFO]: Epoch 039 - training loss: 181264.1684, validation loss: 2.6064
2024-06-02 20:29:05 [INFO]: Epoch 040 - training loss: 181259.7147, validation loss: 2.6104
2024-06-02 20:29:19 [INFO]: Epoch 041 - training loss: 181265.6291, validation loss: 2.6118
2024-06-02 20:29:34 [INFO]: Epoch 042 - training loss: 181253.0052, validation loss: 2.6154
2024-06-02 20:29:48 [INFO]: Epoch 043 - training loss: 181241.6753, validation loss: 2.6077
2024-06-02 20:30:02 [INFO]: Epoch 044 - training loss: 181234.4826, validation loss: 2.6179
2024-06-02 20:30:16 [INFO]: Epoch 045 - training loss: 181227.9728, validation loss: 2.6029
2024-06-02 20:30:31 [INFO]: Epoch 046 - training loss: 181225.3339, validation loss: 2.6063
2024-06-02 20:30:45 [INFO]: Epoch 047 - training loss: 181221.6047, validation loss: 2.5979
2024-06-02 20:31:00 [INFO]: Epoch 048 - training loss: 181219.9896, validation loss: 2.6056
2024-06-02 20:31:12 [INFO]: Epoch 049 - training loss: 181220.9311, validation loss: 2.6002
2024-06-02 20:31:26 [INFO]: Epoch 050 - training loss: 181215.5943, validation loss: 2.5925
2024-06-02 20:31:40 [INFO]: Epoch 051 - training loss: 181206.6071, validation loss: 2.5837
2024-06-02 20:31:54 [INFO]: Epoch 052 - training loss: 181226.6765, validation loss: 2.5915
2024-06-02 20:32:08 [INFO]: Epoch 053 - training loss: 181195.9207, validation loss: 2.5936
2024-06-02 20:32:22 [INFO]: Epoch 054 - training loss: 181189.7986, validation loss: 2.5834
2024-06-02 20:32:37 [INFO]: Epoch 055 - training loss: 181182.0723, validation loss: 2.5773
2024-06-02 20:32:52 [INFO]: Epoch 056 - training loss: 181176.9265, validation loss: 2.5723
2024-06-02 20:33:06 [INFO]: Epoch 057 - training loss: 181178.4745, validation loss: 2.5701
2024-06-02 20:33:20 [INFO]: Epoch 058 - training loss: 181174.2674, validation loss: 2.5649
2024-06-02 20:33:34 [INFO]: Epoch 059 - training loss: 181170.4630, validation loss: 2.5718
2024-06-02 20:33:49 [INFO]: Epoch 060 - training loss: 181161.5966, validation loss: 2.5707
2024-06-02 20:34:02 [INFO]: Epoch 061 - training loss: 181165.7240, validation loss: 2.5752
2024-06-02 20:34:16 [INFO]: Epoch 062 - training loss: 181160.3299, validation loss: 2.5508
2024-06-02 20:34:31 [INFO]: Epoch 063 - training loss: 181156.5943, validation loss: 2.5483
2024-06-02 20:34:45 [INFO]: Epoch 064 - training loss: 181150.9213, validation loss: 2.5516
2024-06-02 20:34:59 [INFO]: Epoch 065 - training loss: 181152.7083, validation loss: 2.5438
2024-06-02 20:35:12 [INFO]: Epoch 066 - training loss: 181173.2679, validation loss: 2.5727
2024-06-02 20:35:26 [INFO]: Epoch 067 - training loss: 181158.2818, validation loss: 2.5517
2024-06-02 20:35:41 [INFO]: Epoch 068 - training loss: 181139.7546, validation loss: 2.5439
2024-06-02 20:35:54 [INFO]: Epoch 069 - training loss: 181143.5995, validation loss: 2.5347
2024-06-02 20:36:08 [INFO]: Epoch 070 - training loss: 181135.2008, validation loss: 2.5139
2024-06-02 20:36:22 [INFO]: Epoch 071 - training loss: 181128.7986, validation loss: 2.5303
2024-06-02 20:36:36 [INFO]: Epoch 072 - training loss: 181119.5897, validation loss: 2.5305
2024-06-02 20:36:50 [INFO]: Epoch 073 - training loss: 181125.3484, validation loss: 2.5208
2024-06-02 20:37:04 [INFO]: Epoch 074 - training loss: 181118.3895, validation loss: 2.5279
2024-06-02 20:37:18 [INFO]: Epoch 075 - training loss: 181145.8576, validation loss: 2.5420
2024-06-02 20:37:32 [INFO]: Epoch 076 - training loss: 181164.6354, validation loss: 2.5197
2024-06-02 20:37:47 [INFO]: Epoch 077 - training loss: 181134.4994, validation loss: 2.5055
2024-06-02 20:38:01 [INFO]: Epoch 078 - training loss: 181108.5469, validation loss: 2.5010
2024-06-02 20:38:15 [INFO]: Epoch 079 - training loss: 181101.3009, validation loss: 2.5041
2024-06-02 20:38:30 [INFO]: Epoch 080 - training loss: 181099.1337, validation loss: 2.4746
2024-06-02 20:38:43 [INFO]: Epoch 081 - training loss: 181101.4184, validation loss: 2.4895
2024-06-02 20:38:55 [INFO]: Epoch 082 - training loss: 181104.1956, validation loss: 2.4902
2024-06-02 20:39:07 [INFO]: Epoch 083 - training loss: 181095.7778, validation loss: 2.4621
2024-06-02 20:39:19 [INFO]: Epoch 084 - training loss: 181094.3212, validation loss: 2.4765
2024-06-02 20:39:30 [INFO]: Epoch 085 - training loss: 181088.1696, validation loss: 2.4867
2024-06-02 20:39:42 [INFO]: Epoch 086 - training loss: 181108.6771, validation loss: 2.4606
2024-06-02 20:39:54 [INFO]: Epoch 087 - training loss: 181109.3953, validation loss: 2.4480
2024-06-02 20:40:05 [INFO]: Epoch 088 - training loss: 181088.9647, validation loss: 2.4832
2024-06-02 20:40:17 [INFO]: Epoch 089 - training loss: 181088.8131, validation loss: 2.4427
2024-06-02 20:40:29 [INFO]: Epoch 090 - training loss: 181087.6701, validation loss: 2.4605
2024-06-02 20:40:41 [INFO]: Epoch 091 - training loss: 181083.3125, validation loss: 2.4330
2024-06-02 20:40:52 [INFO]: Epoch 092 - training loss: 181091.5139, validation loss: 2.4465
2024-06-02 20:41:04 [INFO]: Epoch 093 - training loss: 181086.6528, validation loss: 2.4465
2024-06-02 20:41:15 [INFO]: Epoch 094 - training loss: 181080.6424, validation loss: 2.4592
2024-06-02 20:41:27 [INFO]: Epoch 095 - training loss: 181084.0833, validation loss: 2.4533
2024-06-02 20:41:38 [INFO]: Epoch 096 - training loss: 181101.1001, validation loss: 2.4404
2024-06-02 20:41:49 [INFO]: Epoch 097 - training loss: 181099.2020, validation loss: 2.4319
2024-06-02 20:42:00 [INFO]: Epoch 098 - training loss: 181092.8530, validation loss: 2.4310
2024-06-02 20:42:11 [INFO]: Epoch 099 - training loss: 181082.4311, validation loss: 2.4267
2024-06-02 20:42:22 [INFO]: Epoch 100 - training loss: 181072.4010, validation loss: 2.4286
2024-06-02 20:42:22 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 20:42:22 [INFO]: Saved the model to results_point_rate05/Electricity/GPVAE_Electricity/round_1/20240602_T201854/GPVAE.pypots
2024-06-02 20:42:46 [INFO]: Successfully saved to results_point_rate05/Electricity/GPVAE_Electricity/round_1/imputation.pkl
2024-06-02 20:42:46 [INFO]: Round1 - GPVAE on Electricity: MAE=1.1130, MSE=2.9893, MRE=0.5959
2024-06-02 20:42:46 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 20:42:46 [INFO]: Using the given device: cuda:0
2024-06-02 20:42:46 [INFO]: Model files will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_2/20240602_T204246
2024-06-02 20:42:46 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_2/20240602_T204246/tensorboard
2024-06-02 20:42:46 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,825,022
2024-06-02 20:42:58 [INFO]: Epoch 001 - training loss: 214165.2454, validation loss: 3.6493
2024-06-02 20:43:09 [INFO]: Epoch 002 - training loss: 185552.0637, validation loss: 3.0502
2024-06-02 20:43:19 [INFO]: Epoch 003 - training loss: 183157.1163, validation loss: 2.9289
2024-06-02 20:43:31 [INFO]: Epoch 004 - training loss: 182539.2448, validation loss: 2.8552
2024-06-02 20:43:42 [INFO]: Epoch 005 - training loss: 182215.1424, validation loss: 2.7786
2024-06-02 20:43:53 [INFO]: Epoch 006 - training loss: 182044.4682, validation loss: 2.7553
2024-06-02 20:44:04 [INFO]: Epoch 007 - training loss: 181955.5336, validation loss: 2.7221
2024-06-02 20:44:15 [INFO]: Epoch 008 - training loss: 181907.4057, validation loss: 2.6851
2024-06-02 20:44:25 [INFO]: Epoch 009 - training loss: 181836.1834, validation loss: 2.6961
2024-06-02 20:44:36 [INFO]: Epoch 010 - training loss: 181771.9080, validation loss: 2.7002
2024-06-02 20:44:47 [INFO]: Epoch 011 - training loss: 181734.5162, validation loss: 2.6834
2024-06-02 20:44:58 [INFO]: Epoch 012 - training loss: 181695.2928, validation loss: 2.6612
2024-06-02 20:45:09 [INFO]: Epoch 013 - training loss: 181663.5839, validation loss: 2.6624
2024-06-02 20:45:20 [INFO]: Epoch 014 - training loss: 181628.3374, validation loss: 2.6542
2024-06-02 20:45:31 [INFO]: Epoch 015 - training loss: 181600.7604, validation loss: 2.6398
2024-06-02 20:45:41 [INFO]: Epoch 016 - training loss: 181578.8872, validation loss: 2.6311
2024-06-02 20:45:52 [INFO]: Epoch 017 - training loss: 181548.4716, validation loss: 2.6166
2024-06-02 20:46:03 [INFO]: Epoch 018 - training loss: 181532.5087, validation loss: 2.6141
2024-06-02 20:46:13 [INFO]: Epoch 019 - training loss: 181507.6690, validation loss: 2.6138
2024-06-02 20:46:22 [INFO]: Epoch 020 - training loss: 181485.2118, validation loss: 2.6114
2024-06-02 20:46:30 [INFO]: Epoch 021 - training loss: 181475.5972, validation loss: 2.5990
2024-06-02 20:46:39 [INFO]: Epoch 022 - training loss: 181484.7477, validation loss: 2.6073
2024-06-02 20:46:47 [INFO]: Epoch 023 - training loss: 181494.7147, validation loss: 2.6231
2024-06-02 20:46:55 [INFO]: Epoch 024 - training loss: 181451.0307, validation loss: 2.5962
2024-06-02 20:47:04 [INFO]: Epoch 025 - training loss: 181421.3866, validation loss: 2.5892
2024-06-02 20:47:13 [INFO]: Epoch 026 - training loss: 181409.6956, validation loss: 2.5874
2024-06-02 20:47:22 [INFO]: Epoch 027 - training loss: 181412.4763, validation loss: 2.5962
2024-06-02 20:47:29 [INFO]: Epoch 028 - training loss: 181398.3339, validation loss: 2.5891
2024-06-02 20:47:34 [INFO]: Epoch 029 - training loss: 181385.6186, validation loss: 2.5978
2024-06-02 20:47:40 [INFO]: Epoch 030 - training loss: 181412.4988, validation loss: 2.5821
2024-06-02 20:47:45 [INFO]: Epoch 031 - training loss: 181366.3669, validation loss: 2.5795
2024-06-02 20:47:50 [INFO]: Epoch 032 - training loss: 181352.3339, validation loss: 2.5883
2024-06-02 20:47:56 [INFO]: Epoch 033 - training loss: 181339.0949, validation loss: 2.5910
2024-06-02 20:48:01 [INFO]: Epoch 034 - training loss: 181328.8889, validation loss: 2.5889
2024-06-02 20:48:06 [INFO]: Epoch 035 - training loss: 181349.9456, validation loss: 2.5997
2024-06-02 20:48:10 [INFO]: Epoch 036 - training loss: 181337.2928, validation loss: 2.5906
2024-06-02 20:48:14 [INFO]: Epoch 037 - training loss: 181327.3281, validation loss: 2.5859
2024-06-02 20:48:18 [INFO]: Epoch 038 - training loss: 181300.6267, validation loss: 2.5854
2024-06-02 20:48:22 [INFO]: Epoch 039 - training loss: 181299.3102, validation loss: 2.5867
2024-06-02 20:48:26 [INFO]: Epoch 040 - training loss: 181290.4433, validation loss: 2.5727
2024-06-02 20:48:30 [INFO]: Epoch 041 - training loss: 181284.2384, validation loss: 2.5746
2024-06-02 20:48:34 [INFO]: Epoch 042 - training loss: 181285.7656, validation loss: 2.5693
2024-06-02 20:48:37 [INFO]: Epoch 043 - training loss: 181267.2188, validation loss: 2.5686
2024-06-02 20:48:41 [INFO]: Epoch 044 - training loss: 181257.9965, validation loss: 2.5724
2024-06-02 20:48:45 [INFO]: Epoch 045 - training loss: 181248.5752, validation loss: 2.5662
2024-06-02 20:48:49 [INFO]: Epoch 046 - training loss: 181251.9155, validation loss: 2.5655
2024-06-02 20:48:53 [INFO]: Epoch 047 - training loss: 181240.1285, validation loss: 2.5664
2024-06-02 20:48:57 [INFO]: Epoch 048 - training loss: 181234.4253, validation loss: 2.5591
2024-06-02 20:49:01 [INFO]: Epoch 049 - training loss: 181236.5943, validation loss: 2.5624
2024-06-02 20:49:05 [INFO]: Epoch 050 - training loss: 181240.8646, validation loss: 2.5510
2024-06-02 20:49:09 [INFO]: Epoch 051 - training loss: 181231.3681, validation loss: 2.5562
2024-06-02 20:49:12 [INFO]: Epoch 052 - training loss: 181223.7622, validation loss: 2.5466
2024-06-02 20:49:16 [INFO]: Epoch 053 - training loss: 181221.0150, validation loss: 2.5629
2024-06-02 20:49:20 [INFO]: Epoch 054 - training loss: 181208.9878, validation loss: 2.5749
2024-06-02 20:49:24 [INFO]: Epoch 055 - training loss: 181205.2135, validation loss: 2.5694
2024-06-02 20:49:28 [INFO]: Epoch 056 - training loss: 181198.8420, validation loss: 2.5555
2024-06-02 20:49:32 [INFO]: Epoch 057 - training loss: 181217.5035, validation loss: 2.5593
2024-06-02 20:49:36 [INFO]: Epoch 058 - training loss: 181218.3490, validation loss: 2.5665
2024-06-02 20:49:40 [INFO]: Epoch 059 - training loss: 181192.2940, validation loss: 2.5421
2024-06-02 20:49:44 [INFO]: Epoch 060 - training loss: 181171.4948, validation loss: 2.5489
2024-06-02 20:49:48 [INFO]: Epoch 061 - training loss: 181167.6730, validation loss: 2.5523
2024-06-02 20:49:52 [INFO]: Epoch 062 - training loss: 181163.8449, validation loss: 2.5481
2024-06-02 20:49:56 [INFO]: Epoch 063 - training loss: 181168.2662, validation loss: 2.5422
2024-06-02 20:50:00 [INFO]: Epoch 064 - training loss: 181160.3692, validation loss: 2.5352
2024-06-02 20:50:03 [INFO]: Epoch 065 - training loss: 181157.2986, validation loss: 2.5236
2024-06-02 20:50:07 [INFO]: Epoch 066 - training loss: 181156.3368, validation loss: 2.5427
2024-06-02 20:50:11 [INFO]: Epoch 067 - training loss: 181155.1123, validation loss: 2.5155
2024-06-02 20:50:15 [INFO]: Epoch 068 - training loss: 181144.6997, validation loss: 2.5215
2024-06-02 20:50:19 [INFO]: Epoch 069 - training loss: 181149.0185, validation loss: 2.5171
2024-06-02 20:50:23 [INFO]: Epoch 070 - training loss: 181144.2303, validation loss: 2.5351
2024-06-02 20:50:27 [INFO]: Epoch 071 - training loss: 181136.5723, validation loss: 2.5063
2024-06-02 20:50:30 [INFO]: Epoch 072 - training loss: 181135.3721, validation loss: 2.5083
2024-06-02 20:50:34 [INFO]: Epoch 073 - training loss: 181139.7888, validation loss: 2.5108
2024-06-02 20:50:38 [INFO]: Epoch 074 - training loss: 181135.7656, validation loss: 2.4872
2024-06-02 20:50:42 [INFO]: Epoch 075 - training loss: 181158.2054, validation loss: 2.4873
2024-06-02 20:50:46 [INFO]: Epoch 076 - training loss: 181124.4861, validation loss: 2.5029
2024-06-02 20:50:49 [INFO]: Epoch 077 - training loss: 181123.6047, validation loss: 2.5052
2024-06-02 20:50:53 [INFO]: Epoch 078 - training loss: 181126.5943, validation loss: 2.4652
2024-06-02 20:50:57 [INFO]: Epoch 079 - training loss: 181118.8866, validation loss: 2.4669
2024-06-02 20:51:01 [INFO]: Epoch 080 - training loss: 181111.4074, validation loss: 2.4634
2024-06-02 20:51:05 [INFO]: Epoch 081 - training loss: 181109.9363, validation loss: 2.4464
2024-06-02 20:51:08 [INFO]: Epoch 082 - training loss: 181103.2199, validation loss: 2.4415
2024-06-02 20:51:12 [INFO]: Epoch 083 - training loss: 181104.6314, validation loss: 2.4442
2024-06-02 20:51:16 [INFO]: Epoch 084 - training loss: 181116.3160, validation loss: 2.4437
2024-06-02 20:51:20 [INFO]: Epoch 085 - training loss: 181129.3079, validation loss: 2.4281
2024-06-02 20:51:24 [INFO]: Epoch 086 - training loss: 181122.6429, validation loss: 2.4785
2024-06-02 20:51:27 [INFO]: Epoch 087 - training loss: 181124.4965, validation loss: 2.3864
2024-06-02 20:51:31 [INFO]: Epoch 088 - training loss: 181097.9566, validation loss: 2.3907
2024-06-02 20:51:35 [INFO]: Epoch 089 - training loss: 181092.5538, validation loss: 2.3771
2024-06-02 20:51:39 [INFO]: Epoch 090 - training loss: 181093.6389, validation loss: 2.3636
2024-06-02 20:51:43 [INFO]: Epoch 091 - training loss: 181085.6968, validation loss: 2.3395
2024-06-02 20:51:47 [INFO]: Epoch 092 - training loss: 181089.2668, validation loss: 2.3510
2024-06-02 20:51:50 [INFO]: Epoch 093 - training loss: 181090.8507, validation loss: 2.3782
2024-06-02 20:51:54 [INFO]: Epoch 094 - training loss: 181079.3380, validation loss: 2.3350
2024-06-02 20:51:58 [INFO]: Epoch 095 - training loss: 181087.2014, validation loss: 2.3335
2024-06-02 20:52:02 [INFO]: Epoch 096 - training loss: 181084.8264, validation loss: 2.3284
2024-06-02 20:52:06 [INFO]: Epoch 097 - training loss: 181088.6019, validation loss: 2.3273
2024-06-02 20:52:09 [INFO]: Epoch 098 - training loss: 181077.7789, validation loss: 2.3083
2024-06-02 20:52:13 [INFO]: Epoch 099 - training loss: 181085.6736, validation loss: 2.2929
2024-06-02 20:52:17 [INFO]: Epoch 100 - training loss: 181088.2459, validation loss: 2.2964
2024-06-02 20:52:17 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 20:52:17 [INFO]: Saved the model to results_point_rate05/Electricity/GPVAE_Electricity/round_2/20240602_T204246/GPVAE.pypots
2024-06-02 20:52:26 [INFO]: Successfully saved to results_point_rate05/Electricity/GPVAE_Electricity/round_2/imputation.pkl
2024-06-02 20:52:26 [INFO]: Round2 - GPVAE on Electricity: MAE=1.1555, MSE=2.9398, MRE=0.6187
2024-06-02 20:52:26 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 20:52:26 [INFO]: Using the given device: cuda:0
2024-06-02 20:52:26 [INFO]: Model files will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_3/20240602_T205226
2024-06-02 20:52:26 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_3/20240602_T205226/tensorboard
2024-06-02 20:52:26 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,825,022
2024-06-02 20:52:29 [INFO]: Epoch 001 - training loss: 214119.8721, validation loss: 3.3444
2024-06-02 20:52:33 [INFO]: Epoch 002 - training loss: 185087.1620, validation loss: 3.0265
2024-06-02 20:52:37 [INFO]: Epoch 003 - training loss: 183162.4369, validation loss: 2.9209
2024-06-02 20:52:41 [INFO]: Epoch 004 - training loss: 182575.3137, validation loss: 2.8469
2024-06-02 20:52:44 [INFO]: Epoch 005 - training loss: 182333.8067, validation loss: 2.8107
2024-06-02 20:52:48 [INFO]: Epoch 006 - training loss: 182071.7685, validation loss: 2.7703
2024-06-02 20:52:52 [INFO]: Epoch 007 - training loss: 181930.1661, validation loss: 2.7282
2024-06-02 20:52:55 [INFO]: Epoch 008 - training loss: 181853.3819, validation loss: 2.6998
2024-06-02 20:52:59 [INFO]: Epoch 009 - training loss: 181793.5069, validation loss: 2.6772
2024-06-02 20:53:03 [INFO]: Epoch 010 - training loss: 181734.4479, validation loss: 2.6739
2024-06-02 20:53:06 [INFO]: Epoch 011 - training loss: 181700.2604, validation loss: 2.6605
2024-06-02 20:53:10 [INFO]: Epoch 012 - training loss: 181666.2801, validation loss: 2.6557
2024-06-02 20:53:14 [INFO]: Epoch 013 - training loss: 181645.4468, validation loss: 2.6510
2024-06-02 20:53:18 [INFO]: Epoch 014 - training loss: 181619.7344, validation loss: 2.6577
2024-06-02 20:53:21 [INFO]: Epoch 015 - training loss: 181590.5943, validation loss: 2.6727
2024-06-02 20:53:25 [INFO]: Epoch 016 - training loss: 181571.2905, validation loss: 2.6525
2024-06-02 20:53:29 [INFO]: Epoch 017 - training loss: 181534.2853, validation loss: 2.6543
2024-06-02 20:53:33 [INFO]: Epoch 018 - training loss: 181506.8374, validation loss: 2.6254
2024-06-02 20:53:37 [INFO]: Epoch 019 - training loss: 181499.1250, validation loss: 2.6559
2024-06-02 20:53:41 [INFO]: Epoch 020 - training loss: 181458.3108, validation loss: 2.6292
2024-06-02 20:53:44 [INFO]: Epoch 021 - training loss: 181463.2095, validation loss: 2.6370
2024-06-02 20:53:48 [INFO]: Epoch 022 - training loss: 181441.9178, validation loss: 2.6342
2024-06-02 20:53:52 [INFO]: Epoch 023 - training loss: 181438.3756, validation loss: 2.6365
2024-06-02 20:53:56 [INFO]: Epoch 024 - training loss: 181421.2824, validation loss: 2.6272
2024-06-02 20:54:00 [INFO]: Epoch 025 - training loss: 181402.6360, validation loss: 2.6107
2024-06-02 20:54:04 [INFO]: Epoch 026 - training loss: 181386.8229, validation loss: 2.6237
2024-06-02 20:54:08 [INFO]: Epoch 027 - training loss: 181377.0168, validation loss: 2.6259
2024-06-02 20:54:12 [INFO]: Epoch 028 - training loss: 181378.0168, validation loss: 2.6273
2024-06-02 20:54:16 [INFO]: Epoch 029 - training loss: 181355.0550, validation loss: 2.6152
2024-06-02 20:54:19 [INFO]: Epoch 030 - training loss: 181357.8976, validation loss: 2.6053
2024-06-02 20:54:23 [INFO]: Epoch 031 - training loss: 181344.2124, validation loss: 2.6183
2024-06-02 20:54:27 [INFO]: Epoch 032 - training loss: 181325.4225, validation loss: 2.6173
2024-06-02 20:54:31 [INFO]: Epoch 033 - training loss: 181307.0694, validation loss: 2.6063
2024-06-02 20:54:35 [INFO]: Epoch 034 - training loss: 181300.4300, validation loss: 2.5935
2024-06-02 20:54:39 [INFO]: Epoch 035 - training loss: 181290.8194, validation loss: 2.6229
2024-06-02 20:54:43 [INFO]: Epoch 036 - training loss: 181291.2309, validation loss: 2.6152
2024-06-02 20:54:47 [INFO]: Epoch 037 - training loss: 181278.0081, validation loss: 2.6035
2024-06-02 20:54:51 [INFO]: Epoch 038 - training loss: 181271.5307, validation loss: 2.6089
2024-06-02 20:54:55 [INFO]: Epoch 039 - training loss: 181274.6250, validation loss: 2.6051
2024-06-02 20:54:58 [INFO]: Epoch 040 - training loss: 181257.3565, validation loss: 2.6144
2024-06-02 20:55:02 [INFO]: Epoch 041 - training loss: 181265.9954, validation loss: 2.6091
2024-06-02 20:55:06 [INFO]: Epoch 042 - training loss: 181266.1661, validation loss: 2.5941
2024-06-02 20:55:10 [INFO]: Epoch 043 - training loss: 181250.0677, validation loss: 2.6107
2024-06-02 20:55:14 [INFO]: Epoch 044 - training loss: 181254.0556, validation loss: 2.6114
2024-06-02 20:55:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:55:14 [INFO]: Finished training. The best model is from epoch#34.
2024-06-02 20:55:14 [INFO]: Saved the model to results_point_rate05/Electricity/GPVAE_Electricity/round_3/20240602_T205226/GPVAE.pypots
2024-06-02 20:55:23 [INFO]: Successfully saved to results_point_rate05/Electricity/GPVAE_Electricity/round_3/imputation.pkl
2024-06-02 20:55:23 [INFO]: Round3 - GPVAE on Electricity: MAE=1.0708, MSE=3.0223, MRE=0.5733
2024-06-02 20:55:23 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 20:55:23 [INFO]: Using the given device: cuda:0
2024-06-02 20:55:23 [INFO]: Model files will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_4/20240602_T205523
2024-06-02 20:55:23 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/GPVAE_Electricity/round_4/20240602_T205523/tensorboard
2024-06-02 20:55:23 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,825,022
2024-06-02 20:55:27 [INFO]: Epoch 001 - training loss: 213748.7396, validation loss: 3.3073
2024-06-02 20:55:30 [INFO]: Epoch 002 - training loss: 185509.8767, validation loss: 3.0331
2024-06-02 20:55:34 [INFO]: Epoch 003 - training loss: 183172.2824, validation loss: 2.9342
2024-06-02 20:55:38 [INFO]: Epoch 004 - training loss: 182586.6840, validation loss: 2.8695
2024-06-02 20:55:41 [INFO]: Epoch 005 - training loss: 182316.1418, validation loss: 2.8457
2024-06-02 20:55:45 [INFO]: Epoch 006 - training loss: 182136.1308, validation loss: 2.7839
2024-06-02 20:55:49 [INFO]: Epoch 007 - training loss: 182017.8802, validation loss: 2.7183
2024-06-02 20:55:53 [INFO]: Epoch 008 - training loss: 181918.4381, validation loss: 2.6916
2024-06-02 20:55:56 [INFO]: Epoch 009 - training loss: 181834.6499, validation loss: 2.6678
2024-06-02 20:56:00 [INFO]: Epoch 010 - training loss: 181796.0839, validation loss: 2.6501
2024-06-02 20:56:04 [INFO]: Epoch 011 - training loss: 181769.1678, validation loss: 2.6410
2024-06-02 20:56:08 [INFO]: Epoch 012 - training loss: 181713.3756, validation loss: 2.6279
2024-06-02 20:56:11 [INFO]: Epoch 013 - training loss: 181695.9062, validation loss: 2.6342
2024-06-02 20:56:15 [INFO]: Epoch 014 - training loss: 181651.9201, validation loss: 2.6338
2024-06-02 20:56:19 [INFO]: Epoch 015 - training loss: 181609.3825, validation loss: 2.6466
2024-06-02 20:56:23 [INFO]: Epoch 016 - training loss: 181590.0226, validation loss: 2.6268
2024-06-02 20:56:26 [INFO]: Epoch 017 - training loss: 181566.8021, validation loss: 2.6369
2024-06-02 20:56:30 [INFO]: Epoch 018 - training loss: 181547.1707, validation loss: 2.6386
2024-06-02 20:56:34 [INFO]: Epoch 019 - training loss: 181519.1730, validation loss: 2.6224
2024-06-02 20:56:38 [INFO]: Epoch 020 - training loss: 181527.6152, validation loss: 2.6431
2024-06-02 20:56:41 [INFO]: Epoch 021 - training loss: 181524.3310, validation loss: 2.6309
2024-06-02 20:56:45 [INFO]: Epoch 022 - training loss: 181473.4583, validation loss: 2.6173
2024-06-02 20:56:49 [INFO]: Epoch 023 - training loss: 181456.4578, validation loss: 2.6305
2024-06-02 20:56:53 [INFO]: Epoch 024 - training loss: 181472.3600, validation loss: 2.6356
2024-06-02 20:56:56 [INFO]: Epoch 025 - training loss: 181454.7222, validation loss: 2.6092
2024-06-02 20:57:00 [INFO]: Epoch 026 - training loss: 181427.5990, validation loss: 2.6108
2024-06-02 20:57:04 [INFO]: Epoch 027 - training loss: 181413.1921, validation loss: 2.6032
2024-06-02 20:57:08 [INFO]: Epoch 028 - training loss: 181401.1042, validation loss: 2.6038
2024-06-02 20:57:11 [INFO]: Epoch 029 - training loss: 181389.7240, validation loss: 2.6114
2024-06-02 20:57:15 [INFO]: Epoch 030 - training loss: 181393.3438, validation loss: 2.5935
2024-06-02 20:57:19 [INFO]: Epoch 031 - training loss: 181374.2731, validation loss: 2.5853
2024-06-02 20:57:23 [INFO]: Epoch 032 - training loss: 181367.3686, validation loss: 2.6163
2024-06-02 20:57:26 [INFO]: Epoch 033 - training loss: 181361.1811, validation loss: 2.5962
2024-06-02 20:57:30 [INFO]: Epoch 034 - training loss: 181361.3027, validation loss: 2.5954
2024-06-02 20:57:34 [INFO]: Epoch 035 - training loss: 181349.2066, validation loss: 2.5956
2024-06-02 20:57:38 [INFO]: Epoch 036 - training loss: 181326.6042, validation loss: 2.5864
2024-06-02 20:57:41 [INFO]: Epoch 037 - training loss: 181320.3565, validation loss: 2.5983
2024-06-02 20:57:45 [INFO]: Epoch 038 - training loss: 181319.6215, validation loss: 2.5866
2024-06-02 20:57:49 [INFO]: Epoch 039 - training loss: 181330.1852, validation loss: 2.5921
2024-06-02 20:57:53 [INFO]: Epoch 040 - training loss: 181322.8791, validation loss: 2.5906
2024-06-02 20:57:56 [INFO]: Epoch 041 - training loss: 181316.8281, validation loss: 2.5996
2024-06-02 20:57:56 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-02 20:57:56 [INFO]: Finished training. The best model is from epoch#31.
2024-06-02 20:57:57 [INFO]: Saved the model to results_point_rate05/Electricity/GPVAE_Electricity/round_4/20240602_T205523/GPVAE.pypots
2024-06-02 20:58:05 [INFO]: Successfully saved to results_point_rate05/Electricity/GPVAE_Electricity/round_4/imputation.pkl
2024-06-02 20:58:05 [INFO]: Round4 - GPVAE on Electricity: MAE=1.0743, MSE=2.9151, MRE=0.5752
2024-06-02 20:58:05 [INFO]: Done! Final results:
Averaged GPVAE (1,825,022 params) on Electricity: MAE=1.0990 ± 0.031926359829787095, MSE=2.9731 ± 0.03951843818625224, MRE=0.5884 ± 0.017093584078991385, average inference time=16.73
