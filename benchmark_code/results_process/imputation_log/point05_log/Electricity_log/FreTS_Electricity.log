2024-06-02 19:59:13 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-02 19:59:13 [INFO]: Using the given device: cuda:0
2024-06-02 19:59:13 [INFO]: Model files will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_0/20240602_T195913
2024-06-02 19:59:13 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_0/20240602_T195913/tensorboard
2024-06-02 19:59:14 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 19:59:22 [INFO]: Epoch 001 - training loss: 1.2135, validation loss: 3.6734
2024-06-02 19:59:29 [INFO]: Epoch 002 - training loss: 0.7947, validation loss: 3.1779
2024-06-02 19:59:37 [INFO]: Epoch 003 - training loss: 0.6883, validation loss: 2.9610
2024-06-02 19:59:44 [INFO]: Epoch 004 - training loss: 0.6449, validation loss: 2.9667
2024-06-02 19:59:52 [INFO]: Epoch 005 - training loss: 0.6185, validation loss: 2.7991
2024-06-02 20:00:00 [INFO]: Epoch 006 - training loss: 0.5970, validation loss: 2.6341
2024-06-02 20:00:08 [INFO]: Epoch 007 - training loss: 0.5821, validation loss: 2.5585
2024-06-02 20:00:16 [INFO]: Epoch 008 - training loss: 0.5735, validation loss: 2.5986
2024-06-02 20:00:24 [INFO]: Epoch 009 - training loss: 0.5583, validation loss: 2.5167
2024-06-02 20:00:31 [INFO]: Epoch 010 - training loss: 0.5488, validation loss: 2.4487
2024-06-02 20:00:39 [INFO]: Epoch 011 - training loss: 0.5434, validation loss: 2.4421
2024-06-02 20:00:46 [INFO]: Epoch 012 - training loss: 0.5377, validation loss: 2.3841
2024-06-02 20:00:54 [INFO]: Epoch 013 - training loss: 0.5287, validation loss: 2.3907
2024-06-02 20:01:02 [INFO]: Epoch 014 - training loss: 0.5220, validation loss: 2.3377
2024-06-02 20:01:10 [INFO]: Epoch 015 - training loss: 0.5193, validation loss: 2.3388
2024-06-02 20:01:17 [INFO]: Epoch 016 - training loss: 0.5159, validation loss: 2.2865
2024-06-02 20:01:25 [INFO]: Epoch 017 - training loss: 0.5092, validation loss: 2.2678
2024-06-02 20:01:33 [INFO]: Epoch 018 - training loss: 0.5049, validation loss: 2.2596
2024-06-02 20:01:40 [INFO]: Epoch 019 - training loss: 0.5045, validation loss: 2.2541
2024-06-02 20:01:48 [INFO]: Epoch 020 - training loss: 0.4986, validation loss: 2.1791
2024-06-02 20:01:55 [INFO]: Epoch 021 - training loss: 0.4949, validation loss: 2.2325
2024-06-02 20:02:03 [INFO]: Epoch 022 - training loss: 0.4933, validation loss: 2.1803
2024-06-02 20:02:11 [INFO]: Epoch 023 - training loss: 0.4883, validation loss: 2.2032
2024-06-02 20:02:18 [INFO]: Epoch 024 - training loss: 0.4877, validation loss: 2.1926
2024-06-02 20:02:26 [INFO]: Epoch 025 - training loss: 0.4861, validation loss: 2.1741
2024-06-02 20:02:34 [INFO]: Epoch 026 - training loss: 0.4825, validation loss: 2.1209
2024-06-02 20:02:42 [INFO]: Epoch 027 - training loss: 0.4816, validation loss: 2.0971
2024-06-02 20:02:49 [INFO]: Epoch 028 - training loss: 0.4826, validation loss: 2.1150
2024-06-02 20:02:57 [INFO]: Epoch 029 - training loss: 0.4769, validation loss: 2.1109
2024-06-02 20:03:05 [INFO]: Epoch 030 - training loss: 0.4747, validation loss: 2.0930
2024-06-02 20:03:12 [INFO]: Epoch 031 - training loss: 0.4727, validation loss: 2.0976
2024-06-02 20:03:20 [INFO]: Epoch 032 - training loss: 0.4758, validation loss: 2.0704
2024-06-02 20:03:28 [INFO]: Epoch 033 - training loss: 0.4692, validation loss: 2.0931
2024-06-02 20:03:35 [INFO]: Epoch 034 - training loss: 0.4677, validation loss: 2.0492
2024-06-02 20:03:43 [INFO]: Epoch 035 - training loss: 0.4661, validation loss: 2.0374
2024-06-02 20:03:51 [INFO]: Epoch 036 - training loss: 0.4660, validation loss: 2.0353
2024-06-02 20:03:59 [INFO]: Epoch 037 - training loss: 0.4641, validation loss: 2.0449
2024-06-02 20:04:07 [INFO]: Epoch 038 - training loss: 0.4635, validation loss: 1.9980
2024-06-02 20:04:15 [INFO]: Epoch 039 - training loss: 0.4625, validation loss: 1.9632
2024-06-02 20:04:22 [INFO]: Epoch 040 - training loss: 0.4605, validation loss: 1.9748
2024-06-02 20:04:30 [INFO]: Epoch 041 - training loss: 0.4602, validation loss: 1.9621
2024-06-02 20:04:38 [INFO]: Epoch 042 - training loss: 0.4597, validation loss: 1.9524
2024-06-02 20:04:46 [INFO]: Epoch 043 - training loss: 0.4583, validation loss: 1.9620
2024-06-02 20:04:53 [INFO]: Epoch 044 - training loss: 0.4556, validation loss: 1.9488
2024-06-02 20:05:01 [INFO]: Epoch 045 - training loss: 0.4567, validation loss: 1.9344
2024-06-02 20:05:08 [INFO]: Epoch 046 - training loss: 0.4551, validation loss: 1.9544
2024-06-02 20:05:15 [INFO]: Epoch 047 - training loss: 0.4545, validation loss: 1.9326
2024-06-02 20:05:23 [INFO]: Epoch 048 - training loss: 0.4560, validation loss: 1.9095
2024-06-02 20:05:31 [INFO]: Epoch 049 - training loss: 0.4518, validation loss: 1.9016
2024-06-02 20:05:39 [INFO]: Epoch 050 - training loss: 0.4507, validation loss: 1.8940
2024-06-02 20:05:46 [INFO]: Epoch 051 - training loss: 0.4494, validation loss: 1.8703
2024-06-02 20:05:55 [INFO]: Epoch 052 - training loss: 0.4500, validation loss: 1.8779
2024-06-02 20:06:02 [INFO]: Epoch 053 - training loss: 0.4498, validation loss: 1.8890
2024-06-02 20:06:10 [INFO]: Epoch 054 - training loss: 0.4495, validation loss: 1.8212
2024-06-02 20:06:18 [INFO]: Epoch 055 - training loss: 0.4473, validation loss: 1.8312
2024-06-02 20:06:26 [INFO]: Epoch 056 - training loss: 0.4467, validation loss: 1.8298
2024-06-02 20:06:34 [INFO]: Epoch 057 - training loss: 0.4467, validation loss: 1.8085
2024-06-02 20:06:42 [INFO]: Epoch 058 - training loss: 0.4455, validation loss: 1.8149
2024-06-02 20:06:49 [INFO]: Epoch 059 - training loss: 0.4447, validation loss: 1.7913
2024-06-02 20:06:57 [INFO]: Epoch 060 - training loss: 0.4435, validation loss: 1.7800
2024-06-02 20:07:05 [INFO]: Epoch 061 - training loss: 0.4423, validation loss: 1.7439
2024-06-02 20:07:13 [INFO]: Epoch 062 - training loss: 0.4423, validation loss: 1.7684
2024-06-02 20:07:20 [INFO]: Epoch 063 - training loss: 0.4415, validation loss: 1.7600
2024-06-02 20:07:28 [INFO]: Epoch 064 - training loss: 0.4421, validation loss: 1.7326
2024-06-02 20:07:36 [INFO]: Epoch 065 - training loss: 0.4435, validation loss: 1.7603
2024-06-02 20:07:43 [INFO]: Epoch 066 - training loss: 0.4436, validation loss: 1.7422
2024-06-02 20:07:51 [INFO]: Epoch 067 - training loss: 0.4389, validation loss: 1.7144
2024-06-02 20:07:58 [INFO]: Epoch 068 - training loss: 0.4389, validation loss: 1.7092
2024-06-02 20:08:05 [INFO]: Epoch 069 - training loss: 0.4378, validation loss: 1.7204
2024-06-02 20:08:12 [INFO]: Epoch 070 - training loss: 0.4382, validation loss: 1.7106
2024-06-02 20:08:18 [INFO]: Epoch 071 - training loss: 0.4363, validation loss: 1.6937
2024-06-02 20:08:26 [INFO]: Epoch 072 - training loss: 0.4363, validation loss: 1.6950
2024-06-02 20:08:34 [INFO]: Epoch 073 - training loss: 0.4360, validation loss: 1.7051
2024-06-02 20:08:42 [INFO]: Epoch 074 - training loss: 0.4374, validation loss: 1.6613
2024-06-02 20:08:49 [INFO]: Epoch 075 - training loss: 0.4360, validation loss: 1.6556
2024-06-02 20:08:57 [INFO]: Epoch 076 - training loss: 0.4350, validation loss: 1.6481
2024-06-02 20:09:05 [INFO]: Epoch 077 - training loss: 0.4346, validation loss: 1.6354
2024-06-02 20:09:13 [INFO]: Epoch 078 - training loss: 0.4347, validation loss: 1.6235
2024-06-02 20:09:21 [INFO]: Epoch 079 - training loss: 0.4355, validation loss: 1.6218
2024-06-02 20:09:28 [INFO]: Epoch 080 - training loss: 0.4345, validation loss: 1.6024
2024-06-02 20:09:37 [INFO]: Epoch 081 - training loss: 0.4349, validation loss: 1.5921
2024-06-02 20:09:44 [INFO]: Epoch 082 - training loss: 0.4333, validation loss: 1.6168
2024-06-02 20:09:51 [INFO]: Epoch 083 - training loss: 0.4330, validation loss: 1.6042
2024-06-02 20:09:59 [INFO]: Epoch 084 - training loss: 0.4320, validation loss: 1.5588
2024-06-02 20:10:06 [INFO]: Epoch 085 - training loss: 0.4323, validation loss: 1.5706
2024-06-02 20:10:14 [INFO]: Epoch 086 - training loss: 0.4310, validation loss: 1.5496
2024-06-02 20:10:22 [INFO]: Epoch 087 - training loss: 0.4319, validation loss: 1.5578
2024-06-02 20:10:30 [INFO]: Epoch 088 - training loss: 0.4302, validation loss: 1.5546
2024-06-02 20:10:38 [INFO]: Epoch 089 - training loss: 0.4308, validation loss: 1.5425
2024-06-02 20:10:45 [INFO]: Epoch 090 - training loss: 0.4339, validation loss: 1.5371
2024-06-02 20:10:53 [INFO]: Epoch 091 - training loss: 0.4344, validation loss: 1.5484
2024-06-02 20:11:01 [INFO]: Epoch 092 - training loss: 0.4320, validation loss: 1.5294
2024-06-02 20:11:08 [INFO]: Epoch 093 - training loss: 0.4298, validation loss: 1.5278
2024-06-02 20:11:16 [INFO]: Epoch 094 - training loss: 0.4295, validation loss: 1.5147
2024-06-02 20:11:24 [INFO]: Epoch 095 - training loss: 0.4297, validation loss: 1.5116
2024-06-02 20:11:32 [INFO]: Epoch 096 - training loss: 0.4287, validation loss: 1.5256
2024-06-02 20:11:39 [INFO]: Epoch 097 - training loss: 0.4281, validation loss: 1.5391
2024-06-02 20:11:47 [INFO]: Epoch 098 - training loss: 0.4279, validation loss: 1.5175
2024-06-02 20:11:55 [INFO]: Epoch 099 - training loss: 0.4268, validation loss: 1.5042
2024-06-02 20:12:03 [INFO]: Epoch 100 - training loss: 0.4270, validation loss: 1.5233
2024-06-02 20:12:03 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 20:12:03 [INFO]: Saved the model to results_point_rate05/Electricity/FreTS_Electricity/round_0/20240602_T195913/FreTS.pypots
2024-06-02 20:12:04 [INFO]: Successfully saved to results_point_rate05/Electricity/FreTS_Electricity/round_0/imputation.pkl
2024-06-02 20:12:04 [INFO]: Round0 - FreTS on Electricity: MAE=1.0201, MSE=1.8255, MRE=0.5462
2024-06-02 20:12:04 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-02 20:12:04 [INFO]: Using the given device: cuda:0
2024-06-02 20:12:04 [INFO]: Model files will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_1/20240602_T201204
2024-06-02 20:12:04 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_1/20240602_T201204/tensorboard
2024-06-02 20:12:04 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 20:12:12 [INFO]: Epoch 001 - training loss: 1.2927, validation loss: 4.0371
2024-06-02 20:12:20 [INFO]: Epoch 002 - training loss: 0.8192, validation loss: 3.1845
2024-06-02 20:12:27 [INFO]: Epoch 003 - training loss: 0.7046, validation loss: 2.9244
2024-06-02 20:12:35 [INFO]: Epoch 004 - training loss: 0.6487, validation loss: 2.7258
2024-06-02 20:12:43 [INFO]: Epoch 005 - training loss: 0.6188, validation loss: 2.5901
2024-06-02 20:12:50 [INFO]: Epoch 006 - training loss: 0.5996, validation loss: 2.5381
2024-06-02 20:12:58 [INFO]: Epoch 007 - training loss: 0.5846, validation loss: 2.4837
2024-06-02 20:13:05 [INFO]: Epoch 008 - training loss: 0.5714, validation loss: 2.4391
2024-06-02 20:13:13 [INFO]: Epoch 009 - training loss: 0.5603, validation loss: 2.4139
2024-06-02 20:13:21 [INFO]: Epoch 010 - training loss: 0.5525, validation loss: 2.3674
2024-06-02 20:13:28 [INFO]: Epoch 011 - training loss: 0.5451, validation loss: 2.3437
2024-06-02 20:13:36 [INFO]: Epoch 012 - training loss: 0.5367, validation loss: 2.3027
2024-06-02 20:13:44 [INFO]: Epoch 013 - training loss: 0.5293, validation loss: 2.2693
2024-06-02 20:13:52 [INFO]: Epoch 014 - training loss: 0.5255, validation loss: 2.2170
2024-06-02 20:13:59 [INFO]: Epoch 015 - training loss: 0.5214, validation loss: 2.2404
2024-06-02 20:14:07 [INFO]: Epoch 016 - training loss: 0.5143, validation loss: 2.2140
2024-06-02 20:14:15 [INFO]: Epoch 017 - training loss: 0.5084, validation loss: 2.1553
2024-06-02 20:14:22 [INFO]: Epoch 018 - training loss: 0.5072, validation loss: 2.1609
2024-06-02 20:14:30 [INFO]: Epoch 019 - training loss: 0.5058, validation loss: 2.1703
2024-06-02 20:14:38 [INFO]: Epoch 020 - training loss: 0.5076, validation loss: 2.1119
2024-06-02 20:14:45 [INFO]: Epoch 021 - training loss: 0.4959, validation loss: 2.0988
2024-06-02 20:14:53 [INFO]: Epoch 022 - training loss: 0.4931, validation loss: 2.0726
2024-06-02 20:15:01 [INFO]: Epoch 023 - training loss: 0.4912, validation loss: 2.0794
2024-06-02 20:15:08 [INFO]: Epoch 024 - training loss: 0.4873, validation loss: 2.0526
2024-06-02 20:15:16 [INFO]: Epoch 025 - training loss: 0.4854, validation loss: 2.0321
2024-06-02 20:15:24 [INFO]: Epoch 026 - training loss: 0.4827, validation loss: 2.0162
2024-06-02 20:15:32 [INFO]: Epoch 027 - training loss: 0.4818, validation loss: 2.0261
2024-06-02 20:15:39 [INFO]: Epoch 028 - training loss: 0.4794, validation loss: 1.9783
2024-06-02 20:15:47 [INFO]: Epoch 029 - training loss: 0.4809, validation loss: 1.9680
2024-06-02 20:15:54 [INFO]: Epoch 030 - training loss: 0.4748, validation loss: 1.9409
2024-06-02 20:16:02 [INFO]: Epoch 031 - training loss: 0.4743, validation loss: 1.9369
2024-06-02 20:16:10 [INFO]: Epoch 032 - training loss: 0.4752, validation loss: 1.9221
2024-06-02 20:16:18 [INFO]: Epoch 033 - training loss: 0.4716, validation loss: 1.9108
2024-06-02 20:16:24 [INFO]: Epoch 034 - training loss: 0.4696, validation loss: 1.9186
2024-06-02 20:16:31 [INFO]: Epoch 035 - training loss: 0.4680, validation loss: 1.9175
2024-06-02 20:16:39 [INFO]: Epoch 036 - training loss: 0.4664, validation loss: 1.8937
2024-06-02 20:16:47 [INFO]: Epoch 037 - training loss: 0.4656, validation loss: 1.8729
2024-06-02 20:16:54 [INFO]: Epoch 038 - training loss: 0.4654, validation loss: 1.8549
2024-06-02 20:17:02 [INFO]: Epoch 039 - training loss: 0.4652, validation loss: 1.8496
2024-06-02 20:17:09 [INFO]: Epoch 040 - training loss: 0.4609, validation loss: 1.8237
2024-06-02 20:17:17 [INFO]: Epoch 041 - training loss: 0.4597, validation loss: 1.8207
2024-06-02 20:17:25 [INFO]: Epoch 042 - training loss: 0.4600, validation loss: 1.8050
2024-06-02 20:17:33 [INFO]: Epoch 043 - training loss: 0.4600, validation loss: 1.7810
2024-06-02 20:17:40 [INFO]: Epoch 044 - training loss: 0.4575, validation loss: 1.7942
2024-06-02 20:17:48 [INFO]: Epoch 045 - training loss: 0.4567, validation loss: 1.7505
2024-06-02 20:17:56 [INFO]: Epoch 046 - training loss: 0.4566, validation loss: 1.7758
2024-06-02 20:18:04 [INFO]: Epoch 047 - training loss: 0.4571, validation loss: 1.7492
2024-06-02 20:18:11 [INFO]: Epoch 048 - training loss: 0.4542, validation loss: 1.7332
2024-06-02 20:18:19 [INFO]: Epoch 049 - training loss: 0.4532, validation loss: 1.7329
2024-06-02 20:18:26 [INFO]: Epoch 050 - training loss: 0.4523, validation loss: 1.7262
2024-06-02 20:18:34 [INFO]: Epoch 051 - training loss: 0.4509, validation loss: 1.6967
2024-06-02 20:18:42 [INFO]: Epoch 052 - training loss: 0.4516, validation loss: 1.6842
2024-06-02 20:18:49 [INFO]: Epoch 053 - training loss: 0.4505, validation loss: 1.6848
2024-06-02 20:18:57 [INFO]: Epoch 054 - training loss: 0.4487, validation loss: 1.6718
2024-06-02 20:19:04 [INFO]: Epoch 055 - training loss: 0.4496, validation loss: 1.6617
2024-06-02 20:19:12 [INFO]: Epoch 056 - training loss: 0.4499, validation loss: 1.6378
2024-06-02 20:19:20 [INFO]: Epoch 057 - training loss: 0.4471, validation loss: 1.6355
2024-06-02 20:19:28 [INFO]: Epoch 058 - training loss: 0.4466, validation loss: 1.6383
2024-06-02 20:19:36 [INFO]: Epoch 059 - training loss: 0.4467, validation loss: 1.6265
2024-06-02 20:19:44 [INFO]: Epoch 060 - training loss: 0.4467, validation loss: 1.5950
2024-06-02 20:19:51 [INFO]: Epoch 061 - training loss: 0.4457, validation loss: 1.5935
2024-06-02 20:19:58 [INFO]: Epoch 062 - training loss: 0.4460, validation loss: 1.5826
2024-06-02 20:20:06 [INFO]: Epoch 063 - training loss: 0.4437, validation loss: 1.5701
2024-06-02 20:20:13 [INFO]: Epoch 064 - training loss: 0.4438, validation loss: 1.5667
2024-06-02 20:20:21 [INFO]: Epoch 065 - training loss: 0.4433, validation loss: 1.5426
2024-06-02 20:20:28 [INFO]: Epoch 066 - training loss: 0.4430, validation loss: 1.5603
2024-06-02 20:20:36 [INFO]: Epoch 067 - training loss: 0.4466, validation loss: 1.5539
2024-06-02 20:20:43 [INFO]: Epoch 068 - training loss: 0.4430, validation loss: 1.5285
2024-06-02 20:20:51 [INFO]: Epoch 069 - training loss: 0.4412, validation loss: 1.5141
2024-06-02 20:20:58 [INFO]: Epoch 070 - training loss: 0.4405, validation loss: 1.4903
2024-06-02 20:21:06 [INFO]: Epoch 071 - training loss: 0.4403, validation loss: 1.4816
2024-06-02 20:21:14 [INFO]: Epoch 072 - training loss: 0.4404, validation loss: 1.4688
2024-06-02 20:21:22 [INFO]: Epoch 073 - training loss: 0.4395, validation loss: 1.4670
2024-06-02 20:21:30 [INFO]: Epoch 074 - training loss: 0.4389, validation loss: 1.4605
2024-06-02 20:21:37 [INFO]: Epoch 075 - training loss: 0.4406, validation loss: 1.4623
2024-06-02 20:21:45 [INFO]: Epoch 076 - training loss: 0.4397, validation loss: 1.4299
2024-06-02 20:21:51 [INFO]: Epoch 077 - training loss: 0.4376, validation loss: 1.4152
2024-06-02 20:21:56 [INFO]: Epoch 078 - training loss: 0.4367, validation loss: 1.4159
2024-06-02 20:22:02 [INFO]: Epoch 079 - training loss: 0.4363, validation loss: 1.4240
2024-06-02 20:22:07 [INFO]: Epoch 080 - training loss: 0.4365, validation loss: 1.4119
2024-06-02 20:22:12 [INFO]: Epoch 081 - training loss: 0.4358, validation loss: 1.4058
2024-06-02 20:22:18 [INFO]: Epoch 082 - training loss: 0.4359, validation loss: 1.3846
2024-06-02 20:22:23 [INFO]: Epoch 083 - training loss: 0.4362, validation loss: 1.3823
2024-06-02 20:22:28 [INFO]: Epoch 084 - training loss: 0.4358, validation loss: 1.3889
2024-06-02 20:22:33 [INFO]: Epoch 085 - training loss: 0.4348, validation loss: 1.3850
2024-06-02 20:22:38 [INFO]: Epoch 086 - training loss: 0.4344, validation loss: 1.3743
2024-06-02 20:22:44 [INFO]: Epoch 087 - training loss: 0.4343, validation loss: 1.3734
2024-06-02 20:22:49 [INFO]: Epoch 088 - training loss: 0.4333, validation loss: 1.3620
2024-06-02 20:22:54 [INFO]: Epoch 089 - training loss: 0.4331, validation loss: 1.3543
2024-06-02 20:22:59 [INFO]: Epoch 090 - training loss: 0.4338, validation loss: 1.3573
2024-06-02 20:23:05 [INFO]: Epoch 091 - training loss: 0.4340, validation loss: 1.3454
2024-06-02 20:23:10 [INFO]: Epoch 092 - training loss: 0.4343, validation loss: 1.3375
2024-06-02 20:23:16 [INFO]: Epoch 093 - training loss: 0.4335, validation loss: 1.3304
2024-06-02 20:23:21 [INFO]: Epoch 094 - training loss: 0.4324, validation loss: 1.3137
2024-06-02 20:23:27 [INFO]: Epoch 095 - training loss: 0.4317, validation loss: 1.3282
2024-06-02 20:23:32 [INFO]: Epoch 096 - training loss: 0.4325, validation loss: 1.3132
2024-06-02 20:23:38 [INFO]: Epoch 097 - training loss: 0.4314, validation loss: 1.3007
2024-06-02 20:23:44 [INFO]: Epoch 098 - training loss: 0.4314, validation loss: 1.2985
2024-06-02 20:23:49 [INFO]: Epoch 099 - training loss: 0.4307, validation loss: 1.3072
2024-06-02 20:23:54 [INFO]: Epoch 100 - training loss: 0.4308, validation loss: 1.2924
2024-06-02 20:23:54 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 20:23:54 [INFO]: Saved the model to results_point_rate05/Electricity/FreTS_Electricity/round_1/20240602_T201204/FreTS.pypots
2024-06-02 20:23:55 [INFO]: Successfully saved to results_point_rate05/Electricity/FreTS_Electricity/round_1/imputation.pkl
2024-06-02 20:23:55 [INFO]: Round1 - FreTS on Electricity: MAE=0.8168, MSE=1.1577, MRE=0.4373
2024-06-02 20:23:55 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-02 20:23:55 [INFO]: Using the given device: cuda:0
2024-06-02 20:23:55 [INFO]: Model files will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_2/20240602_T202355
2024-06-02 20:23:55 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_2/20240602_T202355/tensorboard
2024-06-02 20:23:55 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 20:24:01 [INFO]: Epoch 001 - training loss: 1.2911, validation loss: 3.7030
2024-06-02 20:24:06 [INFO]: Epoch 002 - training loss: 0.7983, validation loss: 3.1737
2024-06-02 20:24:12 [INFO]: Epoch 003 - training loss: 0.6951, validation loss: 2.9856
2024-06-02 20:24:17 [INFO]: Epoch 004 - training loss: 0.6506, validation loss: 2.8265
2024-06-02 20:24:22 [INFO]: Epoch 005 - training loss: 0.6269, validation loss: 2.6905
2024-06-02 20:24:27 [INFO]: Epoch 006 - training loss: 0.6063, validation loss: 2.5780
2024-06-02 20:24:32 [INFO]: Epoch 007 - training loss: 0.5907, validation loss: 2.5088
2024-06-02 20:24:36 [INFO]: Epoch 008 - training loss: 0.5770, validation loss: 2.4490
2024-06-02 20:24:41 [INFO]: Epoch 009 - training loss: 0.5692, validation loss: 2.4039
2024-06-02 20:24:47 [INFO]: Epoch 010 - training loss: 0.5582, validation loss: 2.3961
2024-06-02 20:24:52 [INFO]: Epoch 011 - training loss: 0.5496, validation loss: 2.3372
2024-06-02 20:24:57 [INFO]: Epoch 012 - training loss: 0.5407, validation loss: 2.3366
2024-06-02 20:25:03 [INFO]: Epoch 013 - training loss: 0.5342, validation loss: 2.2612
2024-06-02 20:25:08 [INFO]: Epoch 014 - training loss: 0.5301, validation loss: 2.2389
2024-06-02 20:25:13 [INFO]: Epoch 015 - training loss: 0.5251, validation loss: 2.2515
2024-06-02 20:25:19 [INFO]: Epoch 016 - training loss: 0.5183, validation loss: 2.1754
2024-06-02 20:25:24 [INFO]: Epoch 017 - training loss: 0.5179, validation loss: 2.2094
2024-06-02 20:25:29 [INFO]: Epoch 018 - training loss: 0.5090, validation loss: 2.1585
2024-06-02 20:25:35 [INFO]: Epoch 019 - training loss: 0.5041, validation loss: 2.1632
2024-06-02 20:25:40 [INFO]: Epoch 020 - training loss: 0.5031, validation loss: 2.1328
2024-06-02 20:25:46 [INFO]: Epoch 021 - training loss: 0.4994, validation loss: 2.1187
2024-06-02 20:25:51 [INFO]: Epoch 022 - training loss: 0.4972, validation loss: 2.1066
2024-06-02 20:25:56 [INFO]: Epoch 023 - training loss: 0.4936, validation loss: 2.0976
2024-06-02 20:26:02 [INFO]: Epoch 024 - training loss: 0.4921, validation loss: 2.0872
2024-06-02 20:26:07 [INFO]: Epoch 025 - training loss: 0.4884, validation loss: 2.0586
2024-06-02 20:26:13 [INFO]: Epoch 026 - training loss: 0.4887, validation loss: 2.0608
2024-06-02 20:26:18 [INFO]: Epoch 027 - training loss: 0.4840, validation loss: 2.0613
2024-06-02 20:26:23 [INFO]: Epoch 028 - training loss: 0.4807, validation loss: 2.0266
2024-06-02 20:26:29 [INFO]: Epoch 029 - training loss: 0.4792, validation loss: 2.0183
2024-06-02 20:26:34 [INFO]: Epoch 030 - training loss: 0.4774, validation loss: 2.0074
2024-06-02 20:26:39 [INFO]: Epoch 031 - training loss: 0.4826, validation loss: 1.9878
2024-06-02 20:26:45 [INFO]: Epoch 032 - training loss: 0.4809, validation loss: 1.9740
2024-06-02 20:26:50 [INFO]: Epoch 033 - training loss: 0.4733, validation loss: 1.9885
2024-06-02 20:26:55 [INFO]: Epoch 034 - training loss: 0.4699, validation loss: 1.9674
2024-06-02 20:27:01 [INFO]: Epoch 035 - training loss: 0.4693, validation loss: 1.9427
2024-06-02 20:27:06 [INFO]: Epoch 036 - training loss: 0.4680, validation loss: 1.9546
2024-06-02 20:27:12 [INFO]: Epoch 037 - training loss: 0.4688, validation loss: 1.9430
2024-06-02 20:27:17 [INFO]: Epoch 038 - training loss: 0.4678, validation loss: 1.9634
2024-06-02 20:27:22 [INFO]: Epoch 039 - training loss: 0.4671, validation loss: 1.9083
2024-06-02 20:27:28 [INFO]: Epoch 040 - training loss: 0.4637, validation loss: 1.9376
2024-06-02 20:27:34 [INFO]: Epoch 041 - training loss: 0.4619, validation loss: 1.9108
2024-06-02 20:27:39 [INFO]: Epoch 042 - training loss: 0.4609, validation loss: 1.8906
2024-06-02 20:27:44 [INFO]: Epoch 043 - training loss: 0.4669, validation loss: 1.9240
2024-06-02 20:27:49 [INFO]: Epoch 044 - training loss: 0.4670, validation loss: 1.8650
2024-06-02 20:27:55 [INFO]: Epoch 045 - training loss: 0.4589, validation loss: 1.8746
2024-06-02 20:28:00 [INFO]: Epoch 046 - training loss: 0.4560, validation loss: 1.8607
2024-06-02 20:28:06 [INFO]: Epoch 047 - training loss: 0.4554, validation loss: 1.8563
2024-06-02 20:28:11 [INFO]: Epoch 048 - training loss: 0.4555, validation loss: 1.8400
2024-06-02 20:28:16 [INFO]: Epoch 049 - training loss: 0.4534, validation loss: 1.8375
2024-06-02 20:28:21 [INFO]: Epoch 050 - training loss: 0.4529, validation loss: 1.8281
2024-06-02 20:28:27 [INFO]: Epoch 051 - training loss: 0.4518, validation loss: 1.8232
2024-06-02 20:28:32 [INFO]: Epoch 052 - training loss: 0.4524, validation loss: 1.8061
2024-06-02 20:28:37 [INFO]: Epoch 053 - training loss: 0.4512, validation loss: 1.7889
2024-06-02 20:28:43 [INFO]: Epoch 054 - training loss: 0.4515, validation loss: 1.7503
2024-06-02 20:28:48 [INFO]: Epoch 055 - training loss: 0.4522, validation loss: 1.7849
2024-06-02 20:28:53 [INFO]: Epoch 056 - training loss: 0.4504, validation loss: 1.7554
2024-06-02 20:28:59 [INFO]: Epoch 057 - training loss: 0.4485, validation loss: 1.7592
2024-06-02 20:29:04 [INFO]: Epoch 058 - training loss: 0.4473, validation loss: 1.7298
2024-06-02 20:29:09 [INFO]: Epoch 059 - training loss: 0.4459, validation loss: 1.7235
2024-06-02 20:29:15 [INFO]: Epoch 060 - training loss: 0.4450, validation loss: 1.7372
2024-06-02 20:29:20 [INFO]: Epoch 061 - training loss: 0.4460, validation loss: 1.7137
2024-06-02 20:29:26 [INFO]: Epoch 062 - training loss: 0.4460, validation loss: 1.7130
2024-06-02 20:29:31 [INFO]: Epoch 063 - training loss: 0.4444, validation loss: 1.6925
2024-06-02 20:29:36 [INFO]: Epoch 064 - training loss: 0.4439, validation loss: 1.6746
2024-06-02 20:29:42 [INFO]: Epoch 065 - training loss: 0.4481, validation loss: 1.6961
2024-06-02 20:29:47 [INFO]: Epoch 066 - training loss: 0.4439, validation loss: 1.6531
2024-06-02 20:29:52 [INFO]: Epoch 067 - training loss: 0.4424, validation loss: 1.6467
2024-06-02 20:29:58 [INFO]: Epoch 068 - training loss: 0.4404, validation loss: 1.6560
2024-06-02 20:30:03 [INFO]: Epoch 069 - training loss: 0.4410, validation loss: 1.6381
2024-06-02 20:30:08 [INFO]: Epoch 070 - training loss: 0.4399, validation loss: 1.6240
2024-06-02 20:30:14 [INFO]: Epoch 071 - training loss: 0.4405, validation loss: 1.6301
2024-06-02 20:30:19 [INFO]: Epoch 072 - training loss: 0.4406, validation loss: 1.6159
2024-06-02 20:30:25 [INFO]: Epoch 073 - training loss: 0.4418, validation loss: 1.6245
2024-06-02 20:30:30 [INFO]: Epoch 074 - training loss: 0.4426, validation loss: 1.5880
2024-06-02 20:30:36 [INFO]: Epoch 075 - training loss: 0.4409, validation loss: 1.6012
2024-06-02 20:30:41 [INFO]: Epoch 076 - training loss: 0.4384, validation loss: 1.5695
2024-06-02 20:30:46 [INFO]: Epoch 077 - training loss: 0.4383, validation loss: 1.5800
2024-06-02 20:30:52 [INFO]: Epoch 078 - training loss: 0.4372, validation loss: 1.5538
2024-06-02 20:30:57 [INFO]: Epoch 079 - training loss: 0.4380, validation loss: 1.5478
2024-06-02 20:31:02 [INFO]: Epoch 080 - training loss: 0.4379, validation loss: 1.5452
2024-06-02 20:31:07 [INFO]: Epoch 081 - training loss: 0.4361, validation loss: 1.5301
2024-06-02 20:31:11 [INFO]: Epoch 082 - training loss: 0.4351, validation loss: 1.5439
2024-06-02 20:31:16 [INFO]: Epoch 083 - training loss: 0.4368, validation loss: 1.5189
2024-06-02 20:31:21 [INFO]: Epoch 084 - training loss: 0.4369, validation loss: 1.5038
2024-06-02 20:31:26 [INFO]: Epoch 085 - training loss: 0.4344, validation loss: 1.5122
2024-06-02 20:31:31 [INFO]: Epoch 086 - training loss: 0.4339, validation loss: 1.4927
2024-06-02 20:31:36 [INFO]: Epoch 087 - training loss: 0.4335, validation loss: 1.4920
2024-06-02 20:31:42 [INFO]: Epoch 088 - training loss: 0.4353, validation loss: 1.4707
2024-06-02 20:31:47 [INFO]: Epoch 089 - training loss: 0.4330, validation loss: 1.4522
2024-06-02 20:31:52 [INFO]: Epoch 090 - training loss: 0.4316, validation loss: 1.4682
2024-06-02 20:31:58 [INFO]: Epoch 091 - training loss: 0.4318, validation loss: 1.4693
2024-06-02 20:32:03 [INFO]: Epoch 092 - training loss: 0.4319, validation loss: 1.4508
2024-06-02 20:32:08 [INFO]: Epoch 093 - training loss: 0.4320, validation loss: 1.4392
2024-06-02 20:32:14 [INFO]: Epoch 094 - training loss: 0.4323, validation loss: 1.4406
2024-06-02 20:32:19 [INFO]: Epoch 095 - training loss: 0.4313, validation loss: 1.4479
2024-06-02 20:32:25 [INFO]: Epoch 096 - training loss: 0.4308, validation loss: 1.4370
2024-06-02 20:32:30 [INFO]: Epoch 097 - training loss: 0.4329, validation loss: 1.4482
2024-06-02 20:32:36 [INFO]: Epoch 098 - training loss: 0.4320, validation loss: 1.4176
2024-06-02 20:32:42 [INFO]: Epoch 099 - training loss: 0.4314, validation loss: 1.4028
2024-06-02 20:32:47 [INFO]: Epoch 100 - training loss: 0.4335, validation loss: 1.4206
2024-06-02 20:32:47 [INFO]: Finished training. The best model is from epoch#99.
2024-06-02 20:32:47 [INFO]: Saved the model to results_point_rate05/Electricity/FreTS_Electricity/round_2/20240602_T202355/FreTS.pypots
2024-06-02 20:32:48 [INFO]: Successfully saved to results_point_rate05/Electricity/FreTS_Electricity/round_2/imputation.pkl
2024-06-02 20:32:48 [INFO]: Round2 - FreTS on Electricity: MAE=0.8338, MSE=1.1863, MRE=0.4464
2024-06-02 20:32:48 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-02 20:32:48 [INFO]: Using the given device: cuda:0
2024-06-02 20:32:48 [INFO]: Model files will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_3/20240602_T203248
2024-06-02 20:32:48 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_3/20240602_T203248/tensorboard
2024-06-02 20:32:48 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 20:32:54 [INFO]: Epoch 001 - training loss: 1.2356, validation loss: 3.7116
2024-06-02 20:32:59 [INFO]: Epoch 002 - training loss: 0.7813, validation loss: 3.1067
2024-06-02 20:33:05 [INFO]: Epoch 003 - training loss: 0.6901, validation loss: 2.9571
2024-06-02 20:33:10 [INFO]: Epoch 004 - training loss: 0.6493, validation loss: 2.8052
2024-06-02 20:33:16 [INFO]: Epoch 005 - training loss: 0.6199, validation loss: 2.6813
2024-06-02 20:33:21 [INFO]: Epoch 006 - training loss: 0.5995, validation loss: 2.6111
2024-06-02 20:33:27 [INFO]: Epoch 007 - training loss: 0.5853, validation loss: 2.5030
2024-06-02 20:33:32 [INFO]: Epoch 008 - training loss: 0.5727, validation loss: 2.4776
2024-06-02 20:33:38 [INFO]: Epoch 009 - training loss: 0.5647, validation loss: 2.4062
2024-06-02 20:33:43 [INFO]: Epoch 010 - training loss: 0.5517, validation loss: 2.3973
2024-06-02 20:33:48 [INFO]: Epoch 011 - training loss: 0.5452, validation loss: 2.3130
2024-06-02 20:33:54 [INFO]: Epoch 012 - training loss: 0.5390, validation loss: 2.3082
2024-06-02 20:33:59 [INFO]: Epoch 013 - training loss: 0.5323, validation loss: 2.2628
2024-06-02 20:34:04 [INFO]: Epoch 014 - training loss: 0.5328, validation loss: 2.2200
2024-06-02 20:34:09 [INFO]: Epoch 015 - training loss: 0.5215, validation loss: 2.2168
2024-06-02 20:34:15 [INFO]: Epoch 016 - training loss: 0.5153, validation loss: 2.1824
2024-06-02 20:34:20 [INFO]: Epoch 017 - training loss: 0.5136, validation loss: 2.1866
2024-06-02 20:34:26 [INFO]: Epoch 018 - training loss: 0.5089, validation loss: 2.1621
2024-06-02 20:34:31 [INFO]: Epoch 019 - training loss: 0.5050, validation loss: 2.1388
2024-06-02 20:34:36 [INFO]: Epoch 020 - training loss: 0.5011, validation loss: 2.1153
2024-06-02 20:34:42 [INFO]: Epoch 021 - training loss: 0.5019, validation loss: 2.1057
2024-06-02 20:34:47 [INFO]: Epoch 022 - training loss: 0.4961, validation loss: 2.0817
2024-06-02 20:34:53 [INFO]: Epoch 023 - training loss: 0.4914, validation loss: 2.0494
2024-06-02 20:34:59 [INFO]: Epoch 024 - training loss: 0.4889, validation loss: 2.0421
2024-06-02 20:35:05 [INFO]: Epoch 025 - training loss: 0.4892, validation loss: 1.9956
2024-06-02 20:35:10 [INFO]: Epoch 026 - training loss: 0.4875, validation loss: 2.0051
2024-06-02 20:35:15 [INFO]: Epoch 027 - training loss: 0.4847, validation loss: 1.9975
2024-06-02 20:35:21 [INFO]: Epoch 028 - training loss: 0.4820, validation loss: 1.9637
2024-06-02 20:35:26 [INFO]: Epoch 029 - training loss: 0.4777, validation loss: 1.9699
2024-06-02 20:35:31 [INFO]: Epoch 030 - training loss: 0.4770, validation loss: 1.9116
2024-06-02 20:35:37 [INFO]: Epoch 031 - training loss: 0.4789, validation loss: 1.9447
2024-06-02 20:35:42 [INFO]: Epoch 032 - training loss: 0.4764, validation loss: 1.9234
2024-06-02 20:35:48 [INFO]: Epoch 033 - training loss: 0.4726, validation loss: 1.9073
2024-06-02 20:35:53 [INFO]: Epoch 034 - training loss: 0.4704, validation loss: 1.8863
2024-06-02 20:35:58 [INFO]: Epoch 035 - training loss: 0.4699, validation loss: 1.8697
2024-06-02 20:36:03 [INFO]: Epoch 036 - training loss: 0.4677, validation loss: 1.8545
2024-06-02 20:36:09 [INFO]: Epoch 037 - training loss: 0.4680, validation loss: 1.8291
2024-06-02 20:36:14 [INFO]: Epoch 038 - training loss: 0.4656, validation loss: 1.8252
2024-06-02 20:36:19 [INFO]: Epoch 039 - training loss: 0.4649, validation loss: 1.7949
2024-06-02 20:36:25 [INFO]: Epoch 040 - training loss: 0.4641, validation loss: 1.7821
2024-06-02 20:36:30 [INFO]: Epoch 041 - training loss: 0.4640, validation loss: 1.8028
2024-06-02 20:36:35 [INFO]: Epoch 042 - training loss: 0.4639, validation loss: 1.7753
2024-06-02 20:36:41 [INFO]: Epoch 043 - training loss: 0.4595, validation loss: 1.7663
2024-06-02 20:36:46 [INFO]: Epoch 044 - training loss: 0.4579, validation loss: 1.7728
2024-06-02 20:36:51 [INFO]: Epoch 045 - training loss: 0.4574, validation loss: 1.7294
2024-06-02 20:36:56 [INFO]: Epoch 046 - training loss: 0.4577, validation loss: 1.7293
2024-06-02 20:37:02 [INFO]: Epoch 047 - training loss: 0.4580, validation loss: 1.7155
2024-06-02 20:37:07 [INFO]: Epoch 048 - training loss: 0.4545, validation loss: 1.6919
2024-06-02 20:37:12 [INFO]: Epoch 049 - training loss: 0.4538, validation loss: 1.7052
2024-06-02 20:37:17 [INFO]: Epoch 050 - training loss: 0.4530, validation loss: 1.6684
2024-06-02 20:37:23 [INFO]: Epoch 051 - training loss: 0.4521, validation loss: 1.6572
2024-06-02 20:37:28 [INFO]: Epoch 052 - training loss: 0.4546, validation loss: 1.6671
2024-06-02 20:37:33 [INFO]: Epoch 053 - training loss: 0.4503, validation loss: 1.6701
2024-06-02 20:37:39 [INFO]: Epoch 054 - training loss: 0.4518, validation loss: 1.6379
2024-06-02 20:37:44 [INFO]: Epoch 055 - training loss: 0.4501, validation loss: 1.6346
2024-06-02 20:37:49 [INFO]: Epoch 056 - training loss: 0.4491, validation loss: 1.6201
2024-06-02 20:37:55 [INFO]: Epoch 057 - training loss: 0.4484, validation loss: 1.6028
2024-06-02 20:38:00 [INFO]: Epoch 058 - training loss: 0.4490, validation loss: 1.6042
2024-06-02 20:38:05 [INFO]: Epoch 059 - training loss: 0.4496, validation loss: 1.5906
2024-06-02 20:38:10 [INFO]: Epoch 060 - training loss: 0.4459, validation loss: 1.5673
2024-06-02 20:38:16 [INFO]: Epoch 061 - training loss: 0.4470, validation loss: 1.5758
2024-06-02 20:38:21 [INFO]: Epoch 062 - training loss: 0.4465, validation loss: 1.5555
2024-06-02 20:38:26 [INFO]: Epoch 063 - training loss: 0.4442, validation loss: 1.5422
2024-06-02 20:38:32 [INFO]: Epoch 064 - training loss: 0.4438, validation loss: 1.5196
2024-06-02 20:38:37 [INFO]: Epoch 065 - training loss: 0.4442, validation loss: 1.5297
2024-06-02 20:38:41 [INFO]: Epoch 066 - training loss: 0.4440, validation loss: 1.5063
2024-06-02 20:38:46 [INFO]: Epoch 067 - training loss: 0.4424, validation loss: 1.5110
2024-06-02 20:38:50 [INFO]: Epoch 068 - training loss: 0.4433, validation loss: 1.4889
2024-06-02 20:38:54 [INFO]: Epoch 069 - training loss: 0.4420, validation loss: 1.4784
2024-06-02 20:38:58 [INFO]: Epoch 070 - training loss: 0.4413, validation loss: 1.4693
2024-06-02 20:39:02 [INFO]: Epoch 071 - training loss: 0.4442, validation loss: 1.4735
2024-06-02 20:39:06 [INFO]: Epoch 072 - training loss: 0.4426, validation loss: 1.4505
2024-06-02 20:39:10 [INFO]: Epoch 073 - training loss: 0.4400, validation loss: 1.4513
2024-06-02 20:39:14 [INFO]: Epoch 074 - training loss: 0.4393, validation loss: 1.4386
2024-06-02 20:39:18 [INFO]: Epoch 075 - training loss: 0.4385, validation loss: 1.4301
2024-06-02 20:39:22 [INFO]: Epoch 076 - training loss: 0.4381, validation loss: 1.4209
2024-06-02 20:39:26 [INFO]: Epoch 077 - training loss: 0.4374, validation loss: 1.4286
2024-06-02 20:39:30 [INFO]: Epoch 078 - training loss: 0.4373, validation loss: 1.4129
2024-06-02 20:39:34 [INFO]: Epoch 079 - training loss: 0.4389, validation loss: 1.4109
2024-06-02 20:39:38 [INFO]: Epoch 080 - training loss: 0.4380, validation loss: 1.3911
2024-06-02 20:39:42 [INFO]: Epoch 081 - training loss: 0.4373, validation loss: 1.3882
2024-06-02 20:39:46 [INFO]: Epoch 082 - training loss: 0.4363, validation loss: 1.3753
2024-06-02 20:39:50 [INFO]: Epoch 083 - training loss: 0.4353, validation loss: 1.3637
2024-06-02 20:39:54 [INFO]: Epoch 084 - training loss: 0.4355, validation loss: 1.3552
2024-06-02 20:39:58 [INFO]: Epoch 085 - training loss: 0.4349, validation loss: 1.3508
2024-06-02 20:40:02 [INFO]: Epoch 086 - training loss: 0.4350, validation loss: 1.3446
2024-06-02 20:40:06 [INFO]: Epoch 087 - training loss: 0.4344, validation loss: 1.3343
2024-06-02 20:40:10 [INFO]: Epoch 088 - training loss: 0.4337, validation loss: 1.3172
2024-06-02 20:40:14 [INFO]: Epoch 089 - training loss: 0.4339, validation loss: 1.3150
2024-06-02 20:40:18 [INFO]: Epoch 090 - training loss: 0.4350, validation loss: 1.3134
2024-06-02 20:40:22 [INFO]: Epoch 091 - training loss: 0.4335, validation loss: 1.3070
2024-06-02 20:40:26 [INFO]: Epoch 092 - training loss: 0.4337, validation loss: 1.3024
2024-06-02 20:40:30 [INFO]: Epoch 093 - training loss: 0.4326, validation loss: 1.2898
2024-06-02 20:40:34 [INFO]: Epoch 094 - training loss: 0.4316, validation loss: 1.2785
2024-06-02 20:40:38 [INFO]: Epoch 095 - training loss: 0.4302, validation loss: 1.2835
2024-06-02 20:40:42 [INFO]: Epoch 096 - training loss: 0.4311, validation loss: 1.2829
2024-06-02 20:40:46 [INFO]: Epoch 097 - training loss: 0.4317, validation loss: 1.2772
2024-06-02 20:40:50 [INFO]: Epoch 098 - training loss: 0.4310, validation loss: 1.2972
2024-06-02 20:40:54 [INFO]: Epoch 099 - training loss: 0.4300, validation loss: 1.2854
2024-06-02 20:40:58 [INFO]: Epoch 100 - training loss: 0.4308, validation loss: 1.2691
2024-06-02 20:40:58 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 20:40:58 [INFO]: Saved the model to results_point_rate05/Electricity/FreTS_Electricity/round_3/20240602_T203248/FreTS.pypots
2024-06-02 20:40:59 [INFO]: Successfully saved to results_point_rate05/Electricity/FreTS_Electricity/round_3/imputation.pkl
2024-06-02 20:40:59 [INFO]: Round3 - FreTS on Electricity: MAE=0.7839, MSE=1.0489, MRE=0.4197
2024-06-02 20:40:59 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-02 20:40:59 [INFO]: Using the given device: cuda:0
2024-06-02 20:40:59 [INFO]: Model files will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_4/20240602_T204059
2024-06-02 20:40:59 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/FreTS_Electricity/round_4/20240602_T204059/tensorboard
2024-06-02 20:40:59 [INFO]: FreTS initialized with the given hyperparameters, the number of trainable parameters: 3,706,194
2024-06-02 20:41:03 [INFO]: Epoch 001 - training loss: 1.2475, validation loss: 3.5656
2024-06-02 20:41:07 [INFO]: Epoch 002 - training loss: 0.7975, validation loss: 3.1182
2024-06-02 20:41:11 [INFO]: Epoch 003 - training loss: 0.6887, validation loss: 3.0417
2024-06-02 20:41:15 [INFO]: Epoch 004 - training loss: 0.6441, validation loss: 2.8529
2024-06-02 20:41:19 [INFO]: Epoch 005 - training loss: 0.6217, validation loss: 2.7623
2024-06-02 20:41:23 [INFO]: Epoch 006 - training loss: 0.5980, validation loss: 2.6632
2024-06-02 20:41:27 [INFO]: Epoch 007 - training loss: 0.5821, validation loss: 2.5664
2024-06-02 20:41:31 [INFO]: Epoch 008 - training loss: 0.5715, validation loss: 2.4757
2024-06-02 20:41:35 [INFO]: Epoch 009 - training loss: 0.5618, validation loss: 2.4583
2024-06-02 20:41:39 [INFO]: Epoch 010 - training loss: 0.5518, validation loss: 2.3926
2024-06-02 20:41:43 [INFO]: Epoch 011 - training loss: 0.5457, validation loss: 2.3743
2024-06-02 20:41:47 [INFO]: Epoch 012 - training loss: 0.5336, validation loss: 2.2830
2024-06-02 20:41:51 [INFO]: Epoch 013 - training loss: 0.5302, validation loss: 2.2628
2024-06-02 20:41:55 [INFO]: Epoch 014 - training loss: 0.5261, validation loss: 2.2297
2024-06-02 20:41:59 [INFO]: Epoch 015 - training loss: 0.5192, validation loss: 2.2137
2024-06-02 20:42:03 [INFO]: Epoch 016 - training loss: 0.5156, validation loss: 2.1768
2024-06-02 20:42:07 [INFO]: Epoch 017 - training loss: 0.5083, validation loss: 2.1599
2024-06-02 20:42:11 [INFO]: Epoch 018 - training loss: 0.5074, validation loss: 2.1378
2024-06-02 20:42:15 [INFO]: Epoch 019 - training loss: 0.5041, validation loss: 2.1232
2024-06-02 20:42:20 [INFO]: Epoch 020 - training loss: 0.4978, validation loss: 2.1186
2024-06-02 20:42:23 [INFO]: Epoch 021 - training loss: 0.4972, validation loss: 2.0923
2024-06-02 20:42:27 [INFO]: Epoch 022 - training loss: 0.4921, validation loss: 2.0450
2024-06-02 20:42:31 [INFO]: Epoch 023 - training loss: 0.4924, validation loss: 2.0538
2024-06-02 20:42:35 [INFO]: Epoch 024 - training loss: 0.4872, validation loss: 2.0394
2024-06-02 20:42:39 [INFO]: Epoch 025 - training loss: 0.4859, validation loss: 2.0353
2024-06-02 20:42:43 [INFO]: Epoch 026 - training loss: 0.4859, validation loss: 2.0094
2024-06-02 20:42:47 [INFO]: Epoch 027 - training loss: 0.4824, validation loss: 1.9919
2024-06-02 20:42:51 [INFO]: Epoch 028 - training loss: 0.4796, validation loss: 1.9745
2024-06-02 20:42:55 [INFO]: Epoch 029 - training loss: 0.4773, validation loss: 1.9667
2024-06-02 20:42:59 [INFO]: Epoch 030 - training loss: 0.4752, validation loss: 1.9351
2024-06-02 20:43:03 [INFO]: Epoch 031 - training loss: 0.4743, validation loss: 1.9361
2024-06-02 20:43:07 [INFO]: Epoch 032 - training loss: 0.4721, validation loss: 1.9166
2024-06-02 20:43:11 [INFO]: Epoch 033 - training loss: 0.4723, validation loss: 1.8998
2024-06-02 20:43:15 [INFO]: Epoch 034 - training loss: 0.4695, validation loss: 1.8823
2024-06-02 20:43:19 [INFO]: Epoch 035 - training loss: 0.4678, validation loss: 1.8758
2024-06-02 20:43:23 [INFO]: Epoch 036 - training loss: 0.4666, validation loss: 1.8795
2024-06-02 20:43:27 [INFO]: Epoch 037 - training loss: 0.4666, validation loss: 1.8623
2024-06-02 20:43:31 [INFO]: Epoch 038 - training loss: 0.4646, validation loss: 1.8498
2024-06-02 20:43:35 [INFO]: Epoch 039 - training loss: 0.4626, validation loss: 1.8435
2024-06-02 20:43:39 [INFO]: Epoch 040 - training loss: 0.4614, validation loss: 1.8587
2024-06-02 20:43:43 [INFO]: Epoch 041 - training loss: 0.4598, validation loss: 1.8441
2024-06-02 20:43:48 [INFO]: Epoch 042 - training loss: 0.4601, validation loss: 1.8180
2024-06-02 20:43:52 [INFO]: Epoch 043 - training loss: 0.4597, validation loss: 1.7977
2024-06-02 20:43:56 [INFO]: Epoch 044 - training loss: 0.4591, validation loss: 1.8001
2024-06-02 20:44:00 [INFO]: Epoch 045 - training loss: 0.4563, validation loss: 1.7782
2024-06-02 20:44:04 [INFO]: Epoch 046 - training loss: 0.4542, validation loss: 1.7666
2024-06-02 20:44:08 [INFO]: Epoch 047 - training loss: 0.4536, validation loss: 1.7690
2024-06-02 20:44:12 [INFO]: Epoch 048 - training loss: 0.4541, validation loss: 1.7609
2024-06-02 20:44:16 [INFO]: Epoch 049 - training loss: 0.4530, validation loss: 1.7613
2024-06-02 20:44:20 [INFO]: Epoch 050 - training loss: 0.4523, validation loss: 1.7359
2024-06-02 20:44:24 [INFO]: Epoch 051 - training loss: 0.4509, validation loss: 1.7271
2024-06-02 20:44:28 [INFO]: Epoch 052 - training loss: 0.4498, validation loss: 1.7286
2024-06-02 20:44:32 [INFO]: Epoch 053 - training loss: 0.4494, validation loss: 1.7288
2024-06-02 20:44:36 [INFO]: Epoch 054 - training loss: 0.4482, validation loss: 1.7053
2024-06-02 20:44:40 [INFO]: Epoch 055 - training loss: 0.4482, validation loss: 1.7139
2024-06-02 20:44:45 [INFO]: Epoch 056 - training loss: 0.4515, validation loss: 1.6903
2024-06-02 20:44:49 [INFO]: Epoch 057 - training loss: 0.4496, validation loss: 1.6794
2024-06-02 20:44:53 [INFO]: Epoch 058 - training loss: 0.4505, validation loss: 1.6376
2024-06-02 20:44:57 [INFO]: Epoch 059 - training loss: 0.4464, validation loss: 1.6367
2024-06-02 20:45:01 [INFO]: Epoch 060 - training loss: 0.4453, validation loss: 1.6404
2024-06-02 20:45:05 [INFO]: Epoch 061 - training loss: 0.4448, validation loss: 1.6350
2024-06-02 20:45:09 [INFO]: Epoch 062 - training loss: 0.4442, validation loss: 1.6256
2024-06-02 20:45:13 [INFO]: Epoch 063 - training loss: 0.4432, validation loss: 1.6024
2024-06-02 20:45:17 [INFO]: Epoch 064 - training loss: 0.4427, validation loss: 1.5869
2024-06-02 20:45:21 [INFO]: Epoch 065 - training loss: 0.4430, validation loss: 1.5845
2024-06-02 20:45:25 [INFO]: Epoch 066 - training loss: 0.4452, validation loss: 1.5500
2024-06-02 20:45:29 [INFO]: Epoch 067 - training loss: 0.4428, validation loss: 1.5676
2024-06-02 20:45:33 [INFO]: Epoch 068 - training loss: 0.4413, validation loss: 1.5428
2024-06-02 20:45:37 [INFO]: Epoch 069 - training loss: 0.4406, validation loss: 1.5476
2024-06-02 20:45:41 [INFO]: Epoch 070 - training loss: 0.4403, validation loss: 1.5340
2024-06-02 20:45:45 [INFO]: Epoch 071 - training loss: 0.4391, validation loss: 1.5293
2024-06-02 20:45:49 [INFO]: Epoch 072 - training loss: 0.4410, validation loss: 1.5211
2024-06-02 20:45:53 [INFO]: Epoch 073 - training loss: 0.4406, validation loss: 1.5139
2024-06-02 20:45:57 [INFO]: Epoch 074 - training loss: 0.4401, validation loss: 1.4916
2024-06-02 20:46:01 [INFO]: Epoch 075 - training loss: 0.4383, validation loss: 1.4691
2024-06-02 20:46:05 [INFO]: Epoch 076 - training loss: 0.4364, validation loss: 1.4602
2024-06-02 20:46:09 [INFO]: Epoch 077 - training loss: 0.4371, validation loss: 1.4684
2024-06-02 20:46:13 [INFO]: Epoch 078 - training loss: 0.4376, validation loss: 1.4617
2024-06-02 20:46:16 [INFO]: Epoch 079 - training loss: 0.4361, validation loss: 1.4524
2024-06-02 20:46:19 [INFO]: Epoch 080 - training loss: 0.4367, validation loss: 1.4762
2024-06-02 20:46:23 [INFO]: Epoch 081 - training loss: 0.4382, validation loss: 1.4493
2024-06-02 20:46:26 [INFO]: Epoch 082 - training loss: 0.4355, validation loss: 1.4166
2024-06-02 20:46:29 [INFO]: Epoch 083 - training loss: 0.4351, validation loss: 1.4353
2024-06-02 20:46:32 [INFO]: Epoch 084 - training loss: 0.4345, validation loss: 1.4064
2024-06-02 20:46:36 [INFO]: Epoch 085 - training loss: 0.4354, validation loss: 1.3932
2024-06-02 20:46:39 [INFO]: Epoch 086 - training loss: 0.4349, validation loss: 1.3944
2024-06-02 20:46:42 [INFO]: Epoch 087 - training loss: 0.4338, validation loss: 1.4025
2024-06-02 20:46:45 [INFO]: Epoch 088 - training loss: 0.4332, validation loss: 1.3948
2024-06-02 20:46:49 [INFO]: Epoch 089 - training loss: 0.4332, validation loss: 1.3913
2024-06-02 20:46:52 [INFO]: Epoch 090 - training loss: 0.4328, validation loss: 1.3898
2024-06-02 20:46:55 [INFO]: Epoch 091 - training loss: 0.4326, validation loss: 1.3604
2024-06-02 20:46:59 [INFO]: Epoch 092 - training loss: 0.4316, validation loss: 1.3752
2024-06-02 20:47:02 [INFO]: Epoch 093 - training loss: 0.4313, validation loss: 1.3566
2024-06-02 20:47:05 [INFO]: Epoch 094 - training loss: 0.4320, validation loss: 1.3425
2024-06-02 20:47:09 [INFO]: Epoch 095 - training loss: 0.4333, validation loss: 1.3839
2024-06-02 20:47:12 [INFO]: Epoch 096 - training loss: 0.4324, validation loss: 1.3568
2024-06-02 20:47:15 [INFO]: Epoch 097 - training loss: 0.4312, validation loss: 1.3477
2024-06-02 20:47:19 [INFO]: Epoch 098 - training loss: 0.4298, validation loss: 1.3590
2024-06-02 20:47:22 [INFO]: Epoch 099 - training loss: 0.4296, validation loss: 1.3502
2024-06-02 20:47:25 [INFO]: Epoch 100 - training loss: 0.4299, validation loss: 1.3248
2024-06-02 20:47:25 [INFO]: Finished training. The best model is from epoch#100.
2024-06-02 20:47:25 [INFO]: Saved the model to results_point_rate05/Electricity/FreTS_Electricity/round_4/20240602_T204059/FreTS.pypots
2024-06-02 20:47:26 [INFO]: Successfully saved to results_point_rate05/Electricity/FreTS_Electricity/round_4/imputation.pkl
2024-06-02 20:47:26 [INFO]: Round4 - FreTS on Electricity: MAE=0.9009, MSE=1.3805, MRE=0.4824
2024-06-02 20:47:26 [INFO]: Done! Final results:
Averaged FreTS (3,706,194 params) on Electricity: MAE=0.8711 ± 0.08372415298610698, MSE=1.3198 ± 0.2745494140830543, MRE=0.4664 ± 0.04482646490675408, average inference time=0.54
