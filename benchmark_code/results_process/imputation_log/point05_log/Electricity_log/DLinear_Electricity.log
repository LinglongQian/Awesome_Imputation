2024-06-04 02:59:04 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-04 02:59:04 [INFO]: Using the given device: cuda:0
2024-06-04 02:59:04 [INFO]: Model files will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_0/20240604_T025904
2024-06-04 02:59:04 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_0/20240604_T025904/tensorboard
2024-06-04 02:59:05 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-04 02:59:10 [INFO]: Epoch 001 - training loss: 1.0649, validation loss: 3.1216
2024-06-04 02:59:15 [INFO]: Epoch 002 - training loss: 0.7035, validation loss: 2.8398
2024-06-04 02:59:21 [INFO]: Epoch 003 - training loss: 0.6273, validation loss: 2.6721
2024-06-04 02:59:26 [INFO]: Epoch 004 - training loss: 0.5797, validation loss: 2.5351
2024-06-04 02:59:31 [INFO]: Epoch 005 - training loss: 0.5536, validation loss: 2.4382
2024-06-04 02:59:37 [INFO]: Epoch 006 - training loss: 0.5341, validation loss: 2.3509
2024-06-04 02:59:42 [INFO]: Epoch 007 - training loss: 0.5193, validation loss: 2.2782
2024-06-04 02:59:47 [INFO]: Epoch 008 - training loss: 0.5056, validation loss: 2.2043
2024-06-04 02:59:52 [INFO]: Epoch 009 - training loss: 0.4942, validation loss: 2.1019
2024-06-04 02:59:58 [INFO]: Epoch 010 - training loss: 0.4851, validation loss: 2.0437
2024-06-04 03:00:03 [INFO]: Epoch 011 - training loss: 0.4781, validation loss: 1.9453
2024-06-04 03:00:09 [INFO]: Epoch 012 - training loss: 0.4706, validation loss: 1.8993
2024-06-04 03:00:15 [INFO]: Epoch 013 - training loss: 0.4656, validation loss: 1.8274
2024-06-04 03:00:20 [INFO]: Epoch 014 - training loss: 0.4610, validation loss: 1.7808
2024-06-04 03:00:26 [INFO]: Epoch 015 - training loss: 0.4572, validation loss: 1.7285
2024-06-04 03:00:31 [INFO]: Epoch 016 - training loss: 0.4575, validation loss: 1.6562
2024-06-04 03:00:37 [INFO]: Epoch 017 - training loss: 0.4527, validation loss: 1.6291
2024-06-04 03:00:43 [INFO]: Epoch 018 - training loss: 0.4491, validation loss: 1.5802
2024-06-04 03:00:48 [INFO]: Epoch 019 - training loss: 0.4460, validation loss: 1.5392
2024-06-04 03:00:54 [INFO]: Epoch 020 - training loss: 0.4433, validation loss: 1.5020
2024-06-04 03:00:59 [INFO]: Epoch 021 - training loss: 0.4428, validation loss: 1.4326
2024-06-04 03:01:05 [INFO]: Epoch 022 - training loss: 0.4417, validation loss: 1.3865
2024-06-04 03:01:11 [INFO]: Epoch 023 - training loss: 0.4387, validation loss: 1.3488
2024-06-04 03:01:16 [INFO]: Epoch 024 - training loss: 0.4372, validation loss: 1.3097
2024-06-04 03:01:22 [INFO]: Epoch 025 - training loss: 0.4369, validation loss: 1.2961
2024-06-04 03:01:27 [INFO]: Epoch 026 - training loss: 0.4352, validation loss: 1.2420
2024-06-04 03:01:32 [INFO]: Epoch 027 - training loss: 0.4355, validation loss: 1.2504
2024-06-04 03:01:38 [INFO]: Epoch 028 - training loss: 0.4333, validation loss: 1.2228
2024-06-04 03:01:43 [INFO]: Epoch 029 - training loss: 0.4316, validation loss: 1.1831
2024-06-04 03:01:49 [INFO]: Epoch 030 - training loss: 0.4310, validation loss: 1.1866
2024-06-04 03:01:54 [INFO]: Epoch 031 - training loss: 0.4310, validation loss: 1.1536
2024-06-04 03:02:00 [INFO]: Epoch 032 - training loss: 0.4302, validation loss: 1.1529
2024-06-04 03:02:05 [INFO]: Epoch 033 - training loss: 0.4295, validation loss: 1.1316
2024-06-04 03:02:11 [INFO]: Epoch 034 - training loss: 0.4284, validation loss: 1.1214
2024-06-04 03:02:16 [INFO]: Epoch 035 - training loss: 0.4272, validation loss: 1.1170
2024-06-04 03:02:22 [INFO]: Epoch 036 - training loss: 0.4269, validation loss: 1.0948
2024-06-04 03:02:27 [INFO]: Epoch 037 - training loss: 0.4285, validation loss: 1.1097
2024-06-04 03:02:33 [INFO]: Epoch 038 - training loss: 0.4255, validation loss: 1.0592
2024-06-04 03:02:38 [INFO]: Epoch 039 - training loss: 0.4243, validation loss: 1.0762
2024-06-04 03:02:43 [INFO]: Epoch 040 - training loss: 0.4241, validation loss: 1.0705
2024-06-04 03:02:49 [INFO]: Epoch 041 - training loss: 0.4223, validation loss: 1.0214
2024-06-04 03:02:55 [INFO]: Epoch 042 - training loss: 0.4203, validation loss: 1.0196
2024-06-04 03:03:00 [INFO]: Epoch 043 - training loss: 0.4192, validation loss: 1.0166
2024-06-04 03:03:05 [INFO]: Epoch 044 - training loss: 0.4182, validation loss: 1.0164
2024-06-04 03:03:11 [INFO]: Epoch 045 - training loss: 0.4155, validation loss: 1.0093
2024-06-04 03:03:16 [INFO]: Epoch 046 - training loss: 0.4148, validation loss: 0.9826
2024-06-04 03:03:22 [INFO]: Epoch 047 - training loss: 0.4144, validation loss: 0.9871
2024-06-04 03:03:27 [INFO]: Epoch 048 - training loss: 0.4137, validation loss: 0.9517
2024-06-04 03:03:32 [INFO]: Epoch 049 - training loss: 0.4122, validation loss: 0.9607
2024-06-04 03:03:38 [INFO]: Epoch 050 - training loss: 0.4114, validation loss: 0.9521
2024-06-04 03:03:43 [INFO]: Epoch 051 - training loss: 0.4106, validation loss: 0.9443
2024-06-04 03:03:48 [INFO]: Epoch 052 - training loss: 0.4108, validation loss: 0.9385
2024-06-04 03:03:54 [INFO]: Epoch 053 - training loss: 0.4095, validation loss: 0.9357
2024-06-04 03:03:59 [INFO]: Epoch 054 - training loss: 0.4091, validation loss: 0.9309
2024-06-04 03:04:04 [INFO]: Epoch 055 - training loss: 0.4088, validation loss: 0.9326
2024-06-04 03:04:10 [INFO]: Epoch 056 - training loss: 0.4081, validation loss: 0.9202
2024-06-04 03:04:15 [INFO]: Epoch 057 - training loss: 0.4075, validation loss: 0.9352
2024-06-04 03:04:21 [INFO]: Epoch 058 - training loss: 0.4072, validation loss: 0.9123
2024-06-04 03:04:26 [INFO]: Epoch 059 - training loss: 0.4077, validation loss: 0.9211
2024-06-04 03:04:32 [INFO]: Epoch 060 - training loss: 0.4056, validation loss: 0.9127
2024-06-04 03:04:37 [INFO]: Epoch 061 - training loss: 0.4054, validation loss: 0.8918
2024-06-04 03:04:43 [INFO]: Epoch 062 - training loss: 0.4061, validation loss: 0.9082
2024-06-04 03:04:48 [INFO]: Epoch 063 - training loss: 0.4052, validation loss: 0.9086
2024-06-04 03:04:54 [INFO]: Epoch 064 - training loss: 0.4041, validation loss: 0.8950
2024-06-04 03:04:59 [INFO]: Epoch 065 - training loss: 0.4044, validation loss: 0.9053
2024-06-04 03:05:05 [INFO]: Epoch 066 - training loss: 0.4032, validation loss: 0.8973
2024-06-04 03:05:10 [INFO]: Epoch 067 - training loss: 0.4038, validation loss: 0.9163
2024-06-04 03:05:16 [INFO]: Epoch 068 - training loss: 0.4032, validation loss: 0.8848
2024-06-04 03:05:21 [INFO]: Epoch 069 - training loss: 0.4036, validation loss: 0.8784
2024-06-04 03:05:27 [INFO]: Epoch 070 - training loss: 0.4034, validation loss: 0.9008
2024-06-04 03:05:32 [INFO]: Epoch 071 - training loss: 0.4025, validation loss: 0.9009
2024-06-04 03:05:38 [INFO]: Epoch 072 - training loss: 0.4034, validation loss: 0.8890
2024-06-04 03:05:44 [INFO]: Epoch 073 - training loss: 0.4022, validation loss: 0.8878
2024-06-04 03:05:49 [INFO]: Epoch 074 - training loss: 0.4018, validation loss: 0.8907
2024-06-04 03:05:54 [INFO]: Epoch 075 - training loss: 0.4025, validation loss: 0.8856
2024-06-04 03:06:00 [INFO]: Epoch 076 - training loss: 0.4017, validation loss: 0.8822
2024-06-04 03:06:06 [INFO]: Epoch 077 - training loss: 0.4012, validation loss: 0.8838
2024-06-04 03:06:11 [INFO]: Epoch 078 - training loss: 0.4006, validation loss: 0.8941
2024-06-04 03:06:17 [INFO]: Epoch 079 - training loss: 0.4014, validation loss: 0.8782
2024-06-04 03:06:23 [INFO]: Epoch 080 - training loss: 0.4014, validation loss: 0.8704
2024-06-04 03:06:28 [INFO]: Epoch 081 - training loss: 0.4000, validation loss: 0.8796
2024-06-04 03:06:34 [INFO]: Epoch 082 - training loss: 0.3995, validation loss: 0.8773
2024-06-04 03:06:40 [INFO]: Epoch 083 - training loss: 0.3997, validation loss: 0.8640
2024-06-04 03:06:45 [INFO]: Epoch 084 - training loss: 0.4000, validation loss: 0.8770
2024-06-04 03:06:51 [INFO]: Epoch 085 - training loss: 0.3991, validation loss: 0.8845
2024-06-04 03:06:56 [INFO]: Epoch 086 - training loss: 0.3992, validation loss: 0.8977
2024-06-04 03:07:02 [INFO]: Epoch 087 - training loss: 0.3990, validation loss: 0.8712
2024-06-04 03:07:07 [INFO]: Epoch 088 - training loss: 0.3994, validation loss: 0.8749
2024-06-04 03:07:13 [INFO]: Epoch 089 - training loss: 0.3984, validation loss: 0.8730
2024-06-04 03:07:18 [INFO]: Epoch 090 - training loss: 0.3986, validation loss: 0.8725
2024-06-04 03:07:23 [INFO]: Epoch 091 - training loss: 0.3981, validation loss: 0.8678
2024-06-04 03:07:29 [INFO]: Epoch 092 - training loss: 0.3988, validation loss: 0.8623
2024-06-04 03:07:34 [INFO]: Epoch 093 - training loss: 0.3981, validation loss: 0.8709
2024-06-04 03:07:39 [INFO]: Epoch 094 - training loss: 0.3975, validation loss: 0.8775
2024-06-04 03:07:45 [INFO]: Epoch 095 - training loss: 0.3976, validation loss: 0.8633
2024-06-04 03:07:50 [INFO]: Epoch 096 - training loss: 0.3973, validation loss: 0.8711
2024-06-04 03:07:55 [INFO]: Epoch 097 - training loss: 0.3975, validation loss: 0.8706
2024-06-04 03:08:01 [INFO]: Epoch 098 - training loss: 0.3983, validation loss: 0.8799
2024-06-04 03:08:06 [INFO]: Epoch 099 - training loss: 0.3975, validation loss: 0.8618
2024-06-04 03:08:12 [INFO]: Epoch 100 - training loss: 0.3963, validation loss: 0.8638
2024-06-04 03:08:12 [INFO]: Finished training. The best model is from epoch#99.
2024-06-04 03:08:12 [INFO]: Saved the model to results_point_rate05/Electricity/DLinear_Electricity/round_0/20240604_T025904/DLinear.pypots
2024-06-04 03:08:14 [INFO]: Successfully saved to results_point_rate05/Electricity/DLinear_Electricity/round_0/imputation.pkl
2024-06-04 03:08:14 [INFO]: Round0 - DLinear on Electricity: MAE=0.7334, MSE=0.9831, MRE=0.3927
2024-06-04 03:08:14 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-04 03:08:14 [INFO]: Using the given device: cuda:0
2024-06-04 03:08:14 [INFO]: Model files will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_1/20240604_T030814
2024-06-04 03:08:14 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_1/20240604_T030814/tensorboard
2024-06-04 03:08:14 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-04 03:08:20 [INFO]: Epoch 001 - training loss: 1.0776, validation loss: 3.1431
2024-06-04 03:08:26 [INFO]: Epoch 002 - training loss: 0.6873, validation loss: 2.9246
2024-06-04 03:08:31 [INFO]: Epoch 003 - training loss: 0.6162, validation loss: 2.7540
2024-06-04 03:08:36 [INFO]: Epoch 004 - training loss: 0.5773, validation loss: 2.6069
2024-06-04 03:08:41 [INFO]: Epoch 005 - training loss: 0.5503, validation loss: 2.4625
2024-06-04 03:08:45 [INFO]: Epoch 006 - training loss: 0.5292, validation loss: 2.3747
2024-06-04 03:08:50 [INFO]: Epoch 007 - training loss: 0.5122, validation loss: 2.3064
2024-06-04 03:08:54 [INFO]: Epoch 008 - training loss: 0.5000, validation loss: 2.2196
2024-06-04 03:08:59 [INFO]: Epoch 009 - training loss: 0.4897, validation loss: 2.1572
2024-06-04 03:09:04 [INFO]: Epoch 010 - training loss: 0.4821, validation loss: 2.0753
2024-06-04 03:09:08 [INFO]: Epoch 011 - training loss: 0.4741, validation loss: 2.0395
2024-06-04 03:09:13 [INFO]: Epoch 012 - training loss: 0.4714, validation loss: 1.9801
2024-06-04 03:09:17 [INFO]: Epoch 013 - training loss: 0.4652, validation loss: 1.8891
2024-06-04 03:09:22 [INFO]: Epoch 014 - training loss: 0.4596, validation loss: 1.8506
2024-06-04 03:09:26 [INFO]: Epoch 015 - training loss: 0.4557, validation loss: 1.7793
2024-06-04 03:09:31 [INFO]: Epoch 016 - training loss: 0.4537, validation loss: 1.7152
2024-06-04 03:09:35 [INFO]: Epoch 017 - training loss: 0.4502, validation loss: 1.6546
2024-06-04 03:09:40 [INFO]: Epoch 018 - training loss: 0.4481, validation loss: 1.5938
2024-06-04 03:09:45 [INFO]: Epoch 019 - training loss: 0.4447, validation loss: 1.5649
2024-06-04 03:09:50 [INFO]: Epoch 020 - training loss: 0.4431, validation loss: 1.5220
2024-06-04 03:09:54 [INFO]: Epoch 021 - training loss: 0.4421, validation loss: 1.4598
2024-06-04 03:09:59 [INFO]: Epoch 022 - training loss: 0.4415, validation loss: 1.4588
2024-06-04 03:10:03 [INFO]: Epoch 023 - training loss: 0.4390, validation loss: 1.4043
2024-06-04 03:10:08 [INFO]: Epoch 024 - training loss: 0.4367, validation loss: 1.3868
2024-06-04 03:10:13 [INFO]: Epoch 025 - training loss: 0.4356, validation loss: 1.3648
2024-06-04 03:10:17 [INFO]: Epoch 026 - training loss: 0.4352, validation loss: 1.3077
2024-06-04 03:10:22 [INFO]: Epoch 027 - training loss: 0.4358, validation loss: 1.2764
2024-06-04 03:10:26 [INFO]: Epoch 028 - training loss: 0.4325, validation loss: 1.2488
2024-06-04 03:10:31 [INFO]: Epoch 029 - training loss: 0.4319, validation loss: 1.2238
2024-06-04 03:10:36 [INFO]: Epoch 030 - training loss: 0.4323, validation loss: 1.2208
2024-06-04 03:10:40 [INFO]: Epoch 031 - training loss: 0.4309, validation loss: 1.1923
2024-06-04 03:10:45 [INFO]: Epoch 032 - training loss: 0.4305, validation loss: 1.1729
2024-06-04 03:10:49 [INFO]: Epoch 033 - training loss: 0.4289, validation loss: 1.1608
2024-06-04 03:10:54 [INFO]: Epoch 034 - training loss: 0.4294, validation loss: 1.1567
2024-06-04 03:10:58 [INFO]: Epoch 035 - training loss: 0.4288, validation loss: 1.1400
2024-06-04 03:11:03 [INFO]: Epoch 036 - training loss: 0.4276, validation loss: 1.1117
2024-06-04 03:11:08 [INFO]: Epoch 037 - training loss: 0.4293, validation loss: 1.1173
2024-06-04 03:11:12 [INFO]: Epoch 038 - training loss: 0.4280, validation loss: 1.1221
2024-06-04 03:11:17 [INFO]: Epoch 039 - training loss: 0.4263, validation loss: 1.0889
2024-06-04 03:11:21 [INFO]: Epoch 040 - training loss: 0.4263, validation loss: 1.0840
2024-06-04 03:11:25 [INFO]: Epoch 041 - training loss: 0.4270, validation loss: 1.0704
2024-06-04 03:11:30 [INFO]: Epoch 042 - training loss: 0.4266, validation loss: 1.0707
2024-06-04 03:11:34 [INFO]: Epoch 043 - training loss: 0.4257, validation loss: 1.0668
2024-06-04 03:11:39 [INFO]: Epoch 044 - training loss: 0.4254, validation loss: 1.0706
2024-06-04 03:11:43 [INFO]: Epoch 045 - training loss: 0.4246, validation loss: 1.0490
2024-06-04 03:11:48 [INFO]: Epoch 046 - training loss: 0.4239, validation loss: 1.0585
2024-06-04 03:11:52 [INFO]: Epoch 047 - training loss: 0.4237, validation loss: 1.0323
2024-06-04 03:11:57 [INFO]: Epoch 048 - training loss: 0.4249, validation loss: 1.0557
2024-06-04 03:12:01 [INFO]: Epoch 049 - training loss: 0.4233, validation loss: 1.0341
2024-06-04 03:12:06 [INFO]: Epoch 050 - training loss: 0.4223, validation loss: 1.0337
2024-06-04 03:12:11 [INFO]: Epoch 051 - training loss: 0.4229, validation loss: 1.0128
2024-06-04 03:12:15 [INFO]: Epoch 052 - training loss: 0.4237, validation loss: 1.0192
2024-06-04 03:12:20 [INFO]: Epoch 053 - training loss: 0.4234, validation loss: 1.0103
2024-06-04 03:12:24 [INFO]: Epoch 054 - training loss: 0.4235, validation loss: 1.0035
2024-06-04 03:12:29 [INFO]: Epoch 055 - training loss: 0.4229, validation loss: 1.0089
2024-06-04 03:12:33 [INFO]: Epoch 056 - training loss: 0.4220, validation loss: 1.0096
2024-06-04 03:12:37 [INFO]: Epoch 057 - training loss: 0.4230, validation loss: 1.0222
2024-06-04 03:12:42 [INFO]: Epoch 058 - training loss: 0.4220, validation loss: 1.0106
2024-06-04 03:12:47 [INFO]: Epoch 059 - training loss: 0.4215, validation loss: 1.0122
2024-06-04 03:12:51 [INFO]: Epoch 060 - training loss: 0.4213, validation loss: 0.9950
2024-06-04 03:12:56 [INFO]: Epoch 061 - training loss: 0.4211, validation loss: 0.9955
2024-06-04 03:13:00 [INFO]: Epoch 062 - training loss: 0.4208, validation loss: 1.0111
2024-06-04 03:13:05 [INFO]: Epoch 063 - training loss: 0.4203, validation loss: 0.9947
2024-06-04 03:13:09 [INFO]: Epoch 064 - training loss: 0.4203, validation loss: 0.9915
2024-06-04 03:13:14 [INFO]: Epoch 065 - training loss: 0.4194, validation loss: 0.9868
2024-06-04 03:13:19 [INFO]: Epoch 066 - training loss: 0.4182, validation loss: 0.9886
2024-06-04 03:13:23 [INFO]: Epoch 067 - training loss: 0.4175, validation loss: 0.9734
2024-06-04 03:13:28 [INFO]: Epoch 068 - training loss: 0.4179, validation loss: 0.9735
2024-06-04 03:13:32 [INFO]: Epoch 069 - training loss: 0.4166, validation loss: 0.9561
2024-06-04 03:13:37 [INFO]: Epoch 070 - training loss: 0.4156, validation loss: 0.9545
2024-06-04 03:13:41 [INFO]: Epoch 071 - training loss: 0.4154, validation loss: 0.9696
2024-06-04 03:13:46 [INFO]: Epoch 072 - training loss: 0.4145, validation loss: 0.9590
2024-06-04 03:13:50 [INFO]: Epoch 073 - training loss: 0.4142, validation loss: 0.9424
2024-06-04 03:13:55 [INFO]: Epoch 074 - training loss: 0.4131, validation loss: 0.9563
2024-06-04 03:14:00 [INFO]: Epoch 075 - training loss: 0.4120, validation loss: 0.9357
2024-06-04 03:14:04 [INFO]: Epoch 076 - training loss: 0.4115, validation loss: 0.9204
2024-06-04 03:14:09 [INFO]: Epoch 077 - training loss: 0.4105, validation loss: 0.9268
2024-06-04 03:14:13 [INFO]: Epoch 078 - training loss: 0.4101, validation loss: 0.9193
2024-06-04 03:14:18 [INFO]: Epoch 079 - training loss: 0.4102, validation loss: 0.9083
2024-06-04 03:14:22 [INFO]: Epoch 080 - training loss: 0.4084, validation loss: 0.9009
2024-06-04 03:14:26 [INFO]: Epoch 081 - training loss: 0.4075, validation loss: 0.9109
2024-06-04 03:14:31 [INFO]: Epoch 082 - training loss: 0.4087, validation loss: 0.8948
2024-06-04 03:14:36 [INFO]: Epoch 083 - training loss: 0.4070, validation loss: 0.8933
2024-06-04 03:14:40 [INFO]: Epoch 084 - training loss: 0.4059, validation loss: 0.8766
2024-06-04 03:14:45 [INFO]: Epoch 085 - training loss: 0.4067, validation loss: 0.9010
2024-06-04 03:14:49 [INFO]: Epoch 086 - training loss: 0.4066, validation loss: 0.8745
2024-06-04 03:14:54 [INFO]: Epoch 087 - training loss: 0.4062, validation loss: 0.8882
2024-06-04 03:14:58 [INFO]: Epoch 088 - training loss: 0.4049, validation loss: 0.8737
2024-06-04 03:15:03 [INFO]: Epoch 089 - training loss: 0.4046, validation loss: 0.8598
2024-06-04 03:15:07 [INFO]: Epoch 090 - training loss: 0.4035, validation loss: 0.8709
2024-06-04 03:15:12 [INFO]: Epoch 091 - training loss: 0.4029, validation loss: 0.8709
2024-06-04 03:15:16 [INFO]: Epoch 092 - training loss: 0.4029, validation loss: 0.8670
2024-06-04 03:15:20 [INFO]: Epoch 093 - training loss: 0.4040, validation loss: 0.8706
2024-06-04 03:15:25 [INFO]: Epoch 094 - training loss: 0.4030, validation loss: 0.8529
2024-06-04 03:15:29 [INFO]: Epoch 095 - training loss: 0.4043, validation loss: 0.8751
2024-06-04 03:15:34 [INFO]: Epoch 096 - training loss: 0.4027, validation loss: 0.8690
2024-06-04 03:15:38 [INFO]: Epoch 097 - training loss: 0.4024, validation loss: 0.8681
2024-06-04 03:15:42 [INFO]: Epoch 098 - training loss: 0.4011, validation loss: 0.8553
2024-06-04 03:15:47 [INFO]: Epoch 099 - training loss: 0.4013, validation loss: 0.8530
2024-06-04 03:15:51 [INFO]: Epoch 100 - training loss: 0.4007, validation loss: 0.8542
2024-06-04 03:15:51 [INFO]: Finished training. The best model is from epoch#94.
2024-06-04 03:15:51 [INFO]: Saved the model to results_point_rate05/Electricity/DLinear_Electricity/round_1/20240604_T030814/DLinear.pypots
2024-06-04 03:15:53 [INFO]: Successfully saved to results_point_rate05/Electricity/DLinear_Electricity/round_1/imputation.pkl
2024-06-04 03:15:53 [INFO]: Round1 - DLinear on Electricity: MAE=0.7286, MSE=0.9614, MRE=0.3901
2024-06-04 03:15:53 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-04 03:15:53 [INFO]: Using the given device: cuda:0
2024-06-04 03:15:53 [INFO]: Model files will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_2/20240604_T031553
2024-06-04 03:15:53 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_2/20240604_T031553/tensorboard
2024-06-04 03:15:53 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-04 03:15:58 [INFO]: Epoch 001 - training loss: 1.0450, validation loss: 3.0605
2024-06-04 03:16:02 [INFO]: Epoch 002 - training loss: 0.6924, validation loss: 2.9592
2024-06-04 03:16:07 [INFO]: Epoch 003 - training loss: 0.6222, validation loss: 2.7561
2024-06-04 03:16:12 [INFO]: Epoch 004 - training loss: 0.5806, validation loss: 2.5892
2024-06-04 03:16:16 [INFO]: Epoch 005 - training loss: 0.5544, validation loss: 2.4674
2024-06-04 03:16:21 [INFO]: Epoch 006 - training loss: 0.5359, validation loss: 2.3530
2024-06-04 03:16:26 [INFO]: Epoch 007 - training loss: 0.5213, validation loss: 2.2990
2024-06-04 03:16:31 [INFO]: Epoch 008 - training loss: 0.5068, validation loss: 2.2202
2024-06-04 03:16:35 [INFO]: Epoch 009 - training loss: 0.4969, validation loss: 2.1537
2024-06-04 03:16:40 [INFO]: Epoch 010 - training loss: 0.4866, validation loss: 2.0653
2024-06-04 03:16:45 [INFO]: Epoch 011 - training loss: 0.4792, validation loss: 2.0025
2024-06-04 03:16:49 [INFO]: Epoch 012 - training loss: 0.4730, validation loss: 1.9174
2024-06-04 03:16:53 [INFO]: Epoch 013 - training loss: 0.4661, validation loss: 1.8314
2024-06-04 03:16:58 [INFO]: Epoch 014 - training loss: 0.4645, validation loss: 1.7988
2024-06-04 03:17:03 [INFO]: Epoch 015 - training loss: 0.4591, validation loss: 1.7493
2024-06-04 03:17:07 [INFO]: Epoch 016 - training loss: 0.4556, validation loss: 1.6956
2024-06-04 03:17:12 [INFO]: Epoch 017 - training loss: 0.4522, validation loss: 1.6439
2024-06-04 03:17:17 [INFO]: Epoch 018 - training loss: 0.4493, validation loss: 1.5918
2024-06-04 03:17:21 [INFO]: Epoch 019 - training loss: 0.4457, validation loss: 1.5476
2024-06-04 03:17:26 [INFO]: Epoch 020 - training loss: 0.4433, validation loss: 1.5036
2024-06-04 03:17:30 [INFO]: Epoch 021 - training loss: 0.4422, validation loss: 1.4624
2024-06-04 03:17:35 [INFO]: Epoch 022 - training loss: 0.4413, validation loss: 1.4295
2024-06-04 03:17:39 [INFO]: Epoch 023 - training loss: 0.4386, validation loss: 1.3770
2024-06-04 03:17:44 [INFO]: Epoch 024 - training loss: 0.4386, validation loss: 1.3392
2024-06-04 03:17:48 [INFO]: Epoch 025 - training loss: 0.4367, validation loss: 1.3102
2024-06-04 03:17:53 [INFO]: Epoch 026 - training loss: 0.4357, validation loss: 1.2983
2024-06-04 03:17:58 [INFO]: Epoch 027 - training loss: 0.4342, validation loss: 1.2316
2024-06-04 03:18:02 [INFO]: Epoch 028 - training loss: 0.4323, validation loss: 1.2227
2024-06-04 03:18:07 [INFO]: Epoch 029 - training loss: 0.4314, validation loss: 1.1942
2024-06-04 03:18:12 [INFO]: Epoch 030 - training loss: 0.4323, validation loss: 1.1722
2024-06-04 03:18:16 [INFO]: Epoch 031 - training loss: 0.4304, validation loss: 1.1518
2024-06-04 03:18:21 [INFO]: Epoch 032 - training loss: 0.4298, validation loss: 1.1582
2024-06-04 03:18:25 [INFO]: Epoch 033 - training loss: 0.4284, validation loss: 1.1281
2024-06-04 03:18:30 [INFO]: Epoch 034 - training loss: 0.4276, validation loss: 1.1282
2024-06-04 03:18:34 [INFO]: Epoch 035 - training loss: 0.4257, validation loss: 1.1016
2024-06-04 03:18:39 [INFO]: Epoch 036 - training loss: 0.4243, validation loss: 1.0932
2024-06-04 03:18:43 [INFO]: Epoch 037 - training loss: 0.4248, validation loss: 1.0867
2024-06-04 03:18:48 [INFO]: Epoch 038 - training loss: 0.4223, validation loss: 1.0490
2024-06-04 03:18:52 [INFO]: Epoch 039 - training loss: 0.4206, validation loss: 1.0607
2024-06-04 03:18:57 [INFO]: Epoch 040 - training loss: 0.4197, validation loss: 1.0394
2024-06-04 03:19:02 [INFO]: Epoch 041 - training loss: 0.4184, validation loss: 1.0343
2024-06-04 03:19:06 [INFO]: Epoch 042 - training loss: 0.4169, validation loss: 1.0248
2024-06-04 03:19:11 [INFO]: Epoch 043 - training loss: 0.4167, validation loss: 1.0205
2024-06-04 03:19:15 [INFO]: Epoch 044 - training loss: 0.4156, validation loss: 1.0181
2024-06-04 03:19:20 [INFO]: Epoch 045 - training loss: 0.4143, validation loss: 1.0074
2024-06-04 03:19:24 [INFO]: Epoch 046 - training loss: 0.4125, validation loss: 0.9855
2024-06-04 03:19:28 [INFO]: Epoch 047 - training loss: 0.4115, validation loss: 0.9496
2024-06-04 03:19:33 [INFO]: Epoch 048 - training loss: 0.4114, validation loss: 0.9520
2024-06-04 03:19:37 [INFO]: Epoch 049 - training loss: 0.4109, validation loss: 0.9499
2024-06-04 03:19:41 [INFO]: Epoch 050 - training loss: 0.4110, validation loss: 0.9414
2024-06-04 03:19:46 [INFO]: Epoch 051 - training loss: 0.4094, validation loss: 0.9582
2024-06-04 03:19:50 [INFO]: Epoch 052 - training loss: 0.4085, validation loss: 0.9252
2024-06-04 03:19:55 [INFO]: Epoch 053 - training loss: 0.4080, validation loss: 0.9230
2024-06-04 03:19:59 [INFO]: Epoch 054 - training loss: 0.4075, validation loss: 0.9369
2024-06-04 03:20:04 [INFO]: Epoch 055 - training loss: 0.4061, validation loss: 0.9297
2024-06-04 03:20:09 [INFO]: Epoch 056 - training loss: 0.4071, validation loss: 0.9095
2024-06-04 03:20:13 [INFO]: Epoch 057 - training loss: 0.4065, validation loss: 0.9223
2024-06-04 03:20:18 [INFO]: Epoch 058 - training loss: 0.4062, validation loss: 0.9251
2024-06-04 03:20:22 [INFO]: Epoch 059 - training loss: 0.4056, validation loss: 0.9044
2024-06-04 03:20:27 [INFO]: Epoch 060 - training loss: 0.4055, validation loss: 0.9173
2024-06-04 03:20:32 [INFO]: Epoch 061 - training loss: 0.4052, validation loss: 0.8965
2024-06-04 03:20:36 [INFO]: Epoch 062 - training loss: 0.4045, validation loss: 0.9185
2024-06-04 03:20:41 [INFO]: Epoch 063 - training loss: 0.4039, validation loss: 0.9222
2024-06-04 03:20:46 [INFO]: Epoch 064 - training loss: 0.4039, validation loss: 0.9112
2024-06-04 03:20:50 [INFO]: Epoch 065 - training loss: 0.4035, validation loss: 0.9075
2024-06-04 03:20:55 [INFO]: Epoch 066 - training loss: 0.4030, validation loss: 0.8936
2024-06-04 03:20:59 [INFO]: Epoch 067 - training loss: 0.4032, validation loss: 0.8939
2024-06-04 03:21:04 [INFO]: Epoch 068 - training loss: 0.4023, validation loss: 0.8959
2024-06-04 03:21:09 [INFO]: Epoch 069 - training loss: 0.4020, validation loss: 0.8981
2024-06-04 03:21:13 [INFO]: Epoch 070 - training loss: 0.4019, validation loss: 0.9077
2024-06-04 03:21:18 [INFO]: Epoch 071 - training loss: 0.4031, validation loss: 0.8965
2024-06-04 03:21:22 [INFO]: Epoch 072 - training loss: 0.4023, validation loss: 0.9063
2024-06-04 03:21:27 [INFO]: Epoch 073 - training loss: 0.4020, validation loss: 0.9127
2024-06-04 03:21:31 [INFO]: Epoch 074 - training loss: 0.4009, validation loss: 0.8816
2024-06-04 03:21:36 [INFO]: Epoch 075 - training loss: 0.4007, validation loss: 0.9072
2024-06-04 03:21:40 [INFO]: Epoch 076 - training loss: 0.4007, validation loss: 0.8629
2024-06-04 03:21:45 [INFO]: Epoch 077 - training loss: 0.4000, validation loss: 0.8727
2024-06-04 03:21:49 [INFO]: Epoch 078 - training loss: 0.4000, validation loss: 0.8638
2024-06-04 03:21:54 [INFO]: Epoch 079 - training loss: 0.3995, validation loss: 0.8889
2024-06-04 03:21:59 [INFO]: Epoch 080 - training loss: 0.3995, validation loss: 0.8627
2024-06-04 03:22:03 [INFO]: Epoch 081 - training loss: 0.3993, validation loss: 0.8826
2024-06-04 03:22:08 [INFO]: Epoch 082 - training loss: 0.4004, validation loss: 0.8669
2024-06-04 03:22:12 [INFO]: Epoch 083 - training loss: 0.3996, validation loss: 0.8671
2024-06-04 03:22:17 [INFO]: Epoch 084 - training loss: 0.3987, validation loss: 0.8654
2024-06-04 03:22:21 [INFO]: Epoch 085 - training loss: 0.3988, validation loss: 0.8688
2024-06-04 03:22:26 [INFO]: Epoch 086 - training loss: 0.3982, validation loss: 0.8622
2024-06-04 03:22:31 [INFO]: Epoch 087 - training loss: 0.3987, validation loss: 0.8801
2024-06-04 03:22:35 [INFO]: Epoch 088 - training loss: 0.3980, validation loss: 0.8707
2024-06-04 03:22:40 [INFO]: Epoch 089 - training loss: 0.3989, validation loss: 0.8752
2024-06-04 03:22:45 [INFO]: Epoch 090 - training loss: 0.3988, validation loss: 0.8745
2024-06-04 03:22:49 [INFO]: Epoch 091 - training loss: 0.3970, validation loss: 0.8665
2024-06-04 03:22:54 [INFO]: Epoch 092 - training loss: 0.3976, validation loss: 0.8571
2024-06-04 03:22:59 [INFO]: Epoch 093 - training loss: 0.3973, validation loss: 0.8634
2024-06-04 03:23:03 [INFO]: Epoch 094 - training loss: 0.3966, validation loss: 0.8683
2024-06-04 03:23:08 [INFO]: Epoch 095 - training loss: 0.3976, validation loss: 0.8552
2024-06-04 03:23:12 [INFO]: Epoch 096 - training loss: 0.3981, validation loss: 0.8586
2024-06-04 03:23:16 [INFO]: Epoch 097 - training loss: 0.3971, validation loss: 0.8552
2024-06-04 03:23:21 [INFO]: Epoch 098 - training loss: 0.3966, validation loss: 0.8529
2024-06-04 03:23:25 [INFO]: Epoch 099 - training loss: 0.3965, validation loss: 0.8711
2024-06-04 03:23:30 [INFO]: Epoch 100 - training loss: 0.3957, validation loss: 0.8646
2024-06-04 03:23:30 [INFO]: Finished training. The best model is from epoch#98.
2024-06-04 03:23:30 [INFO]: Saved the model to results_point_rate05/Electricity/DLinear_Electricity/round_2/20240604_T031553/DLinear.pypots
2024-06-04 03:23:32 [INFO]: Successfully saved to results_point_rate05/Electricity/DLinear_Electricity/round_2/imputation.pkl
2024-06-04 03:23:32 [INFO]: Round2 - DLinear on Electricity: MAE=0.7503, MSE=1.0456, MRE=0.4017
2024-06-04 03:23:32 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-04 03:23:32 [INFO]: Using the given device: cuda:0
2024-06-04 03:23:32 [INFO]: Model files will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_3/20240604_T032332
2024-06-04 03:23:32 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_3/20240604_T032332/tensorboard
2024-06-04 03:23:32 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-04 03:23:37 [INFO]: Epoch 001 - training loss: 1.0419, validation loss: 3.0481
2024-06-04 03:23:41 [INFO]: Epoch 002 - training loss: 0.6939, validation loss: 2.7969
2024-06-04 03:23:46 [INFO]: Epoch 003 - training loss: 0.6171, validation loss: 2.6629
2024-06-04 03:23:50 [INFO]: Epoch 004 - training loss: 0.5770, validation loss: 2.5419
2024-06-04 03:23:54 [INFO]: Epoch 005 - training loss: 0.5495, validation loss: 2.4277
2024-06-04 03:23:59 [INFO]: Epoch 006 - training loss: 0.5309, validation loss: 2.3518
2024-06-04 03:24:03 [INFO]: Epoch 007 - training loss: 0.5155, validation loss: 2.2701
2024-06-04 03:24:08 [INFO]: Epoch 008 - training loss: 0.5025, validation loss: 2.1763
2024-06-04 03:24:12 [INFO]: Epoch 009 - training loss: 0.4924, validation loss: 2.0986
2024-06-04 03:24:17 [INFO]: Epoch 010 - training loss: 0.4851, validation loss: 2.0370
2024-06-04 03:24:21 [INFO]: Epoch 011 - training loss: 0.4766, validation loss: 1.9711
2024-06-04 03:24:26 [INFO]: Epoch 012 - training loss: 0.4688, validation loss: 1.9144
2024-06-04 03:24:31 [INFO]: Epoch 013 - training loss: 0.4649, validation loss: 1.8501
2024-06-04 03:24:35 [INFO]: Epoch 014 - training loss: 0.4623, validation loss: 1.7747
2024-06-04 03:24:40 [INFO]: Epoch 015 - training loss: 0.4565, validation loss: 1.7307
2024-06-04 03:24:44 [INFO]: Epoch 016 - training loss: 0.4551, validation loss: 1.6969
2024-06-04 03:24:49 [INFO]: Epoch 017 - training loss: 0.4512, validation loss: 1.5949
2024-06-04 03:24:53 [INFO]: Epoch 018 - training loss: 0.4480, validation loss: 1.5699
2024-06-04 03:24:58 [INFO]: Epoch 019 - training loss: 0.4466, validation loss: 1.5096
2024-06-04 03:25:02 [INFO]: Epoch 020 - training loss: 0.4428, validation loss: 1.4710
2024-06-04 03:25:07 [INFO]: Epoch 021 - training loss: 0.4415, validation loss: 1.4289
2024-06-04 03:25:12 [INFO]: Epoch 022 - training loss: 0.4395, validation loss: 1.4051
2024-06-04 03:25:16 [INFO]: Epoch 023 - training loss: 0.4383, validation loss: 1.3570
2024-06-04 03:25:21 [INFO]: Epoch 024 - training loss: 0.4376, validation loss: 1.3204
2024-06-04 03:25:25 [INFO]: Epoch 025 - training loss: 0.4390, validation loss: 1.2911
2024-06-04 03:25:30 [INFO]: Epoch 026 - training loss: 0.4380, validation loss: 1.2823
2024-06-04 03:25:35 [INFO]: Epoch 027 - training loss: 0.4355, validation loss: 1.2513
2024-06-04 03:25:39 [INFO]: Epoch 028 - training loss: 0.4330, validation loss: 1.2170
2024-06-04 03:25:44 [INFO]: Epoch 029 - training loss: 0.4324, validation loss: 1.1846
2024-06-04 03:25:48 [INFO]: Epoch 030 - training loss: 0.4317, validation loss: 1.1756
2024-06-04 03:25:53 [INFO]: Epoch 031 - training loss: 0.4309, validation loss: 1.1646
2024-06-04 03:25:58 [INFO]: Epoch 032 - training loss: 0.4302, validation loss: 1.1555
2024-06-04 03:26:02 [INFO]: Epoch 033 - training loss: 0.4308, validation loss: 1.1412
2024-06-04 03:26:07 [INFO]: Epoch 034 - training loss: 0.4301, validation loss: 1.1151
2024-06-04 03:26:11 [INFO]: Epoch 035 - training loss: 0.4299, validation loss: 1.1046
2024-06-04 03:26:16 [INFO]: Epoch 036 - training loss: 0.4279, validation loss: 1.1171
2024-06-04 03:26:21 [INFO]: Epoch 037 - training loss: 0.4269, validation loss: 1.1049
2024-06-04 03:26:25 [INFO]: Epoch 038 - training loss: 0.4274, validation loss: 1.0817
2024-06-04 03:26:29 [INFO]: Epoch 039 - training loss: 0.4269, validation loss: 1.0866
2024-06-04 03:26:34 [INFO]: Epoch 040 - training loss: 0.4261, validation loss: 1.0691
2024-06-04 03:26:39 [INFO]: Epoch 041 - training loss: 0.4260, validation loss: 1.0583
2024-06-04 03:26:43 [INFO]: Epoch 042 - training loss: 0.4260, validation loss: 1.0593
2024-06-04 03:26:48 [INFO]: Epoch 043 - training loss: 0.4253, validation loss: 1.0543
2024-06-04 03:26:53 [INFO]: Epoch 044 - training loss: 0.4244, validation loss: 1.0252
2024-06-04 03:26:57 [INFO]: Epoch 045 - training loss: 0.4232, validation loss: 1.0480
2024-06-04 03:27:02 [INFO]: Epoch 046 - training loss: 0.4232, validation loss: 1.0291
2024-06-04 03:27:06 [INFO]: Epoch 047 - training loss: 0.4234, validation loss: 1.0420
2024-06-04 03:27:11 [INFO]: Epoch 048 - training loss: 0.4237, validation loss: 1.0236
2024-06-04 03:27:15 [INFO]: Epoch 049 - training loss: 0.4225, validation loss: 0.9928
2024-06-04 03:27:19 [INFO]: Epoch 050 - training loss: 0.4220, validation loss: 1.0188
2024-06-04 03:27:24 [INFO]: Epoch 051 - training loss: 0.4211, validation loss: 1.0257
2024-06-04 03:27:28 [INFO]: Epoch 052 - training loss: 0.4200, validation loss: 0.9963
2024-06-04 03:27:33 [INFO]: Epoch 053 - training loss: 0.4198, validation loss: 1.0117
2024-06-04 03:27:37 [INFO]: Epoch 054 - training loss: 0.4178, validation loss: 0.9949
2024-06-04 03:27:42 [INFO]: Epoch 055 - training loss: 0.4164, validation loss: 0.9719
2024-06-04 03:27:46 [INFO]: Epoch 056 - training loss: 0.4150, validation loss: 0.9852
2024-06-04 03:27:50 [INFO]: Epoch 057 - training loss: 0.4140, validation loss: 0.9729
2024-06-04 03:27:55 [INFO]: Epoch 058 - training loss: 0.4125, validation loss: 0.9456
2024-06-04 03:27:59 [INFO]: Epoch 059 - training loss: 0.4129, validation loss: 0.9362
2024-06-04 03:28:04 [INFO]: Epoch 060 - training loss: 0.4118, validation loss: 0.9283
2024-06-04 03:28:08 [INFO]: Epoch 061 - training loss: 0.4099, validation loss: 0.9241
2024-06-04 03:28:13 [INFO]: Epoch 062 - training loss: 0.4094, validation loss: 0.9262
2024-06-04 03:28:17 [INFO]: Epoch 063 - training loss: 0.4102, validation loss: 0.9110
2024-06-04 03:28:22 [INFO]: Epoch 064 - training loss: 0.4093, validation loss: 0.9089
2024-06-04 03:28:27 [INFO]: Epoch 065 - training loss: 0.4081, validation loss: 0.9164
2024-06-04 03:28:31 [INFO]: Epoch 066 - training loss: 0.4068, validation loss: 0.9035
2024-06-04 03:28:36 [INFO]: Epoch 067 - training loss: 0.4068, validation loss: 0.9086
2024-06-04 03:28:40 [INFO]: Epoch 068 - training loss: 0.4068, validation loss: 0.8945
2024-06-04 03:28:45 [INFO]: Epoch 069 - training loss: 0.4053, validation loss: 0.9047
2024-06-04 03:28:49 [INFO]: Epoch 070 - training loss: 0.4062, validation loss: 0.8763
2024-06-04 03:28:54 [INFO]: Epoch 071 - training loss: 0.4056, validation loss: 0.8887
2024-06-04 03:28:58 [INFO]: Epoch 072 - training loss: 0.4053, validation loss: 0.8852
2024-06-04 03:29:03 [INFO]: Epoch 073 - training loss: 0.4047, validation loss: 0.8813
2024-06-04 03:29:08 [INFO]: Epoch 074 - training loss: 0.4046, validation loss: 0.8735
2024-06-04 03:29:12 [INFO]: Epoch 075 - training loss: 0.4040, validation loss: 0.8626
2024-06-04 03:29:17 [INFO]: Epoch 076 - training loss: 0.4043, validation loss: 0.8509
2024-06-04 03:29:22 [INFO]: Epoch 077 - training loss: 0.4039, validation loss: 0.8590
2024-06-04 03:29:26 [INFO]: Epoch 078 - training loss: 0.4036, validation loss: 0.8664
2024-06-04 03:29:31 [INFO]: Epoch 079 - training loss: 0.4026, validation loss: 0.8686
2024-06-04 03:29:35 [INFO]: Epoch 080 - training loss: 0.4023, validation loss: 0.8668
2024-06-04 03:29:40 [INFO]: Epoch 081 - training loss: 0.4014, validation loss: 0.8525
2024-06-04 03:29:44 [INFO]: Epoch 082 - training loss: 0.4019, validation loss: 0.8597
2024-06-04 03:29:49 [INFO]: Epoch 083 - training loss: 0.4023, validation loss: 0.8606
2024-06-04 03:29:53 [INFO]: Epoch 084 - training loss: 0.4011, validation loss: 0.8614
2024-06-04 03:29:58 [INFO]: Epoch 085 - training loss: 0.4024, validation loss: 0.8577
2024-06-04 03:30:03 [INFO]: Epoch 086 - training loss: 0.4024, validation loss: 0.8550
2024-06-04 03:30:03 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:30:03 [INFO]: Finished training. The best model is from epoch#76.
2024-06-04 03:30:03 [INFO]: Saved the model to results_point_rate05/Electricity/DLinear_Electricity/round_3/20240604_T032332/DLinear.pypots
2024-06-04 03:30:04 [INFO]: Successfully saved to results_point_rate05/Electricity/DLinear_Electricity/round_3/imputation.pkl
2024-06-04 03:30:04 [INFO]: Round3 - DLinear on Electricity: MAE=0.7169, MSE=0.9365, MRE=0.3838
2024-06-04 03:30:04 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-04 03:30:04 [INFO]: Using the given device: cuda:0
2024-06-04 03:30:04 [INFO]: Model files will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_4/20240604_T033004
2024-06-04 03:30:04 [INFO]: Tensorboard file will be saved to results_point_rate05/Electricity/DLinear_Electricity/round_4/20240604_T033004/tensorboard
2024-06-04 03:30:05 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-04 03:30:10 [INFO]: Epoch 001 - training loss: 1.0539, validation loss: 3.1077
2024-06-04 03:30:14 [INFO]: Epoch 002 - training loss: 0.6960, validation loss: 2.8886
2024-06-04 03:30:19 [INFO]: Epoch 003 - training loss: 0.6232, validation loss: 2.7306
2024-06-04 03:30:24 [INFO]: Epoch 004 - training loss: 0.5825, validation loss: 2.5819
2024-06-04 03:30:28 [INFO]: Epoch 005 - training loss: 0.5534, validation loss: 2.4563
2024-06-04 03:30:33 [INFO]: Epoch 006 - training loss: 0.5323, validation loss: 2.3571
2024-06-04 03:30:37 [INFO]: Epoch 007 - training loss: 0.5165, validation loss: 2.2724
2024-06-04 03:30:42 [INFO]: Epoch 008 - training loss: 0.5026, validation loss: 2.2094
2024-06-04 03:30:46 [INFO]: Epoch 009 - training loss: 0.4926, validation loss: 2.1221
2024-06-04 03:30:51 [INFO]: Epoch 010 - training loss: 0.4837, validation loss: 2.0464
2024-06-04 03:30:56 [INFO]: Epoch 011 - training loss: 0.4768, validation loss: 1.9901
2024-06-04 03:31:00 [INFO]: Epoch 012 - training loss: 0.4706, validation loss: 1.9029
2024-06-04 03:31:05 [INFO]: Epoch 013 - training loss: 0.4683, validation loss: 1.8624
2024-06-04 03:31:09 [INFO]: Epoch 014 - training loss: 0.4611, validation loss: 1.7797
2024-06-04 03:31:13 [INFO]: Epoch 015 - training loss: 0.4559, validation loss: 1.7448
2024-06-04 03:31:18 [INFO]: Epoch 016 - training loss: 0.4532, validation loss: 1.6963
2024-06-04 03:31:22 [INFO]: Epoch 017 - training loss: 0.4519, validation loss: 1.6405
2024-06-04 03:31:27 [INFO]: Epoch 018 - training loss: 0.4498, validation loss: 1.5735
2024-06-04 03:31:31 [INFO]: Epoch 019 - training loss: 0.4461, validation loss: 1.5340
2024-06-04 03:31:35 [INFO]: Epoch 020 - training loss: 0.4438, validation loss: 1.4989
2024-06-04 03:31:40 [INFO]: Epoch 021 - training loss: 0.4420, validation loss: 1.4303
2024-06-04 03:31:44 [INFO]: Epoch 022 - training loss: 0.4400, validation loss: 1.3977
2024-06-04 03:31:48 [INFO]: Epoch 023 - training loss: 0.4383, validation loss: 1.3564
2024-06-04 03:31:53 [INFO]: Epoch 024 - training loss: 0.4380, validation loss: 1.3247
2024-06-04 03:31:58 [INFO]: Epoch 025 - training loss: 0.4378, validation loss: 1.2930
2024-06-04 03:32:02 [INFO]: Epoch 026 - training loss: 0.4345, validation loss: 1.2704
2024-06-04 03:32:06 [INFO]: Epoch 027 - training loss: 0.4339, validation loss: 1.2442
2024-06-04 03:32:11 [INFO]: Epoch 028 - training loss: 0.4328, validation loss: 1.1942
2024-06-04 03:32:16 [INFO]: Epoch 029 - training loss: 0.4325, validation loss: 1.1964
2024-06-04 03:32:20 [INFO]: Epoch 030 - training loss: 0.4312, validation loss: 1.1617
2024-06-04 03:32:25 [INFO]: Epoch 031 - training loss: 0.4306, validation loss: 1.1552
2024-06-04 03:32:29 [INFO]: Epoch 032 - training loss: 0.4309, validation loss: 1.1608
2024-06-04 03:32:34 [INFO]: Epoch 033 - training loss: 0.4302, validation loss: 1.1355
2024-06-04 03:32:38 [INFO]: Epoch 034 - training loss: 0.4289, validation loss: 1.1353
2024-06-04 03:32:43 [INFO]: Epoch 035 - training loss: 0.4283, validation loss: 1.1083
2024-06-04 03:32:48 [INFO]: Epoch 036 - training loss: 0.4296, validation loss: 1.1245
2024-06-04 03:32:52 [INFO]: Epoch 037 - training loss: 0.4270, validation loss: 1.0947
2024-06-04 03:32:57 [INFO]: Epoch 038 - training loss: 0.4258, validation loss: 1.0955
2024-06-04 03:33:01 [INFO]: Epoch 039 - training loss: 0.4249, validation loss: 1.0852
2024-06-04 03:33:06 [INFO]: Epoch 040 - training loss: 0.4240, validation loss: 1.0672
2024-06-04 03:33:10 [INFO]: Epoch 041 - training loss: 0.4233, validation loss: 1.0590
2024-06-04 03:33:15 [INFO]: Epoch 042 - training loss: 0.4230, validation loss: 1.0596
2024-06-04 03:33:19 [INFO]: Epoch 043 - training loss: 0.4213, validation loss: 1.0458
2024-06-04 03:33:24 [INFO]: Epoch 044 - training loss: 0.4204, validation loss: 1.0277
2024-06-04 03:33:28 [INFO]: Epoch 045 - training loss: 0.4195, validation loss: 1.0139
2024-06-04 03:33:33 [INFO]: Epoch 046 - training loss: 0.4187, validation loss: 1.0185
2024-06-04 03:33:38 [INFO]: Epoch 047 - training loss: 0.4175, validation loss: 1.0127
2024-06-04 03:33:42 [INFO]: Epoch 048 - training loss: 0.4151, validation loss: 0.9909
2024-06-04 03:33:46 [INFO]: Epoch 049 - training loss: 0.4141, validation loss: 0.9845
2024-06-04 03:33:51 [INFO]: Epoch 050 - training loss: 0.4151, validation loss: 0.9775
2024-06-04 03:33:55 [INFO]: Epoch 051 - training loss: 0.4133, validation loss: 0.9731
2024-06-04 03:34:00 [INFO]: Epoch 052 - training loss: 0.4113, validation loss: 0.9942
2024-06-04 03:34:04 [INFO]: Epoch 053 - training loss: 0.4127, validation loss: 0.9717
2024-06-04 03:34:09 [INFO]: Epoch 054 - training loss: 0.4115, validation loss: 0.9650
2024-06-04 03:34:13 [INFO]: Epoch 055 - training loss: 0.4099, validation loss: 0.9400
2024-06-04 03:34:18 [INFO]: Epoch 056 - training loss: 0.4084, validation loss: 0.9260
2024-06-04 03:34:22 [INFO]: Epoch 057 - training loss: 0.4089, validation loss: 0.9240
2024-06-04 03:34:27 [INFO]: Epoch 058 - training loss: 0.4076, validation loss: 0.9441
2024-06-04 03:34:31 [INFO]: Epoch 059 - training loss: 0.4072, validation loss: 0.9242
2024-06-04 03:34:36 [INFO]: Epoch 060 - training loss: 0.4074, validation loss: 0.9193
2024-06-04 03:34:41 [INFO]: Epoch 061 - training loss: 0.4065, validation loss: 0.9176
2024-06-04 03:34:45 [INFO]: Epoch 062 - training loss: 0.4057, validation loss: 0.9192
2024-06-04 03:34:49 [INFO]: Epoch 063 - training loss: 0.4057, validation loss: 0.9218
2024-06-04 03:34:54 [INFO]: Epoch 064 - training loss: 0.4056, validation loss: 0.8993
2024-06-04 03:34:58 [INFO]: Epoch 065 - training loss: 0.4067, validation loss: 0.9180
2024-06-04 03:35:03 [INFO]: Epoch 066 - training loss: 0.4061, validation loss: 0.8975
2024-06-04 03:35:07 [INFO]: Epoch 067 - training loss: 0.4040, validation loss: 0.8931
2024-06-04 03:35:12 [INFO]: Epoch 068 - training loss: 0.4038, validation loss: 0.8974
2024-06-04 03:35:16 [INFO]: Epoch 069 - training loss: 0.4040, validation loss: 0.9151
2024-06-04 03:35:20 [INFO]: Epoch 070 - training loss: 0.4046, validation loss: 0.9141
2024-06-04 03:35:25 [INFO]: Epoch 071 - training loss: 0.4036, validation loss: 0.8764
2024-06-04 03:35:29 [INFO]: Epoch 072 - training loss: 0.4025, validation loss: 0.8973
2024-06-04 03:35:33 [INFO]: Epoch 073 - training loss: 0.4018, validation loss: 0.8723
2024-06-04 03:35:38 [INFO]: Epoch 074 - training loss: 0.4012, validation loss: 0.8894
2024-06-04 03:35:42 [INFO]: Epoch 075 - training loss: 0.4008, validation loss: 0.8890
2024-06-04 03:35:47 [INFO]: Epoch 076 - training loss: 0.4018, validation loss: 0.8943
2024-06-04 03:35:52 [INFO]: Epoch 077 - training loss: 0.4018, validation loss: 0.8797
2024-06-04 03:35:56 [INFO]: Epoch 078 - training loss: 0.4004, validation loss: 0.8783
2024-06-04 03:36:01 [INFO]: Epoch 079 - training loss: 0.4017, validation loss: 0.8774
2024-06-04 03:36:05 [INFO]: Epoch 080 - training loss: 0.4012, validation loss: 0.8989
2024-06-04 03:36:10 [INFO]: Epoch 081 - training loss: 0.4001, validation loss: 0.8735
2024-06-04 03:36:15 [INFO]: Epoch 082 - training loss: 0.4001, validation loss: 0.8896
2024-06-04 03:36:19 [INFO]: Epoch 083 - training loss: 0.4006, validation loss: 0.8819
2024-06-04 03:36:19 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-04 03:36:19 [INFO]: Finished training. The best model is from epoch#73.
2024-06-04 03:36:19 [INFO]: Saved the model to results_point_rate05/Electricity/DLinear_Electricity/round_4/20240604_T033004/DLinear.pypots
2024-06-04 03:36:21 [INFO]: Successfully saved to results_point_rate05/Electricity/DLinear_Electricity/round_4/imputation.pkl
2024-06-04 03:36:21 [INFO]: Round4 - DLinear on Electricity: MAE=0.7421, MSE=1.0144, MRE=0.3973
2024-06-04 03:36:21 [INFO]: Done! Final results:
Averaged DLinear (2,294,692 params) on Electricity: MAE=0.7343 ± 0.011434025184742324, MSE=0.9882 ± 0.038451494825727156, MRE=0.3931 ± 0.006121852660269308, average inference time=0.27
