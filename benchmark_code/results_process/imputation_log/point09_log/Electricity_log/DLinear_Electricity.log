2024-06-03 01:11:55 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 01:11:55 [INFO]: Using the given device: cuda:0
2024-06-03 01:11:55 [INFO]: Model files will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_0/20240603_T011155
2024-06-03 01:11:55 [INFO]: Tensorboard file will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_0/20240603_T011155/tensorboard
2024-06-03 01:11:55 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-03 01:12:00 [INFO]: Epoch 001 - training loss: 1.2414, validation loss: 3.3680
2024-06-03 01:12:04 [INFO]: Epoch 002 - training loss: 0.7892, validation loss: 3.0444
2024-06-03 01:12:10 [INFO]: Epoch 003 - training loss: 0.6826, validation loss: 2.8793
2024-06-03 01:12:15 [INFO]: Epoch 004 - training loss: 0.6466, validation loss: 2.8516
2024-06-03 01:12:20 [INFO]: Epoch 005 - training loss: 0.6232, validation loss: 2.7800
2024-06-03 01:12:25 [INFO]: Epoch 006 - training loss: 0.6008, validation loss: 2.6860
2024-06-03 01:12:31 [INFO]: Epoch 007 - training loss: 0.5844, validation loss: 2.6019
2024-06-03 01:12:36 [INFO]: Epoch 008 - training loss: 0.5713, validation loss: 2.5732
2024-06-03 01:12:41 [INFO]: Epoch 009 - training loss: 0.5596, validation loss: 2.5062
2024-06-03 01:12:47 [INFO]: Epoch 010 - training loss: 0.5489, validation loss: 2.4941
2024-06-03 01:12:52 [INFO]: Epoch 011 - training loss: 0.5419, validation loss: 2.4600
2024-06-03 01:12:58 [INFO]: Epoch 012 - training loss: 0.5336, validation loss: 2.4235
2024-06-03 01:13:03 [INFO]: Epoch 013 - training loss: 0.5274, validation loss: 2.3475
2024-06-03 01:13:09 [INFO]: Epoch 014 - training loss: 0.5216, validation loss: 2.3354
2024-06-03 01:13:14 [INFO]: Epoch 015 - training loss: 0.5167, validation loss: 2.3175
2024-06-03 01:13:20 [INFO]: Epoch 016 - training loss: 0.5113, validation loss: 2.2935
2024-06-03 01:13:25 [INFO]: Epoch 017 - training loss: 0.5084, validation loss: 2.2633
2024-06-03 01:13:30 [INFO]: Epoch 018 - training loss: 0.5050, validation loss: 2.2369
2024-06-03 01:13:36 [INFO]: Epoch 019 - training loss: 0.5029, validation loss: 2.2085
2024-06-03 01:13:41 [INFO]: Epoch 020 - training loss: 0.4997, validation loss: 2.1901
2024-06-03 01:13:47 [INFO]: Epoch 021 - training loss: 0.4989, validation loss: 2.1805
2024-06-03 01:13:52 [INFO]: Epoch 022 - training loss: 0.4967, validation loss: 2.1483
2024-06-03 01:13:57 [INFO]: Epoch 023 - training loss: 0.4956, validation loss: 2.1432
2024-06-03 01:14:03 [INFO]: Epoch 024 - training loss: 0.4942, validation loss: 2.1265
2024-06-03 01:14:08 [INFO]: Epoch 025 - training loss: 0.4905, validation loss: 2.1055
2024-06-03 01:14:14 [INFO]: Epoch 026 - training loss: 0.4894, validation loss: 2.0906
2024-06-03 01:14:19 [INFO]: Epoch 027 - training loss: 0.4887, validation loss: 2.0765
2024-06-03 01:14:25 [INFO]: Epoch 028 - training loss: 0.4876, validation loss: 2.0731
2024-06-03 01:14:30 [INFO]: Epoch 029 - training loss: 0.4875, validation loss: 2.0398
2024-06-03 01:14:36 [INFO]: Epoch 030 - training loss: 0.4865, validation loss: 2.0438
2024-06-03 01:14:41 [INFO]: Epoch 031 - training loss: 0.4856, validation loss: 2.0224
2024-06-03 01:14:47 [INFO]: Epoch 032 - training loss: 0.4843, validation loss: 2.0166
2024-06-03 01:14:52 [INFO]: Epoch 033 - training loss: 0.4824, validation loss: 1.9995
2024-06-03 01:14:58 [INFO]: Epoch 034 - training loss: 0.4815, validation loss: 2.0079
2024-06-03 01:15:03 [INFO]: Epoch 035 - training loss: 0.4803, validation loss: 1.9833
2024-06-03 01:15:09 [INFO]: Epoch 036 - training loss: 0.4796, validation loss: 1.9688
2024-06-03 01:15:14 [INFO]: Epoch 037 - training loss: 0.4797, validation loss: 1.9684
2024-06-03 01:15:20 [INFO]: Epoch 038 - training loss: 0.4787, validation loss: 1.9467
2024-06-03 01:15:25 [INFO]: Epoch 039 - training loss: 0.4765, validation loss: 1.9390
2024-06-03 01:15:31 [INFO]: Epoch 040 - training loss: 0.4758, validation loss: 1.9298
2024-06-03 01:15:36 [INFO]: Epoch 041 - training loss: 0.4748, validation loss: 1.9164
2024-06-03 01:15:42 [INFO]: Epoch 042 - training loss: 0.4733, validation loss: 1.9245
2024-06-03 01:15:48 [INFO]: Epoch 043 - training loss: 0.4735, validation loss: 1.9074
2024-06-03 01:15:53 [INFO]: Epoch 044 - training loss: 0.4713, validation loss: 1.8990
2024-06-03 01:15:59 [INFO]: Epoch 045 - training loss: 0.4705, validation loss: 1.8929
2024-06-03 01:16:04 [INFO]: Epoch 046 - training loss: 0.4698, validation loss: 1.8964
2024-06-03 01:16:10 [INFO]: Epoch 047 - training loss: 0.4687, validation loss: 1.8742
2024-06-03 01:16:15 [INFO]: Epoch 048 - training loss: 0.4676, validation loss: 1.8580
2024-06-03 01:16:21 [INFO]: Epoch 049 - training loss: 0.4667, validation loss: 1.8451
2024-06-03 01:16:26 [INFO]: Epoch 050 - training loss: 0.4658, validation loss: 1.8499
2024-06-03 01:16:31 [INFO]: Epoch 051 - training loss: 0.4660, validation loss: 1.8439
2024-06-03 01:16:37 [INFO]: Epoch 052 - training loss: 0.4648, validation loss: 1.8400
2024-06-03 01:16:42 [INFO]: Epoch 053 - training loss: 0.4649, validation loss: 1.8296
2024-06-03 01:16:48 [INFO]: Epoch 054 - training loss: 0.4635, validation loss: 1.8175
2024-06-03 01:16:53 [INFO]: Epoch 055 - training loss: 0.4630, validation loss: 1.8245
2024-06-03 01:16:59 [INFO]: Epoch 056 - training loss: 0.4615, validation loss: 1.8230
2024-06-03 01:17:04 [INFO]: Epoch 057 - training loss: 0.4614, validation loss: 1.8222
2024-06-03 01:17:10 [INFO]: Epoch 058 - training loss: 0.4617, validation loss: 1.8217
2024-06-03 01:17:15 [INFO]: Epoch 059 - training loss: 0.4602, validation loss: 1.8203
2024-06-03 01:17:20 [INFO]: Epoch 060 - training loss: 0.4593, validation loss: 1.7918
2024-06-03 01:17:26 [INFO]: Epoch 061 - training loss: 0.4588, validation loss: 1.7834
2024-06-03 01:17:31 [INFO]: Epoch 062 - training loss: 0.4585, validation loss: 1.7865
2024-06-03 01:17:37 [INFO]: Epoch 063 - training loss: 0.4575, validation loss: 1.7915
2024-06-03 01:17:42 [INFO]: Epoch 064 - training loss: 0.4573, validation loss: 1.7825
2024-06-03 01:17:48 [INFO]: Epoch 065 - training loss: 0.4572, validation loss: 1.7920
2024-06-03 01:17:53 [INFO]: Epoch 066 - training loss: 0.4566, validation loss: 1.7821
2024-06-03 01:17:59 [INFO]: Epoch 067 - training loss: 0.4562, validation loss: 1.7755
2024-06-03 01:18:04 [INFO]: Epoch 068 - training loss: 0.4567, validation loss: 1.7765
2024-06-03 01:18:09 [INFO]: Epoch 069 - training loss: 0.4564, validation loss: 1.7750
2024-06-03 01:18:15 [INFO]: Epoch 070 - training loss: 0.4548, validation loss: 1.7675
2024-06-03 01:18:21 [INFO]: Epoch 071 - training loss: 0.4540, validation loss: 1.7657
2024-06-03 01:18:26 [INFO]: Epoch 072 - training loss: 0.4546, validation loss: 1.7705
2024-06-03 01:18:32 [INFO]: Epoch 073 - training loss: 0.4540, validation loss: 1.7721
2024-06-03 01:18:37 [INFO]: Epoch 074 - training loss: 0.4530, validation loss: 1.7565
2024-06-03 01:18:43 [INFO]: Epoch 075 - training loss: 0.4533, validation loss: 1.7555
2024-06-03 01:18:48 [INFO]: Epoch 076 - training loss: 0.4525, validation loss: 1.7517
2024-06-03 01:18:54 [INFO]: Epoch 077 - training loss: 0.4532, validation loss: 1.7589
2024-06-03 01:18:59 [INFO]: Epoch 078 - training loss: 0.4521, validation loss: 1.7648
2024-06-03 01:19:05 [INFO]: Epoch 079 - training loss: 0.4528, validation loss: 1.7679
2024-06-03 01:19:10 [INFO]: Epoch 080 - training loss: 0.4512, validation loss: 1.7678
2024-06-03 01:19:15 [INFO]: Epoch 081 - training loss: 0.4513, validation loss: 1.7696
2024-06-03 01:19:21 [INFO]: Epoch 082 - training loss: 0.4520, validation loss: 1.7620
2024-06-03 01:19:26 [INFO]: Epoch 083 - training loss: 0.4513, validation loss: 1.7628
2024-06-03 01:19:32 [INFO]: Epoch 084 - training loss: 0.4505, validation loss: 1.7549
2024-06-03 01:19:37 [INFO]: Epoch 085 - training loss: 0.4508, validation loss: 1.7491
2024-06-03 01:19:43 [INFO]: Epoch 086 - training loss: 0.4508, validation loss: 1.7528
2024-06-03 01:19:49 [INFO]: Epoch 087 - training loss: 0.4500, validation loss: 1.7498
2024-06-03 01:19:54 [INFO]: Epoch 088 - training loss: 0.4500, validation loss: 1.7322
2024-06-03 01:20:00 [INFO]: Epoch 089 - training loss: 0.4499, validation loss: 1.7584
2024-06-03 01:20:05 [INFO]: Epoch 090 - training loss: 0.4499, validation loss: 1.7447
2024-06-03 01:20:11 [INFO]: Epoch 091 - training loss: 0.4495, validation loss: 1.7506
2024-06-03 01:20:16 [INFO]: Epoch 092 - training loss: 0.4494, validation loss: 1.7557
2024-06-03 01:20:22 [INFO]: Epoch 093 - training loss: 0.4491, validation loss: 1.7665
2024-06-03 01:20:27 [INFO]: Epoch 094 - training loss: 0.4487, validation loss: 1.7575
2024-06-03 01:20:32 [INFO]: Epoch 095 - training loss: 0.4487, validation loss: 1.7525
2024-06-03 01:20:37 [INFO]: Epoch 096 - training loss: 0.4486, validation loss: 1.7426
2024-06-03 01:20:43 [INFO]: Epoch 097 - training loss: 0.4493, validation loss: 1.7363
2024-06-03 01:20:48 [INFO]: Epoch 098 - training loss: 0.4484, validation loss: 1.7452
2024-06-03 01:20:48 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 01:20:48 [INFO]: Finished training. The best model is from epoch#88.
2024-06-03 01:20:48 [INFO]: Saved the model to results_point_rate09/Electricity/DLinear_Electricity/round_0/20240603_T011155/DLinear.pypots
2024-06-03 01:20:49 [INFO]: Successfully saved to results_point_rate09/Electricity/DLinear_Electricity/round_0/imputation.pkl
2024-06-03 01:20:49 [INFO]: Round0 - DLinear on Electricity: MAE=0.8981, MSE=1.7219, MRE=0.4808
2024-06-03 01:20:49 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 01:20:49 [INFO]: Using the given device: cuda:0
2024-06-03 01:20:49 [INFO]: Model files will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_1/20240603_T012049
2024-06-03 01:20:49 [INFO]: Tensorboard file will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_1/20240603_T012049/tensorboard
2024-06-03 01:20:49 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-03 01:20:55 [INFO]: Epoch 001 - training loss: 1.2799, validation loss: 3.4692
2024-06-03 01:21:01 [INFO]: Epoch 002 - training loss: 0.8070, validation loss: 3.0702
2024-06-03 01:21:06 [INFO]: Epoch 003 - training loss: 0.6954, validation loss: 3.0259
2024-06-03 01:21:12 [INFO]: Epoch 004 - training loss: 0.6533, validation loss: 2.9105
2024-06-03 01:21:18 [INFO]: Epoch 005 - training loss: 0.6244, validation loss: 2.8222
2024-06-03 01:21:23 [INFO]: Epoch 006 - training loss: 0.6025, validation loss: 2.7201
2024-06-03 01:21:28 [INFO]: Epoch 007 - training loss: 0.5816, validation loss: 2.6405
2024-06-03 01:21:34 [INFO]: Epoch 008 - training loss: 0.5617, validation loss: 2.5894
2024-06-03 01:21:39 [INFO]: Epoch 009 - training loss: 0.5469, validation loss: 2.5321
2024-06-03 01:21:45 [INFO]: Epoch 010 - training loss: 0.5354, validation loss: 2.5085
2024-06-03 01:21:50 [INFO]: Epoch 011 - training loss: 0.5264, validation loss: 2.4627
2024-06-03 01:21:56 [INFO]: Epoch 012 - training loss: 0.5213, validation loss: 2.4489
2024-06-03 01:22:01 [INFO]: Epoch 013 - training loss: 0.5172, validation loss: 2.4258
2024-06-03 01:22:07 [INFO]: Epoch 014 - training loss: 0.5142, validation loss: 2.4086
2024-06-03 01:22:12 [INFO]: Epoch 015 - training loss: 0.5098, validation loss: 2.3518
2024-06-03 01:22:18 [INFO]: Epoch 016 - training loss: 0.5057, validation loss: 2.3491
2024-06-03 01:22:23 [INFO]: Epoch 017 - training loss: 0.5008, validation loss: 2.3336
2024-06-03 01:22:28 [INFO]: Epoch 018 - training loss: 0.4984, validation loss: 2.3094
2024-06-03 01:22:34 [INFO]: Epoch 019 - training loss: 0.4966, validation loss: 2.2835
2024-06-03 01:22:40 [INFO]: Epoch 020 - training loss: 0.4930, validation loss: 2.2680
2024-06-03 01:22:45 [INFO]: Epoch 021 - training loss: 0.4918, validation loss: 2.2350
2024-06-03 01:22:51 [INFO]: Epoch 022 - training loss: 0.4907, validation loss: 2.2209
2024-06-03 01:22:56 [INFO]: Epoch 023 - training loss: 0.4891, validation loss: 2.2203
2024-06-03 01:23:02 [INFO]: Epoch 024 - training loss: 0.4878, validation loss: 2.2024
2024-06-03 01:23:07 [INFO]: Epoch 025 - training loss: 0.4860, validation loss: 2.1804
2024-06-03 01:23:13 [INFO]: Epoch 026 - training loss: 0.4855, validation loss: 2.1601
2024-06-03 01:23:18 [INFO]: Epoch 027 - training loss: 0.4847, validation loss: 2.1450
2024-06-03 01:23:23 [INFO]: Epoch 028 - training loss: 0.4824, validation loss: 2.1352
2024-06-03 01:23:29 [INFO]: Epoch 029 - training loss: 0.4813, validation loss: 2.1209
2024-06-03 01:23:34 [INFO]: Epoch 030 - training loss: 0.4807, validation loss: 2.0968
2024-06-03 01:23:40 [INFO]: Epoch 031 - training loss: 0.4790, validation loss: 2.0921
2024-06-03 01:23:46 [INFO]: Epoch 032 - training loss: 0.4791, validation loss: 2.0736
2024-06-03 01:23:51 [INFO]: Epoch 033 - training loss: 0.4776, validation loss: 2.0745
2024-06-03 01:23:57 [INFO]: Epoch 034 - training loss: 0.4770, validation loss: 2.0530
2024-06-03 01:24:02 [INFO]: Epoch 035 - training loss: 0.4754, validation loss: 2.0417
2024-06-03 01:24:07 [INFO]: Epoch 036 - training loss: 0.4762, validation loss: 2.0202
2024-06-03 01:24:13 [INFO]: Epoch 037 - training loss: 0.4737, validation loss: 2.0192
2024-06-03 01:24:18 [INFO]: Epoch 038 - training loss: 0.4728, validation loss: 2.0124
2024-06-03 01:24:23 [INFO]: Epoch 039 - training loss: 0.4729, validation loss: 1.9943
2024-06-03 01:24:29 [INFO]: Epoch 040 - training loss: 0.4708, validation loss: 1.9837
2024-06-03 01:24:34 [INFO]: Epoch 041 - training loss: 0.4699, validation loss: 1.9850
2024-06-03 01:24:40 [INFO]: Epoch 042 - training loss: 0.4695, validation loss: 1.9619
2024-06-03 01:24:45 [INFO]: Epoch 043 - training loss: 0.4688, validation loss: 1.9667
2024-06-03 01:24:50 [INFO]: Epoch 044 - training loss: 0.4691, validation loss: 1.9505
2024-06-03 01:24:56 [INFO]: Epoch 045 - training loss: 0.4667, validation loss: 1.9328
2024-06-03 01:25:02 [INFO]: Epoch 046 - training loss: 0.4668, validation loss: 1.9307
2024-06-03 01:25:07 [INFO]: Epoch 047 - training loss: 0.4665, validation loss: 1.9214
2024-06-03 01:25:13 [INFO]: Epoch 048 - training loss: 0.4654, validation loss: 1.9353
2024-06-03 01:25:18 [INFO]: Epoch 049 - training loss: 0.4652, validation loss: 1.9087
2024-06-03 01:25:24 [INFO]: Epoch 050 - training loss: 0.4640, validation loss: 1.8965
2024-06-03 01:25:29 [INFO]: Epoch 051 - training loss: 0.4631, validation loss: 1.8887
2024-06-03 01:25:35 [INFO]: Epoch 052 - training loss: 0.4632, validation loss: 1.8928
2024-06-03 01:25:40 [INFO]: Epoch 053 - training loss: 0.4619, validation loss: 1.8768
2024-06-03 01:25:45 [INFO]: Epoch 054 - training loss: 0.4612, validation loss: 1.8586
2024-06-03 01:25:51 [INFO]: Epoch 055 - training loss: 0.4608, validation loss: 1.8587
2024-06-03 01:25:56 [INFO]: Epoch 056 - training loss: 0.4601, validation loss: 1.8685
2024-06-03 01:26:02 [INFO]: Epoch 057 - training loss: 0.4593, validation loss: 1.8598
2024-06-03 01:26:07 [INFO]: Epoch 058 - training loss: 0.4588, validation loss: 1.8738
2024-06-03 01:26:13 [INFO]: Epoch 059 - training loss: 0.4580, validation loss: 1.8476
2024-06-03 01:26:18 [INFO]: Epoch 060 - training loss: 0.4584, validation loss: 1.8443
2024-06-03 01:26:24 [INFO]: Epoch 061 - training loss: 0.4577, validation loss: 1.8415
2024-06-03 01:26:29 [INFO]: Epoch 062 - training loss: 0.4566, validation loss: 1.8307
2024-06-03 01:26:35 [INFO]: Epoch 063 - training loss: 0.4566, validation loss: 1.8418
2024-06-03 01:26:40 [INFO]: Epoch 064 - training loss: 0.4571, validation loss: 1.8296
2024-06-03 01:26:46 [INFO]: Epoch 065 - training loss: 0.4566, validation loss: 1.8274
2024-06-03 01:26:51 [INFO]: Epoch 066 - training loss: 0.4567, validation loss: 1.8309
2024-06-03 01:26:56 [INFO]: Epoch 067 - training loss: 0.4557, validation loss: 1.8156
2024-06-03 01:27:02 [INFO]: Epoch 068 - training loss: 0.4550, validation loss: 1.8229
2024-06-03 01:27:07 [INFO]: Epoch 069 - training loss: 0.4548, validation loss: 1.8019
2024-06-03 01:27:13 [INFO]: Epoch 070 - training loss: 0.4545, validation loss: 1.8009
2024-06-03 01:27:18 [INFO]: Epoch 071 - training loss: 0.4541, validation loss: 1.8035
2024-06-03 01:27:24 [INFO]: Epoch 072 - training loss: 0.4542, validation loss: 1.8204
2024-06-03 01:27:30 [INFO]: Epoch 073 - training loss: 0.4538, validation loss: 1.8170
2024-06-03 01:27:35 [INFO]: Epoch 074 - training loss: 0.4527, validation loss: 1.8075
2024-06-03 01:27:40 [INFO]: Epoch 075 - training loss: 0.4523, validation loss: 1.7953
2024-06-03 01:27:46 [INFO]: Epoch 076 - training loss: 0.4521, validation loss: 1.8039
2024-06-03 01:27:51 [INFO]: Epoch 077 - training loss: 0.4518, validation loss: 1.7929
2024-06-03 01:27:57 [INFO]: Epoch 078 - training loss: 0.4519, validation loss: 1.8149
2024-06-03 01:28:03 [INFO]: Epoch 079 - training loss: 0.4517, validation loss: 1.7867
2024-06-03 01:28:08 [INFO]: Epoch 080 - training loss: 0.4521, validation loss: 1.7918
2024-06-03 01:28:13 [INFO]: Epoch 081 - training loss: 0.4518, validation loss: 1.8017
2024-06-03 01:28:18 [INFO]: Epoch 082 - training loss: 0.4516, validation loss: 1.7866
2024-06-03 01:28:24 [INFO]: Epoch 083 - training loss: 0.4517, validation loss: 1.7985
2024-06-03 01:28:29 [INFO]: Epoch 084 - training loss: 0.4505, validation loss: 1.7814
2024-06-03 01:28:35 [INFO]: Epoch 085 - training loss: 0.4500, validation loss: 1.7809
2024-06-03 01:28:40 [INFO]: Epoch 086 - training loss: 0.4507, validation loss: 1.7838
2024-06-03 01:28:45 [INFO]: Epoch 087 - training loss: 0.4503, validation loss: 1.7799
2024-06-03 01:28:50 [INFO]: Epoch 088 - training loss: 0.4495, validation loss: 1.7686
2024-06-03 01:28:56 [INFO]: Epoch 089 - training loss: 0.4492, validation loss: 1.7680
2024-06-03 01:29:01 [INFO]: Epoch 090 - training loss: 0.4488, validation loss: 1.7758
2024-06-03 01:29:07 [INFO]: Epoch 091 - training loss: 0.4493, validation loss: 1.7702
2024-06-03 01:29:12 [INFO]: Epoch 092 - training loss: 0.4490, validation loss: 1.7682
2024-06-03 01:29:18 [INFO]: Epoch 093 - training loss: 0.4480, validation loss: 1.7599
2024-06-03 01:29:23 [INFO]: Epoch 094 - training loss: 0.4485, validation loss: 1.7655
2024-06-03 01:29:29 [INFO]: Epoch 095 - training loss: 0.4485, validation loss: 1.7743
2024-06-03 01:29:35 [INFO]: Epoch 096 - training loss: 0.4487, validation loss: 1.7709
2024-06-03 01:29:40 [INFO]: Epoch 097 - training loss: 0.4492, validation loss: 1.7719
2024-06-03 01:29:46 [INFO]: Epoch 098 - training loss: 0.4478, validation loss: 1.7719
2024-06-03 01:29:51 [INFO]: Epoch 099 - training loss: 0.4483, validation loss: 1.7847
2024-06-03 01:29:57 [INFO]: Epoch 100 - training loss: 0.4472, validation loss: 1.7805
2024-06-03 01:29:57 [INFO]: Finished training. The best model is from epoch#93.
2024-06-03 01:29:57 [INFO]: Saved the model to results_point_rate09/Electricity/DLinear_Electricity/round_1/20240603_T012049/DLinear.pypots
2024-06-03 01:29:57 [INFO]: Successfully saved to results_point_rate09/Electricity/DLinear_Electricity/round_1/imputation.pkl
2024-06-03 01:29:57 [INFO]: Round1 - DLinear on Electricity: MAE=0.8527, MSE=1.5923, MRE=0.4565
2024-06-03 01:29:57 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 01:29:57 [INFO]: Using the given device: cuda:0
2024-06-03 01:29:57 [INFO]: Model files will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_2/20240603_T012957
2024-06-03 01:29:57 [INFO]: Tensorboard file will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_2/20240603_T012957/tensorboard
2024-06-03 01:29:58 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-03 01:30:03 [INFO]: Epoch 001 - training loss: 1.1802, validation loss: 3.3444
2024-06-03 01:30:09 [INFO]: Epoch 002 - training loss: 0.7644, validation loss: 3.0413
2024-06-03 01:30:14 [INFO]: Epoch 003 - training loss: 0.6953, validation loss: 2.9705
2024-06-03 01:30:20 [INFO]: Epoch 004 - training loss: 0.6558, validation loss: 2.9006
2024-06-03 01:30:25 [INFO]: Epoch 005 - training loss: 0.6290, validation loss: 2.7994
2024-06-03 01:30:31 [INFO]: Epoch 006 - training loss: 0.6084, validation loss: 2.7388
2024-06-03 01:30:36 [INFO]: Epoch 007 - training loss: 0.5892, validation loss: 2.6550
2024-06-03 01:30:41 [INFO]: Epoch 008 - training loss: 0.5727, validation loss: 2.6100
2024-06-03 01:30:47 [INFO]: Epoch 009 - training loss: 0.5602, validation loss: 2.5829
2024-06-03 01:30:52 [INFO]: Epoch 010 - training loss: 0.5458, validation loss: 2.5257
2024-06-03 01:30:58 [INFO]: Epoch 011 - training loss: 0.5358, validation loss: 2.4748
2024-06-03 01:31:03 [INFO]: Epoch 012 - training loss: 0.5284, validation loss: 2.4281
2024-06-03 01:31:09 [INFO]: Epoch 013 - training loss: 0.5215, validation loss: 2.4162
2024-06-03 01:31:14 [INFO]: Epoch 014 - training loss: 0.5172, validation loss: 2.3817
2024-06-03 01:31:19 [INFO]: Epoch 015 - training loss: 0.5128, validation loss: 2.3604
2024-06-03 01:31:25 [INFO]: Epoch 016 - training loss: 0.5090, validation loss: 2.3630
2024-06-03 01:31:30 [INFO]: Epoch 017 - training loss: 0.5064, validation loss: 2.3254
2024-06-03 01:31:36 [INFO]: Epoch 018 - training loss: 0.5036, validation loss: 2.2957
2024-06-03 01:31:41 [INFO]: Epoch 019 - training loss: 0.4997, validation loss: 2.2806
2024-06-03 01:31:47 [INFO]: Epoch 020 - training loss: 0.4979, validation loss: 2.2725
2024-06-03 01:31:52 [INFO]: Epoch 021 - training loss: 0.4967, validation loss: 2.2331
2024-06-03 01:31:57 [INFO]: Epoch 022 - training loss: 0.4943, validation loss: 2.2320
2024-06-03 01:32:03 [INFO]: Epoch 023 - training loss: 0.4919, validation loss: 2.2132
2024-06-03 01:32:08 [INFO]: Epoch 024 - training loss: 0.4909, validation loss: 2.2096
2024-06-03 01:32:14 [INFO]: Epoch 025 - training loss: 0.4889, validation loss: 2.2031
2024-06-03 01:32:19 [INFO]: Epoch 026 - training loss: 0.4886, validation loss: 2.1592
2024-06-03 01:32:24 [INFO]: Epoch 027 - training loss: 0.4879, validation loss: 2.1521
2024-06-03 01:32:30 [INFO]: Epoch 028 - training loss: 0.4861, validation loss: 2.1326
2024-06-03 01:32:35 [INFO]: Epoch 029 - training loss: 0.4864, validation loss: 2.1267
2024-06-03 01:32:40 [INFO]: Epoch 030 - training loss: 0.4851, validation loss: 2.1077
2024-06-03 01:32:46 [INFO]: Epoch 031 - training loss: 0.4836, validation loss: 2.0845
2024-06-03 01:32:51 [INFO]: Epoch 032 - training loss: 0.4822, validation loss: 2.0706
2024-06-03 01:32:56 [INFO]: Epoch 033 - training loss: 0.4813, validation loss: 2.0821
2024-06-03 01:33:02 [INFO]: Epoch 034 - training loss: 0.4808, validation loss: 2.0710
2024-06-03 01:33:08 [INFO]: Epoch 035 - training loss: 0.4811, validation loss: 2.0422
2024-06-03 01:33:13 [INFO]: Epoch 036 - training loss: 0.4790, validation loss: 2.0453
2024-06-03 01:33:18 [INFO]: Epoch 037 - training loss: 0.4782, validation loss: 2.0460
2024-06-03 01:33:24 [INFO]: Epoch 038 - training loss: 0.4780, validation loss: 2.0077
2024-06-03 01:33:29 [INFO]: Epoch 039 - training loss: 0.4768, validation loss: 2.0270
2024-06-03 01:33:35 [INFO]: Epoch 040 - training loss: 0.4754, validation loss: 1.9934
2024-06-03 01:33:40 [INFO]: Epoch 041 - training loss: 0.4761, validation loss: 2.0142
2024-06-03 01:33:46 [INFO]: Epoch 042 - training loss: 0.4745, validation loss: 1.9915
2024-06-03 01:33:51 [INFO]: Epoch 043 - training loss: 0.4735, validation loss: 1.9747
2024-06-03 01:33:56 [INFO]: Epoch 044 - training loss: 0.4732, validation loss: 1.9914
2024-06-03 01:34:02 [INFO]: Epoch 045 - training loss: 0.4721, validation loss: 1.9738
2024-06-03 01:34:07 [INFO]: Epoch 046 - training loss: 0.4713, validation loss: 1.9582
2024-06-03 01:34:13 [INFO]: Epoch 047 - training loss: 0.4716, validation loss: 1.9681
2024-06-03 01:34:18 [INFO]: Epoch 048 - training loss: 0.4705, validation loss: 1.9306
2024-06-03 01:34:24 [INFO]: Epoch 049 - training loss: 0.4709, validation loss: 1.9375
2024-06-03 01:34:29 [INFO]: Epoch 050 - training loss: 0.4690, validation loss: 1.9250
2024-06-03 01:34:34 [INFO]: Epoch 051 - training loss: 0.4685, validation loss: 1.9205
2024-06-03 01:34:40 [INFO]: Epoch 052 - training loss: 0.4664, validation loss: 1.9121
2024-06-03 01:34:45 [INFO]: Epoch 053 - training loss: 0.4665, validation loss: 1.9158
2024-06-03 01:34:51 [INFO]: Epoch 054 - training loss: 0.4665, validation loss: 1.8880
2024-06-03 01:34:56 [INFO]: Epoch 055 - training loss: 0.4656, validation loss: 1.8796
2024-06-03 01:35:02 [INFO]: Epoch 056 - training loss: 0.4649, validation loss: 1.8801
2024-06-03 01:35:07 [INFO]: Epoch 057 - training loss: 0.4631, validation loss: 1.8742
2024-06-03 01:35:13 [INFO]: Epoch 058 - training loss: 0.4647, validation loss: 1.8633
2024-06-03 01:35:18 [INFO]: Epoch 059 - training loss: 0.4636, validation loss: 1.8756
2024-06-03 01:35:24 [INFO]: Epoch 060 - training loss: 0.4624, validation loss: 1.8541
2024-06-03 01:35:29 [INFO]: Epoch 061 - training loss: 0.4620, validation loss: 1.8496
2024-06-03 01:35:35 [INFO]: Epoch 062 - training loss: 0.4618, validation loss: 1.8501
2024-06-03 01:35:40 [INFO]: Epoch 063 - training loss: 0.4601, validation loss: 1.8567
2024-06-03 01:35:46 [INFO]: Epoch 064 - training loss: 0.4608, validation loss: 1.8324
2024-06-03 01:35:51 [INFO]: Epoch 065 - training loss: 0.4599, validation loss: 1.8411
2024-06-03 01:35:57 [INFO]: Epoch 066 - training loss: 0.4595, validation loss: 1.8445
2024-06-03 01:36:02 [INFO]: Epoch 067 - training loss: 0.4590, validation loss: 1.8427
2024-06-03 01:36:08 [INFO]: Epoch 068 - training loss: 0.4586, validation loss: 1.8218
2024-06-03 01:36:13 [INFO]: Epoch 069 - training loss: 0.4596, validation loss: 1.8218
2024-06-03 01:36:19 [INFO]: Epoch 070 - training loss: 0.4569, validation loss: 1.8116
2024-06-03 01:36:24 [INFO]: Epoch 071 - training loss: 0.4573, validation loss: 1.8105
2024-06-03 01:36:29 [INFO]: Epoch 072 - training loss: 0.4576, validation loss: 1.8230
2024-06-03 01:36:35 [INFO]: Epoch 073 - training loss: 0.4573, validation loss: 1.8211
2024-06-03 01:36:40 [INFO]: Epoch 074 - training loss: 0.4558, validation loss: 1.7970
2024-06-03 01:36:46 [INFO]: Epoch 075 - training loss: 0.4576, validation loss: 1.8014
2024-06-03 01:36:51 [INFO]: Epoch 076 - training loss: 0.4563, validation loss: 1.7961
2024-06-03 01:36:56 [INFO]: Epoch 077 - training loss: 0.4544, validation loss: 1.7904
2024-06-03 01:37:02 [INFO]: Epoch 078 - training loss: 0.4551, validation loss: 1.7791
2024-06-03 01:37:07 [INFO]: Epoch 079 - training loss: 0.4540, validation loss: 1.7931
2024-06-03 01:37:13 [INFO]: Epoch 080 - training loss: 0.4556, validation loss: 1.7811
2024-06-03 01:37:18 [INFO]: Epoch 081 - training loss: 0.4549, validation loss: 1.7984
2024-06-03 01:37:24 [INFO]: Epoch 082 - training loss: 0.4544, validation loss: 1.7965
2024-06-03 01:37:29 [INFO]: Epoch 083 - training loss: 0.4540, validation loss: 1.7773
2024-06-03 01:37:35 [INFO]: Epoch 084 - training loss: 0.4537, validation loss: 1.7830
2024-06-03 01:37:41 [INFO]: Epoch 085 - training loss: 0.4538, validation loss: 1.7800
2024-06-03 01:37:46 [INFO]: Epoch 086 - training loss: 0.4534, validation loss: 1.7838
2024-06-03 01:37:52 [INFO]: Epoch 087 - training loss: 0.4521, validation loss: 1.7770
2024-06-03 01:37:57 [INFO]: Epoch 088 - training loss: 0.4531, validation loss: 1.7678
2024-06-03 01:38:03 [INFO]: Epoch 089 - training loss: 0.4522, validation loss: 1.7741
2024-06-03 01:38:08 [INFO]: Epoch 090 - training loss: 0.4514, validation loss: 1.7912
2024-06-03 01:38:14 [INFO]: Epoch 091 - training loss: 0.4515, validation loss: 1.7892
2024-06-03 01:38:19 [INFO]: Epoch 092 - training loss: 0.4507, validation loss: 1.7757
2024-06-03 01:38:25 [INFO]: Epoch 093 - training loss: 0.4515, validation loss: 1.7762
2024-06-03 01:38:30 [INFO]: Epoch 094 - training loss: 0.4514, validation loss: 1.7613
2024-06-03 01:38:36 [INFO]: Epoch 095 - training loss: 0.4506, validation loss: 1.7654
2024-06-03 01:38:41 [INFO]: Epoch 096 - training loss: 0.4509, validation loss: 1.7682
2024-06-03 01:38:47 [INFO]: Epoch 097 - training loss: 0.4517, validation loss: 1.7626
2024-06-03 01:38:52 [INFO]: Epoch 098 - training loss: 0.4514, validation loss: 1.7651
2024-06-03 01:38:57 [INFO]: Epoch 099 - training loss: 0.4507, validation loss: 1.7695
2024-06-03 01:39:03 [INFO]: Epoch 100 - training loss: 0.4498, validation loss: 1.7662
2024-06-03 01:39:03 [INFO]: Finished training. The best model is from epoch#94.
2024-06-03 01:39:03 [INFO]: Saved the model to results_point_rate09/Electricity/DLinear_Electricity/round_2/20240603_T012957/DLinear.pypots
2024-06-03 01:39:03 [INFO]: Successfully saved to results_point_rate09/Electricity/DLinear_Electricity/round_2/imputation.pkl
2024-06-03 01:39:03 [INFO]: Round2 - DLinear on Electricity: MAE=0.9093, MSE=1.7504, MRE=0.4868
2024-06-03 01:39:03 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 01:39:03 [INFO]: Using the given device: cuda:0
2024-06-03 01:39:03 [INFO]: Model files will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_3/20240603_T013903
2024-06-03 01:39:03 [INFO]: Tensorboard file will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_3/20240603_T013903/tensorboard
2024-06-03 01:39:03 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-03 01:39:09 [INFO]: Epoch 001 - training loss: 1.1820, validation loss: 3.2818
2024-06-03 01:39:15 [INFO]: Epoch 002 - training loss: 0.7604, validation loss: 3.0713
2024-06-03 01:39:21 [INFO]: Epoch 003 - training loss: 0.6960, validation loss: 2.9887
2024-06-03 01:39:26 [INFO]: Epoch 004 - training loss: 0.6588, validation loss: 2.9121
2024-06-03 01:39:32 [INFO]: Epoch 005 - training loss: 0.6322, validation loss: 2.8300
2024-06-03 01:39:37 [INFO]: Epoch 006 - training loss: 0.6136, validation loss: 2.7763
2024-06-03 01:39:42 [INFO]: Epoch 007 - training loss: 0.5953, validation loss: 2.6731
2024-06-03 01:39:48 [INFO]: Epoch 008 - training loss: 0.5786, validation loss: 2.5789
2024-06-03 01:39:53 [INFO]: Epoch 009 - training loss: 0.5647, validation loss: 2.5424
2024-06-03 01:39:58 [INFO]: Epoch 010 - training loss: 0.5489, validation loss: 2.5226
2024-06-03 01:40:04 [INFO]: Epoch 011 - training loss: 0.5371, validation loss: 2.4647
2024-06-03 01:40:09 [INFO]: Epoch 012 - training loss: 0.5283, validation loss: 2.4615
2024-06-03 01:40:14 [INFO]: Epoch 013 - training loss: 0.5208, validation loss: 2.4053
2024-06-03 01:40:20 [INFO]: Epoch 014 - training loss: 0.5147, validation loss: 2.3730
2024-06-03 01:40:25 [INFO]: Epoch 015 - training loss: 0.5111, validation loss: 2.3424
2024-06-03 01:40:30 [INFO]: Epoch 016 - training loss: 0.5066, validation loss: 2.3477
2024-06-03 01:40:36 [INFO]: Epoch 017 - training loss: 0.5045, validation loss: 2.3143
2024-06-03 01:40:41 [INFO]: Epoch 018 - training loss: 0.5011, validation loss: 2.3107
2024-06-03 01:40:46 [INFO]: Epoch 019 - training loss: 0.4996, validation loss: 2.2999
2024-06-03 01:40:52 [INFO]: Epoch 020 - training loss: 0.4975, validation loss: 2.2631
2024-06-03 01:40:57 [INFO]: Epoch 021 - training loss: 0.4948, validation loss: 2.2451
2024-06-03 01:41:02 [INFO]: Epoch 022 - training loss: 0.4923, validation loss: 2.2205
2024-06-03 01:41:08 [INFO]: Epoch 023 - training loss: 0.4924, validation loss: 2.2177
2024-06-03 01:41:13 [INFO]: Epoch 024 - training loss: 0.4900, validation loss: 2.1947
2024-06-03 01:41:19 [INFO]: Epoch 025 - training loss: 0.4876, validation loss: 2.1659
2024-06-03 01:41:24 [INFO]: Epoch 026 - training loss: 0.4866, validation loss: 2.1506
2024-06-03 01:41:30 [INFO]: Epoch 027 - training loss: 0.4861, validation loss: 2.1403
2024-06-03 01:41:35 [INFO]: Epoch 028 - training loss: 0.4841, validation loss: 2.1406
2024-06-03 01:41:41 [INFO]: Epoch 029 - training loss: 0.4831, validation loss: 2.1156
2024-06-03 01:41:46 [INFO]: Epoch 030 - training loss: 0.4817, validation loss: 2.0963
2024-06-03 01:41:52 [INFO]: Epoch 031 - training loss: 0.4806, validation loss: 2.0822
2024-06-03 01:41:57 [INFO]: Epoch 032 - training loss: 0.4815, validation loss: 2.0811
2024-06-03 01:42:03 [INFO]: Epoch 033 - training loss: 0.4797, validation loss: 2.0533
2024-06-03 01:42:08 [INFO]: Epoch 034 - training loss: 0.4794, validation loss: 2.0633
2024-06-03 01:42:14 [INFO]: Epoch 035 - training loss: 0.4800, validation loss: 2.0421
2024-06-03 01:42:19 [INFO]: Epoch 036 - training loss: 0.4784, validation loss: 2.0200
2024-06-03 01:42:25 [INFO]: Epoch 037 - training loss: 0.4761, validation loss: 2.0247
2024-06-03 01:42:30 [INFO]: Epoch 038 - training loss: 0.4754, validation loss: 2.0029
2024-06-03 01:42:35 [INFO]: Epoch 039 - training loss: 0.4747, validation loss: 1.9956
2024-06-03 01:42:40 [INFO]: Epoch 040 - training loss: 0.4742, validation loss: 1.9868
2024-06-03 01:42:44 [INFO]: Epoch 041 - training loss: 0.4731, validation loss: 1.9676
2024-06-03 01:42:49 [INFO]: Epoch 042 - training loss: 0.4730, validation loss: 1.9658
2024-06-03 01:42:53 [INFO]: Epoch 043 - training loss: 0.4715, validation loss: 1.9540
2024-06-03 01:42:58 [INFO]: Epoch 044 - training loss: 0.4723, validation loss: 1.9456
2024-06-03 01:43:02 [INFO]: Epoch 045 - training loss: 0.4706, validation loss: 1.9496
2024-06-03 01:43:06 [INFO]: Epoch 046 - training loss: 0.4700, validation loss: 1.9213
2024-06-03 01:43:11 [INFO]: Epoch 047 - training loss: 0.4683, validation loss: 1.9201
2024-06-03 01:43:16 [INFO]: Epoch 048 - training loss: 0.4675, validation loss: 1.9085
2024-06-03 01:43:20 [INFO]: Epoch 049 - training loss: 0.4662, validation loss: 1.9132
2024-06-03 01:43:25 [INFO]: Epoch 050 - training loss: 0.4666, validation loss: 1.9320
2024-06-03 01:43:29 [INFO]: Epoch 051 - training loss: 0.4654, validation loss: 1.9074
2024-06-03 01:43:33 [INFO]: Epoch 052 - training loss: 0.4653, validation loss: 1.8787
2024-06-03 01:43:38 [INFO]: Epoch 053 - training loss: 0.4647, validation loss: 1.8723
2024-06-03 01:43:42 [INFO]: Epoch 054 - training loss: 0.4642, validation loss: 1.8797
2024-06-03 01:43:47 [INFO]: Epoch 055 - training loss: 0.4631, validation loss: 1.8552
2024-06-03 01:43:51 [INFO]: Epoch 056 - training loss: 0.4629, validation loss: 1.8436
2024-06-03 01:43:56 [INFO]: Epoch 057 - training loss: 0.4625, validation loss: 1.8558
2024-06-03 01:44:00 [INFO]: Epoch 058 - training loss: 0.4619, validation loss: 1.8462
2024-06-03 01:44:05 [INFO]: Epoch 059 - training loss: 0.4617, validation loss: 1.8446
2024-06-03 01:44:09 [INFO]: Epoch 060 - training loss: 0.4610, validation loss: 1.8458
2024-06-03 01:44:14 [INFO]: Epoch 061 - training loss: 0.4607, validation loss: 1.8277
2024-06-03 01:44:18 [INFO]: Epoch 062 - training loss: 0.4598, validation loss: 1.8367
2024-06-03 01:44:23 [INFO]: Epoch 063 - training loss: 0.4596, validation loss: 1.8403
2024-06-03 01:44:27 [INFO]: Epoch 064 - training loss: 0.4591, validation loss: 1.8364
2024-06-03 01:44:31 [INFO]: Epoch 065 - training loss: 0.4589, validation loss: 1.8097
2024-06-03 01:44:36 [INFO]: Epoch 066 - training loss: 0.4579, validation loss: 1.8070
2024-06-03 01:44:40 [INFO]: Epoch 067 - training loss: 0.4579, validation loss: 1.8179
2024-06-03 01:44:45 [INFO]: Epoch 068 - training loss: 0.4575, validation loss: 1.8257
2024-06-03 01:44:49 [INFO]: Epoch 069 - training loss: 0.4567, validation loss: 1.8055
2024-06-03 01:44:53 [INFO]: Epoch 070 - training loss: 0.4570, validation loss: 1.8076
2024-06-03 01:44:58 [INFO]: Epoch 071 - training loss: 0.4568, validation loss: 1.7902
2024-06-03 01:45:02 [INFO]: Epoch 072 - training loss: 0.4569, validation loss: 1.8031
2024-06-03 01:45:07 [INFO]: Epoch 073 - training loss: 0.4554, validation loss: 1.8118
2024-06-03 01:45:12 [INFO]: Epoch 074 - training loss: 0.4545, validation loss: 1.7911
2024-06-03 01:45:16 [INFO]: Epoch 075 - training loss: 0.4547, validation loss: 1.7948
2024-06-03 01:45:21 [INFO]: Epoch 076 - training loss: 0.4543, validation loss: 1.7841
2024-06-03 01:45:25 [INFO]: Epoch 077 - training loss: 0.4529, validation loss: 1.7860
2024-06-03 01:45:30 [INFO]: Epoch 078 - training loss: 0.4538, validation loss: 1.7737
2024-06-03 01:45:34 [INFO]: Epoch 079 - training loss: 0.4542, validation loss: 1.7531
2024-06-03 01:45:39 [INFO]: Epoch 080 - training loss: 0.4533, validation loss: 1.7751
2024-06-03 01:45:43 [INFO]: Epoch 081 - training loss: 0.4521, validation loss: 1.7797
2024-06-03 01:45:48 [INFO]: Epoch 082 - training loss: 0.4523, validation loss: 1.7765
2024-06-03 01:45:53 [INFO]: Epoch 083 - training loss: 0.4530, validation loss: 1.7759
2024-06-03 01:45:57 [INFO]: Epoch 084 - training loss: 0.4516, validation loss: 1.7636
2024-06-03 01:46:02 [INFO]: Epoch 085 - training loss: 0.4520, validation loss: 1.7776
2024-06-03 01:46:06 [INFO]: Epoch 086 - training loss: 0.4516, validation loss: 1.7768
2024-06-03 01:46:11 [INFO]: Epoch 087 - training loss: 0.4511, validation loss: 1.7821
2024-06-03 01:46:15 [INFO]: Epoch 088 - training loss: 0.4523, validation loss: 1.7739
2024-06-03 01:46:20 [INFO]: Epoch 089 - training loss: 0.4520, validation loss: 1.7811
2024-06-03 01:46:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 01:46:20 [INFO]: Finished training. The best model is from epoch#79.
2024-06-03 01:46:20 [INFO]: Saved the model to results_point_rate09/Electricity/DLinear_Electricity/round_3/20240603_T013903/DLinear.pypots
2024-06-03 01:46:21 [INFO]: Successfully saved to results_point_rate09/Electricity/DLinear_Electricity/round_3/imputation.pkl
2024-06-03 01:46:21 [INFO]: Round3 - DLinear on Electricity: MAE=0.9149, MSE=1.7567, MRE=0.4898
2024-06-03 01:46:21 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 01:46:21 [INFO]: Using the given device: cuda:0
2024-06-03 01:46:21 [INFO]: Model files will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_4/20240603_T014621
2024-06-03 01:46:21 [INFO]: Tensorboard file will be saved to results_point_rate09/Electricity/DLinear_Electricity/round_4/20240603_T014621/tensorboard
2024-06-03 01:46:21 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 2,294,692
2024-06-03 01:46:25 [INFO]: Epoch 001 - training loss: 1.2399, validation loss: 3.2954
2024-06-03 01:46:30 [INFO]: Epoch 002 - training loss: 0.7724, validation loss: 3.0333
2024-06-03 01:46:35 [INFO]: Epoch 003 - training loss: 0.6919, validation loss: 3.0097
2024-06-03 01:46:39 [INFO]: Epoch 004 - training loss: 0.6536, validation loss: 2.9161
2024-06-03 01:46:44 [INFO]: Epoch 005 - training loss: 0.6244, validation loss: 2.8341
2024-06-03 01:46:48 [INFO]: Epoch 006 - training loss: 0.6018, validation loss: 2.7580
2024-06-03 01:46:53 [INFO]: Epoch 007 - training loss: 0.5815, validation loss: 2.6549
2024-06-03 01:46:57 [INFO]: Epoch 008 - training loss: 0.5605, validation loss: 2.6070
2024-06-03 01:47:02 [INFO]: Epoch 009 - training loss: 0.5467, validation loss: 2.5679
2024-06-03 01:47:06 [INFO]: Epoch 010 - training loss: 0.5366, validation loss: 2.5225
2024-06-03 01:47:11 [INFO]: Epoch 011 - training loss: 0.5273, validation loss: 2.4946
2024-06-03 01:47:15 [INFO]: Epoch 012 - training loss: 0.5217, validation loss: 2.4590
2024-06-03 01:47:20 [INFO]: Epoch 013 - training loss: 0.5160, validation loss: 2.4405
2024-06-03 01:47:24 [INFO]: Epoch 014 - training loss: 0.5108, validation loss: 2.4038
2024-06-03 01:47:29 [INFO]: Epoch 015 - training loss: 0.5076, validation loss: 2.3756
2024-06-03 01:47:33 [INFO]: Epoch 016 - training loss: 0.5049, validation loss: 2.3711
2024-06-03 01:47:38 [INFO]: Epoch 017 - training loss: 0.5015, validation loss: 2.3431
2024-06-03 01:47:42 [INFO]: Epoch 018 - training loss: 0.5009, validation loss: 2.3474
2024-06-03 01:47:47 [INFO]: Epoch 019 - training loss: 0.4973, validation loss: 2.2966
2024-06-03 01:47:51 [INFO]: Epoch 020 - training loss: 0.4957, validation loss: 2.2889
2024-06-03 01:47:56 [INFO]: Epoch 021 - training loss: 0.4945, validation loss: 2.2664
2024-06-03 01:48:00 [INFO]: Epoch 022 - training loss: 0.4914, validation loss: 2.2536
2024-06-03 01:48:05 [INFO]: Epoch 023 - training loss: 0.4896, validation loss: 2.2414
2024-06-03 01:48:09 [INFO]: Epoch 024 - training loss: 0.4880, validation loss: 2.2050
2024-06-03 01:48:14 [INFO]: Epoch 025 - training loss: 0.4864, validation loss: 2.1889
2024-06-03 01:48:18 [INFO]: Epoch 026 - training loss: 0.4865, validation loss: 2.1836
2024-06-03 01:48:22 [INFO]: Epoch 027 - training loss: 0.4864, validation loss: 2.1750
2024-06-03 01:48:27 [INFO]: Epoch 028 - training loss: 0.4834, validation loss: 2.1458
2024-06-03 01:48:31 [INFO]: Epoch 029 - training loss: 0.4818, validation loss: 2.1426
2024-06-03 01:48:36 [INFO]: Epoch 030 - training loss: 0.4809, validation loss: 2.1145
2024-06-03 01:48:40 [INFO]: Epoch 031 - training loss: 0.4818, validation loss: 2.1005
2024-06-03 01:48:44 [INFO]: Epoch 032 - training loss: 0.4803, validation loss: 2.0841
2024-06-03 01:48:49 [INFO]: Epoch 033 - training loss: 0.4782, validation loss: 2.0828
2024-06-03 01:48:53 [INFO]: Epoch 034 - training loss: 0.4773, validation loss: 2.0695
2024-06-03 01:48:58 [INFO]: Epoch 035 - training loss: 0.4770, validation loss: 2.0639
2024-06-03 01:49:02 [INFO]: Epoch 036 - training loss: 0.4748, validation loss: 2.0384
2024-06-03 01:49:07 [INFO]: Epoch 037 - training loss: 0.4742, validation loss: 2.0150
2024-06-03 01:49:11 [INFO]: Epoch 038 - training loss: 0.4723, validation loss: 2.0253
2024-06-03 01:49:16 [INFO]: Epoch 039 - training loss: 0.4721, validation loss: 2.0198
2024-06-03 01:49:21 [INFO]: Epoch 040 - training loss: 0.4713, validation loss: 2.0022
2024-06-03 01:49:25 [INFO]: Epoch 041 - training loss: 0.4706, validation loss: 1.9868
2024-06-03 01:49:30 [INFO]: Epoch 042 - training loss: 0.4706, validation loss: 1.9855
2024-06-03 01:49:34 [INFO]: Epoch 043 - training loss: 0.4687, validation loss: 1.9865
2024-06-03 01:49:39 [INFO]: Epoch 044 - training loss: 0.4682, validation loss: 1.9536
2024-06-03 01:49:43 [INFO]: Epoch 045 - training loss: 0.4672, validation loss: 1.9412
2024-06-03 01:49:48 [INFO]: Epoch 046 - training loss: 0.4665, validation loss: 1.9311
2024-06-03 01:49:53 [INFO]: Epoch 047 - training loss: 0.4664, validation loss: 1.9180
2024-06-03 01:49:57 [INFO]: Epoch 048 - training loss: 0.4655, validation loss: 1.9129
2024-06-03 01:50:02 [INFO]: Epoch 049 - training loss: 0.4638, validation loss: 1.9051
2024-06-03 01:50:06 [INFO]: Epoch 050 - training loss: 0.4647, validation loss: 1.9118
2024-06-03 01:50:11 [INFO]: Epoch 051 - training loss: 0.4631, validation loss: 1.9033
2024-06-03 01:50:16 [INFO]: Epoch 052 - training loss: 0.4634, validation loss: 1.9009
2024-06-03 01:50:20 [INFO]: Epoch 053 - training loss: 0.4630, validation loss: 1.8824
2024-06-03 01:50:25 [INFO]: Epoch 054 - training loss: 0.4632, validation loss: 1.8761
2024-06-03 01:50:29 [INFO]: Epoch 055 - training loss: 0.4621, validation loss: 1.8597
2024-06-03 01:50:34 [INFO]: Epoch 056 - training loss: 0.4613, validation loss: 1.8601
2024-06-03 01:50:38 [INFO]: Epoch 057 - training loss: 0.4600, validation loss: 1.8516
2024-06-03 01:50:43 [INFO]: Epoch 058 - training loss: 0.4594, validation loss: 1.8611
2024-06-03 01:50:47 [INFO]: Epoch 059 - training loss: 0.4592, validation loss: 1.8440
2024-06-03 01:50:52 [INFO]: Epoch 060 - training loss: 0.4580, validation loss: 1.8387
2024-06-03 01:50:56 [INFO]: Epoch 061 - training loss: 0.4582, validation loss: 1.8467
2024-06-03 01:51:01 [INFO]: Epoch 062 - training loss: 0.4573, validation loss: 1.8487
2024-06-03 01:51:05 [INFO]: Epoch 063 - training loss: 0.4575, validation loss: 1.8485
2024-06-03 01:51:10 [INFO]: Epoch 064 - training loss: 0.4567, validation loss: 1.8330
2024-06-03 01:51:15 [INFO]: Epoch 065 - training loss: 0.4570, validation loss: 1.8202
2024-06-03 01:51:19 [INFO]: Epoch 066 - training loss: 0.4570, validation loss: 1.8284
2024-06-03 01:51:24 [INFO]: Epoch 067 - training loss: 0.4566, validation loss: 1.8173
2024-06-03 01:51:28 [INFO]: Epoch 068 - training loss: 0.4550, validation loss: 1.8091
2024-06-03 01:51:33 [INFO]: Epoch 069 - training loss: 0.4555, validation loss: 1.8050
2024-06-03 01:51:38 [INFO]: Epoch 070 - training loss: 0.4553, validation loss: 1.8009
2024-06-03 01:51:42 [INFO]: Epoch 071 - training loss: 0.4540, validation loss: 1.7950
2024-06-03 01:51:47 [INFO]: Epoch 072 - training loss: 0.4539, validation loss: 1.8145
2024-06-03 01:51:52 [INFO]: Epoch 073 - training loss: 0.4539, validation loss: 1.8039
2024-06-03 01:51:56 [INFO]: Epoch 074 - training loss: 0.4541, validation loss: 1.8005
2024-06-03 01:52:01 [INFO]: Epoch 075 - training loss: 0.4531, validation loss: 1.7854
2024-06-03 01:52:05 [INFO]: Epoch 076 - training loss: 0.4532, validation loss: 1.8055
2024-06-03 01:52:10 [INFO]: Epoch 077 - training loss: 0.4526, validation loss: 1.7979
2024-06-03 01:52:14 [INFO]: Epoch 078 - training loss: 0.4523, validation loss: 1.7851
2024-06-03 01:52:18 [INFO]: Epoch 079 - training loss: 0.4521, validation loss: 1.7935
2024-06-03 01:52:23 [INFO]: Epoch 080 - training loss: 0.4513, validation loss: 1.7934
2024-06-03 01:52:28 [INFO]: Epoch 081 - training loss: 0.4518, validation loss: 1.7964
2024-06-03 01:52:32 [INFO]: Epoch 082 - training loss: 0.4520, validation loss: 1.7961
2024-06-03 01:52:37 [INFO]: Epoch 083 - training loss: 0.4517, validation loss: 1.7926
2024-06-03 01:52:41 [INFO]: Epoch 084 - training loss: 0.4512, validation loss: 1.7775
2024-06-03 01:52:46 [INFO]: Epoch 085 - training loss: 0.4504, validation loss: 1.7974
2024-06-03 01:52:50 [INFO]: Epoch 086 - training loss: 0.4509, validation loss: 1.7847
2024-06-03 01:52:54 [INFO]: Epoch 087 - training loss: 0.4508, validation loss: 1.7854
2024-06-03 01:52:59 [INFO]: Epoch 088 - training loss: 0.4496, validation loss: 1.7894
2024-06-03 01:53:04 [INFO]: Epoch 089 - training loss: 0.4502, validation loss: 1.7842
2024-06-03 01:53:08 [INFO]: Epoch 090 - training loss: 0.4491, validation loss: 1.7789
2024-06-03 01:53:13 [INFO]: Epoch 091 - training loss: 0.4493, validation loss: 1.7828
2024-06-03 01:53:18 [INFO]: Epoch 092 - training loss: 0.4490, validation loss: 1.7692
2024-06-03 01:53:22 [INFO]: Epoch 093 - training loss: 0.4504, validation loss: 1.7814
2024-06-03 01:53:27 [INFO]: Epoch 094 - training loss: 0.4497, validation loss: 1.7872
2024-06-03 01:53:31 [INFO]: Epoch 095 - training loss: 0.4496, validation loss: 1.7814
2024-06-03 01:53:35 [INFO]: Epoch 096 - training loss: 0.4496, validation loss: 1.7744
2024-06-03 01:53:40 [INFO]: Epoch 097 - training loss: 0.4483, validation loss: 1.7765
2024-06-03 01:53:44 [INFO]: Epoch 098 - training loss: 0.4476, validation loss: 1.7790
2024-06-03 01:53:49 [INFO]: Epoch 099 - training loss: 0.4478, validation loss: 1.7809
2024-06-03 01:53:53 [INFO]: Epoch 100 - training loss: 0.4490, validation loss: 1.7853
2024-06-03 01:53:53 [INFO]: Finished training. The best model is from epoch#92.
2024-06-03 01:53:53 [INFO]: Saved the model to results_point_rate09/Electricity/DLinear_Electricity/round_4/20240603_T014621/DLinear.pypots
2024-06-03 01:53:54 [INFO]: Successfully saved to results_point_rate09/Electricity/DLinear_Electricity/round_4/imputation.pkl
2024-06-03 01:53:54 [INFO]: Round4 - DLinear on Electricity: MAE=0.8799, MSE=1.6603, MRE=0.4710
2024-06-03 01:53:54 [INFO]: Done! Final results:
Averaged DLinear (2,294,692 params) on Electricity: MAE=0.8910 ± 0.022588753588211448, MSE=1.6963 ± 0.06222405812559308, MRE=0.4770 ± 0.012092404351748657, average inference time=0.30
