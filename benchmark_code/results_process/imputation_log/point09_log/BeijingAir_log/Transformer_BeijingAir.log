2024-06-03 10:25:46 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:25:46 [INFO]: Using the given device: cuda:0
2024-06-03 10:25:48 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_0/20240603_T102547
2024-06-03 10:25:48 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_0/20240603_T102547/tensorboard
2024-06-03 10:25:48 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 10:25:48 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 10:25:50 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 10:26:03 [INFO]: Epoch 001 - training loss: 1.5123, validation loss: 1.1508
2024-06-03 10:26:10 [INFO]: Epoch 002 - training loss: 1.2179, validation loss: 0.7834
2024-06-03 10:26:16 [INFO]: Epoch 003 - training loss: 0.8983, validation loss: 0.5221
2024-06-03 10:26:23 [INFO]: Epoch 004 - training loss: 0.7598, validation loss: 0.4747
2024-06-03 10:26:30 [INFO]: Epoch 005 - training loss: 0.7036, validation loss: 0.4359
2024-06-03 10:26:37 [INFO]: Epoch 006 - training loss: 0.6684, validation loss: 0.4252
2024-06-03 10:26:44 [INFO]: Epoch 007 - training loss: 0.6373, validation loss: 0.3997
2024-06-03 10:26:50 [INFO]: Epoch 008 - training loss: 0.6090, validation loss: 0.4021
2024-06-03 10:26:57 [INFO]: Epoch 009 - training loss: 0.5906, validation loss: 0.3873
2024-06-03 10:27:04 [INFO]: Epoch 010 - training loss: 0.5759, validation loss: 0.3784
2024-06-03 10:27:11 [INFO]: Epoch 011 - training loss: 0.5700, validation loss: 0.3696
2024-06-03 10:27:18 [INFO]: Epoch 012 - training loss: 0.5420, validation loss: 0.3728
2024-06-03 10:27:25 [INFO]: Epoch 013 - training loss: 0.5392, validation loss: 0.3679
2024-06-03 10:27:31 [INFO]: Epoch 014 - training loss: 0.5368, validation loss: 0.3629
2024-06-03 10:27:38 [INFO]: Epoch 015 - training loss: 0.5240, validation loss: 0.3637
2024-06-03 10:27:45 [INFO]: Epoch 016 - training loss: 0.5170, validation loss: 0.3587
2024-06-03 10:27:52 [INFO]: Epoch 017 - training loss: 0.5093, validation loss: 0.3538
2024-06-03 10:27:59 [INFO]: Epoch 018 - training loss: 0.4954, validation loss: 0.3632
2024-06-03 10:28:05 [INFO]: Epoch 019 - training loss: 0.5005, validation loss: 0.3574
2024-06-03 10:28:12 [INFO]: Epoch 020 - training loss: 0.4803, validation loss: 0.3497
2024-06-03 10:28:19 [INFO]: Epoch 021 - training loss: 0.4746, validation loss: 0.3532
2024-06-03 10:28:26 [INFO]: Epoch 022 - training loss: 0.4712, validation loss: 0.3487
2024-06-03 10:28:32 [INFO]: Epoch 023 - training loss: 0.4682, validation loss: 0.3518
2024-06-03 10:28:39 [INFO]: Epoch 024 - training loss: 0.4658, validation loss: 0.3462
2024-06-03 10:28:46 [INFO]: Epoch 025 - training loss: 0.4632, validation loss: 0.3419
2024-06-03 10:28:52 [INFO]: Epoch 026 - training loss: 0.4549, validation loss: 0.3461
2024-06-03 10:28:59 [INFO]: Epoch 027 - training loss: 0.4582, validation loss: 0.3433
2024-06-03 10:29:06 [INFO]: Epoch 028 - training loss: 0.4446, validation loss: 0.3406
2024-06-03 10:29:12 [INFO]: Epoch 029 - training loss: 0.4384, validation loss: 0.3429
2024-06-03 10:29:19 [INFO]: Epoch 030 - training loss: 0.4366, validation loss: 0.3365
2024-06-03 10:29:25 [INFO]: Epoch 031 - training loss: 0.4322, validation loss: 0.3415
2024-06-03 10:29:32 [INFO]: Epoch 032 - training loss: 0.4371, validation loss: 0.3457
2024-06-03 10:29:39 [INFO]: Epoch 033 - training loss: 0.4333, validation loss: 0.3391
2024-06-03 10:29:45 [INFO]: Epoch 034 - training loss: 0.4220, validation loss: 0.3392
2024-06-03 10:29:52 [INFO]: Epoch 035 - training loss: 0.4214, validation loss: 0.3356
2024-06-03 10:29:59 [INFO]: Epoch 036 - training loss: 0.4156, validation loss: 0.3316
2024-06-03 10:30:06 [INFO]: Epoch 037 - training loss: 0.4144, validation loss: 0.3222
2024-06-03 10:30:12 [INFO]: Epoch 038 - training loss: 0.4087, validation loss: 0.3266
2024-06-03 10:30:19 [INFO]: Epoch 039 - training loss: 0.4024, validation loss: 0.3290
2024-06-03 10:30:26 [INFO]: Epoch 040 - training loss: 0.4039, validation loss: 0.3311
2024-06-03 10:30:33 [INFO]: Epoch 041 - training loss: 0.3971, validation loss: 0.3334
2024-06-03 10:30:39 [INFO]: Epoch 042 - training loss: 0.3951, validation loss: 0.3245
2024-06-03 10:30:46 [INFO]: Epoch 043 - training loss: 0.3980, validation loss: 0.3251
2024-06-03 10:30:53 [INFO]: Epoch 044 - training loss: 0.4009, validation loss: 0.3304
2024-06-03 10:30:59 [INFO]: Epoch 045 - training loss: 0.3859, validation loss: 0.3205
2024-06-03 10:31:06 [INFO]: Epoch 046 - training loss: 0.3829, validation loss: 0.3242
2024-06-03 10:31:13 [INFO]: Epoch 047 - training loss: 0.3868, validation loss: 0.3273
2024-06-03 10:31:20 [INFO]: Epoch 048 - training loss: 0.3806, validation loss: 0.3268
2024-06-03 10:31:27 [INFO]: Epoch 049 - training loss: 0.3812, validation loss: 0.3301
2024-06-03 10:31:34 [INFO]: Epoch 050 - training loss: 0.3746, validation loss: 0.3270
2024-06-03 10:31:40 [INFO]: Epoch 051 - training loss: 0.3783, validation loss: 0.3180
2024-06-03 10:31:47 [INFO]: Epoch 052 - training loss: 0.3651, validation loss: 0.3302
2024-06-03 10:31:54 [INFO]: Epoch 053 - training loss: 0.3659, validation loss: 0.3191
2024-06-03 10:32:01 [INFO]: Epoch 054 - training loss: 0.3600, validation loss: 0.3191
2024-06-03 10:32:07 [INFO]: Epoch 055 - training loss: 0.3588, validation loss: 0.3227
2024-06-03 10:32:14 [INFO]: Epoch 056 - training loss: 0.3614, validation loss: 0.3269
2024-06-03 10:32:21 [INFO]: Epoch 057 - training loss: 0.3597, validation loss: 0.3225
2024-06-03 10:32:27 [INFO]: Epoch 058 - training loss: 0.3541, validation loss: 0.3165
2024-06-03 10:32:34 [INFO]: Epoch 059 - training loss: 0.3482, validation loss: 0.3282
2024-06-03 10:32:41 [INFO]: Epoch 060 - training loss: 0.3466, validation loss: 0.3241
2024-06-03 10:32:47 [INFO]: Epoch 061 - training loss: 0.3509, validation loss: 0.3212
2024-06-03 10:32:54 [INFO]: Epoch 062 - training loss: 0.3430, validation loss: 0.3200
2024-06-03 10:33:01 [INFO]: Epoch 063 - training loss: 0.3557, validation loss: 0.3192
2024-06-03 10:33:07 [INFO]: Epoch 064 - training loss: 0.3427, validation loss: 0.3262
2024-06-03 10:33:14 [INFO]: Epoch 065 - training loss: 0.3451, validation loss: 0.3208
2024-06-03 10:33:21 [INFO]: Epoch 066 - training loss: 0.3368, validation loss: 0.3181
2024-06-03 10:33:27 [INFO]: Epoch 067 - training loss: 0.3321, validation loss: 0.3181
2024-06-03 10:33:34 [INFO]: Epoch 068 - training loss: 0.3329, validation loss: 0.3266
2024-06-03 10:33:34 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:33:34 [INFO]: Finished training. The best model is from epoch#58.
2024-06-03 10:33:39 [INFO]: Saved the model to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_0/20240603_T102547/Transformer.pypots
2024-06-03 10:33:42 [INFO]: Successfully saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_0/imputation.pkl
2024-06-03 10:33:42 [INFO]: Round0 - Transformer on BeijingAir: MAE=0.2870, MSE=0.3457, MRE=0.3865
2024-06-03 10:33:42 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:33:42 [INFO]: Using the given device: cuda:0
2024-06-03 10:33:42 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_1/20240603_T103342
2024-06-03 10:33:42 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_1/20240603_T103342/tensorboard
2024-06-03 10:33:42 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 10:33:42 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 10:33:48 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 10:33:55 [INFO]: Epoch 001 - training loss: 1.4997, validation loss: 1.0083
2024-06-03 10:34:01 [INFO]: Epoch 002 - training loss: 1.1361, validation loss: 0.6349
2024-06-03 10:34:08 [INFO]: Epoch 003 - training loss: 0.8499, validation loss: 0.4996
2024-06-03 10:34:15 [INFO]: Epoch 004 - training loss: 0.7404, validation loss: 0.4758
2024-06-03 10:34:21 [INFO]: Epoch 005 - training loss: 0.7077, validation loss: 0.4511
2024-06-03 10:34:28 [INFO]: Epoch 006 - training loss: 0.6625, validation loss: 0.4263
2024-06-03 10:34:34 [INFO]: Epoch 007 - training loss: 0.6402, validation loss: 0.3993
2024-06-03 10:34:41 [INFO]: Epoch 008 - training loss: 0.6117, validation loss: 0.3926
2024-06-03 10:34:48 [INFO]: Epoch 009 - training loss: 0.5904, validation loss: 0.4032
2024-06-03 10:34:54 [INFO]: Epoch 010 - training loss: 0.5888, validation loss: 0.3850
2024-06-03 10:35:01 [INFO]: Epoch 011 - training loss: 0.5574, validation loss: 0.3833
2024-06-03 10:35:08 [INFO]: Epoch 012 - training loss: 0.5535, validation loss: 0.3668
2024-06-03 10:35:14 [INFO]: Epoch 013 - training loss: 0.5320, validation loss: 0.3589
2024-06-03 10:35:21 [INFO]: Epoch 014 - training loss: 0.5247, validation loss: 0.3732
2024-06-03 10:35:28 [INFO]: Epoch 015 - training loss: 0.5185, validation loss: 0.3591
2024-06-03 10:35:35 [INFO]: Epoch 016 - training loss: 0.5159, validation loss: 0.3653
2024-06-03 10:35:42 [INFO]: Epoch 017 - training loss: 0.5165, validation loss: 0.3640
2024-06-03 10:35:48 [INFO]: Epoch 018 - training loss: 0.5049, validation loss: 0.3521
2024-06-03 10:35:55 [INFO]: Epoch 019 - training loss: 0.4843, validation loss: 0.3476
2024-06-03 10:36:02 [INFO]: Epoch 020 - training loss: 0.4827, validation loss: 0.3598
2024-06-03 10:36:09 [INFO]: Epoch 021 - training loss: 0.4784, validation loss: 0.3503
2024-06-03 10:36:16 [INFO]: Epoch 022 - training loss: 0.4722, validation loss: 0.3464
2024-06-03 10:36:22 [INFO]: Epoch 023 - training loss: 0.4664, validation loss: 0.3458
2024-06-03 10:36:29 [INFO]: Epoch 024 - training loss: 0.4643, validation loss: 0.3439
2024-06-03 10:36:36 [INFO]: Epoch 025 - training loss: 0.4661, validation loss: 0.3468
2024-06-03 10:36:43 [INFO]: Epoch 026 - training loss: 0.4679, validation loss: 0.3504
2024-06-03 10:36:49 [INFO]: Epoch 027 - training loss: 0.4500, validation loss: 0.3400
2024-06-03 10:36:56 [INFO]: Epoch 028 - training loss: 0.4394, validation loss: 0.3439
2024-06-03 10:37:02 [INFO]: Epoch 029 - training loss: 0.4443, validation loss: 0.3411
2024-06-03 10:37:09 [INFO]: Epoch 030 - training loss: 0.4372, validation loss: 0.3356
2024-06-03 10:37:16 [INFO]: Epoch 031 - training loss: 0.4324, validation loss: 0.3435
2024-06-03 10:37:22 [INFO]: Epoch 032 - training loss: 0.4411, validation loss: 0.3362
2024-06-03 10:37:29 [INFO]: Epoch 033 - training loss: 0.4296, validation loss: 0.3382
2024-06-03 10:37:36 [INFO]: Epoch 034 - training loss: 0.4206, validation loss: 0.3345
2024-06-03 10:37:43 [INFO]: Epoch 035 - training loss: 0.4292, validation loss: 0.3327
2024-06-03 10:37:49 [INFO]: Epoch 036 - training loss: 0.4128, validation loss: 0.3342
2024-06-03 10:37:56 [INFO]: Epoch 037 - training loss: 0.4137, validation loss: 0.3270
2024-06-03 10:38:03 [INFO]: Epoch 038 - training loss: 0.3992, validation loss: 0.3268
2024-06-03 10:38:09 [INFO]: Epoch 039 - training loss: 0.4041, validation loss: 0.3345
2024-06-03 10:38:15 [INFO]: Epoch 040 - training loss: 0.4006, validation loss: 0.3220
2024-06-03 10:38:22 [INFO]: Epoch 041 - training loss: 0.3939, validation loss: 0.3309
2024-06-03 10:38:28 [INFO]: Epoch 042 - training loss: 0.3945, validation loss: 0.3258
2024-06-03 10:38:35 [INFO]: Epoch 043 - training loss: 0.4023, validation loss: 0.3231
2024-06-03 10:38:41 [INFO]: Epoch 044 - training loss: 0.3886, validation loss: 0.3236
2024-06-03 10:38:48 [INFO]: Epoch 045 - training loss: 0.3881, validation loss: 0.3214
2024-06-03 10:38:54 [INFO]: Epoch 046 - training loss: 0.3862, validation loss: 0.3228
2024-06-03 10:39:00 [INFO]: Epoch 047 - training loss: 0.3714, validation loss: 0.3229
2024-06-03 10:39:07 [INFO]: Epoch 048 - training loss: 0.3686, validation loss: 0.3227
2024-06-03 10:39:13 [INFO]: Epoch 049 - training loss: 0.3738, validation loss: 0.3189
2024-06-03 10:39:20 [INFO]: Epoch 050 - training loss: 0.3695, validation loss: 0.3230
2024-06-03 10:39:27 [INFO]: Epoch 051 - training loss: 0.3857, validation loss: 0.3306
2024-06-03 10:39:33 [INFO]: Epoch 052 - training loss: 0.3779, validation loss: 0.3275
2024-06-03 10:39:39 [INFO]: Epoch 053 - training loss: 0.3712, validation loss: 0.3297
2024-06-03 10:39:46 [INFO]: Epoch 054 - training loss: 0.3699, validation loss: 0.3217
2024-06-03 10:39:52 [INFO]: Epoch 055 - training loss: 0.3687, validation loss: 0.3210
2024-06-03 10:39:59 [INFO]: Epoch 056 - training loss: 0.3552, validation loss: 0.3252
2024-06-03 10:40:05 [INFO]: Epoch 057 - training loss: 0.3631, validation loss: 0.3210
2024-06-03 10:40:11 [INFO]: Epoch 058 - training loss: 0.3603, validation loss: 0.3170
2024-06-03 10:40:18 [INFO]: Epoch 059 - training loss: 0.3587, validation loss: 0.3306
2024-06-03 10:40:25 [INFO]: Epoch 060 - training loss: 0.3600, validation loss: 0.3195
2024-06-03 10:40:32 [INFO]: Epoch 061 - training loss: 0.3570, validation loss: 0.3246
2024-06-03 10:40:38 [INFO]: Epoch 062 - training loss: 0.3392, validation loss: 0.3160
2024-06-03 10:40:45 [INFO]: Epoch 063 - training loss: 0.3356, validation loss: 0.3233
2024-06-03 10:40:52 [INFO]: Epoch 064 - training loss: 0.3390, validation loss: 0.3206
2024-06-03 10:40:59 [INFO]: Epoch 065 - training loss: 0.3401, validation loss: 0.3253
2024-06-03 10:41:06 [INFO]: Epoch 066 - training loss: 0.3399, validation loss: 0.3188
2024-06-03 10:41:13 [INFO]: Epoch 067 - training loss: 0.3374, validation loss: 0.3198
2024-06-03 10:41:19 [INFO]: Epoch 068 - training loss: 0.3356, validation loss: 0.3177
2024-06-03 10:41:26 [INFO]: Epoch 069 - training loss: 0.3318, validation loss: 0.3174
2024-06-03 10:41:33 [INFO]: Epoch 070 - training loss: 0.3342, validation loss: 0.3232
2024-06-03 10:41:40 [INFO]: Epoch 071 - training loss: 0.3291, validation loss: 0.3147
2024-06-03 10:41:46 [INFO]: Epoch 072 - training loss: 0.3234, validation loss: 0.3257
2024-06-03 10:41:53 [INFO]: Epoch 073 - training loss: 0.3220, validation loss: 0.3228
2024-06-03 10:42:00 [INFO]: Epoch 074 - training loss: 0.3238, validation loss: 0.3167
2024-06-03 10:42:07 [INFO]: Epoch 075 - training loss: 0.3145, validation loss: 0.3152
2024-06-03 10:42:13 [INFO]: Epoch 076 - training loss: 0.3199, validation loss: 0.3175
2024-06-03 10:42:20 [INFO]: Epoch 077 - training loss: 0.3180, validation loss: 0.3192
2024-06-03 10:42:26 [INFO]: Epoch 078 - training loss: 0.3157, validation loss: 0.3151
2024-06-03 10:42:33 [INFO]: Epoch 079 - training loss: 0.3101, validation loss: 0.3222
2024-06-03 10:42:39 [INFO]: Epoch 080 - training loss: 0.3156, validation loss: 0.3214
2024-06-03 10:42:45 [INFO]: Epoch 081 - training loss: 0.3116, validation loss: 0.3161
2024-06-03 10:42:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:42:45 [INFO]: Finished training. The best model is from epoch#71.
2024-06-03 10:42:49 [INFO]: Saved the model to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_1/20240603_T103342/Transformer.pypots
2024-06-03 10:42:51 [INFO]: Successfully saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_1/imputation.pkl
2024-06-03 10:42:51 [INFO]: Round1 - Transformer on BeijingAir: MAE=0.2770, MSE=0.3350, MRE=0.3730
2024-06-03 10:42:51 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:42:51 [INFO]: Using the given device: cuda:0
2024-06-03 10:42:51 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_2/20240603_T104251
2024-06-03 10:42:51 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_2/20240603_T104251/tensorboard
2024-06-03 10:42:51 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 10:42:51 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 10:42:56 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 10:43:02 [INFO]: Epoch 001 - training loss: 1.5084, validation loss: 1.0479
2024-06-03 10:43:08 [INFO]: Epoch 002 - training loss: 1.1362, validation loss: 0.6939
2024-06-03 10:43:14 [INFO]: Epoch 003 - training loss: 0.8572, validation loss: 0.5125
2024-06-03 10:43:20 [INFO]: Epoch 004 - training loss: 0.7527, validation loss: 0.4785
2024-06-03 10:43:26 [INFO]: Epoch 005 - training loss: 0.6989, validation loss: 0.4343
2024-06-03 10:43:33 [INFO]: Epoch 006 - training loss: 0.6581, validation loss: 0.4130
2024-06-03 10:43:39 [INFO]: Epoch 007 - training loss: 0.6338, validation loss: 0.4073
2024-06-03 10:43:45 [INFO]: Epoch 008 - training loss: 0.6068, validation loss: 0.3999
2024-06-03 10:43:51 [INFO]: Epoch 009 - training loss: 0.5940, validation loss: 0.3863
2024-06-03 10:43:57 [INFO]: Epoch 010 - training loss: 0.5687, validation loss: 0.3750
2024-06-03 10:44:03 [INFO]: Epoch 011 - training loss: 0.5608, validation loss: 0.3745
2024-06-03 10:44:09 [INFO]: Epoch 012 - training loss: 0.5468, validation loss: 0.3773
2024-06-03 10:44:15 [INFO]: Epoch 013 - training loss: 0.5300, validation loss: 0.3650
2024-06-03 10:44:21 [INFO]: Epoch 014 - training loss: 0.5289, validation loss: 0.3656
2024-06-03 10:44:27 [INFO]: Epoch 015 - training loss: 0.5198, validation loss: 0.3659
2024-06-03 10:44:33 [INFO]: Epoch 016 - training loss: 0.5083, validation loss: 0.3567
2024-06-03 10:44:39 [INFO]: Epoch 017 - training loss: 0.4998, validation loss: 0.3542
2024-06-03 10:44:45 [INFO]: Epoch 018 - training loss: 0.4940, validation loss: 0.3557
2024-06-03 10:44:51 [INFO]: Epoch 019 - training loss: 0.4911, validation loss: 0.3507
2024-06-03 10:44:57 [INFO]: Epoch 020 - training loss: 0.4785, validation loss: 0.3499
2024-06-03 10:45:03 [INFO]: Epoch 021 - training loss: 0.4826, validation loss: 0.3457
2024-06-03 10:45:10 [INFO]: Epoch 022 - training loss: 0.4675, validation loss: 0.3441
2024-06-03 10:45:16 [INFO]: Epoch 023 - training loss: 0.4709, validation loss: 0.3480
2024-06-03 10:45:22 [INFO]: Epoch 024 - training loss: 0.4584, validation loss: 0.3520
2024-06-03 10:45:28 [INFO]: Epoch 025 - training loss: 0.4643, validation loss: 0.3440
2024-06-03 10:45:34 [INFO]: Epoch 026 - training loss: 0.4536, validation loss: 0.3503
2024-06-03 10:45:39 [INFO]: Epoch 027 - training loss: 0.4547, validation loss: 0.3363
2024-06-03 10:45:45 [INFO]: Epoch 028 - training loss: 0.4488, validation loss: 0.3410
2024-06-03 10:45:51 [INFO]: Epoch 029 - training loss: 0.4386, validation loss: 0.3401
2024-06-03 10:45:58 [INFO]: Epoch 030 - training loss: 0.4444, validation loss: 0.3336
2024-06-03 10:46:03 [INFO]: Epoch 031 - training loss: 0.4396, validation loss: 0.3342
2024-06-03 10:46:09 [INFO]: Epoch 032 - training loss: 0.4294, validation loss: 0.3357
2024-06-03 10:46:14 [INFO]: Epoch 033 - training loss: 0.4206, validation loss: 0.3442
2024-06-03 10:46:19 [INFO]: Epoch 034 - training loss: 0.4294, validation loss: 0.3310
2024-06-03 10:46:23 [INFO]: Epoch 035 - training loss: 0.4221, validation loss: 0.3302
2024-06-03 10:46:28 [INFO]: Epoch 036 - training loss: 0.4166, validation loss: 0.3311
2024-06-03 10:46:32 [INFO]: Epoch 037 - training loss: 0.4151, validation loss: 0.3334
2024-06-03 10:46:36 [INFO]: Epoch 038 - training loss: 0.4050, validation loss: 0.3272
2024-06-03 10:46:41 [INFO]: Epoch 039 - training loss: 0.4045, validation loss: 0.3281
2024-06-03 10:46:45 [INFO]: Epoch 040 - training loss: 0.4030, validation loss: 0.3241
2024-06-03 10:46:49 [INFO]: Epoch 041 - training loss: 0.3942, validation loss: 0.3259
2024-06-03 10:46:54 [INFO]: Epoch 042 - training loss: 0.3949, validation loss: 0.3274
2024-06-03 10:46:58 [INFO]: Epoch 043 - training loss: 0.3916, validation loss: 0.3244
2024-06-03 10:47:03 [INFO]: Epoch 044 - training loss: 0.3875, validation loss: 0.3322
2024-06-03 10:47:07 [INFO]: Epoch 045 - training loss: 0.3881, validation loss: 0.3201
2024-06-03 10:47:11 [INFO]: Epoch 046 - training loss: 0.3855, validation loss: 0.3314
2024-06-03 10:47:16 [INFO]: Epoch 047 - training loss: 0.3831, validation loss: 0.3242
2024-06-03 10:47:20 [INFO]: Epoch 048 - training loss: 0.3761, validation loss: 0.3184
2024-06-03 10:47:24 [INFO]: Epoch 049 - training loss: 0.3787, validation loss: 0.3236
2024-06-03 10:47:29 [INFO]: Epoch 050 - training loss: 0.3701, validation loss: 0.3258
2024-06-03 10:47:33 [INFO]: Epoch 051 - training loss: 0.3672, validation loss: 0.3224
2024-06-03 10:47:38 [INFO]: Epoch 052 - training loss: 0.3650, validation loss: 0.3207
2024-06-03 10:47:42 [INFO]: Epoch 053 - training loss: 0.3696, validation loss: 0.3220
2024-06-03 10:47:47 [INFO]: Epoch 054 - training loss: 0.3618, validation loss: 0.3232
2024-06-03 10:47:51 [INFO]: Epoch 055 - training loss: 0.3583, validation loss: 0.3204
2024-06-03 10:47:56 [INFO]: Epoch 056 - training loss: 0.3552, validation loss: 0.3210
2024-06-03 10:48:00 [INFO]: Epoch 057 - training loss: 0.3539, validation loss: 0.3229
2024-06-03 10:48:04 [INFO]: Epoch 058 - training loss: 0.3584, validation loss: 0.3245
2024-06-03 10:48:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:48:04 [INFO]: Finished training. The best model is from epoch#48.
2024-06-03 10:48:06 [INFO]: Saved the model to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_2/20240603_T104251/Transformer.pypots
2024-06-03 10:48:08 [INFO]: Successfully saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_2/imputation.pkl
2024-06-03 10:48:08 [INFO]: Round2 - Transformer on BeijingAir: MAE=0.2884, MSE=0.3379, MRE=0.3884
2024-06-03 10:48:08 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:48:08 [INFO]: Using the given device: cuda:0
2024-06-03 10:48:08 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_3/20240603_T104808
2024-06-03 10:48:08 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_3/20240603_T104808/tensorboard
2024-06-03 10:48:08 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 10:48:08 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 10:48:11 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 10:48:15 [INFO]: Epoch 001 - training loss: 1.5155, validation loss: 1.0359
2024-06-03 10:48:19 [INFO]: Epoch 002 - training loss: 1.1887, validation loss: 0.6868
2024-06-03 10:48:24 [INFO]: Epoch 003 - training loss: 0.8911, validation loss: 0.5148
2024-06-03 10:48:28 [INFO]: Epoch 004 - training loss: 0.7678, validation loss: 0.4769
2024-06-03 10:48:33 [INFO]: Epoch 005 - training loss: 0.7059, validation loss: 0.4354
2024-06-03 10:48:37 [INFO]: Epoch 006 - training loss: 0.6552, validation loss: 0.4175
2024-06-03 10:48:42 [INFO]: Epoch 007 - training loss: 0.6315, validation loss: 0.3999
2024-06-03 10:48:46 [INFO]: Epoch 008 - training loss: 0.6067, validation loss: 0.3844
2024-06-03 10:48:50 [INFO]: Epoch 009 - training loss: 0.5954, validation loss: 0.3968
2024-06-03 10:48:55 [INFO]: Epoch 010 - training loss: 0.5888, validation loss: 0.3816
2024-06-03 10:48:59 [INFO]: Epoch 011 - training loss: 0.5642, validation loss: 0.3801
2024-06-03 10:49:04 [INFO]: Epoch 012 - training loss: 0.5650, validation loss: 0.3791
2024-06-03 10:49:08 [INFO]: Epoch 013 - training loss: 0.5421, validation loss: 0.3627
2024-06-03 10:49:12 [INFO]: Epoch 014 - training loss: 0.5220, validation loss: 0.3626
2024-06-03 10:49:17 [INFO]: Epoch 015 - training loss: 0.5129, validation loss: 0.3579
2024-06-03 10:49:21 [INFO]: Epoch 016 - training loss: 0.5283, validation loss: 0.3641
2024-06-03 10:49:26 [INFO]: Epoch 017 - training loss: 0.5033, validation loss: 0.3508
2024-06-03 10:49:30 [INFO]: Epoch 018 - training loss: 0.4939, validation loss: 0.3538
2024-06-03 10:49:35 [INFO]: Epoch 019 - training loss: 0.4875, validation loss: 0.3592
2024-06-03 10:49:39 [INFO]: Epoch 020 - training loss: 0.4880, validation loss: 0.3540
2024-06-03 10:49:43 [INFO]: Epoch 021 - training loss: 0.4811, validation loss: 0.3565
2024-06-03 10:49:48 [INFO]: Epoch 022 - training loss: 0.4723, validation loss: 0.3500
2024-06-03 10:49:52 [INFO]: Epoch 023 - training loss: 0.4621, validation loss: 0.3459
2024-06-03 10:49:56 [INFO]: Epoch 024 - training loss: 0.4678, validation loss: 0.3502
2024-06-03 10:50:01 [INFO]: Epoch 025 - training loss: 0.4618, validation loss: 0.3448
2024-06-03 10:50:05 [INFO]: Epoch 026 - training loss: 0.4572, validation loss: 0.3470
2024-06-03 10:50:09 [INFO]: Epoch 027 - training loss: 0.4556, validation loss: 0.3366
2024-06-03 10:50:14 [INFO]: Epoch 028 - training loss: 0.4488, validation loss: 0.3449
2024-06-03 10:50:18 [INFO]: Epoch 029 - training loss: 0.4413, validation loss: 0.3349
2024-06-03 10:50:23 [INFO]: Epoch 030 - training loss: 0.4468, validation loss: 0.3383
2024-06-03 10:50:27 [INFO]: Epoch 031 - training loss: 0.4289, validation loss: 0.3430
2024-06-03 10:50:32 [INFO]: Epoch 032 - training loss: 0.4335, validation loss: 0.3333
2024-06-03 10:50:36 [INFO]: Epoch 033 - training loss: 0.4242, validation loss: 0.3319
2024-06-03 10:50:40 [INFO]: Epoch 034 - training loss: 0.4154, validation loss: 0.3364
2024-06-03 10:50:44 [INFO]: Epoch 035 - training loss: 0.4160, validation loss: 0.3312
2024-06-03 10:50:49 [INFO]: Epoch 036 - training loss: 0.4190, validation loss: 0.3302
2024-06-03 10:50:53 [INFO]: Epoch 037 - training loss: 0.4066, validation loss: 0.3300
2024-06-03 10:50:58 [INFO]: Epoch 038 - training loss: 0.4055, validation loss: 0.3358
2024-06-03 10:51:02 [INFO]: Epoch 039 - training loss: 0.4052, validation loss: 0.3426
2024-06-03 10:51:06 [INFO]: Epoch 040 - training loss: 0.4061, validation loss: 0.3223
2024-06-03 10:51:09 [INFO]: Epoch 041 - training loss: 0.4014, validation loss: 0.3276
2024-06-03 10:51:11 [INFO]: Epoch 042 - training loss: 0.4007, validation loss: 0.3241
2024-06-03 10:51:14 [INFO]: Epoch 043 - training loss: 0.3881, validation loss: 0.3209
2024-06-03 10:51:17 [INFO]: Epoch 044 - training loss: 0.3869, validation loss: 0.3271
2024-06-03 10:51:20 [INFO]: Epoch 045 - training loss: 0.3864, validation loss: 0.3309
2024-06-03 10:51:22 [INFO]: Epoch 046 - training loss: 0.3822, validation loss: 0.3212
2024-06-03 10:51:25 [INFO]: Epoch 047 - training loss: 0.3785, validation loss: 0.3249
2024-06-03 10:51:28 [INFO]: Epoch 048 - training loss: 0.3837, validation loss: 0.3185
2024-06-03 10:51:30 [INFO]: Epoch 049 - training loss: 0.3792, validation loss: 0.3303
2024-06-03 10:51:33 [INFO]: Epoch 050 - training loss: 0.3793, validation loss: 0.3225
2024-06-03 10:51:36 [INFO]: Epoch 051 - training loss: 0.3671, validation loss: 0.3273
2024-06-03 10:51:39 [INFO]: Epoch 052 - training loss: 0.3644, validation loss: 0.3225
2024-06-03 10:51:41 [INFO]: Epoch 053 - training loss: 0.3667, validation loss: 0.3237
2024-06-03 10:51:44 [INFO]: Epoch 054 - training loss: 0.3604, validation loss: 0.3239
2024-06-03 10:51:47 [INFO]: Epoch 055 - training loss: 0.3661, validation loss: 0.3192
2024-06-03 10:51:50 [INFO]: Epoch 056 - training loss: 0.3506, validation loss: 0.3200
2024-06-03 10:51:52 [INFO]: Epoch 057 - training loss: 0.3608, validation loss: 0.3248
2024-06-03 10:51:55 [INFO]: Epoch 058 - training loss: 0.3523, validation loss: 0.3125
2024-06-03 10:51:58 [INFO]: Epoch 059 - training loss: 0.3484, validation loss: 0.3164
2024-06-03 10:52:00 [INFO]: Epoch 060 - training loss: 0.3482, validation loss: 0.3236
2024-06-03 10:52:03 [INFO]: Epoch 061 - training loss: 0.3513, validation loss: 0.3223
2024-06-03 10:52:06 [INFO]: Epoch 062 - training loss: 0.3497, validation loss: 0.3128
2024-06-03 10:52:09 [INFO]: Epoch 063 - training loss: 0.3618, validation loss: 0.3217
2024-06-03 10:52:11 [INFO]: Epoch 064 - training loss: 0.3525, validation loss: 0.3205
2024-06-03 10:52:14 [INFO]: Epoch 065 - training loss: 0.3459, validation loss: 0.3224
2024-06-03 10:52:17 [INFO]: Epoch 066 - training loss: 0.3461, validation loss: 0.3184
2024-06-03 10:52:19 [INFO]: Epoch 067 - training loss: 0.3344, validation loss: 0.3196
2024-06-03 10:52:22 [INFO]: Epoch 068 - training loss: 0.3367, validation loss: 0.3164
2024-06-03 10:52:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:52:22 [INFO]: Finished training. The best model is from epoch#58.
2024-06-03 10:52:23 [INFO]: Saved the model to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_3/20240603_T104808/Transformer.pypots
2024-06-03 10:52:24 [INFO]: Successfully saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_3/imputation.pkl
2024-06-03 10:52:24 [INFO]: Round3 - Transformer on BeijingAir: MAE=0.2784, MSE=0.3330, MRE=0.3750
2024-06-03 10:52:24 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:52:24 [INFO]: Using the given device: cuda:0
2024-06-03 10:52:24 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_4/20240603_T105224
2024-06-03 10:52:24 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_4/20240603_T105224/tensorboard
2024-06-03 10:52:24 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 10:52:24 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 10:52:26 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 10:52:29 [INFO]: Epoch 001 - training loss: 1.4991, validation loss: 1.0127
2024-06-03 10:52:31 [INFO]: Epoch 002 - training loss: 1.1357, validation loss: 0.5852
2024-06-03 10:52:34 [INFO]: Epoch 003 - training loss: 0.8301, validation loss: 0.5258
2024-06-03 10:52:37 [INFO]: Epoch 004 - training loss: 0.7377, validation loss: 0.4643
2024-06-03 10:52:40 [INFO]: Epoch 005 - training loss: 0.7010, validation loss: 0.4598
2024-06-03 10:52:42 [INFO]: Epoch 006 - training loss: 0.6672, validation loss: 0.4196
2024-06-03 10:52:45 [INFO]: Epoch 007 - training loss: 0.6367, validation loss: 0.4028
2024-06-03 10:52:48 [INFO]: Epoch 008 - training loss: 0.6101, validation loss: 0.3925
2024-06-03 10:52:50 [INFO]: Epoch 009 - training loss: 0.5866, validation loss: 0.3768
2024-06-03 10:52:53 [INFO]: Epoch 010 - training loss: 0.5732, validation loss: 0.3758
2024-06-03 10:52:56 [INFO]: Epoch 011 - training loss: 0.5756, validation loss: 0.3770
2024-06-03 10:52:59 [INFO]: Epoch 012 - training loss: 0.5481, validation loss: 0.3646
2024-06-03 10:53:01 [INFO]: Epoch 013 - training loss: 0.5344, validation loss: 0.3697
2024-06-03 10:53:04 [INFO]: Epoch 014 - training loss: 0.5191, validation loss: 0.3623
2024-06-03 10:53:07 [INFO]: Epoch 015 - training loss: 0.5192, validation loss: 0.3641
2024-06-03 10:53:10 [INFO]: Epoch 016 - training loss: 0.5190, validation loss: 0.3575
2024-06-03 10:53:12 [INFO]: Epoch 017 - training loss: 0.5010, validation loss: 0.3540
2024-06-03 10:53:15 [INFO]: Epoch 018 - training loss: 0.4963, validation loss: 0.3588
2024-06-03 10:53:18 [INFO]: Epoch 019 - training loss: 0.4897, validation loss: 0.3452
2024-06-03 10:53:20 [INFO]: Epoch 020 - training loss: 0.4867, validation loss: 0.3505
2024-06-03 10:53:23 [INFO]: Epoch 021 - training loss: 0.4800, validation loss: 0.3526
2024-06-03 10:53:26 [INFO]: Epoch 022 - training loss: 0.4854, validation loss: 0.3504
2024-06-03 10:53:29 [INFO]: Epoch 023 - training loss: 0.4706, validation loss: 0.3413
2024-06-03 10:53:31 [INFO]: Epoch 024 - training loss: 0.4570, validation loss: 0.3434
2024-06-03 10:53:34 [INFO]: Epoch 025 - training loss: 0.4502, validation loss: 0.3495
2024-06-03 10:53:37 [INFO]: Epoch 026 - training loss: 0.4549, validation loss: 0.3521
2024-06-03 10:53:39 [INFO]: Epoch 027 - training loss: 0.4587, validation loss: 0.3450
2024-06-03 10:53:42 [INFO]: Epoch 028 - training loss: 0.4493, validation loss: 0.3453
2024-06-03 10:53:45 [INFO]: Epoch 029 - training loss: 0.4405, validation loss: 0.3353
2024-06-03 10:53:48 [INFO]: Epoch 030 - training loss: 0.4360, validation loss: 0.3362
2024-06-03 10:53:50 [INFO]: Epoch 031 - training loss: 0.4240, validation loss: 0.3297
2024-06-03 10:53:53 [INFO]: Epoch 032 - training loss: 0.4296, validation loss: 0.3449
2024-06-03 10:53:56 [INFO]: Epoch 033 - training loss: 0.4283, validation loss: 0.3304
2024-06-03 10:53:59 [INFO]: Epoch 034 - training loss: 0.4244, validation loss: 0.3258
2024-06-03 10:54:01 [INFO]: Epoch 035 - training loss: 0.4264, validation loss: 0.3451
2024-06-03 10:54:04 [INFO]: Epoch 036 - training loss: 0.4292, validation loss: 0.3320
2024-06-03 10:54:07 [INFO]: Epoch 037 - training loss: 0.4126, validation loss: 0.3337
2024-06-03 10:54:09 [INFO]: Epoch 038 - training loss: 0.4110, validation loss: 0.3301
2024-06-03 10:54:12 [INFO]: Epoch 039 - training loss: 0.4014, validation loss: 0.3250
2024-06-03 10:54:15 [INFO]: Epoch 040 - training loss: 0.4030, validation loss: 0.3292
2024-06-03 10:54:18 [INFO]: Epoch 041 - training loss: 0.3953, validation loss: 0.3303
2024-06-03 10:54:20 [INFO]: Epoch 042 - training loss: 0.3999, validation loss: 0.3240
2024-06-03 10:54:23 [INFO]: Epoch 043 - training loss: 0.3949, validation loss: 0.3222
2024-06-03 10:54:26 [INFO]: Epoch 044 - training loss: 0.3871, validation loss: 0.3269
2024-06-03 10:54:29 [INFO]: Epoch 045 - training loss: 0.3829, validation loss: 0.3257
2024-06-03 10:54:31 [INFO]: Epoch 046 - training loss: 0.3837, validation loss: 0.3290
2024-06-03 10:54:34 [INFO]: Epoch 047 - training loss: 0.3809, validation loss: 0.3258
2024-06-03 10:54:37 [INFO]: Epoch 048 - training loss: 0.3898, validation loss: 0.3237
2024-06-03 10:54:39 [INFO]: Epoch 049 - training loss: 0.3906, validation loss: 0.3246
2024-06-03 10:54:42 [INFO]: Epoch 050 - training loss: 0.3805, validation loss: 0.3269
2024-06-03 10:54:45 [INFO]: Epoch 051 - training loss: 0.3728, validation loss: 0.3225
2024-06-03 10:54:48 [INFO]: Epoch 052 - training loss: 0.3663, validation loss: 0.3199
2024-06-03 10:54:50 [INFO]: Epoch 053 - training loss: 0.3718, validation loss: 0.3212
2024-06-03 10:54:53 [INFO]: Epoch 054 - training loss: 0.3683, validation loss: 0.3184
2024-06-03 10:54:56 [INFO]: Epoch 055 - training loss: 0.3633, validation loss: 0.3186
2024-06-03 10:54:59 [INFO]: Epoch 056 - training loss: 0.3586, validation loss: 0.3190
2024-06-03 10:55:01 [INFO]: Epoch 057 - training loss: 0.3580, validation loss: 0.3176
2024-06-03 10:55:04 [INFO]: Epoch 058 - training loss: 0.3545, validation loss: 0.3205
2024-06-03 10:55:07 [INFO]: Epoch 059 - training loss: 0.3557, validation loss: 0.3170
2024-06-03 10:55:09 [INFO]: Epoch 060 - training loss: 0.3496, validation loss: 0.3229
2024-06-03 10:55:12 [INFO]: Epoch 061 - training loss: 0.3479, validation loss: 0.3206
2024-06-03 10:55:15 [INFO]: Epoch 062 - training loss: 0.3496, validation loss: 0.3239
2024-06-03 10:55:18 [INFO]: Epoch 063 - training loss: 0.3460, validation loss: 0.3209
2024-06-03 10:55:20 [INFO]: Epoch 064 - training loss: 0.3417, validation loss: 0.3179
2024-06-03 10:55:23 [INFO]: Epoch 065 - training loss: 0.3394, validation loss: 0.3133
2024-06-03 10:55:26 [INFO]: Epoch 066 - training loss: 0.3367, validation loss: 0.3163
2024-06-03 10:55:29 [INFO]: Epoch 067 - training loss: 0.3336, validation loss: 0.3194
2024-06-03 10:55:31 [INFO]: Epoch 068 - training loss: 0.3377, validation loss: 0.3096
2024-06-03 10:55:34 [INFO]: Epoch 069 - training loss: 0.3320, validation loss: 0.3160
2024-06-03 10:55:37 [INFO]: Epoch 070 - training loss: 0.3281, validation loss: 0.3132
2024-06-03 10:55:39 [INFO]: Epoch 071 - training loss: 0.3355, validation loss: 0.3215
2024-06-03 10:55:42 [INFO]: Epoch 072 - training loss: 0.3299, validation loss: 0.3187
2024-06-03 10:55:45 [INFO]: Epoch 073 - training loss: 0.3250, validation loss: 0.3177
2024-06-03 10:55:48 [INFO]: Epoch 074 - training loss: 0.3230, validation loss: 0.3179
2024-06-03 10:55:50 [INFO]: Epoch 075 - training loss: 0.3160, validation loss: 0.3122
2024-06-03 10:55:53 [INFO]: Epoch 076 - training loss: 0.3164, validation loss: 0.3167
2024-06-03 10:55:56 [INFO]: Epoch 077 - training loss: 0.3203, validation loss: 0.3198
2024-06-03 10:55:59 [INFO]: Epoch 078 - training loss: 0.3175, validation loss: 0.3162
2024-06-03 10:55:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:55:59 [INFO]: Finished training. The best model is from epoch#68.
2024-06-03 10:56:00 [INFO]: Saved the model to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_4/20240603_T105224/Transformer.pypots
2024-06-03 10:56:01 [INFO]: Successfully saved to results_point_rate09/BeijingAir/Transformer_BeijingAir/round_4/imputation.pkl
2024-06-03 10:56:01 [INFO]: Round4 - Transformer on BeijingAir: MAE=0.2798, MSE=0.3371, MRE=0.3768
2024-06-03 10:56:01 [INFO]: Done! Final results:
Averaged Transformer (203,038,852 params) on BeijingAir: MAE=0.2773 ± 0.004917907026723745, MSE=0.3333 ± 0.00432215619826339, MRE=0.3679 ± 0.006524250151105843, average inference time=0.40