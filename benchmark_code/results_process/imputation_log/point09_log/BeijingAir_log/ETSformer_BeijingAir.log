2024-06-03 00:37:46 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 00:37:46 [INFO]: Using the given device: cuda:0
2024-06-03 00:37:47 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_0/20240603_T003747
2024-06-03 00:37:47 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_0/20240603_T003747/tensorboard
2024-06-03 00:37:48 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 7,928,510
2024-06-03 00:38:07 [INFO]: Epoch 001 - training loss: 1.3257, validation loss: 1.0047
2024-06-03 00:38:24 [INFO]: Epoch 002 - training loss: 1.2090, validation loss: 0.9755
2024-06-03 00:38:41 [INFO]: Epoch 003 - training loss: 1.1299, validation loss: 0.9451
2024-06-03 00:38:58 [INFO]: Epoch 004 - training loss: 1.0416, validation loss: 0.8819
2024-06-03 00:39:15 [INFO]: Epoch 005 - training loss: 0.9457, validation loss: 0.8138
2024-06-03 00:39:31 [INFO]: Epoch 006 - training loss: 0.8964, validation loss: 0.7584
2024-06-03 00:39:48 [INFO]: Epoch 007 - training loss: 0.8223, validation loss: 0.6904
2024-06-03 00:40:05 [INFO]: Epoch 008 - training loss: 0.7777, validation loss: 0.6527
2024-06-03 00:40:22 [INFO]: Epoch 009 - training loss: 0.7494, validation loss: 0.6411
2024-06-03 00:40:38 [INFO]: Epoch 010 - training loss: 0.7376, validation loss: 0.6249
2024-06-03 00:40:55 [INFO]: Epoch 011 - training loss: 0.7182, validation loss: 0.6105
2024-06-03 00:41:12 [INFO]: Epoch 012 - training loss: 0.6995, validation loss: 0.6184
2024-06-03 00:41:29 [INFO]: Epoch 013 - training loss: 0.6911, validation loss: 0.5937
2024-06-03 00:41:46 [INFO]: Epoch 014 - training loss: 0.6697, validation loss: 0.5835
2024-06-03 00:42:02 [INFO]: Epoch 015 - training loss: 0.6643, validation loss: 0.5924
2024-06-03 00:42:20 [INFO]: Epoch 016 - training loss: 0.6552, validation loss: 0.5793
2024-06-03 00:42:37 [INFO]: Epoch 017 - training loss: 0.6386, validation loss: 0.5770
2024-06-03 00:42:55 [INFO]: Epoch 018 - training loss: 0.6304, validation loss: 0.5683
2024-06-03 00:43:12 [INFO]: Epoch 019 - training loss: 0.6271, validation loss: 0.5684
2024-06-03 00:43:28 [INFO]: Epoch 020 - training loss: 0.6205, validation loss: 0.5690
2024-06-03 00:43:46 [INFO]: Epoch 021 - training loss: 0.6174, validation loss: 0.5554
2024-06-03 00:44:03 [INFO]: Epoch 022 - training loss: 0.6096, validation loss: 0.5562
2024-06-03 00:44:20 [INFO]: Epoch 023 - training loss: 0.6006, validation loss: 0.5522
2024-06-03 00:44:36 [INFO]: Epoch 024 - training loss: 0.5951, validation loss: 0.5460
2024-06-03 00:44:54 [INFO]: Epoch 025 - training loss: 0.5882, validation loss: 0.5435
2024-06-03 00:45:12 [INFO]: Epoch 026 - training loss: 0.5823, validation loss: 0.5440
2024-06-03 00:45:29 [INFO]: Epoch 027 - training loss: 0.5804, validation loss: 0.5392
2024-06-03 00:45:47 [INFO]: Epoch 028 - training loss: 0.5736, validation loss: 0.5378
2024-06-03 00:46:03 [INFO]: Epoch 029 - training loss: 0.5700, validation loss: 0.5339
2024-06-03 00:46:20 [INFO]: Epoch 030 - training loss: 0.5686, validation loss: 0.5280
2024-06-03 00:46:38 [INFO]: Epoch 031 - training loss: 0.5676, validation loss: 0.5256
2024-06-03 00:46:54 [INFO]: Epoch 032 - training loss: 0.5592, validation loss: 0.5217
2024-06-03 00:47:09 [INFO]: Epoch 033 - training loss: 0.5610, validation loss: 0.5200
2024-06-03 00:47:25 [INFO]: Epoch 034 - training loss: 0.5523, validation loss: 0.5137
2024-06-03 00:47:40 [INFO]: Epoch 035 - training loss: 0.5504, validation loss: 0.5168
2024-06-03 00:47:56 [INFO]: Epoch 036 - training loss: 0.5540, validation loss: 0.5089
2024-06-03 00:48:12 [INFO]: Epoch 037 - training loss: 0.5473, validation loss: 0.5153
2024-06-03 00:48:28 [INFO]: Epoch 038 - training loss: 0.5438, validation loss: 0.5199
2024-06-03 00:48:43 [INFO]: Epoch 039 - training loss: 0.5479, validation loss: 0.5104
2024-06-03 00:48:59 [INFO]: Epoch 040 - training loss: 0.5371, validation loss: 0.5070
2024-06-03 00:49:15 [INFO]: Epoch 041 - training loss: 0.5381, validation loss: 0.5067
2024-06-03 00:49:31 [INFO]: Epoch 042 - training loss: 0.5359, validation loss: 0.5042
2024-06-03 00:49:46 [INFO]: Epoch 043 - training loss: 0.5321, validation loss: 0.5007
2024-06-03 00:50:02 [INFO]: Epoch 044 - training loss: 0.5346, validation loss: 0.5016
2024-06-03 00:50:18 [INFO]: Epoch 045 - training loss: 0.5319, validation loss: 0.4973
2024-06-03 00:50:33 [INFO]: Epoch 046 - training loss: 0.5249, validation loss: 0.4965
2024-06-03 00:50:46 [INFO]: Epoch 047 - training loss: 0.5267, validation loss: 0.4961
2024-06-03 00:51:00 [INFO]: Epoch 048 - training loss: 0.5248, validation loss: 0.4948
2024-06-03 00:51:13 [INFO]: Epoch 049 - training loss: 0.5209, validation loss: 0.4977
2024-06-03 00:51:27 [INFO]: Epoch 050 - training loss: 0.5205, validation loss: 0.4948
2024-06-03 00:51:41 [INFO]: Epoch 051 - training loss: 0.5206, validation loss: 0.4887
2024-06-03 00:51:55 [INFO]: Epoch 052 - training loss: 0.5190, validation loss: 0.4933
2024-06-03 00:52:08 [INFO]: Epoch 053 - training loss: 0.5190, validation loss: 0.4891
2024-06-03 00:52:22 [INFO]: Epoch 054 - training loss: 0.5120, validation loss: 0.4869
2024-06-03 00:52:35 [INFO]: Epoch 055 - training loss: 0.5105, validation loss: 0.4842
2024-06-03 00:52:50 [INFO]: Epoch 056 - training loss: 0.5094, validation loss: 0.4814
2024-06-03 00:53:03 [INFO]: Epoch 057 - training loss: 0.5106, validation loss: 0.4824
2024-06-03 00:53:17 [INFO]: Epoch 058 - training loss: 0.5048, validation loss: 0.4861
2024-06-03 00:53:31 [INFO]: Epoch 059 - training loss: 0.5070, validation loss: 0.4801
2024-06-03 00:53:44 [INFO]: Epoch 060 - training loss: 0.5071, validation loss: 0.4821
2024-06-03 00:53:58 [INFO]: Epoch 061 - training loss: 0.5032, validation loss: 0.4803
2024-06-03 00:54:12 [INFO]: Epoch 062 - training loss: 0.5020, validation loss: 0.4811
2024-06-03 00:54:26 [INFO]: Epoch 063 - training loss: 0.4967, validation loss: 0.4771
2024-06-03 00:54:40 [INFO]: Epoch 064 - training loss: 0.5007, validation loss: 0.4791
2024-06-03 00:54:54 [INFO]: Epoch 065 - training loss: 0.5014, validation loss: 0.4770
2024-06-03 00:55:07 [INFO]: Epoch 066 - training loss: 0.4974, validation loss: 0.4747
2024-06-03 00:55:21 [INFO]: Epoch 067 - training loss: 0.4980, validation loss: 0.4756
2024-06-03 00:55:35 [INFO]: Epoch 068 - training loss: 0.5027, validation loss: 0.4755
2024-06-03 00:55:49 [INFO]: Epoch 069 - training loss: 0.4908, validation loss: 0.4714
2024-06-03 00:56:03 [INFO]: Epoch 070 - training loss: 0.4909, validation loss: 0.4734
2024-06-03 00:56:16 [INFO]: Epoch 071 - training loss: 0.4936, validation loss: 0.4717
2024-06-03 00:56:30 [INFO]: Epoch 072 - training loss: 0.4944, validation loss: 0.4681
2024-06-03 00:56:44 [INFO]: Epoch 073 - training loss: 0.4935, validation loss: 0.4682
2024-06-03 00:56:58 [INFO]: Epoch 074 - training loss: 0.4868, validation loss: 0.4663
2024-06-03 00:57:12 [INFO]: Epoch 075 - training loss: 0.4910, validation loss: 0.4707
2024-06-03 00:57:25 [INFO]: Epoch 076 - training loss: 0.4871, validation loss: 0.4679
2024-06-03 00:57:39 [INFO]: Epoch 077 - training loss: 0.4890, validation loss: 0.4705
2024-06-03 00:57:53 [INFO]: Epoch 078 - training loss: 0.4884, validation loss: 0.4679
2024-06-03 00:58:07 [INFO]: Epoch 079 - training loss: 0.4900, validation loss: 0.4660
2024-06-03 00:58:21 [INFO]: Epoch 080 - training loss: 0.4842, validation loss: 0.4659
2024-06-03 00:58:34 [INFO]: Epoch 081 - training loss: 0.4838, validation loss: 0.4654
2024-06-03 00:58:48 [INFO]: Epoch 082 - training loss: 0.4825, validation loss: 0.4603
2024-06-03 00:59:01 [INFO]: Epoch 083 - training loss: 0.4828, validation loss: 0.4628
2024-06-03 00:59:15 [INFO]: Epoch 084 - training loss: 0.4793, validation loss: 0.4617
2024-06-03 00:59:29 [INFO]: Epoch 085 - training loss: 0.4829, validation loss: 0.4642
2024-06-03 00:59:42 [INFO]: Epoch 086 - training loss: 0.4840, validation loss: 0.4612
2024-06-03 00:59:56 [INFO]: Epoch 087 - training loss: 0.4792, validation loss: 0.4581
2024-06-03 01:00:10 [INFO]: Epoch 088 - training loss: 0.4824, validation loss: 0.4558
2024-06-03 01:00:23 [INFO]: Epoch 089 - training loss: 0.4772, validation loss: 0.4596
2024-06-03 01:00:37 [INFO]: Epoch 090 - training loss: 0.4795, validation loss: 0.4580
2024-06-03 01:00:51 [INFO]: Epoch 091 - training loss: 0.4765, validation loss: 0.4583
2024-06-03 01:01:04 [INFO]: Epoch 092 - training loss: 0.4776, validation loss: 0.4570
2024-06-03 01:01:18 [INFO]: Epoch 093 - training loss: 0.4738, validation loss: 0.4561
2024-06-03 01:01:32 [INFO]: Epoch 094 - training loss: 0.4728, validation loss: 0.4538
2024-06-03 01:01:45 [INFO]: Epoch 095 - training loss: 0.4754, validation loss: 0.4525
2024-06-03 01:02:00 [INFO]: Epoch 096 - training loss: 0.4707, validation loss: 0.4530
2024-06-03 01:02:14 [INFO]: Epoch 097 - training loss: 0.4736, validation loss: 0.4556
2024-06-03 01:02:27 [INFO]: Epoch 098 - training loss: 0.4767, validation loss: 0.4533
2024-06-03 01:02:41 [INFO]: Epoch 099 - training loss: 0.4709, validation loss: 0.4526
2024-06-03 01:02:55 [INFO]: Epoch 100 - training loss: 0.4669, validation loss: 0.4524
2024-06-03 01:02:55 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 01:02:55 [INFO]: Saved the model to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_0/20240603_T003747/ETSformer.pypots
2024-06-03 01:03:06 [INFO]: Successfully saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_0/imputation.pkl
2024-06-03 01:03:06 [INFO]: Round0 - ETSformer on BeijingAir: MAE=0.3583, MSE=0.4645, MRE=0.4825
2024-06-03 01:03:06 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 01:03:06 [INFO]: Using the given device: cuda:0
2024-06-03 01:03:06 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_1/20240603_T010306
2024-06-03 01:03:06 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_1/20240603_T010306/tensorboard
2024-06-03 01:03:07 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 7,928,510
2024-06-03 01:03:20 [INFO]: Epoch 001 - training loss: 1.3069, validation loss: 1.0116
2024-06-03 01:03:33 [INFO]: Epoch 002 - training loss: 1.2055, validation loss: 0.9907
2024-06-03 01:03:44 [INFO]: Epoch 003 - training loss: 1.1299, validation loss: 0.9948
2024-06-03 01:03:56 [INFO]: Epoch 004 - training loss: 1.0490, validation loss: 0.9903
2024-06-03 01:04:07 [INFO]: Epoch 005 - training loss: 0.9452, validation loss: 0.8926
2024-06-03 01:04:19 [INFO]: Epoch 006 - training loss: 0.8341, validation loss: 0.7764
2024-06-03 01:04:31 [INFO]: Epoch 007 - training loss: 0.7643, validation loss: 0.6863
2024-06-03 01:04:43 [INFO]: Epoch 008 - training loss: 0.7230, validation loss: 0.6337
2024-06-03 01:04:55 [INFO]: Epoch 009 - training loss: 0.7025, validation loss: 0.6366
2024-06-03 01:05:07 [INFO]: Epoch 010 - training loss: 0.7002, validation loss: 0.6252
2024-06-03 01:05:19 [INFO]: Epoch 011 - training loss: 0.6720, validation loss: 0.6143
2024-06-03 01:05:31 [INFO]: Epoch 012 - training loss: 0.6627, validation loss: 0.6062
2024-06-03 01:05:42 [INFO]: Epoch 013 - training loss: 0.6484, validation loss: 0.6019
2024-06-03 01:05:54 [INFO]: Epoch 014 - training loss: 0.6410, validation loss: 0.5888
2024-06-03 01:06:06 [INFO]: Epoch 015 - training loss: 0.6347, validation loss: 0.5882
2024-06-03 01:06:18 [INFO]: Epoch 016 - training loss: 0.6262, validation loss: 0.5863
2024-06-03 01:06:29 [INFO]: Epoch 017 - training loss: 0.6283, validation loss: 0.5864
2024-06-03 01:06:41 [INFO]: Epoch 018 - training loss: 0.6192, validation loss: 0.5811
2024-06-03 01:06:53 [INFO]: Epoch 019 - training loss: 0.6115, validation loss: 0.5780
2024-06-03 01:07:05 [INFO]: Epoch 020 - training loss: 0.6062, validation loss: 0.5700
2024-06-03 01:07:17 [INFO]: Epoch 021 - training loss: 0.5998, validation loss: 0.5694
2024-06-03 01:07:29 [INFO]: Epoch 022 - training loss: 0.5972, validation loss: 0.5576
2024-06-03 01:07:41 [INFO]: Epoch 023 - training loss: 0.5920, validation loss: 0.5601
2024-06-03 01:07:53 [INFO]: Epoch 024 - training loss: 0.5922, validation loss: 0.5538
2024-06-03 01:08:05 [INFO]: Epoch 025 - training loss: 0.5846, validation loss: 0.5586
2024-06-03 01:08:17 [INFO]: Epoch 026 - training loss: 0.5796, validation loss: 0.5558
2024-06-03 01:08:28 [INFO]: Epoch 027 - training loss: 0.5778, validation loss: 0.5539
2024-06-03 01:08:41 [INFO]: Epoch 028 - training loss: 0.5747, validation loss: 0.5487
2024-06-03 01:08:53 [INFO]: Epoch 029 - training loss: 0.5733, validation loss: 0.5479
2024-06-03 01:09:05 [INFO]: Epoch 030 - training loss: 0.5670, validation loss: 0.5398
2024-06-03 01:09:17 [INFO]: Epoch 031 - training loss: 0.5605, validation loss: 0.5492
2024-06-03 01:09:29 [INFO]: Epoch 032 - training loss: 0.5611, validation loss: 0.5412
2024-06-03 01:09:41 [INFO]: Epoch 033 - training loss: 0.5580, validation loss: 0.5326
2024-06-03 01:09:53 [INFO]: Epoch 034 - training loss: 0.5528, validation loss: 0.5359
2024-06-03 01:10:05 [INFO]: Epoch 035 - training loss: 0.5505, validation loss: 0.5330
2024-06-03 01:10:17 [INFO]: Epoch 036 - training loss: 0.5504, validation loss: 0.5390
2024-06-03 01:10:29 [INFO]: Epoch 037 - training loss: 0.5443, validation loss: 0.5298
2024-06-03 01:10:41 [INFO]: Epoch 038 - training loss: 0.5469, validation loss: 0.5273
2024-06-03 01:10:53 [INFO]: Epoch 039 - training loss: 0.5497, validation loss: 0.5300
2024-06-03 01:11:05 [INFO]: Epoch 040 - training loss: 0.5438, validation loss: 0.5205
2024-06-03 01:11:17 [INFO]: Epoch 041 - training loss: 0.5438, validation loss: 0.5278
2024-06-03 01:11:30 [INFO]: Epoch 042 - training loss: 0.5368, validation loss: 0.5273
2024-06-03 01:11:42 [INFO]: Epoch 043 - training loss: 0.5359, validation loss: 0.5199
2024-06-03 01:11:53 [INFO]: Epoch 044 - training loss: 0.5310, validation loss: 0.5257
2024-06-03 01:12:05 [INFO]: Epoch 045 - training loss: 0.5284, validation loss: 0.5201
2024-06-03 01:12:17 [INFO]: Epoch 046 - training loss: 0.5327, validation loss: 0.5211
2024-06-03 01:12:29 [INFO]: Epoch 047 - training loss: 0.5262, validation loss: 0.5182
2024-06-03 01:12:41 [INFO]: Epoch 048 - training loss: 0.5249, validation loss: 0.5175
2024-06-03 01:12:53 [INFO]: Epoch 049 - training loss: 0.5275, validation loss: 0.5180
2024-06-03 01:13:06 [INFO]: Epoch 050 - training loss: 0.5233, validation loss: 0.5181
2024-06-03 01:13:18 [INFO]: Epoch 051 - training loss: 0.5253, validation loss: 0.5134
2024-06-03 01:13:30 [INFO]: Epoch 052 - training loss: 0.5176, validation loss: 0.5121
2024-06-03 01:13:41 [INFO]: Epoch 053 - training loss: 0.5183, validation loss: 0.5197
2024-06-03 01:13:53 [INFO]: Epoch 054 - training loss: 0.5150, validation loss: 0.5148
2024-06-03 01:14:05 [INFO]: Epoch 055 - training loss: 0.5169, validation loss: 0.5232
2024-06-03 01:14:17 [INFO]: Epoch 056 - training loss: 0.5142, validation loss: 0.5153
2024-06-03 01:14:29 [INFO]: Epoch 057 - training loss: 0.5105, validation loss: 0.5104
2024-06-03 01:14:41 [INFO]: Epoch 058 - training loss: 0.5131, validation loss: 0.5081
2024-06-03 01:14:53 [INFO]: Epoch 059 - training loss: 0.5138, validation loss: 0.5112
2024-06-03 01:15:05 [INFO]: Epoch 060 - training loss: 0.5068, validation loss: 0.5066
2024-06-03 01:15:18 [INFO]: Epoch 061 - training loss: 0.5114, validation loss: 0.5062
2024-06-03 01:15:30 [INFO]: Epoch 062 - training loss: 0.5071, validation loss: 0.5059
2024-06-03 01:15:42 [INFO]: Epoch 063 - training loss: 0.5070, validation loss: 0.5085
2024-06-03 01:15:53 [INFO]: Epoch 064 - training loss: 0.5087, validation loss: 0.4974
2024-06-03 01:16:05 [INFO]: Epoch 065 - training loss: 0.5031, validation loss: 0.5042
2024-06-03 01:16:17 [INFO]: Epoch 066 - training loss: 0.5051, validation loss: 0.5028
2024-06-03 01:16:28 [INFO]: Epoch 067 - training loss: 0.5065, validation loss: 0.5000
2024-06-03 01:16:41 [INFO]: Epoch 068 - training loss: 0.4976, validation loss: 0.5028
2024-06-03 01:16:53 [INFO]: Epoch 069 - training loss: 0.5013, validation loss: 0.4974
2024-06-03 01:17:05 [INFO]: Epoch 070 - training loss: 0.5004, validation loss: 0.5000
2024-06-03 01:17:17 [INFO]: Epoch 071 - training loss: 0.4989, validation loss: 0.5026
2024-06-03 01:17:29 [INFO]: Epoch 072 - training loss: 0.5007, validation loss: 0.4936
2024-06-03 01:17:41 [INFO]: Epoch 073 - training loss: 0.5022, validation loss: 0.4958
2024-06-03 01:17:53 [INFO]: Epoch 074 - training loss: 0.4950, validation loss: 0.4927
2024-06-03 01:18:05 [INFO]: Epoch 075 - training loss: 0.4972, validation loss: 0.4927
2024-06-03 01:18:18 [INFO]: Epoch 076 - training loss: 0.4893, validation loss: 0.4948
2024-06-03 01:18:30 [INFO]: Epoch 077 - training loss: 0.4957, validation loss: 0.4972
2024-06-03 01:18:42 [INFO]: Epoch 078 - training loss: 0.4909, validation loss: 0.4960
2024-06-03 01:18:54 [INFO]: Epoch 079 - training loss: 0.4917, validation loss: 0.4953
2024-06-03 01:19:06 [INFO]: Epoch 080 - training loss: 0.4932, validation loss: 0.4872
2024-06-03 01:19:18 [INFO]: Epoch 081 - training loss: 0.4918, validation loss: 0.4887
2024-06-03 01:19:31 [INFO]: Epoch 082 - training loss: 0.4904, validation loss: 0.4887
2024-06-03 01:19:43 [INFO]: Epoch 083 - training loss: 0.4904, validation loss: 0.4959
2024-06-03 01:19:55 [INFO]: Epoch 084 - training loss: 0.4910, validation loss: 0.4933
2024-06-03 01:20:08 [INFO]: Epoch 085 - training loss: 0.4925, validation loss: 0.4817
2024-06-03 01:20:19 [INFO]: Epoch 086 - training loss: 0.4858, validation loss: 0.4862
2024-06-03 01:20:32 [INFO]: Epoch 087 - training loss: 0.4872, validation loss: 0.4854
2024-06-03 01:20:44 [INFO]: Epoch 088 - training loss: 0.4821, validation loss: 0.4853
2024-06-03 01:20:56 [INFO]: Epoch 089 - training loss: 0.4858, validation loss: 0.4824
2024-06-03 01:21:08 [INFO]: Epoch 090 - training loss: 0.4836, validation loss: 0.4826
2024-06-03 01:21:20 [INFO]: Epoch 091 - training loss: 0.4859, validation loss: 0.4814
2024-06-03 01:21:32 [INFO]: Epoch 092 - training loss: 0.4821, validation loss: 0.4790
2024-06-03 01:21:44 [INFO]: Epoch 093 - training loss: 0.4799, validation loss: 0.4778
2024-06-03 01:21:56 [INFO]: Epoch 094 - training loss: 0.4808, validation loss: 0.4816
2024-06-03 01:22:08 [INFO]: Epoch 095 - training loss: 0.4821, validation loss: 0.4764
2024-06-03 01:22:20 [INFO]: Epoch 096 - training loss: 0.4791, validation loss: 0.4782
2024-06-03 01:22:32 [INFO]: Epoch 097 - training loss: 0.4787, validation loss: 0.4787
2024-06-03 01:22:44 [INFO]: Epoch 098 - training loss: 0.4789, validation loss: 0.4815
2024-06-03 01:22:55 [INFO]: Epoch 099 - training loss: 0.4779, validation loss: 0.4798
2024-06-03 01:23:07 [INFO]: Epoch 100 - training loss: 0.4806, validation loss: 0.4741
2024-06-03 01:23:07 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 01:23:08 [INFO]: Saved the model to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_1/20240603_T010306/ETSformer.pypots
2024-06-03 01:23:17 [INFO]: Successfully saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_1/imputation.pkl
2024-06-03 01:23:17 [INFO]: Round1 - ETSformer on BeijingAir: MAE=0.3648, MSE=0.4807, MRE=0.4912
2024-06-03 01:23:17 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 01:23:17 [INFO]: Using the given device: cuda:0
2024-06-03 01:23:17 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_2/20240603_T012317
2024-06-03 01:23:17 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_2/20240603_T012317/tensorboard
2024-06-03 01:23:17 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 7,928,510
2024-06-03 01:23:30 [INFO]: Epoch 001 - training loss: 1.3022, validation loss: 1.0172
2024-06-03 01:23:42 [INFO]: Epoch 002 - training loss: 1.1999, validation loss: 1.0041
2024-06-03 01:23:54 [INFO]: Epoch 003 - training loss: 1.1129, validation loss: 0.9845
2024-06-03 01:24:06 [INFO]: Epoch 004 - training loss: 1.0195, validation loss: 0.9253
2024-06-03 01:24:18 [INFO]: Epoch 005 - training loss: 0.9016, validation loss: 0.8330
2024-06-03 01:24:30 [INFO]: Epoch 006 - training loss: 0.8184, validation loss: 0.7358
2024-06-03 01:24:42 [INFO]: Epoch 007 - training loss: 0.7662, validation loss: 0.6937
2024-06-03 01:24:55 [INFO]: Epoch 008 - training loss: 0.7200, validation loss: 0.6548
2024-06-03 01:25:07 [INFO]: Epoch 009 - training loss: 0.7017, validation loss: 0.6342
2024-06-03 01:25:19 [INFO]: Epoch 010 - training loss: 0.6824, validation loss: 0.6216
2024-06-03 01:25:31 [INFO]: Epoch 011 - training loss: 0.6676, validation loss: 0.6115
2024-06-03 01:25:43 [INFO]: Epoch 012 - training loss: 0.6554, validation loss: 0.6081
2024-06-03 01:25:55 [INFO]: Epoch 013 - training loss: 0.6499, validation loss: 0.6098
2024-06-03 01:26:07 [INFO]: Epoch 014 - training loss: 0.6385, validation loss: 0.6035
2024-06-03 01:26:19 [INFO]: Epoch 015 - training loss: 0.6341, validation loss: 0.5963
2024-06-03 01:26:31 [INFO]: Epoch 016 - training loss: 0.6258, validation loss: 0.5923
2024-06-03 01:26:43 [INFO]: Epoch 017 - training loss: 0.6150, validation loss: 0.5850
2024-06-03 01:26:55 [INFO]: Epoch 018 - training loss: 0.6112, validation loss: 0.5874
2024-06-03 01:27:07 [INFO]: Epoch 019 - training loss: 0.6074, validation loss: 0.5829
2024-06-03 01:27:19 [INFO]: Epoch 020 - training loss: 0.5974, validation loss: 0.5735
2024-06-03 01:27:31 [INFO]: Epoch 021 - training loss: 0.5922, validation loss: 0.5768
2024-06-03 01:27:43 [INFO]: Epoch 022 - training loss: 0.5931, validation loss: 0.5762
2024-06-03 01:27:55 [INFO]: Epoch 023 - training loss: 0.5884, validation loss: 0.5757
2024-06-03 01:28:07 [INFO]: Epoch 024 - training loss: 0.5753, validation loss: 0.5644
2024-06-03 01:28:20 [INFO]: Epoch 025 - training loss: 0.5779, validation loss: 0.5659
2024-06-03 01:28:32 [INFO]: Epoch 026 - training loss: 0.5754, validation loss: 0.5636
2024-06-03 01:28:44 [INFO]: Epoch 027 - training loss: 0.5728, validation loss: 0.5651
2024-06-03 01:28:56 [INFO]: Epoch 028 - training loss: 0.5681, validation loss: 0.5595
2024-06-03 01:29:08 [INFO]: Epoch 029 - training loss: 0.5638, validation loss: 0.5597
2024-06-03 01:29:20 [INFO]: Epoch 030 - training loss: 0.5547, validation loss: 0.5518
2024-06-03 01:29:32 [INFO]: Epoch 031 - training loss: 0.5620, validation loss: 0.5598
2024-06-03 01:29:44 [INFO]: Epoch 032 - training loss: 0.5565, validation loss: 0.5569
2024-06-03 01:29:57 [INFO]: Epoch 033 - training loss: 0.5513, validation loss: 0.5533
2024-06-03 01:30:09 [INFO]: Epoch 034 - training loss: 0.5530, validation loss: 0.5468
2024-06-03 01:30:20 [INFO]: Epoch 035 - training loss: 0.5506, validation loss: 0.5509
2024-06-03 01:30:32 [INFO]: Epoch 036 - training loss: 0.5464, validation loss: 0.5449
2024-06-03 01:30:44 [INFO]: Epoch 037 - training loss: 0.5454, validation loss: 0.5422
2024-06-03 01:30:56 [INFO]: Epoch 038 - training loss: 0.5464, validation loss: 0.5439
2024-06-03 01:31:08 [INFO]: Epoch 039 - training loss: 0.5387, validation loss: 0.5389
2024-06-03 01:31:20 [INFO]: Epoch 040 - training loss: 0.5436, validation loss: 0.5361
2024-06-03 01:31:30 [INFO]: Epoch 041 - training loss: 0.5371, validation loss: 0.5395
2024-06-03 01:31:40 [INFO]: Epoch 042 - training loss: 0.5356, validation loss: 0.5375
2024-06-03 01:31:49 [INFO]: Epoch 043 - training loss: 0.5340, validation loss: 0.5374
2024-06-03 01:31:59 [INFO]: Epoch 044 - training loss: 0.5345, validation loss: 0.5382
2024-06-03 01:32:09 [INFO]: Epoch 045 - training loss: 0.5332, validation loss: 0.5357
2024-06-03 01:32:19 [INFO]: Epoch 046 - training loss: 0.5293, validation loss: 0.5316
2024-06-03 01:32:29 [INFO]: Epoch 047 - training loss: 0.5240, validation loss: 0.5378
2024-06-03 01:32:39 [INFO]: Epoch 048 - training loss: 0.5264, validation loss: 0.5287
2024-06-03 01:32:49 [INFO]: Epoch 049 - training loss: 0.5263, validation loss: 0.5356
2024-06-03 01:32:59 [INFO]: Epoch 050 - training loss: 0.5274, validation loss: 0.5304
2024-06-03 01:33:09 [INFO]: Epoch 051 - training loss: 0.5215, validation loss: 0.5310
2024-06-03 01:33:19 [INFO]: Epoch 052 - training loss: 0.5247, validation loss: 0.5305
2024-06-03 01:33:29 [INFO]: Epoch 053 - training loss: 0.5202, validation loss: 0.5285
2024-06-03 01:33:39 [INFO]: Epoch 054 - training loss: 0.5175, validation loss: 0.5280
2024-06-03 01:33:49 [INFO]: Epoch 055 - training loss: 0.5206, validation loss: 0.5239
2024-06-03 01:33:59 [INFO]: Epoch 056 - training loss: 0.5115, validation loss: 0.5284
2024-06-03 01:34:08 [INFO]: Epoch 057 - training loss: 0.5139, validation loss: 0.5252
2024-06-03 01:34:18 [INFO]: Epoch 058 - training loss: 0.5108, validation loss: 0.5180
2024-06-03 01:34:28 [INFO]: Epoch 059 - training loss: 0.5121, validation loss: 0.5287
2024-06-03 01:34:38 [INFO]: Epoch 060 - training loss: 0.5126, validation loss: 0.5229
2024-06-03 01:34:48 [INFO]: Epoch 061 - training loss: 0.5105, validation loss: 0.5207
2024-06-03 01:34:58 [INFO]: Epoch 062 - training loss: 0.5119, validation loss: 0.5182
2024-06-03 01:35:08 [INFO]: Epoch 063 - training loss: 0.5076, validation loss: 0.5147
2024-06-03 01:35:18 [INFO]: Epoch 064 - training loss: 0.5048, validation loss: 0.5187
2024-06-03 01:35:28 [INFO]: Epoch 065 - training loss: 0.5094, validation loss: 0.5164
2024-06-03 01:35:39 [INFO]: Epoch 066 - training loss: 0.5052, validation loss: 0.5190
2024-06-03 01:35:48 [INFO]: Epoch 067 - training loss: 0.5055, validation loss: 0.5190
2024-06-03 01:35:59 [INFO]: Epoch 068 - training loss: 0.5049, validation loss: 0.5185
2024-06-03 01:36:09 [INFO]: Epoch 069 - training loss: 0.5017, validation loss: 0.5148
2024-06-03 01:36:19 [INFO]: Epoch 070 - training loss: 0.5016, validation loss: 0.5167
2024-06-03 01:36:29 [INFO]: Epoch 071 - training loss: 0.5028, validation loss: 0.5119
2024-06-03 01:36:39 [INFO]: Epoch 072 - training loss: 0.5006, validation loss: 0.5119
2024-06-03 01:36:49 [INFO]: Epoch 073 - training loss: 0.4989, validation loss: 0.5072
2024-06-03 01:36:59 [INFO]: Epoch 074 - training loss: 0.5016, validation loss: 0.5081
2024-06-03 01:37:09 [INFO]: Epoch 075 - training loss: 0.4981, validation loss: 0.5080
2024-06-03 01:37:19 [INFO]: Epoch 076 - training loss: 0.4947, validation loss: 0.5028
2024-06-03 01:37:29 [INFO]: Epoch 077 - training loss: 0.4958, validation loss: 0.5031
2024-06-03 01:37:39 [INFO]: Epoch 078 - training loss: 0.4927, validation loss: 0.5047
2024-06-03 01:37:49 [INFO]: Epoch 079 - training loss: 0.4950, validation loss: 0.5061
2024-06-03 01:37:59 [INFO]: Epoch 080 - training loss: 0.4916, validation loss: 0.5016
2024-06-03 01:38:09 [INFO]: Epoch 081 - training loss: 0.4912, validation loss: 0.4984
2024-06-03 01:38:19 [INFO]: Epoch 082 - training loss: 0.4953, validation loss: 0.5005
2024-06-03 01:38:29 [INFO]: Epoch 083 - training loss: 0.4912, validation loss: 0.4986
2024-06-03 01:38:39 [INFO]: Epoch 084 - training loss: 0.4943, validation loss: 0.5000
2024-06-03 01:38:49 [INFO]: Epoch 085 - training loss: 0.4907, validation loss: 0.5008
2024-06-03 01:38:59 [INFO]: Epoch 086 - training loss: 0.4895, validation loss: 0.4999
2024-06-03 01:39:09 [INFO]: Epoch 087 - training loss: 0.4902, validation loss: 0.4949
2024-06-03 01:39:18 [INFO]: Epoch 088 - training loss: 0.4837, validation loss: 0.4972
2024-06-03 01:39:29 [INFO]: Epoch 089 - training loss: 0.4833, validation loss: 0.4965
2024-06-03 01:39:39 [INFO]: Epoch 090 - training loss: 0.4885, validation loss: 0.5011
2024-06-03 01:39:49 [INFO]: Epoch 091 - training loss: 0.4888, validation loss: 0.4984
2024-06-03 01:39:59 [INFO]: Epoch 092 - training loss: 0.4855, validation loss: 0.4974
2024-06-03 01:40:09 [INFO]: Epoch 093 - training loss: 0.4893, validation loss: 0.4982
2024-06-03 01:40:18 [INFO]: Epoch 094 - training loss: 0.4855, validation loss: 0.4959
2024-06-03 01:40:29 [INFO]: Epoch 095 - training loss: 0.4818, validation loss: 0.4927
2024-06-03 01:40:39 [INFO]: Epoch 096 - training loss: 0.4811, validation loss: 0.4910
2024-06-03 01:40:49 [INFO]: Epoch 097 - training loss: 0.4832, validation loss: 0.4929
2024-06-03 01:40:58 [INFO]: Epoch 098 - training loss: 0.4799, validation loss: 0.4900
2024-06-03 01:41:08 [INFO]: Epoch 099 - training loss: 0.4822, validation loss: 0.4884
2024-06-03 01:41:18 [INFO]: Epoch 100 - training loss: 0.4798, validation loss: 0.4895
2024-06-03 01:41:18 [INFO]: Finished training. The best model is from epoch#99.
2024-06-03 01:41:18 [INFO]: Saved the model to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_2/20240603_T012317/ETSformer.pypots
2024-06-03 01:41:26 [INFO]: Successfully saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_2/imputation.pkl
2024-06-03 01:41:26 [INFO]: Round2 - ETSformer on BeijingAir: MAE=0.3731, MSE=0.4978, MRE=0.5024
2024-06-03 01:41:26 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 01:41:26 [INFO]: Using the given device: cuda:0
2024-06-03 01:41:26 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_3/20240603_T014126
2024-06-03 01:41:26 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_3/20240603_T014126/tensorboard
2024-06-03 01:41:26 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 7,928,510
2024-06-03 01:41:36 [INFO]: Epoch 001 - training loss: 1.3243, validation loss: 1.0161
2024-06-03 01:41:46 [INFO]: Epoch 002 - training loss: 1.2077, validation loss: 1.0085
2024-06-03 01:41:57 [INFO]: Epoch 003 - training loss: 1.1239, validation loss: 0.9797
2024-06-03 01:42:07 [INFO]: Epoch 004 - training loss: 1.0371, validation loss: 1.0175
2024-06-03 01:42:17 [INFO]: Epoch 005 - training loss: 0.9456, validation loss: 0.9279
2024-06-03 01:42:27 [INFO]: Epoch 006 - training loss: 0.8665, validation loss: 0.8334
2024-06-03 01:42:37 [INFO]: Epoch 007 - training loss: 0.8054, validation loss: 0.7830
2024-06-03 01:42:46 [INFO]: Epoch 008 - training loss: 0.7577, validation loss: 0.6974
2024-06-03 01:42:56 [INFO]: Epoch 009 - training loss: 0.7163, validation loss: 0.6656
2024-06-03 01:43:05 [INFO]: Epoch 010 - training loss: 0.6979, validation loss: 0.6401
2024-06-03 01:43:15 [INFO]: Epoch 011 - training loss: 0.6796, validation loss: 0.6370
2024-06-03 01:43:25 [INFO]: Epoch 012 - training loss: 0.6679, validation loss: 0.6178
2024-06-03 01:43:35 [INFO]: Epoch 013 - training loss: 0.6500, validation loss: 0.6070
2024-06-03 01:43:45 [INFO]: Epoch 014 - training loss: 0.6437, validation loss: 0.6003
2024-06-03 01:43:55 [INFO]: Epoch 015 - training loss: 0.6372, validation loss: 0.6081
2024-06-03 01:44:05 [INFO]: Epoch 016 - training loss: 0.6256, validation loss: 0.5895
2024-06-03 01:44:15 [INFO]: Epoch 017 - training loss: 0.6224, validation loss: 0.6022
2024-06-03 01:44:25 [INFO]: Epoch 018 - training loss: 0.6147, validation loss: 0.5881
2024-06-03 01:44:35 [INFO]: Epoch 019 - training loss: 0.6052, validation loss: 0.5854
2024-06-03 01:44:46 [INFO]: Epoch 020 - training loss: 0.6016, validation loss: 0.5820
2024-06-03 01:44:56 [INFO]: Epoch 021 - training loss: 0.5926, validation loss: 0.5802
2024-06-03 01:45:06 [INFO]: Epoch 022 - training loss: 0.5891, validation loss: 0.5746
2024-06-03 01:45:16 [INFO]: Epoch 023 - training loss: 0.5794, validation loss: 0.5760
2024-06-03 01:45:26 [INFO]: Epoch 024 - training loss: 0.5820, validation loss: 0.5696
2024-06-03 01:45:36 [INFO]: Epoch 025 - training loss: 0.5798, validation loss: 0.5656
2024-06-03 01:45:45 [INFO]: Epoch 026 - training loss: 0.5755, validation loss: 0.5652
2024-06-03 01:45:55 [INFO]: Epoch 027 - training loss: 0.5680, validation loss: 0.5608
2024-06-03 01:46:06 [INFO]: Epoch 028 - training loss: 0.5648, validation loss: 0.5572
2024-06-03 01:46:15 [INFO]: Epoch 029 - training loss: 0.5659, validation loss: 0.5568
2024-06-03 01:46:26 [INFO]: Epoch 030 - training loss: 0.5617, validation loss: 0.5598
2024-06-03 01:46:36 [INFO]: Epoch 031 - training loss: 0.5578, validation loss: 0.5582
2024-06-03 01:46:46 [INFO]: Epoch 032 - training loss: 0.5531, validation loss: 0.5573
2024-06-03 01:46:56 [INFO]: Epoch 033 - training loss: 0.5537, validation loss: 0.5606
2024-06-03 01:47:05 [INFO]: Epoch 034 - training loss: 0.5492, validation loss: 0.5599
2024-06-03 01:47:15 [INFO]: Epoch 035 - training loss: 0.5461, validation loss: 0.5561
2024-06-03 01:47:26 [INFO]: Epoch 036 - training loss: 0.5471, validation loss: 0.5542
2024-06-03 01:47:36 [INFO]: Epoch 037 - training loss: 0.5488, validation loss: 0.5499
2024-06-03 01:47:46 [INFO]: Epoch 038 - training loss: 0.5359, validation loss: 0.5541
2024-06-03 01:47:56 [INFO]: Epoch 039 - training loss: 0.5415, validation loss: 0.5564
2024-06-03 01:48:06 [INFO]: Epoch 040 - training loss: 0.5409, validation loss: 0.5557
2024-06-03 01:48:16 [INFO]: Epoch 041 - training loss: 0.5361, validation loss: 0.5542
2024-06-03 01:48:26 [INFO]: Epoch 042 - training loss: 0.5327, validation loss: 0.5450
2024-06-03 01:48:36 [INFO]: Epoch 043 - training loss: 0.5338, validation loss: 0.5441
2024-06-03 01:48:46 [INFO]: Epoch 044 - training loss: 0.5298, validation loss: 0.5485
2024-06-03 01:48:56 [INFO]: Epoch 045 - training loss: 0.5281, validation loss: 0.5426
2024-06-03 01:49:06 [INFO]: Epoch 046 - training loss: 0.5303, validation loss: 0.5418
2024-06-03 01:49:16 [INFO]: Epoch 047 - training loss: 0.5286, validation loss: 0.5414
2024-06-03 01:49:26 [INFO]: Epoch 048 - training loss: 0.5285, validation loss: 0.5475
2024-06-03 01:49:36 [INFO]: Epoch 049 - training loss: 0.5278, validation loss: 0.5406
2024-06-03 01:49:46 [INFO]: Epoch 050 - training loss: 0.5241, validation loss: 0.5413
2024-06-03 01:49:56 [INFO]: Epoch 051 - training loss: 0.5229, validation loss: 0.5416
2024-06-03 01:50:06 [INFO]: Epoch 052 - training loss: 0.5155, validation loss: 0.5386
2024-06-03 01:50:16 [INFO]: Epoch 053 - training loss: 0.5166, validation loss: 0.5407
2024-06-03 01:50:26 [INFO]: Epoch 054 - training loss: 0.5209, validation loss: 0.5403
2024-06-03 01:50:36 [INFO]: Epoch 055 - training loss: 0.5182, validation loss: 0.5360
2024-06-03 01:50:46 [INFO]: Epoch 056 - training loss: 0.5172, validation loss: 0.5360
2024-06-03 01:50:56 [INFO]: Epoch 057 - training loss: 0.5141, validation loss: 0.5290
2024-06-03 01:51:06 [INFO]: Epoch 058 - training loss: 0.5167, validation loss: 0.5362
2024-06-03 01:51:15 [INFO]: Epoch 059 - training loss: 0.5164, validation loss: 0.5322
2024-06-03 01:51:26 [INFO]: Epoch 060 - training loss: 0.5098, validation loss: 0.5327
2024-06-03 01:51:36 [INFO]: Epoch 061 - training loss: 0.5100, validation loss: 0.5335
2024-06-03 01:51:46 [INFO]: Epoch 062 - training loss: 0.5088, validation loss: 0.5318
2024-06-03 01:51:55 [INFO]: Epoch 063 - training loss: 0.5103, validation loss: 0.5290
2024-06-03 01:52:05 [INFO]: Epoch 064 - training loss: 0.5067, validation loss: 0.5349
2024-06-03 01:52:15 [INFO]: Epoch 065 - training loss: 0.5052, validation loss: 0.5281
2024-06-03 01:52:25 [INFO]: Epoch 066 - training loss: 0.5052, validation loss: 0.5269
2024-06-03 01:52:35 [INFO]: Epoch 067 - training loss: 0.5029, validation loss: 0.5315
2024-06-03 01:52:44 [INFO]: Epoch 068 - training loss: 0.5075, validation loss: 0.5246
2024-06-03 01:52:54 [INFO]: Epoch 069 - training loss: 0.5037, validation loss: 0.5303
2024-06-03 01:53:04 [INFO]: Epoch 070 - training loss: 0.5063, validation loss: 0.5258
2024-06-03 01:53:14 [INFO]: Epoch 071 - training loss: 0.5000, validation loss: 0.5309
2024-06-03 01:53:24 [INFO]: Epoch 072 - training loss: 0.4994, validation loss: 0.5267
2024-06-03 01:53:33 [INFO]: Epoch 073 - training loss: 0.4982, validation loss: 0.5269
2024-06-03 01:53:43 [INFO]: Epoch 074 - training loss: 0.4984, validation loss: 0.5250
2024-06-03 01:53:53 [INFO]: Epoch 075 - training loss: 0.5019, validation loss: 0.5266
2024-06-03 01:54:02 [INFO]: Epoch 076 - training loss: 0.4991, validation loss: 0.5205
2024-06-03 01:54:12 [INFO]: Epoch 077 - training loss: 0.4928, validation loss: 0.5221
2024-06-03 01:54:22 [INFO]: Epoch 078 - training loss: 0.4989, validation loss: 0.5233
2024-06-03 01:54:32 [INFO]: Epoch 079 - training loss: 0.4965, validation loss: 0.5250
2024-06-03 01:54:41 [INFO]: Epoch 080 - training loss: 0.4970, validation loss: 0.5264
2024-06-03 01:54:51 [INFO]: Epoch 081 - training loss: 0.4943, validation loss: 0.5237
2024-06-03 01:55:00 [INFO]: Epoch 082 - training loss: 0.4920, validation loss: 0.5226
2024-06-03 01:55:10 [INFO]: Epoch 083 - training loss: 0.4922, validation loss: 0.5244
2024-06-03 01:55:20 [INFO]: Epoch 084 - training loss: 0.4920, validation loss: 0.5237
2024-06-03 01:55:30 [INFO]: Epoch 085 - training loss: 0.4870, validation loss: 0.5210
2024-06-03 01:55:40 [INFO]: Epoch 086 - training loss: 0.4939, validation loss: 0.5185
2024-06-03 01:55:49 [INFO]: Epoch 087 - training loss: 0.4886, validation loss: 0.5171
2024-06-03 01:55:59 [INFO]: Epoch 088 - training loss: 0.4862, validation loss: 0.5167
2024-06-03 01:56:09 [INFO]: Epoch 089 - training loss: 0.4894, validation loss: 0.5228
2024-06-03 01:56:19 [INFO]: Epoch 090 - training loss: 0.4911, validation loss: 0.5108
2024-06-03 01:56:29 [INFO]: Epoch 091 - training loss: 0.4882, validation loss: 0.5146
2024-06-03 01:56:38 [INFO]: Epoch 092 - training loss: 0.4908, validation loss: 0.5163
2024-06-03 01:56:47 [INFO]: Epoch 093 - training loss: 0.4865, validation loss: 0.5124
2024-06-03 01:56:57 [INFO]: Epoch 094 - training loss: 0.4909, validation loss: 0.5110
2024-06-03 01:57:07 [INFO]: Epoch 095 - training loss: 0.4819, validation loss: 0.5126
2024-06-03 01:57:16 [INFO]: Epoch 096 - training loss: 0.4829, validation loss: 0.5121
2024-06-03 01:57:26 [INFO]: Epoch 097 - training loss: 0.4847, validation loss: 0.5134
2024-06-03 01:57:36 [INFO]: Epoch 098 - training loss: 0.4864, validation loss: 0.5063
2024-06-03 01:57:46 [INFO]: Epoch 099 - training loss: 0.4829, validation loss: 0.5084
2024-06-03 01:57:55 [INFO]: Epoch 100 - training loss: 0.4839, validation loss: 0.5109
2024-06-03 01:57:55 [INFO]: Finished training. The best model is from epoch#98.
2024-06-03 01:57:56 [INFO]: Saved the model to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_3/20240603_T014126/ETSformer.pypots
2024-06-03 01:58:03 [INFO]: Successfully saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_3/imputation.pkl
2024-06-03 01:58:03 [INFO]: Round3 - ETSformer on BeijingAir: MAE=0.3877, MSE=0.5193, MRE=0.5221
2024-06-03 01:58:03 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 01:58:03 [INFO]: Using the given device: cuda:0
2024-06-03 01:58:03 [INFO]: Model files will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_4/20240603_T015803
2024-06-03 01:58:03 [INFO]: Tensorboard file will be saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_4/20240603_T015803/tensorboard
2024-06-03 01:58:03 [INFO]: ETSformer initialized with the given hyperparameters, the number of trainable parameters: 7,928,510
2024-06-03 01:58:13 [INFO]: Epoch 001 - training loss: 1.3150, validation loss: 1.0131
2024-06-03 01:58:23 [INFO]: Epoch 002 - training loss: 1.2065, validation loss: 0.9987
2024-06-03 01:58:33 [INFO]: Epoch 003 - training loss: 1.1209, validation loss: 0.9862
2024-06-03 01:58:42 [INFO]: Epoch 004 - training loss: 1.0238, validation loss: 0.9522
2024-06-03 01:58:52 [INFO]: Epoch 005 - training loss: 0.9188, validation loss: 0.8428
2024-06-03 01:59:02 [INFO]: Epoch 006 - training loss: 0.8265, validation loss: 0.7542
2024-06-03 01:59:11 [INFO]: Epoch 007 - training loss: 0.7585, validation loss: 0.6818
2024-06-03 01:59:21 [INFO]: Epoch 008 - training loss: 0.7263, validation loss: 0.6544
2024-06-03 01:59:31 [INFO]: Epoch 009 - training loss: 0.7086, validation loss: 0.6404
2024-06-03 01:59:40 [INFO]: Epoch 010 - training loss: 0.6813, validation loss: 0.6212
2024-06-03 01:59:50 [INFO]: Epoch 011 - training loss: 0.6653, validation loss: 0.6093
2024-06-03 01:59:59 [INFO]: Epoch 012 - training loss: 0.6624, validation loss: 0.5956
2024-06-03 02:00:09 [INFO]: Epoch 013 - training loss: 0.6481, validation loss: 0.5987
2024-06-03 02:00:19 [INFO]: Epoch 014 - training loss: 0.6393, validation loss: 0.6013
2024-06-03 02:00:28 [INFO]: Epoch 015 - training loss: 0.6279, validation loss: 0.5881
2024-06-03 02:00:38 [INFO]: Epoch 016 - training loss: 0.6216, validation loss: 0.5966
2024-06-03 02:00:48 [INFO]: Epoch 017 - training loss: 0.6117, validation loss: 0.5884
2024-06-03 02:00:57 [INFO]: Epoch 018 - training loss: 0.6041, validation loss: 0.5848
2024-06-03 02:01:07 [INFO]: Epoch 019 - training loss: 0.6013, validation loss: 0.5760
2024-06-03 02:01:17 [INFO]: Epoch 020 - training loss: 0.5905, validation loss: 0.5694
2024-06-03 02:01:27 [INFO]: Epoch 021 - training loss: 0.5890, validation loss: 0.5709
2024-06-03 02:01:37 [INFO]: Epoch 022 - training loss: 0.5877, validation loss: 0.5730
2024-06-03 02:01:47 [INFO]: Epoch 023 - training loss: 0.5763, validation loss: 0.5656
2024-06-03 02:01:56 [INFO]: Epoch 024 - training loss: 0.5774, validation loss: 0.5679
2024-06-03 02:02:06 [INFO]: Epoch 025 - training loss: 0.5705, validation loss: 0.5581
2024-06-03 02:02:16 [INFO]: Epoch 026 - training loss: 0.5670, validation loss: 0.5606
2024-06-03 02:02:26 [INFO]: Epoch 027 - training loss: 0.5671, validation loss: 0.5642
2024-06-03 02:02:36 [INFO]: Epoch 028 - training loss: 0.5645, validation loss: 0.5566
2024-06-03 02:02:45 [INFO]: Epoch 029 - training loss: 0.5639, validation loss: 0.5601
2024-06-03 02:02:55 [INFO]: Epoch 030 - training loss: 0.5538, validation loss: 0.5589
2024-06-03 02:03:05 [INFO]: Epoch 031 - training loss: 0.5536, validation loss: 0.5530
2024-06-03 02:03:15 [INFO]: Epoch 032 - training loss: 0.5515, validation loss: 0.5592
2024-06-03 02:03:25 [INFO]: Epoch 033 - training loss: 0.5478, validation loss: 0.5541
2024-06-03 02:03:35 [INFO]: Epoch 034 - training loss: 0.5415, validation loss: 0.5527
2024-06-03 02:03:45 [INFO]: Epoch 035 - training loss: 0.5446, validation loss: 0.5505
2024-06-03 02:03:54 [INFO]: Epoch 036 - training loss: 0.5416, validation loss: 0.5500
2024-06-03 02:04:04 [INFO]: Epoch 037 - training loss: 0.5409, validation loss: 0.5462
2024-06-03 02:04:14 [INFO]: Epoch 038 - training loss: 0.5408, validation loss: 0.5475
2024-06-03 02:04:24 [INFO]: Epoch 039 - training loss: 0.5406, validation loss: 0.5515
2024-06-03 02:04:33 [INFO]: Epoch 040 - training loss: 0.5345, validation loss: 0.5519
2024-06-03 02:04:43 [INFO]: Epoch 041 - training loss: 0.5382, validation loss: 0.5498
2024-06-03 02:04:52 [INFO]: Epoch 042 - training loss: 0.5372, validation loss: 0.5417
2024-06-03 02:05:02 [INFO]: Epoch 043 - training loss: 0.5312, validation loss: 0.5435
2024-06-03 02:05:12 [INFO]: Epoch 044 - training loss: 0.5293, validation loss: 0.5437
2024-06-03 02:05:21 [INFO]: Epoch 045 - training loss: 0.5295, validation loss: 0.5358
2024-06-03 02:05:31 [INFO]: Epoch 046 - training loss: 0.5236, validation loss: 0.5429
2024-06-03 02:05:41 [INFO]: Epoch 047 - training loss: 0.5260, validation loss: 0.5420
2024-06-03 02:05:51 [INFO]: Epoch 048 - training loss: 0.5191, validation loss: 0.5393
2024-06-03 02:06:01 [INFO]: Epoch 049 - training loss: 0.5211, validation loss: 0.5440
2024-06-03 02:06:10 [INFO]: Epoch 050 - training loss: 0.5197, validation loss: 0.5377
2024-06-03 02:06:20 [INFO]: Epoch 051 - training loss: 0.5177, validation loss: 0.5358
2024-06-03 02:06:30 [INFO]: Epoch 052 - training loss: 0.5184, validation loss: 0.5346
2024-06-03 02:06:40 [INFO]: Epoch 053 - training loss: 0.5162, validation loss: 0.5337
2024-06-03 02:06:49 [INFO]: Epoch 054 - training loss: 0.5179, validation loss: 0.5384
2024-06-03 02:06:59 [INFO]: Epoch 055 - training loss: 0.5164, validation loss: 0.5376
2024-06-03 02:07:09 [INFO]: Epoch 056 - training loss: 0.5112, validation loss: 0.5346
2024-06-03 02:07:18 [INFO]: Epoch 057 - training loss: 0.5107, validation loss: 0.5377
2024-06-03 02:07:28 [INFO]: Epoch 058 - training loss: 0.5120, validation loss: 0.5303
2024-06-03 02:07:38 [INFO]: Epoch 059 - training loss: 0.5070, validation loss: 0.5260
2024-06-03 02:07:47 [INFO]: Epoch 060 - training loss: 0.5107, validation loss: 0.5251
2024-06-03 02:07:57 [INFO]: Epoch 061 - training loss: 0.5068, validation loss: 0.5256
2024-06-03 02:08:07 [INFO]: Epoch 062 - training loss: 0.5077, validation loss: 0.5236
2024-06-03 02:08:17 [INFO]: Epoch 063 - training loss: 0.5103, validation loss: 0.5359
2024-06-03 02:08:26 [INFO]: Epoch 064 - training loss: 0.5090, validation loss: 0.5239
2024-06-03 02:08:36 [INFO]: Epoch 065 - training loss: 0.5056, validation loss: 0.5313
2024-06-03 02:08:45 [INFO]: Epoch 066 - training loss: 0.5045, validation loss: 0.5265
2024-06-03 02:08:55 [INFO]: Epoch 067 - training loss: 0.5005, validation loss: 0.5234
2024-06-03 02:09:05 [INFO]: Epoch 068 - training loss: 0.5026, validation loss: 0.5216
2024-06-03 02:09:15 [INFO]: Epoch 069 - training loss: 0.5010, validation loss: 0.5282
2024-06-03 02:09:25 [INFO]: Epoch 070 - training loss: 0.5057, validation loss: 0.5238
2024-06-03 02:09:35 [INFO]: Epoch 071 - training loss: 0.5000, validation loss: 0.5214
2024-06-03 02:09:44 [INFO]: Epoch 072 - training loss: 0.4992, validation loss: 0.5268
2024-06-03 02:09:54 [INFO]: Epoch 073 - training loss: 0.4997, validation loss: 0.5234
2024-06-03 02:10:04 [INFO]: Epoch 074 - training loss: 0.4986, validation loss: 0.5155
2024-06-03 02:10:14 [INFO]: Epoch 075 - training loss: 0.5021, validation loss: 0.5206
2024-06-03 02:10:24 [INFO]: Epoch 076 - training loss: 0.4979, validation loss: 0.5158
2024-06-03 02:10:33 [INFO]: Epoch 077 - training loss: 0.4936, validation loss: 0.5161
2024-06-03 02:10:43 [INFO]: Epoch 078 - training loss: 0.4961, validation loss: 0.5172
2024-06-03 02:10:53 [INFO]: Epoch 079 - training loss: 0.4942, validation loss: 0.5137
2024-06-03 02:11:02 [INFO]: Epoch 080 - training loss: 0.4926, validation loss: 0.5168
2024-06-03 02:11:12 [INFO]: Epoch 081 - training loss: 0.4917, validation loss: 0.5210
2024-06-03 02:11:22 [INFO]: Epoch 082 - training loss: 0.4930, validation loss: 0.5167
2024-06-03 02:11:31 [INFO]: Epoch 083 - training loss: 0.4934, validation loss: 0.5148
2024-06-03 02:11:41 [INFO]: Epoch 084 - training loss: 0.4890, validation loss: 0.5150
2024-06-03 02:11:51 [INFO]: Epoch 085 - training loss: 0.4887, validation loss: 0.5132
2024-06-03 02:12:01 [INFO]: Epoch 086 - training loss: 0.4844, validation loss: 0.5057
2024-06-03 02:12:11 [INFO]: Epoch 087 - training loss: 0.4929, validation loss: 0.5151
2024-06-03 02:12:21 [INFO]: Epoch 088 - training loss: 0.4895, validation loss: 0.5132
2024-06-03 02:12:30 [INFO]: Epoch 089 - training loss: 0.4903, validation loss: 0.5082
2024-06-03 02:12:40 [INFO]: Epoch 090 - training loss: 0.4858, validation loss: 0.5046
2024-06-03 02:12:50 [INFO]: Epoch 091 - training loss: 0.4841, validation loss: 0.5047
2024-06-03 02:12:59 [INFO]: Epoch 092 - training loss: 0.4849, validation loss: 0.5077
2024-06-03 02:13:09 [INFO]: Epoch 093 - training loss: 0.4867, validation loss: 0.5077
2024-06-03 02:13:19 [INFO]: Epoch 094 - training loss: 0.4853, validation loss: 0.5034
2024-06-03 02:13:29 [INFO]: Epoch 095 - training loss: 0.4877, validation loss: 0.5022
2024-06-03 02:13:39 [INFO]: Epoch 096 - training loss: 0.4876, validation loss: 0.5020
2024-06-03 02:13:49 [INFO]: Epoch 097 - training loss: 0.4840, validation loss: 0.5054
2024-06-03 02:13:59 [INFO]: Epoch 098 - training loss: 0.4837, validation loss: 0.5028
2024-06-03 02:14:09 [INFO]: Epoch 099 - training loss: 0.4846, validation loss: 0.5021
2024-06-03 02:14:19 [INFO]: Epoch 100 - training loss: 0.4869, validation loss: 0.4997
2024-06-03 02:14:19 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 02:14:19 [INFO]: Saved the model to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_4/20240603_T015803/ETSformer.pypots
2024-06-03 02:14:26 [INFO]: Successfully saved to results_point_rate09/BeijingAir/ETSformer_BeijingAir/round_4/imputation.pkl
2024-06-03 02:14:26 [INFO]: Round4 - ETSformer on BeijingAir: MAE=0.3801, MSE=0.5048, MRE=0.5118
2024-06-03 02:14:26 [INFO]: Done! Final results:
Averaged ETSformer (7,928,510 params) on BeijingAir: MAE=0.3713 ± 0.010931354768925247, MSE=0.4952 ± 0.019813766297798175, MRE=0.4925 ± 0.014501879074860011, average inference time=1.82