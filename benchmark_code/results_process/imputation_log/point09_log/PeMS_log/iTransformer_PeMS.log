2024-06-03 03:53:27 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 03:53:27 [INFO]: Using the given device: cuda:0
2024-06-03 03:53:27 [INFO]: Model files will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_0/20240603_T035327
2024-06-03 03:53:27 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_0/20240603_T035327/tensorboard
2024-06-03 03:53:27 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 03:53:27 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 03:53:28 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 03:53:46 [INFO]: Epoch 001 - training loss: 1.1281, validation loss: 0.9652
2024-06-03 03:53:53 [INFO]: Epoch 002 - training loss: 0.7088, validation loss: 0.9097
2024-06-03 03:54:00 [INFO]: Epoch 003 - training loss: 0.6146, validation loss: 0.8532
2024-06-03 03:54:08 [INFO]: Epoch 004 - training loss: 0.5747, validation loss: 0.8412
2024-06-03 03:54:15 [INFO]: Epoch 005 - training loss: 0.5423, validation loss: 0.8547
2024-06-03 03:54:22 [INFO]: Epoch 006 - training loss: 0.5259, validation loss: 0.8101
2024-06-03 03:54:29 [INFO]: Epoch 007 - training loss: 0.5178, validation loss: 0.8109
2024-06-03 03:54:35 [INFO]: Epoch 008 - training loss: 0.5203, validation loss: 0.8145
2024-06-03 03:54:43 [INFO]: Epoch 009 - training loss: 0.5043, validation loss: 0.7761
2024-06-03 03:54:50 [INFO]: Epoch 010 - training loss: 0.4983, validation loss: 0.7740
2024-06-03 03:54:56 [INFO]: Epoch 011 - training loss: 0.4901, validation loss: 0.7835
2024-06-03 03:55:03 [INFO]: Epoch 012 - training loss: 0.4764, validation loss: 0.7561
2024-06-03 03:55:10 [INFO]: Epoch 013 - training loss: 0.4677, validation loss: 0.7517
2024-06-03 03:55:17 [INFO]: Epoch 014 - training loss: 0.4623, validation loss: 0.7491
2024-06-03 03:55:24 [INFO]: Epoch 015 - training loss: 0.4655, validation loss: 0.7599
2024-06-03 03:55:30 [INFO]: Epoch 016 - training loss: 0.4585, validation loss: 0.7369
2024-06-03 03:55:38 [INFO]: Epoch 017 - training loss: 0.4563, validation loss: 0.7427
2024-06-03 03:55:45 [INFO]: Epoch 018 - training loss: 0.4614, validation loss: 0.7249
2024-06-03 03:55:52 [INFO]: Epoch 019 - training loss: 0.4469, validation loss: 0.7214
2024-06-03 03:55:59 [INFO]: Epoch 020 - training loss: 0.4483, validation loss: 0.7279
2024-06-03 03:56:06 [INFO]: Epoch 021 - training loss: 0.4457, validation loss: 0.7111
2024-06-03 03:56:13 [INFO]: Epoch 022 - training loss: 0.4357, validation loss: 0.7129
2024-06-03 03:56:19 [INFO]: Epoch 023 - training loss: 0.4238, validation loss: 0.7113
2024-06-03 03:56:26 [INFO]: Epoch 024 - training loss: 0.4251, validation loss: 0.7047
2024-06-03 03:56:33 [INFO]: Epoch 025 - training loss: 0.4237, validation loss: 0.6923
2024-06-03 03:56:41 [INFO]: Epoch 026 - training loss: 0.4230, validation loss: 0.7040
2024-06-03 03:56:47 [INFO]: Epoch 027 - training loss: 0.4220, validation loss: 0.6930
2024-06-03 03:56:55 [INFO]: Epoch 028 - training loss: 0.4114, validation loss: 0.6933
2024-06-03 03:57:02 [INFO]: Epoch 029 - training loss: 0.4153, validation loss: 0.6824
2024-06-03 03:57:08 [INFO]: Epoch 030 - training loss: 0.4124, validation loss: 0.6833
2024-06-03 03:57:15 [INFO]: Epoch 031 - training loss: 0.4197, validation loss: 0.6584
2024-06-03 03:57:22 [INFO]: Epoch 032 - training loss: 0.4165, validation loss: 0.6811
2024-06-03 03:57:29 [INFO]: Epoch 033 - training loss: 0.4054, validation loss: 0.6762
2024-06-03 03:57:36 [INFO]: Epoch 034 - training loss: 0.4032, validation loss: 0.6829
2024-06-03 03:57:43 [INFO]: Epoch 035 - training loss: 0.4076, validation loss: 0.6688
2024-06-03 03:57:50 [INFO]: Epoch 036 - training loss: 0.4020, validation loss: 0.6541
2024-06-03 03:57:57 [INFO]: Epoch 037 - training loss: 0.4027, validation loss: 0.6526
2024-06-03 03:58:04 [INFO]: Epoch 038 - training loss: 0.4006, validation loss: 0.6492
2024-06-03 03:58:10 [INFO]: Epoch 039 - training loss: 0.3989, validation loss: 0.6740
2024-06-03 03:58:16 [INFO]: Epoch 040 - training loss: 0.3982, validation loss: 0.6472
2024-06-03 03:58:22 [INFO]: Epoch 041 - training loss: 0.3901, validation loss: 0.6507
2024-06-03 03:58:29 [INFO]: Epoch 042 - training loss: 0.3939, validation loss: 0.6438
2024-06-03 03:58:35 [INFO]: Epoch 043 - training loss: 0.3968, validation loss: 0.6369
2024-06-03 03:58:42 [INFO]: Epoch 044 - training loss: 0.3960, validation loss: 0.6438
2024-06-03 03:58:48 [INFO]: Epoch 045 - training loss: 0.3952, validation loss: 0.6445
2024-06-03 03:58:54 [INFO]: Epoch 046 - training loss: 0.3922, validation loss: 0.6493
2024-06-03 03:59:00 [INFO]: Epoch 047 - training loss: 0.3882, validation loss: 0.6491
2024-06-03 03:59:07 [INFO]: Epoch 048 - training loss: 0.3870, validation loss: 0.6496
2024-06-03 03:59:13 [INFO]: Epoch 049 - training loss: 0.3922, validation loss: 0.6321
2024-06-03 03:59:19 [INFO]: Epoch 050 - training loss: 0.3846, validation loss: 0.6573
2024-06-03 03:59:26 [INFO]: Epoch 051 - training loss: 0.3846, validation loss: 0.6295
2024-06-03 03:59:32 [INFO]: Epoch 052 - training loss: 0.3773, validation loss: 0.6411
2024-06-03 03:59:38 [INFO]: Epoch 053 - training loss: 0.3897, validation loss: 0.6370
2024-06-03 03:59:45 [INFO]: Epoch 054 - training loss: 0.3902, validation loss: 0.6355
2024-06-03 03:59:51 [INFO]: Epoch 055 - training loss: 0.3821, validation loss: 0.6315
2024-06-03 03:59:58 [INFO]: Epoch 056 - training loss: 0.3930, validation loss: 0.6256
2024-06-03 04:00:04 [INFO]: Epoch 057 - training loss: 0.3830, validation loss: 0.6352
2024-06-03 04:00:10 [INFO]: Epoch 058 - training loss: 0.3850, validation loss: 0.6140
2024-06-03 04:00:17 [INFO]: Epoch 059 - training loss: 0.3812, validation loss: 0.6222
2024-06-03 04:00:23 [INFO]: Epoch 060 - training loss: 0.3758, validation loss: 0.6262
2024-06-03 04:00:29 [INFO]: Epoch 061 - training loss: 0.3808, validation loss: 0.6221
2024-06-03 04:00:36 [INFO]: Epoch 062 - training loss: 0.3827, validation loss: 0.6241
2024-06-03 04:00:42 [INFO]: Epoch 063 - training loss: 0.3797, validation loss: 0.6271
2024-06-03 04:00:48 [INFO]: Epoch 064 - training loss: 0.3771, validation loss: 0.6286
2024-06-03 04:00:55 [INFO]: Epoch 065 - training loss: 0.3795, validation loss: 0.6148
2024-06-03 04:01:01 [INFO]: Epoch 066 - training loss: 0.3746, validation loss: 0.6150
2024-06-03 04:01:08 [INFO]: Epoch 067 - training loss: 0.3695, validation loss: 0.6102
2024-06-03 04:01:14 [INFO]: Epoch 068 - training loss: 0.3673, validation loss: 0.6108
2024-06-03 04:01:21 [INFO]: Epoch 069 - training loss: 0.3749, validation loss: 0.6557
2024-06-03 04:01:27 [INFO]: Epoch 070 - training loss: 0.3714, validation loss: 0.6097
2024-06-03 04:01:34 [INFO]: Epoch 071 - training loss: 0.3682, validation loss: 0.6311
2024-06-03 04:01:40 [INFO]: Epoch 072 - training loss: 0.3694, validation loss: 0.6346
2024-06-03 04:01:46 [INFO]: Epoch 073 - training loss: 0.3752, validation loss: 0.6266
2024-06-03 04:01:53 [INFO]: Epoch 074 - training loss: 0.3721, validation loss: 0.6139
2024-06-03 04:01:59 [INFO]: Epoch 075 - training loss: 0.3723, validation loss: 0.6199
2024-06-03 04:02:06 [INFO]: Epoch 076 - training loss: 0.3723, validation loss: 0.6139
2024-06-03 04:02:12 [INFO]: Epoch 077 - training loss: 0.3754, validation loss: 0.6064
2024-06-03 04:02:18 [INFO]: Epoch 078 - training loss: 0.3750, validation loss: 0.6174
2024-06-03 04:02:24 [INFO]: Epoch 079 - training loss: 0.3719, validation loss: 0.6051
2024-06-03 04:02:31 [INFO]: Epoch 080 - training loss: 0.3694, validation loss: 0.6087
2024-06-03 04:02:37 [INFO]: Epoch 081 - training loss: 0.3718, validation loss: 0.6037
2024-06-03 04:02:43 [INFO]: Epoch 082 - training loss: 0.3676, validation loss: 0.6140
2024-06-03 04:02:50 [INFO]: Epoch 083 - training loss: 0.3702, validation loss: 0.6261
2024-06-03 04:02:56 [INFO]: Epoch 084 - training loss: 0.3714, validation loss: 0.6155
2024-06-03 04:03:03 [INFO]: Epoch 085 - training loss: 0.3706, validation loss: 0.6205
2024-06-03 04:03:09 [INFO]: Epoch 086 - training loss: 0.3698, validation loss: 0.6103
2024-06-03 04:03:15 [INFO]: Epoch 087 - training loss: 0.3640, validation loss: 0.6115
2024-06-03 04:03:21 [INFO]: Epoch 088 - training loss: 0.3647, validation loss: 0.6031
2024-06-03 04:03:28 [INFO]: Epoch 089 - training loss: 0.3634, validation loss: 0.6059
2024-06-03 04:03:34 [INFO]: Epoch 090 - training loss: 0.3648, validation loss: 0.5974
2024-06-03 04:03:41 [INFO]: Epoch 091 - training loss: 0.3661, validation loss: 0.5968
2024-06-03 04:03:47 [INFO]: Epoch 092 - training loss: 0.3675, validation loss: 0.6004
2024-06-03 04:03:53 [INFO]: Epoch 093 - training loss: 0.3670, validation loss: 0.6041
2024-06-03 04:04:00 [INFO]: Epoch 094 - training loss: 0.3597, validation loss: 0.6102
2024-06-03 04:04:06 [INFO]: Epoch 095 - training loss: 0.3622, validation loss: 0.6067
2024-06-03 04:04:12 [INFO]: Epoch 096 - training loss: 0.3649, validation loss: 0.5967
2024-06-03 04:04:18 [INFO]: Epoch 097 - training loss: 0.3597, validation loss: 0.5982
2024-06-03 04:04:24 [INFO]: Epoch 098 - training loss: 0.3603, validation loss: 0.6057
2024-06-03 04:04:31 [INFO]: Epoch 099 - training loss: 0.3577, validation loss: 0.6022
2024-06-03 04:04:37 [INFO]: Epoch 100 - training loss: 0.3597, validation loss: 0.6098
2024-06-03 04:04:37 [INFO]: Finished training. The best model is from epoch#96.
2024-06-03 04:04:37 [INFO]: Saved the model to results_point_rate09/PeMS/iTransformer_PeMS/round_0/20240603_T035327/iTransformer.pypots
2024-06-03 04:04:40 [INFO]: Successfully saved to results_point_rate09/PeMS/iTransformer_PeMS/round_0/imputation.pkl
2024-06-03 04:04:40 [INFO]: Round0 - iTransformer on PeMS: MAE=0.4434, MSE=0.8571, MRE=0.5502
2024-06-03 04:04:40 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 04:04:40 [INFO]: Using the given device: cuda:0
2024-06-03 04:04:40 [INFO]: Model files will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_1/20240603_T040440
2024-06-03 04:04:40 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_1/20240603_T040440/tensorboard
2024-06-03 04:04:40 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 04:04:40 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 04:04:40 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 04:04:47 [INFO]: Epoch 001 - training loss: 1.1257, validation loss: 0.9173
2024-06-03 04:04:53 [INFO]: Epoch 002 - training loss: 0.7256, validation loss: 0.9057
2024-06-03 04:05:00 [INFO]: Epoch 003 - training loss: 0.6235, validation loss: 0.8640
2024-06-03 04:05:06 [INFO]: Epoch 004 - training loss: 0.5700, validation loss: 0.8702
2024-06-03 04:05:13 [INFO]: Epoch 005 - training loss: 0.5561, validation loss: 0.8331
2024-06-03 04:05:19 [INFO]: Epoch 006 - training loss: 0.5253, validation loss: 0.8179
2024-06-03 04:05:25 [INFO]: Epoch 007 - training loss: 0.5125, validation loss: 0.7884
2024-06-03 04:05:31 [INFO]: Epoch 008 - training loss: 0.5066, validation loss: 0.7729
2024-06-03 04:05:38 [INFO]: Epoch 009 - training loss: 0.4936, validation loss: 0.7652
2024-06-03 04:05:44 [INFO]: Epoch 010 - training loss: 0.5051, validation loss: 0.8131
2024-06-03 04:05:50 [INFO]: Epoch 011 - training loss: 0.4901, validation loss: 0.7700
2024-06-03 04:05:57 [INFO]: Epoch 012 - training loss: 0.4663, validation loss: 0.7498
2024-06-03 04:06:04 [INFO]: Epoch 013 - training loss: 0.4587, validation loss: 0.7488
2024-06-03 04:06:10 [INFO]: Epoch 014 - training loss: 0.4572, validation loss: 0.7454
2024-06-03 04:06:16 [INFO]: Epoch 015 - training loss: 0.4591, validation loss: 0.7531
2024-06-03 04:06:23 [INFO]: Epoch 016 - training loss: 0.4575, validation loss: 0.7341
2024-06-03 04:06:29 [INFO]: Epoch 017 - training loss: 0.4400, validation loss: 0.7247
2024-06-03 04:06:35 [INFO]: Epoch 018 - training loss: 0.4422, validation loss: 0.7127
2024-06-03 04:06:41 [INFO]: Epoch 019 - training loss: 0.4482, validation loss: 0.7383
2024-06-03 04:06:47 [INFO]: Epoch 020 - training loss: 0.4375, validation loss: 0.7455
2024-06-03 04:06:53 [INFO]: Epoch 021 - training loss: 0.4277, validation loss: 0.7091
2024-06-03 04:06:59 [INFO]: Epoch 022 - training loss: 0.4275, validation loss: 0.7066
2024-06-03 04:07:06 [INFO]: Epoch 023 - training loss: 0.4247, validation loss: 0.6837
2024-06-03 04:07:12 [INFO]: Epoch 024 - training loss: 0.4211, validation loss: 0.6967
2024-06-03 04:07:19 [INFO]: Epoch 025 - training loss: 0.4275, validation loss: 0.6859
2024-06-03 04:07:25 [INFO]: Epoch 026 - training loss: 0.4142, validation loss: 0.6901
2024-06-03 04:07:32 [INFO]: Epoch 027 - training loss: 0.4116, validation loss: 0.6598
2024-06-03 04:07:38 [INFO]: Epoch 028 - training loss: 0.4135, validation loss: 0.6881
2024-06-03 04:07:44 [INFO]: Epoch 029 - training loss: 0.4121, validation loss: 0.6768
2024-06-03 04:07:51 [INFO]: Epoch 030 - training loss: 0.4145, validation loss: 0.7062
2024-06-03 04:07:57 [INFO]: Epoch 031 - training loss: 0.4120, validation loss: 0.6640
2024-06-03 04:08:04 [INFO]: Epoch 032 - training loss: 0.4059, validation loss: 0.6678
2024-06-03 04:08:10 [INFO]: Epoch 033 - training loss: 0.4175, validation loss: 0.6813
2024-06-03 04:08:16 [INFO]: Epoch 034 - training loss: 0.4049, validation loss: 0.6477
2024-06-03 04:08:22 [INFO]: Epoch 035 - training loss: 0.4019, validation loss: 0.6637
2024-06-03 04:08:29 [INFO]: Epoch 036 - training loss: 0.3918, validation loss: 0.6480
2024-06-03 04:08:35 [INFO]: Epoch 037 - training loss: 0.3954, validation loss: 0.6427
2024-06-03 04:08:41 [INFO]: Epoch 038 - training loss: 0.4005, validation loss: 0.6369
2024-06-03 04:08:47 [INFO]: Epoch 039 - training loss: 0.3879, validation loss: 0.6524
2024-06-03 04:08:53 [INFO]: Epoch 040 - training loss: 0.3970, validation loss: 0.6537
2024-06-03 04:09:00 [INFO]: Epoch 041 - training loss: 0.3894, validation loss: 0.6446
2024-06-03 04:09:06 [INFO]: Epoch 042 - training loss: 0.3911, validation loss: 0.6381
2024-06-03 04:09:12 [INFO]: Epoch 043 - training loss: 0.3832, validation loss: 0.6488
2024-06-03 04:09:19 [INFO]: Epoch 044 - training loss: 0.3931, validation loss: 0.6406
2024-06-03 04:09:25 [INFO]: Epoch 045 - training loss: 0.3872, validation loss: 0.6273
2024-06-03 04:09:31 [INFO]: Epoch 046 - training loss: 0.3864, validation loss: 0.6757
2024-06-03 04:09:38 [INFO]: Epoch 047 - training loss: 0.3832, validation loss: 0.6352
2024-06-03 04:09:44 [INFO]: Epoch 048 - training loss: 0.3936, validation loss: 0.6395
2024-06-03 04:09:51 [INFO]: Epoch 049 - training loss: 0.3876, validation loss: 0.6273
2024-06-03 04:09:57 [INFO]: Epoch 050 - training loss: 0.3876, validation loss: 0.6293
2024-06-03 04:10:03 [INFO]: Epoch 051 - training loss: 0.3857, validation loss: 0.6234
2024-06-03 04:10:10 [INFO]: Epoch 052 - training loss: 0.3830, validation loss: 0.6275
2024-06-03 04:10:16 [INFO]: Epoch 053 - training loss: 0.3834, validation loss: 0.6372
2024-06-03 04:10:23 [INFO]: Epoch 054 - training loss: 0.3783, validation loss: 0.6264
2024-06-03 04:10:30 [INFO]: Epoch 055 - training loss: 0.3791, validation loss: 0.6262
2024-06-03 04:10:36 [INFO]: Epoch 056 - training loss: 0.3758, validation loss: 0.6267
2024-06-03 04:10:43 [INFO]: Epoch 057 - training loss: 0.3797, validation loss: 0.6308
2024-06-03 04:10:49 [INFO]: Epoch 058 - training loss: 0.3769, validation loss: 0.6292
2024-06-03 04:10:55 [INFO]: Epoch 059 - training loss: 0.3715, validation loss: 0.6380
2024-06-03 04:11:02 [INFO]: Epoch 060 - training loss: 0.3710, validation loss: 0.6159
2024-06-03 04:11:08 [INFO]: Epoch 061 - training loss: 0.3776, validation loss: 0.6204
2024-06-03 04:11:14 [INFO]: Epoch 062 - training loss: 0.3711, validation loss: 0.6193
2024-06-03 04:11:21 [INFO]: Epoch 063 - training loss: 0.3811, validation loss: 0.6117
2024-06-03 04:11:27 [INFO]: Epoch 064 - training loss: 0.3766, validation loss: 0.6310
2024-06-03 04:11:34 [INFO]: Epoch 065 - training loss: 0.3699, validation loss: 0.6191
2024-06-03 04:11:40 [INFO]: Epoch 066 - training loss: 0.3726, validation loss: 0.6161
2024-06-03 04:11:47 [INFO]: Epoch 067 - training loss: 0.3776, validation loss: 0.6149
2024-06-03 04:11:53 [INFO]: Epoch 068 - training loss: 0.3727, validation loss: 0.6089
2024-06-03 04:12:00 [INFO]: Epoch 069 - training loss: 0.3719, validation loss: 0.6259
2024-06-03 04:12:06 [INFO]: Epoch 070 - training loss: 0.3696, validation loss: 0.6160
2024-06-03 04:12:12 [INFO]: Epoch 071 - training loss: 0.3634, validation loss: 0.6154
2024-06-03 04:12:19 [INFO]: Epoch 072 - training loss: 0.3722, validation loss: 0.6255
2024-06-03 04:12:25 [INFO]: Epoch 073 - training loss: 0.3824, validation loss: 0.6189
2024-06-03 04:12:31 [INFO]: Epoch 074 - training loss: 0.3701, validation loss: 0.6093
2024-06-03 04:12:38 [INFO]: Epoch 075 - training loss: 0.3766, validation loss: 0.6235
2024-06-03 04:12:44 [INFO]: Epoch 076 - training loss: 0.3699, validation loss: 0.6153
2024-06-03 04:12:51 [INFO]: Epoch 077 - training loss: 0.3644, validation loss: 0.6124
2024-06-03 04:12:57 [INFO]: Epoch 078 - training loss: 0.3686, validation loss: 0.6164
2024-06-03 04:12:57 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:12:57 [INFO]: Finished training. The best model is from epoch#68.
2024-06-03 04:12:57 [INFO]: Saved the model to results_point_rate09/PeMS/iTransformer_PeMS/round_1/20240603_T040440/iTransformer.pypots
2024-06-03 04:13:00 [INFO]: Successfully saved to results_point_rate09/PeMS/iTransformer_PeMS/round_1/imputation.pkl
2024-06-03 04:13:00 [INFO]: Round1 - iTransformer on PeMS: MAE=0.4419, MSE=0.8701, MRE=0.5483
2024-06-03 04:13:00 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 04:13:00 [INFO]: Using the given device: cuda:0
2024-06-03 04:13:00 [INFO]: Model files will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_2/20240603_T041300
2024-06-03 04:13:00 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_2/20240603_T041300/tensorboard
2024-06-03 04:13:00 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 04:13:00 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 04:13:00 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 04:13:07 [INFO]: Epoch 001 - training loss: 1.1433, validation loss: 0.9553
2024-06-03 04:13:13 [INFO]: Epoch 002 - training loss: 0.7069, validation loss: 0.8836
2024-06-03 04:13:19 [INFO]: Epoch 003 - training loss: 0.6136, validation loss: 0.8891
2024-06-03 04:13:26 [INFO]: Epoch 004 - training loss: 0.5837, validation loss: 0.8703
2024-06-03 04:13:32 [INFO]: Epoch 005 - training loss: 0.5478, validation loss: 0.8114
2024-06-03 04:13:39 [INFO]: Epoch 006 - training loss: 0.5279, validation loss: 0.7976
2024-06-03 04:13:45 [INFO]: Epoch 007 - training loss: 0.5189, validation loss: 0.7948
2024-06-03 04:13:52 [INFO]: Epoch 008 - training loss: 0.5085, validation loss: 0.7713
2024-06-03 04:13:58 [INFO]: Epoch 009 - training loss: 0.4961, validation loss: 0.7713
2024-06-03 04:14:05 [INFO]: Epoch 010 - training loss: 0.4864, validation loss: 0.7590
2024-06-03 04:14:11 [INFO]: Epoch 011 - training loss: 0.4904, validation loss: 0.7873
2024-06-03 04:14:17 [INFO]: Epoch 012 - training loss: 0.4822, validation loss: 0.7542
2024-06-03 04:14:23 [INFO]: Epoch 013 - training loss: 0.4669, validation loss: 0.7460
2024-06-03 04:14:30 [INFO]: Epoch 014 - training loss: 0.4734, validation loss: 0.7262
2024-06-03 04:14:36 [INFO]: Epoch 015 - training loss: 0.4694, validation loss: 0.7414
2024-06-03 04:14:43 [INFO]: Epoch 016 - training loss: 0.4486, validation loss: 0.7225
2024-06-03 04:14:49 [INFO]: Epoch 017 - training loss: 0.4461, validation loss: 0.7285
2024-06-03 04:14:56 [INFO]: Epoch 018 - training loss: 0.4382, validation loss: 0.7240
2024-06-03 04:15:02 [INFO]: Epoch 019 - training loss: 0.4492, validation loss: 0.7122
2024-06-03 04:15:09 [INFO]: Epoch 020 - training loss: 0.4443, validation loss: 0.7058
2024-06-03 04:15:15 [INFO]: Epoch 021 - training loss: 0.4374, validation loss: 0.6974
2024-06-03 04:15:22 [INFO]: Epoch 022 - training loss: 0.4353, validation loss: 0.6946
2024-06-03 04:15:28 [INFO]: Epoch 023 - training loss: 0.4202, validation loss: 0.6879
2024-06-03 04:15:34 [INFO]: Epoch 024 - training loss: 0.4279, validation loss: 0.6830
2024-06-03 04:15:41 [INFO]: Epoch 025 - training loss: 0.4236, validation loss: 0.6781
2024-06-03 04:15:47 [INFO]: Epoch 026 - training loss: 0.4207, validation loss: 0.6785
2024-06-03 04:15:53 [INFO]: Epoch 027 - training loss: 0.4268, validation loss: 0.6815
2024-06-03 04:15:59 [INFO]: Epoch 028 - training loss: 0.4134, validation loss: 0.6840
2024-06-03 04:16:05 [INFO]: Epoch 029 - training loss: 0.4199, validation loss: 0.6789
2024-06-03 04:16:11 [INFO]: Epoch 030 - training loss: 0.4120, validation loss: 0.6638
2024-06-03 04:16:18 [INFO]: Epoch 031 - training loss: 0.4108, validation loss: 0.6563
2024-06-03 04:16:24 [INFO]: Epoch 032 - training loss: 0.4070, validation loss: 0.6688
2024-06-03 04:16:30 [INFO]: Epoch 033 - training loss: 0.4065, validation loss: 0.6492
2024-06-03 04:16:36 [INFO]: Epoch 034 - training loss: 0.4107, validation loss: 0.6578
2024-06-03 04:16:43 [INFO]: Epoch 035 - training loss: 0.4039, validation loss: 0.6714
2024-06-03 04:16:50 [INFO]: Epoch 036 - training loss: 0.4007, validation loss: 0.6528
2024-06-03 04:16:56 [INFO]: Epoch 037 - training loss: 0.3989, validation loss: 0.6535
2024-06-03 04:17:03 [INFO]: Epoch 038 - training loss: 0.3921, validation loss: 0.6576
2024-06-03 04:17:09 [INFO]: Epoch 039 - training loss: 0.4037, validation loss: 0.6669
2024-06-03 04:17:16 [INFO]: Epoch 040 - training loss: 0.3963, validation loss: 0.6513
2024-06-03 04:17:22 [INFO]: Epoch 041 - training loss: 0.3938, validation loss: 0.6512
2024-06-03 04:17:29 [INFO]: Epoch 042 - training loss: 0.3980, validation loss: 0.6463
2024-06-03 04:17:35 [INFO]: Epoch 043 - training loss: 0.3960, validation loss: 0.6620
2024-06-03 04:17:42 [INFO]: Epoch 044 - training loss: 0.3908, validation loss: 0.6355
2024-06-03 04:17:48 [INFO]: Epoch 045 - training loss: 0.3972, validation loss: 0.6524
2024-06-03 04:17:54 [INFO]: Epoch 046 - training loss: 0.3944, validation loss: 0.6543
2024-06-03 04:18:01 [INFO]: Epoch 047 - training loss: 0.3855, validation loss: 0.6386
2024-06-03 04:18:08 [INFO]: Epoch 048 - training loss: 0.3968, validation loss: 0.6595
2024-06-03 04:18:15 [INFO]: Epoch 049 - training loss: 0.3873, validation loss: 0.6434
2024-06-03 04:18:21 [INFO]: Epoch 050 - training loss: 0.3987, validation loss: 0.6496
2024-06-03 04:18:28 [INFO]: Epoch 051 - training loss: 0.3963, validation loss: 0.6519
2024-06-03 04:18:35 [INFO]: Epoch 052 - training loss: 0.3877, validation loss: 0.6401
2024-06-03 04:18:41 [INFO]: Epoch 053 - training loss: 0.3802, validation loss: 0.6510
2024-06-03 04:18:48 [INFO]: Epoch 054 - training loss: 0.3774, validation loss: 0.6363
2024-06-03 04:18:48 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:18:48 [INFO]: Finished training. The best model is from epoch#44.
2024-06-03 04:18:48 [INFO]: Saved the model to results_point_rate09/PeMS/iTransformer_PeMS/round_2/20240603_T041300/iTransformer.pypots
2024-06-03 04:18:51 [INFO]: Successfully saved to results_point_rate09/PeMS/iTransformer_PeMS/round_2/imputation.pkl
2024-06-03 04:18:51 [INFO]: Round2 - iTransformer on PeMS: MAE=0.4623, MSE=0.9000, MRE=0.5736
2024-06-03 04:18:51 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 04:18:51 [INFO]: Using the given device: cuda:0
2024-06-03 04:18:51 [INFO]: Model files will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_3/20240603_T041851
2024-06-03 04:18:51 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_3/20240603_T041851/tensorboard
2024-06-03 04:18:51 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 04:18:51 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 04:18:51 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 04:18:57 [INFO]: Epoch 001 - training loss: 1.1491, validation loss: 0.9016
2024-06-03 04:19:04 [INFO]: Epoch 002 - training loss: 0.7214, validation loss: 0.8699
2024-06-03 04:19:10 [INFO]: Epoch 003 - training loss: 0.6094, validation loss: 0.8296
2024-06-03 04:19:17 [INFO]: Epoch 004 - training loss: 0.5684, validation loss: 0.8338
2024-06-03 04:19:23 [INFO]: Epoch 005 - training loss: 0.5375, validation loss: 0.8173
2024-06-03 04:19:30 [INFO]: Epoch 006 - training loss: 0.5286, validation loss: 0.8188
2024-06-03 04:19:36 [INFO]: Epoch 007 - training loss: 0.5194, validation loss: 0.7932
2024-06-03 04:19:43 [INFO]: Epoch 008 - training loss: 0.5309, validation loss: 0.7711
2024-06-03 04:19:49 [INFO]: Epoch 009 - training loss: 0.5022, validation loss: 0.7686
2024-06-03 04:19:56 [INFO]: Epoch 010 - training loss: 0.4897, validation loss: 0.7936
2024-06-03 04:20:02 [INFO]: Epoch 011 - training loss: 0.4756, validation loss: 0.7635
2024-06-03 04:20:08 [INFO]: Epoch 012 - training loss: 0.4685, validation loss: 0.7589
2024-06-03 04:20:13 [INFO]: Epoch 013 - training loss: 0.4683, validation loss: 0.7465
2024-06-03 04:20:17 [INFO]: Epoch 014 - training loss: 0.4597, validation loss: 0.7366
2024-06-03 04:20:22 [INFO]: Epoch 015 - training loss: 0.4615, validation loss: 0.7593
2024-06-03 04:20:26 [INFO]: Epoch 016 - training loss: 0.4510, validation loss: 0.7430
2024-06-03 04:20:31 [INFO]: Epoch 017 - training loss: 0.4470, validation loss: 0.7340
2024-06-03 04:20:36 [INFO]: Epoch 018 - training loss: 0.4500, validation loss: 0.7273
2024-06-03 04:20:40 [INFO]: Epoch 019 - training loss: 0.4422, validation loss: 0.7224
2024-06-03 04:20:45 [INFO]: Epoch 020 - training loss: 0.4391, validation loss: 0.7110
2024-06-03 04:20:50 [INFO]: Epoch 021 - training loss: 0.4254, validation loss: 0.7111
2024-06-03 04:20:54 [INFO]: Epoch 022 - training loss: 0.4275, validation loss: 0.7026
2024-06-03 04:20:59 [INFO]: Epoch 023 - training loss: 0.4272, validation loss: 0.6959
2024-06-03 04:21:04 [INFO]: Epoch 024 - training loss: 0.4428, validation loss: 0.6943
2024-06-03 04:21:08 [INFO]: Epoch 025 - training loss: 0.4366, validation loss: 0.6709
2024-06-03 04:21:13 [INFO]: Epoch 026 - training loss: 0.4330, validation loss: 0.6931
2024-06-03 04:21:18 [INFO]: Epoch 027 - training loss: 0.4205, validation loss: 0.6786
2024-06-03 04:21:23 [INFO]: Epoch 028 - training loss: 0.4280, validation loss: 0.6958
2024-06-03 04:21:27 [INFO]: Epoch 029 - training loss: 0.4221, validation loss: 0.6785
2024-06-03 04:21:32 [INFO]: Epoch 030 - training loss: 0.4125, validation loss: 0.6779
2024-06-03 04:21:37 [INFO]: Epoch 031 - training loss: 0.4043, validation loss: 0.6675
2024-06-03 04:21:41 [INFO]: Epoch 032 - training loss: 0.4138, validation loss: 0.6638
2024-06-03 04:21:46 [INFO]: Epoch 033 - training loss: 0.4148, validation loss: 0.6740
2024-06-03 04:21:51 [INFO]: Epoch 034 - training loss: 0.4055, validation loss: 0.6609
2024-06-03 04:21:55 [INFO]: Epoch 035 - training loss: 0.4043, validation loss: 0.6676
2024-06-03 04:22:00 [INFO]: Epoch 036 - training loss: 0.4057, validation loss: 0.6571
2024-06-03 04:22:04 [INFO]: Epoch 037 - training loss: 0.4078, validation loss: 0.6871
2024-06-03 04:22:09 [INFO]: Epoch 038 - training loss: 0.4179, validation loss: 0.6626
2024-06-03 04:22:13 [INFO]: Epoch 039 - training loss: 0.3983, validation loss: 0.6569
2024-06-03 04:22:18 [INFO]: Epoch 040 - training loss: 0.3944, validation loss: 0.6649
2024-06-03 04:22:23 [INFO]: Epoch 041 - training loss: 0.3964, validation loss: 0.6590
2024-06-03 04:22:27 [INFO]: Epoch 042 - training loss: 0.3875, validation loss: 0.6446
2024-06-03 04:22:32 [INFO]: Epoch 043 - training loss: 0.3869, validation loss: 0.6492
2024-06-03 04:22:37 [INFO]: Epoch 044 - training loss: 0.3988, validation loss: 0.6492
2024-06-03 04:22:42 [INFO]: Epoch 045 - training loss: 0.3993, validation loss: 0.6469
2024-06-03 04:22:46 [INFO]: Epoch 046 - training loss: 0.3883, validation loss: 0.6474
2024-06-03 04:22:51 [INFO]: Epoch 047 - training loss: 0.3909, validation loss: 0.6487
2024-06-03 04:22:55 [INFO]: Epoch 048 - training loss: 0.3941, validation loss: 0.6407
2024-06-03 04:23:00 [INFO]: Epoch 049 - training loss: 0.4018, validation loss: 0.6605
2024-06-03 04:23:04 [INFO]: Epoch 050 - training loss: 0.3966, validation loss: 0.6507
2024-06-03 04:23:09 [INFO]: Epoch 051 - training loss: 0.3908, validation loss: 0.6300
2024-06-03 04:23:13 [INFO]: Epoch 052 - training loss: 0.3889, validation loss: 0.6451
2024-06-03 04:23:18 [INFO]: Epoch 053 - training loss: 0.3864, validation loss: 0.6385
2024-06-03 04:23:23 [INFO]: Epoch 054 - training loss: 0.3827, validation loss: 0.6432
2024-06-03 04:23:28 [INFO]: Epoch 055 - training loss: 0.3805, validation loss: 0.6399
2024-06-03 04:23:32 [INFO]: Epoch 056 - training loss: 0.3837, validation loss: 0.6337
2024-06-03 04:23:37 [INFO]: Epoch 057 - training loss: 0.3805, validation loss: 0.6343
2024-06-03 04:23:42 [INFO]: Epoch 058 - training loss: 0.3863, validation loss: 0.6360
2024-06-03 04:23:46 [INFO]: Epoch 059 - training loss: 0.3857, validation loss: 0.6308
2024-06-03 04:23:51 [INFO]: Epoch 060 - training loss: 0.3792, validation loss: 0.6291
2024-06-03 04:23:56 [INFO]: Epoch 061 - training loss: 0.3753, validation loss: 0.6247
2024-06-03 04:24:01 [INFO]: Epoch 062 - training loss: 0.3781, validation loss: 0.6305
2024-06-03 04:24:05 [INFO]: Epoch 063 - training loss: 0.3794, validation loss: 0.6244
2024-06-03 04:24:10 [INFO]: Epoch 064 - training loss: 0.3870, validation loss: 0.6285
2024-06-03 04:24:15 [INFO]: Epoch 065 - training loss: 0.3742, validation loss: 0.6166
2024-06-03 04:24:20 [INFO]: Epoch 066 - training loss: 0.3765, validation loss: 0.6223
2024-06-03 04:24:24 [INFO]: Epoch 067 - training loss: 0.3823, validation loss: 0.6253
2024-06-03 04:24:29 [INFO]: Epoch 068 - training loss: 0.3876, validation loss: 0.6565
2024-06-03 04:24:34 [INFO]: Epoch 069 - training loss: 0.3810, validation loss: 0.6334
2024-06-03 04:24:38 [INFO]: Epoch 070 - training loss: 0.3737, validation loss: 0.6386
2024-06-03 04:24:43 [INFO]: Epoch 071 - training loss: 0.3715, validation loss: 0.6447
2024-06-03 04:24:48 [INFO]: Epoch 072 - training loss: 0.3777, validation loss: 0.6366
2024-06-03 04:24:52 [INFO]: Epoch 073 - training loss: 0.3697, validation loss: 0.6289
2024-06-03 04:24:57 [INFO]: Epoch 074 - training loss: 0.3778, validation loss: 0.6158
2024-06-03 04:25:02 [INFO]: Epoch 075 - training loss: 0.3733, validation loss: 0.6283
2024-06-03 04:25:06 [INFO]: Epoch 076 - training loss: 0.3693, validation loss: 0.6277
2024-06-03 04:25:11 [INFO]: Epoch 077 - training loss: 0.3687, validation loss: 0.6226
2024-06-03 04:25:16 [INFO]: Epoch 078 - training loss: 0.3747, validation loss: 0.6588
2024-06-03 04:25:20 [INFO]: Epoch 079 - training loss: 0.3675, validation loss: 0.6125
2024-06-03 04:25:25 [INFO]: Epoch 080 - training loss: 0.3659, validation loss: 0.6174
2024-06-03 04:25:30 [INFO]: Epoch 081 - training loss: 0.3706, validation loss: 0.6224
2024-06-03 04:25:34 [INFO]: Epoch 082 - training loss: 0.3726, validation loss: 0.6367
2024-06-03 04:25:39 [INFO]: Epoch 083 - training loss: 0.3646, validation loss: 0.6421
2024-06-03 04:25:44 [INFO]: Epoch 084 - training loss: 0.3684, validation loss: 0.6438
2024-06-03 04:25:48 [INFO]: Epoch 085 - training loss: 0.3676, validation loss: 0.6370
2024-06-03 04:25:53 [INFO]: Epoch 086 - training loss: 0.3625, validation loss: 0.6306
2024-06-03 04:25:58 [INFO]: Epoch 087 - training loss: 0.3603, validation loss: 0.6222
2024-06-03 04:26:03 [INFO]: Epoch 088 - training loss: 0.3614, validation loss: 0.6214
2024-06-03 04:26:07 [INFO]: Epoch 089 - training loss: 0.3690, validation loss: 0.6339
2024-06-03 04:26:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:26:07 [INFO]: Finished training. The best model is from epoch#79.
2024-06-03 04:26:07 [INFO]: Saved the model to results_point_rate09/PeMS/iTransformer_PeMS/round_3/20240603_T041851/iTransformer.pypots
2024-06-03 04:26:10 [INFO]: Successfully saved to results_point_rate09/PeMS/iTransformer_PeMS/round_3/imputation.pkl
2024-06-03 04:26:10 [INFO]: Round3 - iTransformer on PeMS: MAE=0.4512, MSE=0.8841, MRE=0.5599
2024-06-03 04:26:10 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 04:26:10 [INFO]: Using the given device: cuda:0
2024-06-03 04:26:10 [INFO]: Model files will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_4/20240603_T042610
2024-06-03 04:26:10 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/iTransformer_PeMS/round_4/20240603_T042610/tensorboard
2024-06-03 04:26:10 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=1024, n_heads=2, d_k=128
2024-06-03 04:26:10 [WARNING]: ⚠️ d_model is reset to 256 = n_heads (2) * d_k (128)
2024-06-03 04:26:10 [INFO]: iTransformer initialized with the given hyperparameters, the number of trainable parameters: 1,854,744
2024-06-03 04:26:15 [INFO]: Epoch 001 - training loss: 1.1179, validation loss: 0.9876
2024-06-03 04:26:19 [INFO]: Epoch 002 - training loss: 0.7228, validation loss: 0.8630
2024-06-03 04:26:24 [INFO]: Epoch 003 - training loss: 0.6192, validation loss: 0.8357
2024-06-03 04:26:29 [INFO]: Epoch 004 - training loss: 0.5793, validation loss: 0.8237
2024-06-03 04:26:33 [INFO]: Epoch 005 - training loss: 0.5614, validation loss: 0.8042
2024-06-03 04:26:38 [INFO]: Epoch 006 - training loss: 0.5420, validation loss: 0.7883
2024-06-03 04:26:42 [INFO]: Epoch 007 - training loss: 0.5250, validation loss: 0.8196
2024-06-03 04:26:47 [INFO]: Epoch 008 - training loss: 0.5031, validation loss: 0.7894
2024-06-03 04:26:52 [INFO]: Epoch 009 - training loss: 0.4947, validation loss: 0.7893
2024-06-03 04:26:56 [INFO]: Epoch 010 - training loss: 0.4840, validation loss: 0.7627
2024-06-03 04:27:01 [INFO]: Epoch 011 - training loss: 0.4829, validation loss: 0.7581
2024-06-03 04:27:05 [INFO]: Epoch 012 - training loss: 0.4693, validation loss: 0.7551
2024-06-03 04:27:10 [INFO]: Epoch 013 - training loss: 0.4625, validation loss: 0.7503
2024-06-03 04:27:14 [INFO]: Epoch 014 - training loss: 0.4606, validation loss: 0.7513
2024-06-03 04:27:19 [INFO]: Epoch 015 - training loss: 0.4647, validation loss: 0.7145
2024-06-03 04:27:23 [INFO]: Epoch 016 - training loss: 0.4592, validation loss: 0.7183
2024-06-03 04:27:28 [INFO]: Epoch 017 - training loss: 0.4504, validation loss: 0.7165
2024-06-03 04:27:33 [INFO]: Epoch 018 - training loss: 0.4399, validation loss: 0.7080
2024-06-03 04:27:37 [INFO]: Epoch 019 - training loss: 0.4415, validation loss: 0.7104
2024-06-03 04:27:42 [INFO]: Epoch 020 - training loss: 0.4347, validation loss: 0.6840
2024-06-03 04:27:47 [INFO]: Epoch 021 - training loss: 0.4471, validation loss: 0.7045
2024-06-03 04:27:52 [INFO]: Epoch 022 - training loss: 0.4313, validation loss: 0.6930
2024-06-03 04:27:56 [INFO]: Epoch 023 - training loss: 0.4299, validation loss: 0.6895
2024-06-03 04:28:01 [INFO]: Epoch 024 - training loss: 0.4308, validation loss: 0.6963
2024-06-03 04:28:06 [INFO]: Epoch 025 - training loss: 0.4209, validation loss: 0.6706
2024-06-03 04:28:10 [INFO]: Epoch 026 - training loss: 0.4184, validation loss: 0.6787
2024-06-03 04:28:15 [INFO]: Epoch 027 - training loss: 0.4295, validation loss: 0.6686
2024-06-03 04:28:20 [INFO]: Epoch 028 - training loss: 0.4169, validation loss: 0.6782
2024-06-03 04:28:24 [INFO]: Epoch 029 - training loss: 0.4160, validation loss: 0.6743
2024-06-03 04:28:29 [INFO]: Epoch 030 - training loss: 0.4115, validation loss: 0.6618
2024-06-03 04:28:33 [INFO]: Epoch 031 - training loss: 0.4124, validation loss: 0.6700
2024-06-03 04:28:38 [INFO]: Epoch 032 - training loss: 0.4154, validation loss: 0.6692
2024-06-03 04:28:43 [INFO]: Epoch 033 - training loss: 0.4100, validation loss: 0.6629
2024-06-03 04:28:47 [INFO]: Epoch 034 - training loss: 0.4025, validation loss: 0.6663
2024-06-03 04:28:52 [INFO]: Epoch 035 - training loss: 0.4185, validation loss: 0.6578
2024-06-03 04:28:57 [INFO]: Epoch 036 - training loss: 0.4093, validation loss: 0.6730
2024-06-03 04:29:01 [INFO]: Epoch 037 - training loss: 0.4000, validation loss: 0.6488
2024-06-03 04:29:06 [INFO]: Epoch 038 - training loss: 0.4023, validation loss: 0.6412
2024-06-03 04:29:11 [INFO]: Epoch 039 - training loss: 0.4015, validation loss: 0.6623
2024-06-03 04:29:15 [INFO]: Epoch 040 - training loss: 0.3973, validation loss: 0.6506
2024-06-03 04:29:20 [INFO]: Epoch 041 - training loss: 0.3933, validation loss: 0.6520
2024-06-03 04:29:24 [INFO]: Epoch 042 - training loss: 0.3935, validation loss: 0.6537
2024-06-03 04:29:29 [INFO]: Epoch 043 - training loss: 0.3950, validation loss: 0.6553
2024-06-03 04:29:34 [INFO]: Epoch 044 - training loss: 0.3870, validation loss: 0.6397
2024-06-03 04:29:38 [INFO]: Epoch 045 - training loss: 0.3979, validation loss: 0.6560
2024-06-03 04:29:43 [INFO]: Epoch 046 - training loss: 0.3871, validation loss: 0.6429
2024-06-03 04:29:48 [INFO]: Epoch 047 - training loss: 0.3881, validation loss: 0.6418
2024-06-03 04:29:52 [INFO]: Epoch 048 - training loss: 0.3876, validation loss: 0.6570
2024-06-03 04:29:57 [INFO]: Epoch 049 - training loss: 0.3887, validation loss: 0.6437
2024-06-03 04:30:02 [INFO]: Epoch 050 - training loss: 0.3909, validation loss: 0.6429
2024-06-03 04:30:07 [INFO]: Epoch 051 - training loss: 0.3869, validation loss: 0.6328
2024-06-03 04:30:11 [INFO]: Epoch 052 - training loss: 0.3869, validation loss: 0.6453
2024-06-03 04:30:16 [INFO]: Epoch 053 - training loss: 0.3862, validation loss: 0.6468
2024-06-03 04:30:21 [INFO]: Epoch 054 - training loss: 0.3770, validation loss: 0.6402
2024-06-03 04:30:25 [INFO]: Epoch 055 - training loss: 0.3848, validation loss: 0.6378
2024-06-03 04:30:30 [INFO]: Epoch 056 - training loss: 0.3848, validation loss: 0.6299
2024-06-03 04:30:35 [INFO]: Epoch 057 - training loss: 0.3793, validation loss: 0.6425
2024-06-03 04:30:39 [INFO]: Epoch 058 - training loss: 0.3772, validation loss: 0.6235
2024-06-03 04:30:44 [INFO]: Epoch 059 - training loss: 0.3817, validation loss: 0.6265
2024-06-03 04:30:49 [INFO]: Epoch 060 - training loss: 0.3806, validation loss: 0.6354
2024-06-03 04:30:54 [INFO]: Epoch 061 - training loss: 0.3936, validation loss: 0.6254
2024-06-03 04:30:58 [INFO]: Epoch 062 - training loss: 0.3837, validation loss: 0.6376
2024-06-03 04:31:03 [INFO]: Epoch 063 - training loss: 0.3820, validation loss: 0.6390
2024-06-03 04:31:08 [INFO]: Epoch 064 - training loss: 0.3784, validation loss: 0.6164
2024-06-03 04:31:12 [INFO]: Epoch 065 - training loss: 0.3750, validation loss: 0.6291
2024-06-03 04:31:17 [INFO]: Epoch 066 - training loss: 0.3827, validation loss: 0.6183
2024-06-03 04:31:21 [INFO]: Epoch 067 - training loss: 0.3805, validation loss: 0.6350
2024-06-03 04:31:26 [INFO]: Epoch 068 - training loss: 0.3756, validation loss: 0.6276
2024-06-03 04:31:31 [INFO]: Epoch 069 - training loss: 0.3779, validation loss: 0.6211
2024-06-03 04:31:35 [INFO]: Epoch 070 - training loss: 0.3726, validation loss: 0.6250
2024-06-03 04:31:40 [INFO]: Epoch 071 - training loss: 0.3749, validation loss: 0.6286
2024-06-03 04:31:44 [INFO]: Epoch 072 - training loss: 0.3687, validation loss: 0.6320
2024-06-03 04:31:49 [INFO]: Epoch 073 - training loss: 0.3734, validation loss: 0.6193
2024-06-03 04:31:54 [INFO]: Epoch 074 - training loss: 0.3724, validation loss: 0.6177
2024-06-03 04:31:54 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:31:54 [INFO]: Finished training. The best model is from epoch#64.
2024-06-03 04:31:54 [INFO]: Saved the model to results_point_rate09/PeMS/iTransformer_PeMS/round_4/20240603_T042610/iTransformer.pypots
2024-06-03 04:31:56 [INFO]: Successfully saved to results_point_rate09/PeMS/iTransformer_PeMS/round_4/imputation.pkl
2024-06-03 04:31:56 [INFO]: Round4 - iTransformer on PeMS: MAE=0.4511, MSE=0.8634, MRE=0.5597
2024-06-03 04:31:56 [INFO]: Done! Final results:
Averaged iTransformer (1,854,744 params) on PeMS: MAE=0.4500 ± 0.007250726157093903, MSE=0.8749 ± 0.01540547810053032, MRE=0.5584 ± 0.008996925055551424, average inference time=0.49
