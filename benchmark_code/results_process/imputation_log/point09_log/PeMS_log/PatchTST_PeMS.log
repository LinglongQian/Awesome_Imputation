2024-06-03 03:53:27 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 03:53:27 [INFO]: Using the given device: cuda:0
2024-06-03 03:53:27 [INFO]: Model files will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_0/20240603_T035327
2024-06-03 03:53:27 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_0/20240603_T035327/tensorboard
2024-06-03 03:53:27 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 03:53:27 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 03:53:28 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 03:53:44 [INFO]: Epoch 001 - training loss: 1.2597, validation loss: 0.9440
2024-06-03 03:53:49 [INFO]: Epoch 002 - training loss: 0.8535, validation loss: 0.9236
2024-06-03 03:53:54 [INFO]: Epoch 003 - training loss: 0.7095, validation loss: 0.8147
2024-06-03 03:53:59 [INFO]: Epoch 004 - training loss: 0.6531, validation loss: 0.7291
2024-06-03 03:54:04 [INFO]: Epoch 005 - training loss: 0.6181, validation loss: 0.6859
2024-06-03 03:54:09 [INFO]: Epoch 006 - training loss: 0.5884, validation loss: 0.6519
2024-06-03 03:54:14 [INFO]: Epoch 007 - training loss: 0.5590, validation loss: 0.6248
2024-06-03 03:54:19 [INFO]: Epoch 008 - training loss: 0.5389, validation loss: 0.6166
2024-06-03 03:54:24 [INFO]: Epoch 009 - training loss: 0.5225, validation loss: 0.5959
2024-06-03 03:54:30 [INFO]: Epoch 010 - training loss: 0.5097, validation loss: 0.5892
2024-06-03 03:54:34 [INFO]: Epoch 011 - training loss: 0.4960, validation loss: 0.6042
2024-06-03 03:54:39 [INFO]: Epoch 012 - training loss: 0.4862, validation loss: 0.5884
2024-06-03 03:54:44 [INFO]: Epoch 013 - training loss: 0.4820, validation loss: 0.5943
2024-06-03 03:54:49 [INFO]: Epoch 014 - training loss: 0.4674, validation loss: 0.5651
2024-06-03 03:54:55 [INFO]: Epoch 015 - training loss: 0.4665, validation loss: 0.5731
2024-06-03 03:55:00 [INFO]: Epoch 016 - training loss: 0.4570, validation loss: 0.5800
2024-06-03 03:55:05 [INFO]: Epoch 017 - training loss: 0.4474, validation loss: 0.5832
2024-06-03 03:55:10 [INFO]: Epoch 018 - training loss: 0.4414, validation loss: 0.5800
2024-06-03 03:55:16 [INFO]: Epoch 019 - training loss: 0.4351, validation loss: 0.5729
2024-06-03 03:55:21 [INFO]: Epoch 020 - training loss: 0.4316, validation loss: 0.5628
2024-06-03 03:55:25 [INFO]: Epoch 021 - training loss: 0.4204, validation loss: 0.5726
2024-06-03 03:55:31 [INFO]: Epoch 022 - training loss: 0.4185, validation loss: 0.5651
2024-06-03 03:55:36 [INFO]: Epoch 023 - training loss: 0.4109, validation loss: 0.5718
2024-06-03 03:55:41 [INFO]: Epoch 024 - training loss: 0.4030, validation loss: 0.5605
2024-06-03 03:55:46 [INFO]: Epoch 025 - training loss: 0.3990, validation loss: 0.5655
2024-06-03 03:55:51 [INFO]: Epoch 026 - training loss: 0.3954, validation loss: 0.5555
2024-06-03 03:55:57 [INFO]: Epoch 027 - training loss: 0.3925, validation loss: 0.5508
2024-06-03 03:56:02 [INFO]: Epoch 028 - training loss: 0.3898, validation loss: 0.5645
2024-06-03 03:56:07 [INFO]: Epoch 029 - training loss: 0.3848, validation loss: 0.5577
2024-06-03 03:56:12 [INFO]: Epoch 030 - training loss: 0.3823, validation loss: 0.5592
2024-06-03 03:56:16 [INFO]: Epoch 031 - training loss: 0.3760, validation loss: 0.5612
2024-06-03 03:56:22 [INFO]: Epoch 032 - training loss: 0.3755, validation loss: 0.5604
2024-06-03 03:56:27 [INFO]: Epoch 033 - training loss: 0.3710, validation loss: 0.5507
2024-06-03 03:56:32 [INFO]: Epoch 034 - training loss: 0.3680, validation loss: 0.5560
2024-06-03 03:56:37 [INFO]: Epoch 035 - training loss: 0.3664, validation loss: 0.5498
2024-06-03 03:56:42 [INFO]: Epoch 036 - training loss: 0.3602, validation loss: 0.5497
2024-06-03 03:56:48 [INFO]: Epoch 037 - training loss: 0.3622, validation loss: 0.5522
2024-06-03 03:56:53 [INFO]: Epoch 038 - training loss: 0.3628, validation loss: 0.5406
2024-06-03 03:56:58 [INFO]: Epoch 039 - training loss: 0.3586, validation loss: 0.5473
2024-06-03 03:57:03 [INFO]: Epoch 040 - training loss: 0.3507, validation loss: 0.5545
2024-06-03 03:57:08 [INFO]: Epoch 041 - training loss: 0.3487, validation loss: 0.5456
2024-06-03 03:57:13 [INFO]: Epoch 042 - training loss: 0.3451, validation loss: 0.5489
2024-06-03 03:57:18 [INFO]: Epoch 043 - training loss: 0.3433, validation loss: 0.5420
2024-06-03 03:57:23 [INFO]: Epoch 044 - training loss: 0.3420, validation loss: 0.5440
2024-06-03 03:57:29 [INFO]: Epoch 045 - training loss: 0.3440, validation loss: 0.5428
2024-06-03 03:57:34 [INFO]: Epoch 046 - training loss: 0.3385, validation loss: 0.5412
2024-06-03 03:57:39 [INFO]: Epoch 047 - training loss: 0.3404, validation loss: 0.5533
2024-06-03 03:57:44 [INFO]: Epoch 048 - training loss: 0.3410, validation loss: 0.5410
2024-06-03 03:57:44 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:57:44 [INFO]: Finished training. The best model is from epoch#38.
2024-06-03 03:57:44 [INFO]: Saved the model to results_point_rate09/PeMS/PatchTST_PeMS/round_0/20240603_T035327/PatchTST.pypots
2024-06-03 03:57:46 [INFO]: Successfully saved to results_point_rate09/PeMS/PatchTST_PeMS/round_0/imputation.pkl
2024-06-03 03:57:46 [INFO]: Round0 - PatchTST on PeMS: MAE=0.4311, MSE=0.7678, MRE=0.5349
2024-06-03 03:57:46 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 03:57:46 [INFO]: Using the given device: cuda:0
2024-06-03 03:57:46 [INFO]: Model files will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_1/20240603_T035746
2024-06-03 03:57:46 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_1/20240603_T035746/tensorboard
2024-06-03 03:57:46 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 03:57:46 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 03:57:46 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 03:57:52 [INFO]: Epoch 001 - training loss: 1.2534, validation loss: 1.0056
2024-06-03 03:57:57 [INFO]: Epoch 002 - training loss: 0.9227, validation loss: 0.8867
2024-06-03 03:58:01 [INFO]: Epoch 003 - training loss: 0.7872, validation loss: 0.8060
2024-06-03 03:58:06 [INFO]: Epoch 004 - training loss: 0.6987, validation loss: 0.7921
2024-06-03 03:58:11 [INFO]: Epoch 005 - training loss: 0.6510, validation loss: 0.7051
2024-06-03 03:58:16 [INFO]: Epoch 006 - training loss: 0.6213, validation loss: 0.6958
2024-06-03 03:58:20 [INFO]: Epoch 007 - training loss: 0.5912, validation loss: 0.7317
2024-06-03 03:58:24 [INFO]: Epoch 008 - training loss: 0.5651, validation loss: 0.7340
2024-06-03 03:58:29 [INFO]: Epoch 009 - training loss: 0.5505, validation loss: 0.7165
2024-06-03 03:58:34 [INFO]: Epoch 010 - training loss: 0.5367, validation loss: 0.7041
2024-06-03 03:58:38 [INFO]: Epoch 011 - training loss: 0.5206, validation loss: 0.7413
2024-06-03 03:58:43 [INFO]: Epoch 012 - training loss: 0.5169, validation loss: 0.6832
2024-06-03 03:58:48 [INFO]: Epoch 013 - training loss: 0.5110, validation loss: 0.6834
2024-06-03 03:58:52 [INFO]: Epoch 014 - training loss: 0.4999, validation loss: 0.6959
2024-06-03 03:58:57 [INFO]: Epoch 015 - training loss: 0.4867, validation loss: 0.7054
2024-06-03 03:59:01 [INFO]: Epoch 016 - training loss: 0.4755, validation loss: 0.6962
2024-06-03 03:59:06 [INFO]: Epoch 017 - training loss: 0.4709, validation loss: 0.6929
2024-06-03 03:59:10 [INFO]: Epoch 018 - training loss: 0.4662, validation loss: 0.7166
2024-06-03 03:59:15 [INFO]: Epoch 019 - training loss: 0.4598, validation loss: 0.6868
2024-06-03 03:59:20 [INFO]: Epoch 020 - training loss: 0.4507, validation loss: 0.6798
2024-06-03 03:59:24 [INFO]: Epoch 021 - training loss: 0.4418, validation loss: 0.6977
2024-06-03 03:59:28 [INFO]: Epoch 022 - training loss: 0.4401, validation loss: 0.7023
2024-06-03 03:59:33 [INFO]: Epoch 023 - training loss: 0.4367, validation loss: 0.6810
2024-06-03 03:59:38 [INFO]: Epoch 024 - training loss: 0.4342, validation loss: 0.7001
2024-06-03 03:59:42 [INFO]: Epoch 025 - training loss: 0.4254, validation loss: 0.6717
2024-06-03 03:59:47 [INFO]: Epoch 026 - training loss: 0.4257, validation loss: 0.6813
2024-06-03 03:59:51 [INFO]: Epoch 027 - training loss: 0.4181, validation loss: 0.6980
2024-06-03 03:59:56 [INFO]: Epoch 028 - training loss: 0.4220, validation loss: 0.6755
2024-06-03 04:00:00 [INFO]: Epoch 029 - training loss: 0.4121, validation loss: 0.6941
2024-06-03 04:00:04 [INFO]: Epoch 030 - training loss: 0.4091, validation loss: 0.6988
2024-06-03 04:00:09 [INFO]: Epoch 031 - training loss: 0.4053, validation loss: 0.6940
2024-06-03 04:00:13 [INFO]: Epoch 032 - training loss: 0.4001, validation loss: 0.6932
2024-06-03 04:00:18 [INFO]: Epoch 033 - training loss: 0.3957, validation loss: 0.6856
2024-06-03 04:00:23 [INFO]: Epoch 034 - training loss: 0.3979, validation loss: 0.6692
2024-06-03 04:00:27 [INFO]: Epoch 035 - training loss: 0.3944, validation loss: 0.6632
2024-06-03 04:00:32 [INFO]: Epoch 036 - training loss: 0.3912, validation loss: 0.6881
2024-06-03 04:00:36 [INFO]: Epoch 037 - training loss: 0.3878, validation loss: 0.6604
2024-06-03 04:00:40 [INFO]: Epoch 038 - training loss: 0.3876, validation loss: 0.6913
2024-06-03 04:00:45 [INFO]: Epoch 039 - training loss: 0.3786, validation loss: 0.6747
2024-06-03 04:00:49 [INFO]: Epoch 040 - training loss: 0.3754, validation loss: 0.6764
2024-06-03 04:00:54 [INFO]: Epoch 041 - training loss: 0.3748, validation loss: 0.6590
2024-06-03 04:00:59 [INFO]: Epoch 042 - training loss: 0.3762, validation loss: 0.6817
2024-06-03 04:01:03 [INFO]: Epoch 043 - training loss: 0.3755, validation loss: 0.6821
2024-06-03 04:01:08 [INFO]: Epoch 044 - training loss: 0.3711, validation loss: 0.6925
2024-06-03 04:01:12 [INFO]: Epoch 045 - training loss: 0.3681, validation loss: 0.6813
2024-06-03 04:01:17 [INFO]: Epoch 046 - training loss: 0.3676, validation loss: 0.6807
2024-06-03 04:01:21 [INFO]: Epoch 047 - training loss: 0.3616, validation loss: 0.6894
2024-06-03 04:01:25 [INFO]: Epoch 048 - training loss: 0.3616, validation loss: 0.6773
2024-06-03 04:01:30 [INFO]: Epoch 049 - training loss: 0.3604, validation loss: 0.6987
2024-06-03 04:01:35 [INFO]: Epoch 050 - training loss: 0.3587, validation loss: 0.6752
2024-06-03 04:01:39 [INFO]: Epoch 051 - training loss: 0.3576, validation loss: 0.6834
2024-06-03 04:01:39 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:01:39 [INFO]: Finished training. The best model is from epoch#41.
2024-06-03 04:01:39 [INFO]: Saved the model to results_point_rate09/PeMS/PatchTST_PeMS/round_1/20240603_T035746/PatchTST.pypots
2024-06-03 04:01:41 [INFO]: Successfully saved to results_point_rate09/PeMS/PatchTST_PeMS/round_1/imputation.pkl
2024-06-03 04:01:41 [INFO]: Round1 - PatchTST on PeMS: MAE=0.5513, MSE=1.0041, MRE=0.6841
2024-06-03 04:01:41 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 04:01:41 [INFO]: Using the given device: cuda:0
2024-06-03 04:01:41 [INFO]: Model files will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_2/20240603_T040141
2024-06-03 04:01:41 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_2/20240603_T040141/tensorboard
2024-06-03 04:01:41 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 04:01:41 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 04:01:41 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 04:01:46 [INFO]: Epoch 001 - training loss: 1.4112, validation loss: 0.9670
2024-06-03 04:01:51 [INFO]: Epoch 002 - training loss: 0.9483, validation loss: 0.9749
2024-06-03 04:01:55 [INFO]: Epoch 003 - training loss: 0.8358, validation loss: 0.8444
2024-06-03 04:02:00 [INFO]: Epoch 004 - training loss: 0.7512, validation loss: 0.8003
2024-06-03 04:02:04 [INFO]: Epoch 005 - training loss: 0.6586, validation loss: 0.7157
2024-06-03 04:02:09 [INFO]: Epoch 006 - training loss: 0.5946, validation loss: 0.6641
2024-06-03 04:02:13 [INFO]: Epoch 007 - training loss: 0.5614, validation loss: 0.6322
2024-06-03 04:02:18 [INFO]: Epoch 008 - training loss: 0.5432, validation loss: 0.6005
2024-06-03 04:02:22 [INFO]: Epoch 009 - training loss: 0.5228, validation loss: 0.6038
2024-06-03 04:02:27 [INFO]: Epoch 010 - training loss: 0.5189, validation loss: 0.5956
2024-06-03 04:02:31 [INFO]: Epoch 011 - training loss: 0.5029, validation loss: 0.5916
2024-06-03 04:02:36 [INFO]: Epoch 012 - training loss: 0.4973, validation loss: 0.5841
2024-06-03 04:02:40 [INFO]: Epoch 013 - training loss: 0.4833, validation loss: 0.5847
2024-06-03 04:02:45 [INFO]: Epoch 014 - training loss: 0.4724, validation loss: 0.5795
2024-06-03 04:02:49 [INFO]: Epoch 015 - training loss: 0.4596, validation loss: 0.5705
2024-06-03 04:02:54 [INFO]: Epoch 016 - training loss: 0.4497, validation loss: 0.5700
2024-06-03 04:02:58 [INFO]: Epoch 017 - training loss: 0.4421, validation loss: 0.5759
2024-06-03 04:03:03 [INFO]: Epoch 018 - training loss: 0.4367, validation loss: 0.5610
2024-06-03 04:03:08 [INFO]: Epoch 019 - training loss: 0.4257, validation loss: 0.5579
2024-06-03 04:03:13 [INFO]: Epoch 020 - training loss: 0.4213, validation loss: 0.5562
2024-06-03 04:03:18 [INFO]: Epoch 021 - training loss: 0.4133, validation loss: 0.5614
2024-06-03 04:03:22 [INFO]: Epoch 022 - training loss: 0.4103, validation loss: 0.5472
2024-06-03 04:03:27 [INFO]: Epoch 023 - training loss: 0.4018, validation loss: 0.5470
2024-06-03 04:03:31 [INFO]: Epoch 024 - training loss: 0.3996, validation loss: 0.5455
2024-06-03 04:03:36 [INFO]: Epoch 025 - training loss: 0.3892, validation loss: 0.5529
2024-06-03 04:03:40 [INFO]: Epoch 026 - training loss: 0.3923, validation loss: 0.5473
2024-06-03 04:03:45 [INFO]: Epoch 027 - training loss: 0.3840, validation loss: 0.5432
2024-06-03 04:03:49 [INFO]: Epoch 028 - training loss: 0.3796, validation loss: 0.5406
2024-06-03 04:03:54 [INFO]: Epoch 029 - training loss: 0.3745, validation loss: 0.5423
2024-06-03 04:03:58 [INFO]: Epoch 030 - training loss: 0.3721, validation loss: 0.5399
2024-06-03 04:04:03 [INFO]: Epoch 031 - training loss: 0.3729, validation loss: 0.5403
2024-06-03 04:04:07 [INFO]: Epoch 032 - training loss: 0.3691, validation loss: 0.5408
2024-06-03 04:04:12 [INFO]: Epoch 033 - training loss: 0.3619, validation loss: 0.5351
2024-06-03 04:04:16 [INFO]: Epoch 034 - training loss: 0.3604, validation loss: 0.5367
2024-06-03 04:04:21 [INFO]: Epoch 035 - training loss: 0.3570, validation loss: 0.5354
2024-06-03 04:04:25 [INFO]: Epoch 036 - training loss: 0.3584, validation loss: 0.5363
2024-06-03 04:04:30 [INFO]: Epoch 037 - training loss: 0.3535, validation loss: 0.5347
2024-06-03 04:04:34 [INFO]: Epoch 038 - training loss: 0.3507, validation loss: 0.5319
2024-06-03 04:04:39 [INFO]: Epoch 039 - training loss: 0.3449, validation loss: 0.5355
2024-06-03 04:04:43 [INFO]: Epoch 040 - training loss: 0.3442, validation loss: 0.5345
2024-06-03 04:04:48 [INFO]: Epoch 041 - training loss: 0.3421, validation loss: 0.5362
2024-06-03 04:04:52 [INFO]: Epoch 042 - training loss: 0.3388, validation loss: 0.5286
2024-06-03 04:04:57 [INFO]: Epoch 043 - training loss: 0.3386, validation loss: 0.5331
2024-06-03 04:05:01 [INFO]: Epoch 044 - training loss: 0.3342, validation loss: 0.5304
2024-06-03 04:05:06 [INFO]: Epoch 045 - training loss: 0.3347, validation loss: 0.5326
2024-06-03 04:05:11 [INFO]: Epoch 046 - training loss: 0.3330, validation loss: 0.5282
2024-06-03 04:05:15 [INFO]: Epoch 047 - training loss: 0.3327, validation loss: 0.5285
2024-06-03 04:05:19 [INFO]: Epoch 048 - training loss: 0.3311, validation loss: 0.5291
2024-06-03 04:05:24 [INFO]: Epoch 049 - training loss: 0.3259, validation loss: 0.5339
2024-06-03 04:05:28 [INFO]: Epoch 050 - training loss: 0.3265, validation loss: 0.5281
2024-06-03 04:05:33 [INFO]: Epoch 051 - training loss: 0.3285, validation loss: 0.5232
2024-06-03 04:05:38 [INFO]: Epoch 052 - training loss: 0.3212, validation loss: 0.5214
2024-06-03 04:05:42 [INFO]: Epoch 053 - training loss: 0.3217, validation loss: 0.5295
2024-06-03 04:05:47 [INFO]: Epoch 054 - training loss: 0.3186, validation loss: 0.5254
2024-06-03 04:05:51 [INFO]: Epoch 055 - training loss: 0.3178, validation loss: 0.5256
2024-06-03 04:05:56 [INFO]: Epoch 056 - training loss: 0.3162, validation loss: 0.5263
2024-06-03 04:06:00 [INFO]: Epoch 057 - training loss: 0.3173, validation loss: 0.5254
2024-06-03 04:06:04 [INFO]: Epoch 058 - training loss: 0.3138, validation loss: 0.5236
2024-06-03 04:06:09 [INFO]: Epoch 059 - training loss: 0.3119, validation loss: 0.5231
2024-06-03 04:06:13 [INFO]: Epoch 060 - training loss: 0.3105, validation loss: 0.5196
2024-06-03 04:06:18 [INFO]: Epoch 061 - training loss: 0.3106, validation loss: 0.5263
2024-06-03 04:06:22 [INFO]: Epoch 062 - training loss: 0.3078, validation loss: 0.5280
2024-06-03 04:06:27 [INFO]: Epoch 063 - training loss: 0.3068, validation loss: 0.5229
2024-06-03 04:06:31 [INFO]: Epoch 064 - training loss: 0.3075, validation loss: 0.5268
2024-06-03 04:06:36 [INFO]: Epoch 065 - training loss: 0.3061, validation loss: 0.5212
2024-06-03 04:06:40 [INFO]: Epoch 066 - training loss: 0.3034, validation loss: 0.5193
2024-06-03 04:06:44 [INFO]: Epoch 067 - training loss: 0.3022, validation loss: 0.5194
2024-06-03 04:06:49 [INFO]: Epoch 068 - training loss: 0.3003, validation loss: 0.5216
2024-06-03 04:06:53 [INFO]: Epoch 069 - training loss: 0.3001, validation loss: 0.5219
2024-06-03 04:06:57 [INFO]: Epoch 070 - training loss: 0.2971, validation loss: 0.5242
2024-06-03 04:07:02 [INFO]: Epoch 071 - training loss: 0.2999, validation loss: 0.5219
2024-06-03 04:07:06 [INFO]: Epoch 072 - training loss: 0.2973, validation loss: 0.5226
2024-06-03 04:07:10 [INFO]: Epoch 073 - training loss: 0.3010, validation loss: 0.5224
2024-06-03 04:07:15 [INFO]: Epoch 074 - training loss: 0.2992, validation loss: 0.5203
2024-06-03 04:07:19 [INFO]: Epoch 075 - training loss: 0.2955, validation loss: 0.5211
2024-06-03 04:07:24 [INFO]: Epoch 076 - training loss: 0.2912, validation loss: 0.5190
2024-06-03 04:07:28 [INFO]: Epoch 077 - training loss: 0.2914, validation loss: 0.5192
2024-06-03 04:07:32 [INFO]: Epoch 078 - training loss: 0.2924, validation loss: 0.5175
2024-06-03 04:07:37 [INFO]: Epoch 079 - training loss: 0.2907, validation loss: 0.5206
2024-06-03 04:07:41 [INFO]: Epoch 080 - training loss: 0.2902, validation loss: 0.5198
2024-06-03 04:07:46 [INFO]: Epoch 081 - training loss: 0.2895, validation loss: 0.5233
2024-06-03 04:07:50 [INFO]: Epoch 082 - training loss: 0.2893, validation loss: 0.5189
2024-06-03 04:07:55 [INFO]: Epoch 083 - training loss: 0.2874, validation loss: 0.5160
2024-06-03 04:07:59 [INFO]: Epoch 084 - training loss: 0.2884, validation loss: 0.5190
2024-06-03 04:08:03 [INFO]: Epoch 085 - training loss: 0.2868, validation loss: 0.5216
2024-06-03 04:08:08 [INFO]: Epoch 086 - training loss: 0.2882, validation loss: 0.5178
2024-06-03 04:08:12 [INFO]: Epoch 087 - training loss: 0.2863, validation loss: 0.5187
2024-06-03 04:08:17 [INFO]: Epoch 088 - training loss: 0.2847, validation loss: 0.5169
2024-06-03 04:08:21 [INFO]: Epoch 089 - training loss: 0.2837, validation loss: 0.5167
2024-06-03 04:08:25 [INFO]: Epoch 090 - training loss: 0.2825, validation loss: 0.5203
2024-06-03 04:08:30 [INFO]: Epoch 091 - training loss: 0.2850, validation loss: 0.5188
2024-06-03 04:08:34 [INFO]: Epoch 092 - training loss: 0.2827, validation loss: 0.5184
2024-06-03 04:08:38 [INFO]: Epoch 093 - training loss: 0.2797, validation loss: 0.5176
2024-06-03 04:08:38 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:08:38 [INFO]: Finished training. The best model is from epoch#83.
2024-06-03 04:08:39 [INFO]: Saved the model to results_point_rate09/PeMS/PatchTST_PeMS/round_2/20240603_T040141/PatchTST.pypots
2024-06-03 04:08:40 [INFO]: Successfully saved to results_point_rate09/PeMS/PatchTST_PeMS/round_2/imputation.pkl
2024-06-03 04:08:40 [INFO]: Round2 - PatchTST on PeMS: MAE=0.4011, MSE=0.7433, MRE=0.4977
2024-06-03 04:08:40 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 04:08:40 [INFO]: Using the given device: cuda:0
2024-06-03 04:08:40 [INFO]: Model files will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_3/20240603_T040840
2024-06-03 04:08:40 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_3/20240603_T040840/tensorboard
2024-06-03 04:08:40 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 04:08:40 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 04:08:40 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 04:08:45 [INFO]: Epoch 001 - training loss: 1.2677, validation loss: 0.8997
2024-06-03 04:08:49 [INFO]: Epoch 002 - training loss: 0.9036, validation loss: 0.8817
2024-06-03 04:08:54 [INFO]: Epoch 003 - training loss: 0.7549, validation loss: 0.7798
2024-06-03 04:08:58 [INFO]: Epoch 004 - training loss: 0.6744, validation loss: 0.6869
2024-06-03 04:09:02 [INFO]: Epoch 005 - training loss: 0.6145, validation loss: 0.6333
2024-06-03 04:09:07 [INFO]: Epoch 006 - training loss: 0.5693, validation loss: 0.6044
2024-06-03 04:09:11 [INFO]: Epoch 007 - training loss: 0.5461, validation loss: 0.6123
2024-06-03 04:09:15 [INFO]: Epoch 008 - training loss: 0.5326, validation loss: 0.5887
2024-06-03 04:09:20 [INFO]: Epoch 009 - training loss: 0.5179, validation loss: 0.5696
2024-06-03 04:09:24 [INFO]: Epoch 010 - training loss: 0.5027, validation loss: 0.5578
2024-06-03 04:09:29 [INFO]: Epoch 011 - training loss: 0.4884, validation loss: 0.5523
2024-06-03 04:09:34 [INFO]: Epoch 012 - training loss: 0.4785, validation loss: 0.5663
2024-06-03 04:09:38 [INFO]: Epoch 013 - training loss: 0.4690, validation loss: 0.5531
2024-06-03 04:09:42 [INFO]: Epoch 014 - training loss: 0.4558, validation loss: 0.5465
2024-06-03 04:09:47 [INFO]: Epoch 015 - training loss: 0.4493, validation loss: 0.5515
2024-06-03 04:09:51 [INFO]: Epoch 016 - training loss: 0.4416, validation loss: 0.5468
2024-06-03 04:09:56 [INFO]: Epoch 017 - training loss: 0.4322, validation loss: 0.5437
2024-06-03 04:10:00 [INFO]: Epoch 018 - training loss: 0.4257, validation loss: 0.5462
2024-06-03 04:10:04 [INFO]: Epoch 019 - training loss: 0.4180, validation loss: 0.5350
2024-06-03 04:10:09 [INFO]: Epoch 020 - training loss: 0.4162, validation loss: 0.5393
2024-06-03 04:10:13 [INFO]: Epoch 021 - training loss: 0.4065, validation loss: 0.5411
2024-06-03 04:10:17 [INFO]: Epoch 022 - training loss: 0.4036, validation loss: 0.5374
2024-06-03 04:10:22 [INFO]: Epoch 023 - training loss: 0.3980, validation loss: 0.5321
2024-06-03 04:10:26 [INFO]: Epoch 024 - training loss: 0.3911, validation loss: 0.5335
2024-06-03 04:10:31 [INFO]: Epoch 025 - training loss: 0.3903, validation loss: 0.5376
2024-06-03 04:10:35 [INFO]: Epoch 026 - training loss: 0.3875, validation loss: 0.5386
2024-06-03 04:10:39 [INFO]: Epoch 027 - training loss: 0.3848, validation loss: 0.5379
2024-06-03 04:10:44 [INFO]: Epoch 028 - training loss: 0.3767, validation loss: 0.5343
2024-06-03 04:10:48 [INFO]: Epoch 029 - training loss: 0.3735, validation loss: 0.5310
2024-06-03 04:10:53 [INFO]: Epoch 030 - training loss: 0.3709, validation loss: 0.5291
2024-06-03 04:10:57 [INFO]: Epoch 031 - training loss: 0.3646, validation loss: 0.5311
2024-06-03 04:11:01 [INFO]: Epoch 032 - training loss: 0.3658, validation loss: 0.5367
2024-06-03 04:11:06 [INFO]: Epoch 033 - training loss: 0.3618, validation loss: 0.5312
2024-06-03 04:11:10 [INFO]: Epoch 034 - training loss: 0.3607, validation loss: 0.5279
2024-06-03 04:11:15 [INFO]: Epoch 035 - training loss: 0.3559, validation loss: 0.5313
2024-06-03 04:11:19 [INFO]: Epoch 036 - training loss: 0.3583, validation loss: 0.5264
2024-06-03 04:11:24 [INFO]: Epoch 037 - training loss: 0.3579, validation loss: 0.5272
2024-06-03 04:11:28 [INFO]: Epoch 038 - training loss: 0.3542, validation loss: 0.5288
2024-06-03 04:11:32 [INFO]: Epoch 039 - training loss: 0.3491, validation loss: 0.5295
2024-06-03 04:11:36 [INFO]: Epoch 040 - training loss: 0.3464, validation loss: 0.5341
2024-06-03 04:11:41 [INFO]: Epoch 041 - training loss: 0.3444, validation loss: 0.5301
2024-06-03 04:11:45 [INFO]: Epoch 042 - training loss: 0.3411, validation loss: 0.5306
2024-06-03 04:11:50 [INFO]: Epoch 043 - training loss: 0.3424, validation loss: 0.5272
2024-06-03 04:11:54 [INFO]: Epoch 044 - training loss: 0.3397, validation loss: 0.5285
2024-06-03 04:11:58 [INFO]: Epoch 045 - training loss: 0.3354, validation loss: 0.5296
2024-06-03 04:12:02 [INFO]: Epoch 046 - training loss: 0.3342, validation loss: 0.5243
2024-06-03 04:12:07 [INFO]: Epoch 047 - training loss: 0.3332, validation loss: 0.5300
2024-06-03 04:12:11 [INFO]: Epoch 048 - training loss: 0.3302, validation loss: 0.5284
2024-06-03 04:12:15 [INFO]: Epoch 049 - training loss: 0.3302, validation loss: 0.5291
2024-06-03 04:12:20 [INFO]: Epoch 050 - training loss: 0.3297, validation loss: 0.5254
2024-06-03 04:12:24 [INFO]: Epoch 051 - training loss: 0.3272, validation loss: 0.5253
2024-06-03 04:12:28 [INFO]: Epoch 052 - training loss: 0.3273, validation loss: 0.5298
2024-06-03 04:12:32 [INFO]: Epoch 053 - training loss: 0.3246, validation loss: 0.5286
2024-06-03 04:12:37 [INFO]: Epoch 054 - training loss: 0.3221, validation loss: 0.5342
2024-06-03 04:12:41 [INFO]: Epoch 055 - training loss: 0.3223, validation loss: 0.5280
2024-06-03 04:12:45 [INFO]: Epoch 056 - training loss: 0.3214, validation loss: 0.5268
2024-06-03 04:12:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:12:45 [INFO]: Finished training. The best model is from epoch#46.
2024-06-03 04:12:45 [INFO]: Saved the model to results_point_rate09/PeMS/PatchTST_PeMS/round_3/20240603_T040840/PatchTST.pypots
2024-06-03 04:12:47 [INFO]: Successfully saved to results_point_rate09/PeMS/PatchTST_PeMS/round_3/imputation.pkl
2024-06-03 04:12:47 [INFO]: Round3 - PatchTST on PeMS: MAE=0.4164, MSE=0.7388, MRE=0.5167
2024-06-03 04:12:47 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 04:12:47 [INFO]: Using the given device: cuda:0
2024-06-03 04:12:47 [INFO]: Model files will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_4/20240603_T041247
2024-06-03 04:12:47 [INFO]: Tensorboard file will be saved to results_point_rate09/PeMS/PatchTST_PeMS/round_4/20240603_T041247/tensorboard
2024-06-03 04:12:47 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=2, d_k=256
2024-06-03 04:12:47 [WARNING]: ⚠️ d_model is reset to 512 = n_heads (2) * d_k (256)
2024-06-03 04:12:47 [INFO]: PatchTST initialized with the given hyperparameters, the number of trainable parameters: 3,045,238
2024-06-03 04:12:52 [INFO]: Epoch 001 - training loss: 1.2864, validation loss: 0.9610
2024-06-03 04:12:56 [INFO]: Epoch 002 - training loss: 0.9099, validation loss: 0.8494
2024-06-03 04:13:01 [INFO]: Epoch 003 - training loss: 0.7591, validation loss: 0.7217
2024-06-03 04:13:05 [INFO]: Epoch 004 - training loss: 0.6693, validation loss: 0.6804
2024-06-03 04:13:09 [INFO]: Epoch 005 - training loss: 0.6192, validation loss: 0.6300
2024-06-03 04:13:14 [INFO]: Epoch 006 - training loss: 0.5821, validation loss: 0.6095
2024-06-03 04:13:19 [INFO]: Epoch 007 - training loss: 0.5613, validation loss: 0.5942
2024-06-03 04:13:23 [INFO]: Epoch 008 - training loss: 0.5432, validation loss: 0.5785
2024-06-03 04:13:27 [INFO]: Epoch 009 - training loss: 0.5195, validation loss: 0.5656
2024-06-03 04:13:32 [INFO]: Epoch 010 - training loss: 0.5162, validation loss: 0.5582
2024-06-03 04:13:36 [INFO]: Epoch 011 - training loss: 0.5003, validation loss: 0.5525
2024-06-03 04:13:40 [INFO]: Epoch 012 - training loss: 0.4838, validation loss: 0.5500
2024-06-03 04:13:45 [INFO]: Epoch 013 - training loss: 0.4766, validation loss: 0.5539
2024-06-03 04:13:49 [INFO]: Epoch 014 - training loss: 0.4717, validation loss: 0.5443
2024-06-03 04:13:53 [INFO]: Epoch 015 - training loss: 0.4536, validation loss: 0.5400
2024-06-03 04:13:58 [INFO]: Epoch 016 - training loss: 0.4515, validation loss: 0.5375
2024-06-03 04:14:02 [INFO]: Epoch 017 - training loss: 0.4422, validation loss: 0.5365
2024-06-03 04:14:07 [INFO]: Epoch 018 - training loss: 0.4368, validation loss: 0.5378
2024-06-03 04:14:11 [INFO]: Epoch 019 - training loss: 0.4295, validation loss: 0.5303
2024-06-03 04:14:16 [INFO]: Epoch 020 - training loss: 0.4227, validation loss: 0.5322
2024-06-03 04:14:20 [INFO]: Epoch 021 - training loss: 0.4199, validation loss: 0.5326
2024-06-03 04:14:25 [INFO]: Epoch 022 - training loss: 0.4147, validation loss: 0.5337
2024-06-03 04:14:29 [INFO]: Epoch 023 - training loss: 0.4089, validation loss: 0.5343
2024-06-03 04:14:34 [INFO]: Epoch 024 - training loss: 0.4055, validation loss: 0.5295
2024-06-03 04:14:38 [INFO]: Epoch 025 - training loss: 0.4043, validation loss: 0.5301
2024-06-03 04:14:43 [INFO]: Epoch 026 - training loss: 0.3978, validation loss: 0.5301
2024-06-03 04:14:47 [INFO]: Epoch 027 - training loss: 0.3947, validation loss: 0.5307
2024-06-03 04:14:51 [INFO]: Epoch 028 - training loss: 0.3896, validation loss: 0.5293
2024-06-03 04:14:56 [INFO]: Epoch 029 - training loss: 0.3883, validation loss: 0.5232
2024-06-03 04:15:00 [INFO]: Epoch 030 - training loss: 0.3810, validation loss: 0.5329
2024-06-03 04:15:04 [INFO]: Epoch 031 - training loss: 0.3773, validation loss: 0.5217
2024-06-03 04:15:09 [INFO]: Epoch 032 - training loss: 0.3740, validation loss: 0.5250
2024-06-03 04:15:13 [INFO]: Epoch 033 - training loss: 0.3693, validation loss: 0.5214
2024-06-03 04:15:18 [INFO]: Epoch 034 - training loss: 0.3715, validation loss: 0.5246
2024-06-03 04:15:22 [INFO]: Epoch 035 - training loss: 0.3657, validation loss: 0.5225
2024-06-03 04:15:27 [INFO]: Epoch 036 - training loss: 0.3613, validation loss: 0.5140
2024-06-03 04:15:31 [INFO]: Epoch 037 - training loss: 0.3613, validation loss: 0.5295
2024-06-03 04:15:35 [INFO]: Epoch 038 - training loss: 0.3557, validation loss: 0.5130
2024-06-03 04:15:40 [INFO]: Epoch 039 - training loss: 0.3506, validation loss: 0.5246
2024-06-03 04:15:44 [INFO]: Epoch 040 - training loss: 0.3554, validation loss: 0.5142
2024-06-03 04:15:48 [INFO]: Epoch 041 - training loss: 0.3501, validation loss: 0.5185
2024-06-03 04:15:53 [INFO]: Epoch 042 - training loss: 0.3456, validation loss: 0.5123
2024-06-03 04:15:57 [INFO]: Epoch 043 - training loss: 0.3448, validation loss: 0.5181
2024-06-03 04:16:02 [INFO]: Epoch 044 - training loss: 0.3425, validation loss: 0.5145
2024-06-03 04:16:05 [INFO]: Epoch 045 - training loss: 0.3421, validation loss: 0.5162
2024-06-03 04:16:10 [INFO]: Epoch 046 - training loss: 0.3380, validation loss: 0.5121
2024-06-03 04:16:14 [INFO]: Epoch 047 - training loss: 0.3376, validation loss: 0.5146
2024-06-03 04:16:18 [INFO]: Epoch 048 - training loss: 0.3339, validation loss: 0.5138
2024-06-03 04:16:23 [INFO]: Epoch 049 - training loss: 0.3302, validation loss: 0.5164
2024-06-03 04:16:27 [INFO]: Epoch 050 - training loss: 0.3334, validation loss: 0.5158
2024-06-03 04:16:31 [INFO]: Epoch 051 - training loss: 0.3305, validation loss: 0.5178
2024-06-03 04:16:35 [INFO]: Epoch 052 - training loss: 0.3316, validation loss: 0.5165
2024-06-03 04:16:39 [INFO]: Epoch 053 - training loss: 0.3290, validation loss: 0.5149
2024-06-03 04:16:44 [INFO]: Epoch 054 - training loss: 0.3255, validation loss: 0.5136
2024-06-03 04:16:48 [INFO]: Epoch 055 - training loss: 0.3251, validation loss: 0.5106
2024-06-03 04:16:52 [INFO]: Epoch 056 - training loss: 0.3259, validation loss: 0.5127
2024-06-03 04:16:57 [INFO]: Epoch 057 - training loss: 0.3192, validation loss: 0.5121
2024-06-03 04:17:01 [INFO]: Epoch 058 - training loss: 0.3209, validation loss: 0.5089
2024-06-03 04:17:05 [INFO]: Epoch 059 - training loss: 0.3171, validation loss: 0.5070
2024-06-03 04:17:09 [INFO]: Epoch 060 - training loss: 0.3155, validation loss: 0.5044
2024-06-03 04:17:14 [INFO]: Epoch 061 - training loss: 0.3181, validation loss: 0.5079
2024-06-03 04:17:18 [INFO]: Epoch 062 - training loss: 0.3154, validation loss: 0.5147
2024-06-03 04:17:23 [INFO]: Epoch 063 - training loss: 0.3115, validation loss: 0.5103
2024-06-03 04:17:27 [INFO]: Epoch 064 - training loss: 0.3116, validation loss: 0.5058
2024-06-03 04:17:32 [INFO]: Epoch 065 - training loss: 0.3110, validation loss: 0.5037
2024-06-03 04:17:36 [INFO]: Epoch 066 - training loss: 0.3109, validation loss: 0.5098
2024-06-03 04:17:41 [INFO]: Epoch 067 - training loss: 0.3096, validation loss: 0.5065
2024-06-03 04:17:45 [INFO]: Epoch 068 - training loss: 0.3064, validation loss: 0.5055
2024-06-03 04:17:49 [INFO]: Epoch 069 - training loss: 0.3047, validation loss: 0.5064
2024-06-03 04:17:54 [INFO]: Epoch 070 - training loss: 0.3037, validation loss: 0.5067
2024-06-03 04:17:58 [INFO]: Epoch 071 - training loss: 0.3024, validation loss: 0.5080
2024-06-03 04:18:03 [INFO]: Epoch 072 - training loss: 0.3010, validation loss: 0.5036
2024-06-03 04:18:07 [INFO]: Epoch 073 - training loss: 0.2998, validation loss: 0.5053
2024-06-03 04:18:11 [INFO]: Epoch 074 - training loss: 0.2977, validation loss: 0.5082
2024-06-03 04:18:15 [INFO]: Epoch 075 - training loss: 0.3000, validation loss: 0.5073
2024-06-03 04:18:20 [INFO]: Epoch 076 - training loss: 0.2989, validation loss: 0.5077
2024-06-03 04:18:24 [INFO]: Epoch 077 - training loss: 0.2965, validation loss: 0.5087
2024-06-03 04:18:28 [INFO]: Epoch 078 - training loss: 0.2947, validation loss: 0.5086
2024-06-03 04:18:32 [INFO]: Epoch 079 - training loss: 0.2964, validation loss: 0.5070
2024-06-03 04:18:37 [INFO]: Epoch 080 - training loss: 0.2939, validation loss: 0.5038
2024-06-03 04:18:41 [INFO]: Epoch 081 - training loss: 0.2943, validation loss: 0.5019
2024-06-03 04:18:45 [INFO]: Epoch 082 - training loss: 0.2949, validation loss: 0.5061
2024-06-03 04:18:50 [INFO]: Epoch 083 - training loss: 0.2943, validation loss: 0.5051
2024-06-03 04:18:54 [INFO]: Epoch 084 - training loss: 0.2900, validation loss: 0.5045
2024-06-03 04:18:58 [INFO]: Epoch 085 - training loss: 0.2899, validation loss: 0.5072
2024-06-03 04:19:03 [INFO]: Epoch 086 - training loss: 0.2905, validation loss: 0.5017
2024-06-03 04:19:07 [INFO]: Epoch 087 - training loss: 0.2910, validation loss: 0.5028
2024-06-03 04:19:12 [INFO]: Epoch 088 - training loss: 0.2911, validation loss: 0.5037
2024-06-03 04:19:16 [INFO]: Epoch 089 - training loss: 0.2883, validation loss: 0.5048
2024-06-03 04:19:20 [INFO]: Epoch 090 - training loss: 0.2855, validation loss: 0.5031
2024-06-03 04:19:25 [INFO]: Epoch 091 - training loss: 0.2832, validation loss: 0.5052
2024-06-03 04:19:29 [INFO]: Epoch 092 - training loss: 0.2865, validation loss: 0.5064
2024-06-03 04:19:34 [INFO]: Epoch 093 - training loss: 0.2829, validation loss: 0.5057
2024-06-03 04:19:38 [INFO]: Epoch 094 - training loss: 0.2828, validation loss: 0.5007
2024-06-03 04:19:42 [INFO]: Epoch 095 - training loss: 0.2865, validation loss: 0.5032
2024-06-03 04:19:47 [INFO]: Epoch 096 - training loss: 0.2852, validation loss: 0.5034
2024-06-03 04:19:51 [INFO]: Epoch 097 - training loss: 0.2826, validation loss: 0.5044
2024-06-03 04:19:55 [INFO]: Epoch 098 - training loss: 0.2800, validation loss: 0.5064
2024-06-03 04:20:00 [INFO]: Epoch 099 - training loss: 0.2814, validation loss: 0.5037
2024-06-03 04:20:04 [INFO]: Epoch 100 - training loss: 0.2786, validation loss: 0.5038
2024-06-03 04:20:04 [INFO]: Finished training. The best model is from epoch#94.
2024-06-03 04:20:04 [INFO]: Saved the model to results_point_rate09/PeMS/PatchTST_PeMS/round_4/20240603_T041247/PatchTST.pypots
2024-06-03 04:20:06 [INFO]: Successfully saved to results_point_rate09/PeMS/PatchTST_PeMS/round_4/imputation.pkl
2024-06-03 04:20:06 [INFO]: Round4 - PatchTST on PeMS: MAE=0.3986, MSE=0.7093, MRE=0.4946
2024-06-03 04:20:06 [INFO]: Done! Final results:
Averaged PatchTST (3,045,238 params) on PeMS: MAE=0.4397 ± 0.05702022164831945, MSE=0.7927 ± 0.10733268048775024, MRE=0.5456 ± 0.0707524528862465, average inference time=0.34
