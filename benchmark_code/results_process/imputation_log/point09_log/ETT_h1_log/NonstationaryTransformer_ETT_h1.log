2024-06-03 00:41:51 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 00:41:51 [INFO]: Using the given device: cuda:0
2024-06-03 00:41:53 [INFO]: Model files will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/20240603_T004153
2024-06-03 00:41:53 [INFO]: Tensorboard file will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/20240603_T004153/tensorboard
2024-06-03 00:41:54 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 00:41:59 [INFO]: Epoch 001 - training loss: 1.0747, validation loss: 0.5754
2024-06-03 00:41:59 [INFO]: Epoch 002 - training loss: 0.7764, validation loss: 0.5850
2024-06-03 00:42:00 [INFO]: Epoch 003 - training loss: 0.7731, validation loss: 0.4725
2024-06-03 00:42:01 [INFO]: Epoch 004 - training loss: 0.6962, validation loss: 0.4605
2024-06-03 00:42:01 [INFO]: Epoch 005 - training loss: 0.6884, validation loss: 0.4074
2024-06-03 00:42:02 [INFO]: Epoch 006 - training loss: 0.7014, validation loss: 0.3961
2024-06-03 00:42:02 [INFO]: Epoch 007 - training loss: 0.6714, validation loss: 0.4066
2024-06-03 00:42:03 [INFO]: Epoch 008 - training loss: 0.6785, validation loss: 0.3891
2024-06-03 00:42:03 [INFO]: Epoch 009 - training loss: 0.6709, validation loss: 0.3782
2024-06-03 00:42:04 [INFO]: Epoch 010 - training loss: 0.6658, validation loss: 0.3791
2024-06-03 00:42:04 [INFO]: Epoch 011 - training loss: 0.6706, validation loss: 0.3786
2024-06-03 00:42:05 [INFO]: Epoch 012 - training loss: 0.6617, validation loss: 0.3992
2024-06-03 00:42:05 [INFO]: Epoch 013 - training loss: 0.6646, validation loss: 0.3742
2024-06-03 00:42:06 [INFO]: Epoch 014 - training loss: 0.6422, validation loss: 0.3694
2024-06-03 00:42:06 [INFO]: Epoch 015 - training loss: 0.6634, validation loss: 0.3844
2024-06-03 00:42:07 [INFO]: Epoch 016 - training loss: 0.6797, validation loss: 0.3843
2024-06-03 00:42:07 [INFO]: Epoch 017 - training loss: 0.6473, validation loss: 0.3846
2024-06-03 00:42:08 [INFO]: Epoch 018 - training loss: 0.6544, validation loss: 0.3814
2024-06-03 00:42:08 [INFO]: Epoch 019 - training loss: 0.6563, validation loss: 0.3683
2024-06-03 00:42:09 [INFO]: Epoch 020 - training loss: 0.6338, validation loss: 0.3865
2024-06-03 00:42:09 [INFO]: Epoch 021 - training loss: 0.6583, validation loss: 0.3948
2024-06-03 00:42:10 [INFO]: Epoch 022 - training loss: 0.6455, validation loss: 0.4100
2024-06-03 00:42:10 [INFO]: Epoch 023 - training loss: 0.6504, validation loss: 0.3784
2024-06-03 00:42:11 [INFO]: Epoch 024 - training loss: 0.6471, validation loss: 0.3757
2024-06-03 00:42:11 [INFO]: Epoch 025 - training loss: 0.6225, validation loss: 0.3917
2024-06-03 00:42:12 [INFO]: Epoch 026 - training loss: 0.6409, validation loss: 0.3877
2024-06-03 00:42:13 [INFO]: Epoch 027 - training loss: 0.6604, validation loss: 0.3944
2024-06-03 00:42:13 [INFO]: Epoch 028 - training loss: 0.6561, validation loss: 0.3777
2024-06-03 00:42:14 [INFO]: Epoch 029 - training loss: 0.6329, validation loss: 0.3708
2024-06-03 00:42:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:42:14 [INFO]: Finished training. The best model is from epoch#19.
2024-06-03 00:42:14 [INFO]: Saved the model to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/20240603_T004153/NonstationaryTransformer.pypots
2024-06-03 00:42:14 [INFO]: Successfully saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/imputation.pkl
2024-06-03 00:42:14 [INFO]: Round0 - NonstationaryTransformer on ETT_h1: MAE=0.5212, MSE=0.6129, MRE=0.6134
2024-06-03 00:42:14 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 00:42:14 [INFO]: Using the given device: cuda:0
2024-06-03 00:42:14 [INFO]: Model files will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/20240603_T004214
2024-06-03 00:42:14 [INFO]: Tensorboard file will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/20240603_T004214/tensorboard
2024-06-03 00:42:14 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 00:42:15 [INFO]: Epoch 001 - training loss: 0.8840, validation loss: 0.5968
2024-06-03 00:42:15 [INFO]: Epoch 002 - training loss: 0.7468, validation loss: 0.4833
2024-06-03 00:42:16 [INFO]: Epoch 003 - training loss: 0.7221, validation loss: 0.4116
2024-06-03 00:42:16 [INFO]: Epoch 004 - training loss: 0.7016, validation loss: 0.3994
2024-06-03 00:42:17 [INFO]: Epoch 005 - training loss: 0.6754, validation loss: 0.3880
2024-06-03 00:42:17 [INFO]: Epoch 006 - training loss: 0.6634, validation loss: 0.3883
2024-06-03 00:42:18 [INFO]: Epoch 007 - training loss: 0.6583, validation loss: 0.4005
2024-06-03 00:42:19 [INFO]: Epoch 008 - training loss: 0.6665, validation loss: 0.3986
2024-06-03 00:42:19 [INFO]: Epoch 009 - training loss: 0.6661, validation loss: 0.3970
2024-06-03 00:42:20 [INFO]: Epoch 010 - training loss: 0.6570, validation loss: 0.3885
2024-06-03 00:42:20 [INFO]: Epoch 011 - training loss: 0.6602, validation loss: 0.3819
2024-06-03 00:42:21 [INFO]: Epoch 012 - training loss: 0.6428, validation loss: 0.3841
2024-06-03 00:42:21 [INFO]: Epoch 013 - training loss: 0.6689, validation loss: 0.3847
2024-06-03 00:42:22 [INFO]: Epoch 014 - training loss: 0.6447, validation loss: 0.3777
2024-06-03 00:42:22 [INFO]: Epoch 015 - training loss: 0.6616, validation loss: 0.3802
2024-06-03 00:42:23 [INFO]: Epoch 016 - training loss: 0.6585, validation loss: 0.3710
2024-06-03 00:42:23 [INFO]: Epoch 017 - training loss: 0.6514, validation loss: 0.3869
2024-06-03 00:42:24 [INFO]: Epoch 018 - training loss: 0.6419, validation loss: 0.3774
2024-06-03 00:42:24 [INFO]: Epoch 019 - training loss: 0.6313, validation loss: 0.3754
2024-06-03 00:42:25 [INFO]: Epoch 020 - training loss: 0.6567, validation loss: 0.3813
2024-06-03 00:42:25 [INFO]: Epoch 021 - training loss: 0.6306, validation loss: 0.3826
2024-06-03 00:42:26 [INFO]: Epoch 022 - training loss: 0.6286, validation loss: 0.3739
2024-06-03 00:42:26 [INFO]: Epoch 023 - training loss: 0.6279, validation loss: 0.3846
2024-06-03 00:42:27 [INFO]: Epoch 024 - training loss: 0.6466, validation loss: 0.3809
2024-06-03 00:42:27 [INFO]: Epoch 025 - training loss: 0.6649, validation loss: 0.3805
2024-06-03 00:42:28 [INFO]: Epoch 026 - training loss: 0.6541, validation loss: 0.3720
2024-06-03 00:42:28 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:42:28 [INFO]: Finished training. The best model is from epoch#16.
2024-06-03 00:42:28 [INFO]: Saved the model to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/20240603_T004214/NonstationaryTransformer.pypots
2024-06-03 00:42:28 [INFO]: Successfully saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/imputation.pkl
2024-06-03 00:42:28 [INFO]: Round1 - NonstationaryTransformer on ETT_h1: MAE=0.5214, MSE=0.6125, MRE=0.6137
2024-06-03 00:42:28 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 00:42:28 [INFO]: Using the given device: cuda:0
2024-06-03 00:42:28 [INFO]: Model files will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/20240603_T004228
2024-06-03 00:42:28 [INFO]: Tensorboard file will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/20240603_T004228/tensorboard
2024-06-03 00:42:28 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 00:42:29 [INFO]: Epoch 001 - training loss: 1.0147, validation loss: 0.5008
2024-06-03 00:42:29 [INFO]: Epoch 002 - training loss: 0.7792, validation loss: 0.4727
2024-06-03 00:42:30 [INFO]: Epoch 003 - training loss: 0.7374, validation loss: 0.4230
2024-06-03 00:42:30 [INFO]: Epoch 004 - training loss: 0.7119, validation loss: 0.4309
2024-06-03 00:42:31 [INFO]: Epoch 005 - training loss: 0.7051, validation loss: 0.4108
2024-06-03 00:42:31 [INFO]: Epoch 006 - training loss: 0.6520, validation loss: 0.4051
2024-06-03 00:42:32 [INFO]: Epoch 007 - training loss: 0.6795, validation loss: 0.3919
2024-06-03 00:42:32 [INFO]: Epoch 008 - training loss: 0.6709, validation loss: 0.3963
2024-06-03 00:42:33 [INFO]: Epoch 009 - training loss: 0.6817, validation loss: 0.3829
2024-06-03 00:42:34 [INFO]: Epoch 010 - training loss: 0.6667, validation loss: 0.3761
2024-06-03 00:42:34 [INFO]: Epoch 011 - training loss: 0.6593, validation loss: 0.3977
2024-06-03 00:42:35 [INFO]: Epoch 012 - training loss: 0.6751, validation loss: 0.3902
2024-06-03 00:42:35 [INFO]: Epoch 013 - training loss: 0.6650, validation loss: 0.3866
2024-06-03 00:42:36 [INFO]: Epoch 014 - training loss: 0.6469, validation loss: 0.3818
2024-06-03 00:42:36 [INFO]: Epoch 015 - training loss: 0.6499, validation loss: 0.3799
2024-06-03 00:42:37 [INFO]: Epoch 016 - training loss: 0.6498, validation loss: 0.3889
2024-06-03 00:42:37 [INFO]: Epoch 017 - training loss: 0.6455, validation loss: 0.3859
2024-06-03 00:42:38 [INFO]: Epoch 018 - training loss: 0.6602, validation loss: 0.3833
2024-06-03 00:42:38 [INFO]: Epoch 019 - training loss: 0.6541, validation loss: 0.3758
2024-06-03 00:42:39 [INFO]: Epoch 020 - training loss: 0.6441, validation loss: 0.3762
2024-06-03 00:42:39 [INFO]: Epoch 021 - training loss: 0.6372, validation loss: 0.3751
2024-06-03 00:42:40 [INFO]: Epoch 022 - training loss: 0.6491, validation loss: 0.3843
2024-06-03 00:42:40 [INFO]: Epoch 023 - training loss: 0.6421, validation loss: 0.3755
2024-06-03 00:42:41 [INFO]: Epoch 024 - training loss: 0.6489, validation loss: 0.3638
2024-06-03 00:42:41 [INFO]: Epoch 025 - training loss: 0.6447, validation loss: 0.3632
2024-06-03 00:42:41 [INFO]: Epoch 026 - training loss: 0.6358, validation loss: 0.3769
2024-06-03 00:42:42 [INFO]: Epoch 027 - training loss: 0.6636, validation loss: 0.3786
2024-06-03 00:42:42 [INFO]: Epoch 028 - training loss: 0.6474, validation loss: 0.3710
2024-06-03 00:42:43 [INFO]: Epoch 029 - training loss: 0.6606, validation loss: 0.3709
2024-06-03 00:42:43 [INFO]: Epoch 030 - training loss: 0.6591, validation loss: 0.3789
2024-06-03 00:42:44 [INFO]: Epoch 031 - training loss: 0.6225, validation loss: 0.3695
2024-06-03 00:42:44 [INFO]: Epoch 032 - training loss: 0.6431, validation loss: 0.3678
2024-06-03 00:42:45 [INFO]: Epoch 033 - training loss: 0.6500, validation loss: 0.3839
2024-06-03 00:42:45 [INFO]: Epoch 034 - training loss: 0.6443, validation loss: 0.3668
2024-06-03 00:42:46 [INFO]: Epoch 035 - training loss: 0.6240, validation loss: 0.3781
2024-06-03 00:42:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:42:46 [INFO]: Finished training. The best model is from epoch#25.
2024-06-03 00:42:46 [INFO]: Saved the model to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/20240603_T004228/NonstationaryTransformer.pypots
2024-06-03 00:42:46 [INFO]: Successfully saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/imputation.pkl
2024-06-03 00:42:46 [INFO]: Round2 - NonstationaryTransformer on ETT_h1: MAE=0.5345, MSE=0.6448, MRE=0.6290
2024-06-03 00:42:46 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 00:42:46 [INFO]: Using the given device: cuda:0
2024-06-03 00:42:46 [INFO]: Model files will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/20240603_T004246
2024-06-03 00:42:46 [INFO]: Tensorboard file will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/20240603_T004246/tensorboard
2024-06-03 00:42:46 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 00:42:47 [INFO]: Epoch 001 - training loss: 1.0602, validation loss: 0.5887
2024-06-03 00:42:47 [INFO]: Epoch 002 - training loss: 0.8038, validation loss: 0.4816
2024-06-03 00:42:48 [INFO]: Epoch 003 - training loss: 0.7601, validation loss: 0.4511
2024-06-03 00:42:48 [INFO]: Epoch 004 - training loss: 0.7226, validation loss: 0.4239
2024-06-03 00:42:49 [INFO]: Epoch 005 - training loss: 0.6804, validation loss: 0.4072
2024-06-03 00:42:49 [INFO]: Epoch 006 - training loss: 0.6733, validation loss: 0.3962
2024-06-03 00:42:50 [INFO]: Epoch 007 - training loss: 0.7053, validation loss: 0.4014
2024-06-03 00:42:50 [INFO]: Epoch 008 - training loss: 0.6757, validation loss: 0.3792
2024-06-03 00:42:51 [INFO]: Epoch 009 - training loss: 0.6617, validation loss: 0.4025
2024-06-03 00:42:51 [INFO]: Epoch 010 - training loss: 0.6643, validation loss: 0.3790
2024-06-03 00:42:52 [INFO]: Epoch 011 - training loss: 0.6676, validation loss: 0.4055
2024-06-03 00:42:52 [INFO]: Epoch 012 - training loss: 0.6677, validation loss: 0.3864
2024-06-03 00:42:52 [INFO]: Epoch 013 - training loss: 0.6548, validation loss: 0.3901
2024-06-03 00:42:53 [INFO]: Epoch 014 - training loss: 0.6693, validation loss: 0.3720
2024-06-03 00:42:53 [INFO]: Epoch 015 - training loss: 0.6475, validation loss: 0.3826
2024-06-03 00:42:54 [INFO]: Epoch 016 - training loss: 0.6648, validation loss: 0.3807
2024-06-03 00:42:55 [INFO]: Epoch 017 - training loss: 0.6451, validation loss: 0.3795
2024-06-03 00:42:55 [INFO]: Epoch 018 - training loss: 0.6655, validation loss: 0.3755
2024-06-03 00:42:56 [INFO]: Epoch 019 - training loss: 0.6422, validation loss: 0.3661
2024-06-03 00:42:56 [INFO]: Epoch 020 - training loss: 0.6685, validation loss: 0.3743
2024-06-03 00:42:57 [INFO]: Epoch 021 - training loss: 0.6643, validation loss: 0.3790
2024-06-03 00:42:57 [INFO]: Epoch 022 - training loss: 0.6532, validation loss: 0.3895
2024-06-03 00:42:58 [INFO]: Epoch 023 - training loss: 0.6423, validation loss: 0.3822
2024-06-03 00:42:58 [INFO]: Epoch 024 - training loss: 0.6682, validation loss: 0.3787
2024-06-03 00:42:59 [INFO]: Epoch 025 - training loss: 0.6536, validation loss: 0.3750
2024-06-03 00:42:59 [INFO]: Epoch 026 - training loss: 0.6271, validation loss: 0.3801
2024-06-03 00:43:00 [INFO]: Epoch 027 - training loss: 0.6462, validation loss: 0.3672
2024-06-03 00:43:00 [INFO]: Epoch 028 - training loss: 0.6474, validation loss: 0.3675
2024-06-03 00:43:01 [INFO]: Epoch 029 - training loss: 0.6287, validation loss: 0.3799
2024-06-03 00:43:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:43:01 [INFO]: Finished training. The best model is from epoch#19.
2024-06-03 00:43:01 [INFO]: Saved the model to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/20240603_T004246/NonstationaryTransformer.pypots
2024-06-03 00:43:01 [INFO]: Successfully saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/imputation.pkl
2024-06-03 00:43:01 [INFO]: Round3 - NonstationaryTransformer on ETT_h1: MAE=0.5290, MSE=0.6352, MRE=0.6226
2024-06-03 00:43:01 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 00:43:01 [INFO]: Using the given device: cuda:0
2024-06-03 00:43:01 [INFO]: Model files will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/20240603_T004301
2024-06-03 00:43:01 [INFO]: Tensorboard file will be saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/20240603_T004301/tensorboard
2024-06-03 00:43:01 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 00:43:02 [INFO]: Epoch 001 - training loss: 0.8959, validation loss: 0.5399
2024-06-03 00:43:02 [INFO]: Epoch 002 - training loss: 0.7555, validation loss: 0.4576
2024-06-03 00:43:03 [INFO]: Epoch 003 - training loss: 0.6998, validation loss: 0.4257
2024-06-03 00:43:03 [INFO]: Epoch 004 - training loss: 0.6785, validation loss: 0.4035
2024-06-03 00:43:04 [INFO]: Epoch 005 - training loss: 0.6565, validation loss: 0.4019
2024-06-03 00:43:05 [INFO]: Epoch 006 - training loss: 0.6808, validation loss: 0.4095
2024-06-03 00:43:05 [INFO]: Epoch 007 - training loss: 0.6631, validation loss: 0.3981
2024-06-03 00:43:06 [INFO]: Epoch 008 - training loss: 0.6704, validation loss: 0.3961
2024-06-03 00:43:06 [INFO]: Epoch 009 - training loss: 0.6690, validation loss: 0.3976
2024-06-03 00:43:07 [INFO]: Epoch 010 - training loss: 0.6722, validation loss: 0.3905
2024-06-03 00:43:07 [INFO]: Epoch 011 - training loss: 0.6630, validation loss: 0.3843
2024-06-03 00:43:08 [INFO]: Epoch 012 - training loss: 0.6470, validation loss: 0.3684
2024-06-03 00:43:08 [INFO]: Epoch 013 - training loss: 0.6618, validation loss: 0.3886
2024-06-03 00:43:09 [INFO]: Epoch 014 - training loss: 0.6475, validation loss: 0.3844
2024-06-03 00:43:09 [INFO]: Epoch 015 - training loss: 0.6529, validation loss: 0.3677
2024-06-03 00:43:10 [INFO]: Epoch 016 - training loss: 0.6502, validation loss: 0.3661
2024-06-03 00:43:10 [INFO]: Epoch 017 - training loss: 0.6498, validation loss: 0.3714
2024-06-03 00:43:11 [INFO]: Epoch 018 - training loss: 0.6466, validation loss: 0.3732
2024-06-03 00:43:11 [INFO]: Epoch 019 - training loss: 0.6155, validation loss: 0.3812
2024-06-03 00:43:12 [INFO]: Epoch 020 - training loss: 0.6521, validation loss: 0.3887
2024-06-03 00:43:13 [INFO]: Epoch 021 - training loss: 0.6494, validation loss: 0.3758
2024-06-03 00:43:13 [INFO]: Epoch 022 - training loss: 0.6526, validation loss: 0.3783
2024-06-03 00:43:13 [INFO]: Epoch 023 - training loss: 0.6451, validation loss: 0.3786
2024-06-03 00:43:14 [INFO]: Epoch 024 - training loss: 0.6511, validation loss: 0.3799
2024-06-03 00:43:14 [INFO]: Epoch 025 - training loss: 0.6282, validation loss: 0.3755
2024-06-03 00:43:15 [INFO]: Epoch 026 - training loss: 0.6462, validation loss: 0.3836
2024-06-03 00:43:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 00:43:15 [INFO]: Finished training. The best model is from epoch#16.
2024-06-03 00:43:15 [INFO]: Saved the model to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/20240603_T004301/NonstationaryTransformer.pypots
2024-06-03 00:43:15 [INFO]: Successfully saved to results_point_rate09/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/imputation.pkl
2024-06-03 00:43:15 [INFO]: Round4 - NonstationaryTransformer on ETT_h1: MAE=0.5237, MSE=0.6168, MRE=0.6163
2024-06-03 00:43:15 [INFO]: Done! Final results:
Averaged NonstationaryTransformer (589,927 params) on ETT_h1: MAE=0.5260 ± 0.005113718334071019, MSE=0.6244 ± 0.013159095258299898, MRE=0.6190 ± 0.006018276767874039, average inference time=0.08
