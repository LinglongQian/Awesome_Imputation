2024-06-03 02:27:52 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 02:27:52 [INFO]: Using the given device: cuda:0
2024-06-03 02:27:52 [INFO]: Model files will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_0/20240603_T022752
2024-06-03 02:27:52 [INFO]: Tensorboard file will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_0/20240603_T022752/tensorboard
2024-06-03 02:27:53 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 401,415
2024-06-03 02:28:02 [INFO]: Epoch 001 - training loss: 1.2011, validation loss: 0.9610
2024-06-03 02:28:03 [INFO]: Epoch 002 - training loss: 0.8284, validation loss: 0.9404
2024-06-03 02:28:05 [INFO]: Epoch 003 - training loss: 0.8421, validation loss: 0.9311
2024-06-03 02:28:06 [INFO]: Epoch 004 - training loss: 0.7849, validation loss: 0.9245
2024-06-03 02:28:08 [INFO]: Epoch 005 - training loss: 0.7687, validation loss: 0.9204
2024-06-03 02:28:09 [INFO]: Epoch 006 - training loss: 0.7782, validation loss: 0.9175
2024-06-03 02:28:10 [INFO]: Epoch 007 - training loss: 0.7599, validation loss: 0.9158
2024-06-03 02:28:12 [INFO]: Epoch 008 - training loss: 0.7519, validation loss: 0.9142
2024-06-03 02:28:13 [INFO]: Epoch 009 - training loss: 0.7304, validation loss: 0.9130
2024-06-03 02:28:15 [INFO]: Epoch 010 - training loss: 0.7242, validation loss: 0.9120
2024-06-03 02:28:16 [INFO]: Epoch 011 - training loss: 0.7416, validation loss: 0.9115
2024-06-03 02:28:17 [INFO]: Epoch 012 - training loss: 0.7313, validation loss: 0.9107
2024-06-03 02:28:19 [INFO]: Epoch 013 - training loss: 0.7367, validation loss: 0.9103
2024-06-03 02:28:20 [INFO]: Epoch 014 - training loss: 0.7388, validation loss: 0.9100
2024-06-03 02:28:22 [INFO]: Epoch 015 - training loss: 0.7442, validation loss: 0.9095
2024-06-03 02:28:23 [INFO]: Epoch 016 - training loss: 0.7418, validation loss: 0.9092
2024-06-03 02:28:25 [INFO]: Epoch 017 - training loss: 0.7335, validation loss: 0.9089
2024-06-03 02:28:26 [INFO]: Epoch 018 - training loss: 0.7348, validation loss: 0.9087
2024-06-03 02:28:28 [INFO]: Epoch 019 - training loss: 0.7198, validation loss: 0.9085
2024-06-03 02:28:29 [INFO]: Epoch 020 - training loss: 0.7464, validation loss: 0.9084
2024-06-03 02:28:30 [INFO]: Epoch 021 - training loss: 0.7475, validation loss: 0.9082
2024-06-03 02:28:32 [INFO]: Epoch 022 - training loss: 0.7180, validation loss: 0.9081
2024-06-03 02:28:33 [INFO]: Epoch 023 - training loss: 0.7152, validation loss: 0.9080
2024-06-03 02:28:34 [INFO]: Epoch 024 - training loss: 0.7309, validation loss: 0.9078
2024-06-03 02:28:36 [INFO]: Epoch 025 - training loss: 0.7417, validation loss: 0.9077
2024-06-03 02:28:37 [INFO]: Epoch 026 - training loss: 0.7360, validation loss: 0.9076
2024-06-03 02:28:39 [INFO]: Epoch 027 - training loss: 0.7198, validation loss: 0.9075
2024-06-03 02:28:40 [INFO]: Epoch 028 - training loss: 0.7147, validation loss: 0.9075
2024-06-03 02:28:42 [INFO]: Epoch 029 - training loss: 0.7208, validation loss: 0.9074
2024-06-03 02:28:43 [INFO]: Epoch 030 - training loss: 0.7268, validation loss: 0.9073
2024-06-03 02:28:44 [INFO]: Epoch 031 - training loss: 0.7303, validation loss: 0.9073
2024-06-03 02:28:46 [INFO]: Epoch 032 - training loss: 0.7294, validation loss: 0.9072
2024-06-03 02:28:47 [INFO]: Epoch 033 - training loss: 0.7302, validation loss: 0.9071
2024-06-03 02:28:48 [INFO]: Epoch 034 - training loss: 0.7260, validation loss: 0.9071
2024-06-03 02:28:50 [INFO]: Epoch 035 - training loss: 0.7195, validation loss: 0.9070
2024-06-03 02:28:51 [INFO]: Epoch 036 - training loss: 0.7247, validation loss: 0.9070
2024-06-03 02:28:53 [INFO]: Epoch 037 - training loss: 0.7404, validation loss: 0.9069
2024-06-03 02:28:54 [INFO]: Epoch 038 - training loss: 0.7187, validation loss: 0.9069
2024-06-03 02:28:55 [INFO]: Epoch 039 - training loss: 0.7212, validation loss: 0.9069
2024-06-03 02:28:57 [INFO]: Epoch 040 - training loss: 0.7177, validation loss: 0.9068
2024-06-03 02:28:58 [INFO]: Epoch 041 - training loss: 0.7198, validation loss: 0.9068
2024-06-03 02:29:00 [INFO]: Epoch 042 - training loss: 0.7136, validation loss: 0.9068
2024-06-03 02:29:01 [INFO]: Epoch 043 - training loss: 0.7249, validation loss: 0.9068
2024-06-03 02:29:02 [INFO]: Epoch 044 - training loss: 0.7231, validation loss: 0.9067
2024-06-03 02:29:03 [INFO]: Epoch 045 - training loss: 0.7292, validation loss: 0.9067
2024-06-03 02:29:05 [INFO]: Epoch 046 - training loss: 0.7149, validation loss: 0.9067
2024-06-03 02:29:06 [INFO]: Epoch 047 - training loss: 0.7445, validation loss: 0.9067
2024-06-03 02:29:07 [INFO]: Epoch 048 - training loss: 0.7314, validation loss: 0.9066
2024-06-03 02:29:09 [INFO]: Epoch 049 - training loss: 0.7230, validation loss: 0.9066
2024-06-03 02:29:10 [INFO]: Epoch 050 - training loss: 0.7286, validation loss: 0.9066
2024-06-03 02:29:11 [INFO]: Epoch 051 - training loss: 0.7035, validation loss: 0.9066
2024-06-03 02:29:12 [INFO]: Epoch 052 - training loss: 0.7233, validation loss: 0.9066
2024-06-03 02:29:14 [INFO]: Epoch 053 - training loss: 0.7308, validation loss: 0.9065
2024-06-03 02:29:15 [INFO]: Epoch 054 - training loss: 0.7171, validation loss: 0.9065
2024-06-03 02:29:16 [INFO]: Epoch 055 - training loss: 0.7125, validation loss: 0.9065
2024-06-03 02:29:17 [INFO]: Epoch 056 - training loss: 0.7213, validation loss: 0.9065
2024-06-03 02:29:19 [INFO]: Epoch 057 - training loss: 0.7103, validation loss: 0.9065
2024-06-03 02:29:20 [INFO]: Epoch 058 - training loss: 0.7317, validation loss: 0.9065
2024-06-03 02:29:21 [INFO]: Epoch 059 - training loss: 0.7242, validation loss: 0.9065
2024-06-03 02:29:23 [INFO]: Epoch 060 - training loss: 0.7202, validation loss: 0.9064
2024-06-03 02:29:24 [INFO]: Epoch 061 - training loss: 0.7187, validation loss: 0.9064
2024-06-03 02:29:25 [INFO]: Epoch 062 - training loss: 0.7228, validation loss: 0.9064
2024-06-03 02:29:27 [INFO]: Epoch 063 - training loss: 0.7213, validation loss: 0.9064
2024-06-03 02:29:28 [INFO]: Epoch 064 - training loss: 0.7196, validation loss: 0.9064
2024-06-03 02:29:29 [INFO]: Epoch 065 - training loss: 0.7241, validation loss: 0.9064
2024-06-03 02:29:31 [INFO]: Epoch 066 - training loss: 0.7200, validation loss: 0.9064
2024-06-03 02:29:32 [INFO]: Epoch 067 - training loss: 0.7277, validation loss: 0.9064
2024-06-03 02:29:34 [INFO]: Epoch 068 - training loss: 0.7167, validation loss: 0.9064
2024-06-03 02:29:35 [INFO]: Epoch 069 - training loss: 0.6994, validation loss: 0.9064
2024-06-03 02:29:36 [INFO]: Epoch 070 - training loss: 0.7199, validation loss: 0.9063
2024-06-03 02:29:38 [INFO]: Epoch 071 - training loss: 0.7126, validation loss: 0.9063
2024-06-03 02:29:39 [INFO]: Epoch 072 - training loss: 0.7123, validation loss: 0.9063
2024-06-03 02:29:40 [INFO]: Epoch 073 - training loss: 0.7186, validation loss: 0.9063
2024-06-03 02:29:42 [INFO]: Epoch 074 - training loss: 0.7213, validation loss: 0.9063
2024-06-03 02:29:43 [INFO]: Epoch 075 - training loss: 0.7234, validation loss: 0.9063
2024-06-03 02:29:44 [INFO]: Epoch 076 - training loss: 0.7109, validation loss: 0.9063
2024-06-03 02:29:46 [INFO]: Epoch 077 - training loss: 0.7079, validation loss: 0.9063
2024-06-03 02:29:47 [INFO]: Epoch 078 - training loss: 0.7237, validation loss: 0.9063
2024-06-03 02:29:49 [INFO]: Epoch 079 - training loss: 0.7178, validation loss: 0.9063
2024-06-03 02:29:50 [INFO]: Epoch 080 - training loss: 0.7152, validation loss: 0.9063
2024-06-03 02:29:51 [INFO]: Epoch 081 - training loss: 0.7237, validation loss: 0.9063
2024-06-03 02:29:53 [INFO]: Epoch 082 - training loss: 0.7307, validation loss: 0.9063
2024-06-03 02:29:54 [INFO]: Epoch 083 - training loss: 0.7309, validation loss: 0.9063
2024-06-03 02:29:55 [INFO]: Epoch 084 - training loss: 0.7175, validation loss: 0.9063
2024-06-03 02:29:56 [INFO]: Epoch 085 - training loss: 0.7238, validation loss: 0.9063
2024-06-03 02:29:58 [INFO]: Epoch 086 - training loss: 0.7123, validation loss: 0.9062
2024-06-03 02:29:59 [INFO]: Epoch 087 - training loss: 0.7056, validation loss: 0.9062
2024-06-03 02:30:01 [INFO]: Epoch 088 - training loss: 0.7098, validation loss: 0.9062
2024-06-03 02:30:02 [INFO]: Epoch 089 - training loss: 0.7076, validation loss: 0.9062
2024-06-03 02:30:03 [INFO]: Epoch 090 - training loss: 0.7158, validation loss: 0.9062
2024-06-03 02:30:05 [INFO]: Epoch 091 - training loss: 0.7120, validation loss: 0.9062
2024-06-03 02:30:06 [INFO]: Epoch 092 - training loss: 0.7055, validation loss: 0.9062
2024-06-03 02:30:07 [INFO]: Epoch 093 - training loss: 0.7149, validation loss: 0.9062
2024-06-03 02:30:09 [INFO]: Epoch 094 - training loss: 0.7072, validation loss: 0.9062
2024-06-03 02:30:10 [INFO]: Epoch 095 - training loss: 0.7139, validation loss: 0.9062
2024-06-03 02:30:11 [INFO]: Epoch 096 - training loss: 0.7146, validation loss: 0.9062
2024-06-03 02:30:12 [INFO]: Epoch 097 - training loss: 0.7314, validation loss: 0.9062
2024-06-03 02:30:14 [INFO]: Epoch 098 - training loss: 0.7217, validation loss: 0.9062
2024-06-03 02:30:15 [INFO]: Epoch 099 - training loss: 0.7217, validation loss: 0.9062
2024-06-03 02:30:16 [INFO]: Epoch 100 - training loss: 0.7131, validation loss: 0.9062
2024-06-03 02:30:16 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 02:30:16 [INFO]: Saved the model to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_0/20240603_T022752/MRNN.pypots
2024-06-03 02:30:33 [INFO]: Successfully saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_0/imputation.pkl
2024-06-03 02:30:33 [INFO]: Round0 - MRNN on Pedestrian: MAE=0.7673, MSE=0.9915, MRE=1.0022
2024-06-03 02:30:33 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 02:30:33 [INFO]: Using the given device: cuda:0
2024-06-03 02:30:33 [INFO]: Model files will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_1/20240603_T023033
2024-06-03 02:30:33 [INFO]: Tensorboard file will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_1/20240603_T023033/tensorboard
2024-06-03 02:30:33 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 401,415
2024-06-03 02:30:38 [INFO]: Epoch 001 - training loss: 1.3336, validation loss: 1.0090
2024-06-03 02:30:39 [INFO]: Epoch 002 - training loss: 0.8683, validation loss: 0.9728
2024-06-03 02:30:40 [INFO]: Epoch 003 - training loss: 0.8128, validation loss: 0.9535
2024-06-03 02:30:41 [INFO]: Epoch 004 - training loss: 0.8039, validation loss: 0.9409
2024-06-03 02:30:42 [INFO]: Epoch 005 - training loss: 0.7765, validation loss: 0.9343
2024-06-03 02:30:42 [INFO]: Epoch 006 - training loss: 0.7882, validation loss: 0.9284
2024-06-03 02:30:43 [INFO]: Epoch 007 - training loss: 0.7597, validation loss: 0.9246
2024-06-03 02:30:44 [INFO]: Epoch 008 - training loss: 0.7369, validation loss: 0.9215
2024-06-03 02:30:45 [INFO]: Epoch 009 - training loss: 0.7483, validation loss: 0.9190
2024-06-03 02:30:45 [INFO]: Epoch 010 - training loss: 0.7355, validation loss: 0.9171
2024-06-03 02:30:46 [INFO]: Epoch 011 - training loss: 0.7512, validation loss: 0.9156
2024-06-03 02:30:47 [INFO]: Epoch 012 - training loss: 0.7503, validation loss: 0.9144
2024-06-03 02:30:47 [INFO]: Epoch 013 - training loss: 0.7457, validation loss: 0.9135
2024-06-03 02:30:48 [INFO]: Epoch 014 - training loss: 0.7377, validation loss: 0.9126
2024-06-03 02:30:49 [INFO]: Epoch 015 - training loss: 0.7626, validation loss: 0.9120
2024-06-03 02:30:49 [INFO]: Epoch 016 - training loss: 0.7249, validation loss: 0.9114
2024-06-03 02:30:50 [INFO]: Epoch 017 - training loss: 0.7230, validation loss: 0.9108
2024-06-03 02:30:51 [INFO]: Epoch 018 - training loss: 0.7302, validation loss: 0.9104
2024-06-03 02:30:52 [INFO]: Epoch 019 - training loss: 0.7196, validation loss: 0.9101
2024-06-03 02:30:52 [INFO]: Epoch 020 - training loss: 0.7207, validation loss: 0.9096
2024-06-03 02:30:53 [INFO]: Epoch 021 - training loss: 0.7205, validation loss: 0.9094
2024-06-03 02:30:54 [INFO]: Epoch 022 - training loss: 0.7334, validation loss: 0.9092
2024-06-03 02:30:55 [INFO]: Epoch 023 - training loss: 0.7441, validation loss: 0.9090
2024-06-03 02:30:55 [INFO]: Epoch 024 - training loss: 0.7194, validation loss: 0.9087
2024-06-03 02:30:56 [INFO]: Epoch 025 - training loss: 0.7588, validation loss: 0.9086
2024-06-03 02:30:57 [INFO]: Epoch 026 - training loss: 0.7351, validation loss: 0.9085
2024-06-03 02:30:58 [INFO]: Epoch 027 - training loss: 0.7062, validation loss: 0.9083
2024-06-03 02:30:58 [INFO]: Epoch 028 - training loss: 0.7319, validation loss: 0.9082
2024-06-03 02:30:59 [INFO]: Epoch 029 - training loss: 0.7283, validation loss: 0.9081
2024-06-03 02:31:00 [INFO]: Epoch 030 - training loss: 0.7226, validation loss: 0.9080
2024-06-03 02:31:00 [INFO]: Epoch 031 - training loss: 0.7304, validation loss: 0.9078
2024-06-03 02:31:01 [INFO]: Epoch 032 - training loss: 0.7088, validation loss: 0.9078
2024-06-03 02:31:02 [INFO]: Epoch 033 - training loss: 0.7212, validation loss: 0.9077
2024-06-03 02:31:02 [INFO]: Epoch 034 - training loss: 0.7280, validation loss: 0.9076
2024-06-03 02:31:03 [INFO]: Epoch 035 - training loss: 0.7160, validation loss: 0.9075
2024-06-03 02:31:04 [INFO]: Epoch 036 - training loss: 0.7261, validation loss: 0.9075
2024-06-03 02:31:05 [INFO]: Epoch 037 - training loss: 0.7341, validation loss: 0.9074
2024-06-03 02:31:05 [INFO]: Epoch 038 - training loss: 0.7312, validation loss: 0.9073
2024-06-03 02:31:06 [INFO]: Epoch 039 - training loss: 0.7306, validation loss: 0.9073
2024-06-03 02:31:07 [INFO]: Epoch 040 - training loss: 0.7196, validation loss: 0.9073
2024-06-03 02:31:08 [INFO]: Epoch 041 - training loss: 0.7155, validation loss: 0.9072
2024-06-03 02:31:08 [INFO]: Epoch 042 - training loss: 0.7117, validation loss: 0.9072
2024-06-03 02:31:09 [INFO]: Epoch 043 - training loss: 0.7282, validation loss: 0.9071
2024-06-03 02:31:10 [INFO]: Epoch 044 - training loss: 0.7227, validation loss: 0.9071
2024-06-03 02:31:11 [INFO]: Epoch 045 - training loss: 0.7424, validation loss: 0.9070
2024-06-03 02:31:11 [INFO]: Epoch 046 - training loss: 0.7113, validation loss: 0.9070
2024-06-03 02:31:12 [INFO]: Epoch 047 - training loss: 0.7432, validation loss: 0.9070
2024-06-03 02:31:13 [INFO]: Epoch 048 - training loss: 0.7353, validation loss: 0.9070
2024-06-03 02:31:13 [INFO]: Epoch 049 - training loss: 0.7186, validation loss: 0.9069
2024-06-03 02:31:14 [INFO]: Epoch 050 - training loss: 0.7228, validation loss: 0.9069
2024-06-03 02:31:14 [INFO]: Epoch 051 - training loss: 0.7310, validation loss: 0.9069
2024-06-03 02:31:15 [INFO]: Epoch 052 - training loss: 0.7398, validation loss: 0.9068
2024-06-03 02:31:15 [INFO]: Epoch 053 - training loss: 0.7262, validation loss: 0.9068
2024-06-03 02:31:16 [INFO]: Epoch 054 - training loss: 0.7349, validation loss: 0.9068
2024-06-03 02:31:17 [INFO]: Epoch 055 - training loss: 0.7168, validation loss: 0.9068
2024-06-03 02:31:17 [INFO]: Epoch 056 - training loss: 0.7137, validation loss: 0.9068
2024-06-03 02:31:18 [INFO]: Epoch 057 - training loss: 0.7391, validation loss: 0.9067
2024-06-03 02:31:19 [INFO]: Epoch 058 - training loss: 0.7093, validation loss: 0.9067
2024-06-03 02:31:19 [INFO]: Epoch 059 - training loss: 0.7161, validation loss: 0.9067
2024-06-03 02:31:20 [INFO]: Epoch 060 - training loss: 0.7067, validation loss: 0.9067
2024-06-03 02:31:21 [INFO]: Epoch 061 - training loss: 0.7287, validation loss: 0.9067
2024-06-03 02:31:21 [INFO]: Epoch 062 - training loss: 0.7125, validation loss: 0.9066
2024-06-03 02:31:22 [INFO]: Epoch 063 - training loss: 0.7325, validation loss: 0.9066
2024-06-03 02:31:23 [INFO]: Epoch 064 - training loss: 0.7156, validation loss: 0.9066
2024-06-03 02:31:24 [INFO]: Epoch 065 - training loss: 0.7110, validation loss: 0.9066
2024-06-03 02:31:24 [INFO]: Epoch 066 - training loss: 0.7148, validation loss: 0.9066
2024-06-03 02:31:25 [INFO]: Epoch 067 - training loss: 0.7284, validation loss: 0.9066
2024-06-03 02:31:26 [INFO]: Epoch 068 - training loss: 0.7390, validation loss: 0.9066
2024-06-03 02:31:26 [INFO]: Epoch 069 - training loss: 0.7162, validation loss: 0.9065
2024-06-03 02:31:27 [INFO]: Epoch 070 - training loss: 0.7221, validation loss: 0.9065
2024-06-03 02:31:28 [INFO]: Epoch 071 - training loss: 0.7146, validation loss: 0.9065
2024-06-03 02:31:29 [INFO]: Epoch 072 - training loss: 0.7129, validation loss: 0.9065
2024-06-03 02:31:30 [INFO]: Epoch 073 - training loss: 0.7191, validation loss: 0.9065
2024-06-03 02:31:30 [INFO]: Epoch 074 - training loss: 0.7164, validation loss: 0.9065
2024-06-03 02:31:31 [INFO]: Epoch 075 - training loss: 0.7148, validation loss: 0.9065
2024-06-03 02:31:32 [INFO]: Epoch 076 - training loss: 0.7186, validation loss: 0.9065
2024-06-03 02:31:33 [INFO]: Epoch 077 - training loss: 0.7031, validation loss: 0.9064
2024-06-03 02:31:33 [INFO]: Epoch 078 - training loss: 0.7125, validation loss: 0.9064
2024-06-03 02:31:34 [INFO]: Epoch 079 - training loss: 0.7058, validation loss: 0.9064
2024-06-03 02:31:35 [INFO]: Epoch 080 - training loss: 0.7183, validation loss: 0.9064
2024-06-03 02:31:35 [INFO]: Epoch 081 - training loss: 0.7239, validation loss: 0.9064
2024-06-03 02:31:36 [INFO]: Epoch 082 - training loss: 0.7153, validation loss: 0.9064
2024-06-03 02:31:37 [INFO]: Epoch 083 - training loss: 0.7158, validation loss: 0.9064
2024-06-03 02:31:37 [INFO]: Epoch 084 - training loss: 0.7173, validation loss: 0.9064
2024-06-03 02:31:38 [INFO]: Epoch 085 - training loss: 0.7194, validation loss: 0.9064
2024-06-03 02:31:39 [INFO]: Epoch 086 - training loss: 0.7259, validation loss: 0.9064
2024-06-03 02:31:39 [INFO]: Epoch 087 - training loss: 0.7174, validation loss: 0.9064
2024-06-03 02:31:40 [INFO]: Epoch 088 - training loss: 0.7288, validation loss: 0.9064
2024-06-03 02:31:40 [INFO]: Epoch 089 - training loss: 0.7169, validation loss: 0.9063
2024-06-03 02:31:41 [INFO]: Epoch 090 - training loss: 0.7328, validation loss: 0.9063
2024-06-03 02:31:41 [INFO]: Epoch 091 - training loss: 0.7121, validation loss: 0.9063
2024-06-03 02:31:42 [INFO]: Epoch 092 - training loss: 0.7103, validation loss: 0.9063
2024-06-03 02:31:42 [INFO]: Epoch 093 - training loss: 0.7215, validation loss: 0.9063
2024-06-03 02:31:43 [INFO]: Epoch 094 - training loss: 0.7155, validation loss: 0.9063
2024-06-03 02:31:44 [INFO]: Epoch 095 - training loss: 0.7152, validation loss: 0.9063
2024-06-03 02:31:44 [INFO]: Epoch 096 - training loss: 0.7103, validation loss: 0.9063
2024-06-03 02:31:45 [INFO]: Epoch 097 - training loss: 0.7091, validation loss: 0.9063
2024-06-03 02:31:46 [INFO]: Epoch 098 - training loss: 0.7134, validation loss: 0.9063
2024-06-03 02:31:46 [INFO]: Epoch 099 - training loss: 0.7176, validation loss: 0.9063
2024-06-03 02:31:47 [INFO]: Epoch 100 - training loss: 0.7247, validation loss: 0.9063
2024-06-03 02:31:47 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 02:31:47 [INFO]: Saved the model to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_1/20240603_T023033/MRNN.pypots
2024-06-03 02:31:56 [INFO]: Successfully saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_1/imputation.pkl
2024-06-03 02:31:56 [INFO]: Round1 - MRNN on Pedestrian: MAE=0.7680, MSE=0.9915, MRE=1.0031
2024-06-03 02:31:56 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 02:31:56 [INFO]: Using the given device: cuda:0
2024-06-03 02:31:56 [INFO]: Model files will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_2/20240603_T023156
2024-06-03 02:31:56 [INFO]: Tensorboard file will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_2/20240603_T023156/tensorboard
2024-06-03 02:31:56 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 401,415
2024-06-03 02:32:00 [INFO]: Epoch 001 - training loss: 1.3434, validation loss: 1.0029
2024-06-03 02:32:00 [INFO]: Epoch 002 - training loss: 0.8512, validation loss: 0.9708
2024-06-03 02:32:01 [INFO]: Epoch 003 - training loss: 0.8243, validation loss: 0.9520
2024-06-03 02:32:02 [INFO]: Epoch 004 - training loss: 0.7838, validation loss: 0.9384
2024-06-03 02:32:02 [INFO]: Epoch 005 - training loss: 0.7694, validation loss: 0.9313
2024-06-03 02:32:03 [INFO]: Epoch 006 - training loss: 0.7641, validation loss: 0.9259
2024-06-03 02:32:04 [INFO]: Epoch 007 - training loss: 0.7686, validation loss: 0.9225
2024-06-03 02:32:05 [INFO]: Epoch 008 - training loss: 0.7454, validation loss: 0.9195
2024-06-03 02:32:05 [INFO]: Epoch 009 - training loss: 0.7631, validation loss: 0.9178
2024-06-03 02:32:06 [INFO]: Epoch 010 - training loss: 0.7365, validation loss: 0.9164
2024-06-03 02:32:07 [INFO]: Epoch 011 - training loss: 0.7276, validation loss: 0.9150
2024-06-03 02:32:07 [INFO]: Epoch 012 - training loss: 0.7339, validation loss: 0.9141
2024-06-03 02:32:08 [INFO]: Epoch 013 - training loss: 0.7352, validation loss: 0.9130
2024-06-03 02:32:09 [INFO]: Epoch 014 - training loss: 0.7373, validation loss: 0.9126
2024-06-03 02:32:09 [INFO]: Epoch 015 - training loss: 0.7320, validation loss: 0.9118
2024-06-03 02:32:10 [INFO]: Epoch 016 - training loss: 0.7162, validation loss: 0.9116
2024-06-03 02:32:11 [INFO]: Epoch 017 - training loss: 0.7432, validation loss: 0.9110
2024-06-03 02:32:12 [INFO]: Epoch 018 - training loss: 0.7268, validation loss: 0.9105
2024-06-03 02:32:12 [INFO]: Epoch 019 - training loss: 0.7220, validation loss: 0.9103
2024-06-03 02:32:13 [INFO]: Epoch 020 - training loss: 0.7356, validation loss: 0.9100
2024-06-03 02:32:14 [INFO]: Epoch 021 - training loss: 0.7482, validation loss: 0.9097
2024-06-03 02:32:15 [INFO]: Epoch 022 - training loss: 0.7335, validation loss: 0.9095
2024-06-03 02:32:15 [INFO]: Epoch 023 - training loss: 0.7457, validation loss: 0.9093
2024-06-03 02:32:16 [INFO]: Epoch 024 - training loss: 0.7388, validation loss: 0.9090
2024-06-03 02:32:17 [INFO]: Epoch 025 - training loss: 0.7401, validation loss: 0.9089
2024-06-03 02:32:18 [INFO]: Epoch 026 - training loss: 0.7209, validation loss: 0.9087
2024-06-03 02:32:18 [INFO]: Epoch 027 - training loss: 0.7252, validation loss: 0.9086
2024-06-03 02:32:19 [INFO]: Epoch 028 - training loss: 0.7374, validation loss: 0.9084
2024-06-03 02:32:19 [INFO]: Epoch 029 - training loss: 0.7235, validation loss: 0.9084
2024-06-03 02:32:20 [INFO]: Epoch 030 - training loss: 0.7236, validation loss: 0.9083
2024-06-03 02:32:20 [INFO]: Epoch 031 - training loss: 0.7205, validation loss: 0.9081
2024-06-03 02:32:21 [INFO]: Epoch 032 - training loss: 0.7116, validation loss: 0.9081
2024-06-03 02:32:22 [INFO]: Epoch 033 - training loss: 0.7340, validation loss: 0.9080
2024-06-03 02:32:22 [INFO]: Epoch 034 - training loss: 0.7332, validation loss: 0.9079
2024-06-03 02:32:23 [INFO]: Epoch 035 - training loss: 0.7433, validation loss: 0.9078
2024-06-03 02:32:23 [INFO]: Epoch 036 - training loss: 0.7260, validation loss: 0.9077
2024-06-03 02:32:24 [INFO]: Epoch 037 - training loss: 0.7319, validation loss: 0.9076
2024-06-03 02:32:25 [INFO]: Epoch 038 - training loss: 0.7252, validation loss: 0.9076
2024-06-03 02:32:25 [INFO]: Epoch 039 - training loss: 0.7266, validation loss: 0.9075
2024-06-03 02:32:26 [INFO]: Epoch 040 - training loss: 0.7326, validation loss: 0.9075
2024-06-03 02:32:26 [INFO]: Epoch 041 - training loss: 0.7302, validation loss: 0.9075
2024-06-03 02:32:27 [INFO]: Epoch 042 - training loss: 0.7141, validation loss: 0.9074
2024-06-03 02:32:27 [INFO]: Epoch 043 - training loss: 0.7368, validation loss: 0.9074
2024-06-03 02:32:28 [INFO]: Epoch 044 - training loss: 0.7407, validation loss: 0.9073
2024-06-03 02:32:29 [INFO]: Epoch 045 - training loss: 0.7111, validation loss: 0.9073
2024-06-03 02:32:29 [INFO]: Epoch 046 - training loss: 0.7176, validation loss: 0.9072
2024-06-03 02:32:30 [INFO]: Epoch 047 - training loss: 0.7300, validation loss: 0.9072
2024-06-03 02:32:30 [INFO]: Epoch 048 - training loss: 0.7529, validation loss: 0.9071
2024-06-03 02:32:31 [INFO]: Epoch 049 - training loss: 0.7222, validation loss: 0.9071
2024-06-03 02:32:31 [INFO]: Epoch 050 - training loss: 0.7146, validation loss: 0.9071
2024-06-03 02:32:32 [INFO]: Epoch 051 - training loss: 0.7238, validation loss: 0.9071
2024-06-03 02:32:32 [INFO]: Epoch 052 - training loss: 0.7118, validation loss: 0.9070
2024-06-03 02:32:33 [INFO]: Epoch 053 - training loss: 0.7153, validation loss: 0.9070
2024-06-03 02:32:33 [INFO]: Epoch 054 - training loss: 0.7178, validation loss: 0.9069
2024-06-03 02:32:34 [INFO]: Epoch 055 - training loss: 0.7305, validation loss: 0.9069
2024-06-03 02:32:34 [INFO]: Epoch 056 - training loss: 0.7471, validation loss: 0.9069
2024-06-03 02:32:35 [INFO]: Epoch 057 - training loss: 0.7305, validation loss: 0.9069
2024-06-03 02:32:35 [INFO]: Epoch 058 - training loss: 0.7224, validation loss: 0.9069
2024-06-03 02:32:36 [INFO]: Epoch 059 - training loss: 0.7121, validation loss: 0.9068
2024-06-03 02:32:36 [INFO]: Epoch 060 - training loss: 0.7102, validation loss: 0.9068
2024-06-03 02:32:37 [INFO]: Epoch 061 - training loss: 0.7177, validation loss: 0.9068
2024-06-03 02:32:37 [INFO]: Epoch 062 - training loss: 0.7232, validation loss: 0.9068
2024-06-03 02:32:38 [INFO]: Epoch 063 - training loss: 0.7196, validation loss: 0.9068
2024-06-03 02:32:38 [INFO]: Epoch 064 - training loss: 0.7259, validation loss: 0.9067
2024-06-03 02:32:39 [INFO]: Epoch 065 - training loss: 0.7062, validation loss: 0.9067
2024-06-03 02:32:39 [INFO]: Epoch 066 - training loss: 0.7024, validation loss: 0.9067
2024-06-03 02:32:40 [INFO]: Epoch 067 - training loss: 0.7180, validation loss: 0.9067
2024-06-03 02:32:41 [INFO]: Epoch 068 - training loss: 0.7054, validation loss: 0.9067
2024-06-03 02:32:41 [INFO]: Epoch 069 - training loss: 0.7113, validation loss: 0.9067
2024-06-03 02:32:42 [INFO]: Epoch 070 - training loss: 0.7139, validation loss: 0.9066
2024-06-03 02:32:42 [INFO]: Epoch 071 - training loss: 0.7334, validation loss: 0.9066
2024-06-03 02:32:43 [INFO]: Epoch 072 - training loss: 0.7189, validation loss: 0.9066
2024-06-03 02:32:43 [INFO]: Epoch 073 - training loss: 0.7284, validation loss: 0.9066
2024-06-03 02:32:43 [INFO]: Epoch 074 - training loss: 0.7128, validation loss: 0.9066
2024-06-03 02:32:44 [INFO]: Epoch 075 - training loss: 0.7132, validation loss: 0.9066
2024-06-03 02:32:44 [INFO]: Epoch 076 - training loss: 0.7199, validation loss: 0.9066
2024-06-03 02:32:44 [INFO]: Epoch 077 - training loss: 0.7138, validation loss: 0.9065
2024-06-03 02:32:45 [INFO]: Epoch 078 - training loss: 0.7207, validation loss: 0.9065
2024-06-03 02:32:46 [INFO]: Epoch 079 - training loss: 0.7208, validation loss: 0.9065
2024-06-03 02:32:46 [INFO]: Epoch 080 - training loss: 0.7131, validation loss: 0.9065
2024-06-03 02:32:46 [INFO]: Epoch 081 - training loss: 0.7280, validation loss: 0.9065
2024-06-03 02:32:47 [INFO]: Epoch 082 - training loss: 0.7117, validation loss: 0.9065
2024-06-03 02:32:47 [INFO]: Epoch 083 - training loss: 0.7209, validation loss: 0.9065
2024-06-03 02:32:48 [INFO]: Epoch 084 - training loss: 0.7241, validation loss: 0.9065
2024-06-03 02:32:48 [INFO]: Epoch 085 - training loss: 0.7091, validation loss: 0.9065
2024-06-03 02:32:49 [INFO]: Epoch 086 - training loss: 0.7219, validation loss: 0.9065
2024-06-03 02:32:49 [INFO]: Epoch 087 - training loss: 0.7249, validation loss: 0.9065
2024-06-03 02:32:49 [INFO]: Epoch 088 - training loss: 0.7182, validation loss: 0.9064
2024-06-03 02:32:50 [INFO]: Epoch 089 - training loss: 0.7029, validation loss: 0.9064
2024-06-03 02:32:51 [INFO]: Epoch 090 - training loss: 0.7117, validation loss: 0.9064
2024-06-03 02:32:51 [INFO]: Epoch 091 - training loss: 0.7151, validation loss: 0.9064
2024-06-03 02:32:52 [INFO]: Epoch 092 - training loss: 0.7134, validation loss: 0.9064
2024-06-03 02:32:52 [INFO]: Epoch 093 - training loss: 0.7145, validation loss: 0.9064
2024-06-03 02:32:53 [INFO]: Epoch 094 - training loss: 0.7217, validation loss: 0.9064
2024-06-03 02:32:53 [INFO]: Epoch 095 - training loss: 0.7407, validation loss: 0.9064
2024-06-03 02:32:54 [INFO]: Epoch 096 - training loss: 0.7316, validation loss: 0.9064
2024-06-03 02:32:54 [INFO]: Epoch 097 - training loss: 0.7203, validation loss: 0.9064
2024-06-03 02:32:55 [INFO]: Epoch 098 - training loss: 0.7136, validation loss: 0.9064
2024-06-03 02:32:56 [INFO]: Epoch 099 - training loss: 0.7182, validation loss: 0.9064
2024-06-03 02:32:56 [INFO]: Epoch 100 - training loss: 0.7174, validation loss: 0.9064
2024-06-03 02:32:56 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 02:32:56 [INFO]: Saved the model to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_2/20240603_T023156/MRNN.pypots
2024-06-03 02:33:03 [INFO]: Successfully saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_2/imputation.pkl
2024-06-03 02:33:03 [INFO]: Round2 - MRNN on Pedestrian: MAE=0.7686, MSE=0.9914, MRE=1.0038
2024-06-03 02:33:03 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 02:33:03 [INFO]: Using the given device: cuda:0
2024-06-03 02:33:03 [INFO]: Model files will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_3/20240603_T023303
2024-06-03 02:33:03 [INFO]: Tensorboard file will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_3/20240603_T023303/tensorboard
2024-06-03 02:33:03 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 401,415
2024-06-03 02:33:06 [INFO]: Epoch 001 - training loss: 1.3641, validation loss: 1.0944
2024-06-03 02:33:07 [INFO]: Epoch 002 - training loss: 0.8647, validation loss: 1.0371
2024-06-03 02:33:07 [INFO]: Epoch 003 - training loss: 0.8200, validation loss: 1.0013
2024-06-03 02:33:08 [INFO]: Epoch 004 - training loss: 0.7870, validation loss: 0.9790
2024-06-03 02:33:08 [INFO]: Epoch 005 - training loss: 0.7903, validation loss: 0.9616
2024-06-03 02:33:09 [INFO]: Epoch 006 - training loss: 0.7690, validation loss: 0.9501
2024-06-03 02:33:10 [INFO]: Epoch 007 - training loss: 0.7861, validation loss: 0.9410
2024-06-03 02:33:10 [INFO]: Epoch 008 - training loss: 0.7840, validation loss: 0.9347
2024-06-03 02:33:11 [INFO]: Epoch 009 - training loss: 0.7504, validation loss: 0.9298
2024-06-03 02:33:11 [INFO]: Epoch 010 - training loss: 0.7435, validation loss: 0.9259
2024-06-03 02:33:12 [INFO]: Epoch 011 - training loss: 0.7678, validation loss: 0.9232
2024-06-03 02:33:12 [INFO]: Epoch 012 - training loss: 0.7345, validation loss: 0.9204
2024-06-03 02:33:13 [INFO]: Epoch 013 - training loss: 0.7368, validation loss: 0.9185
2024-06-03 02:33:13 [INFO]: Epoch 014 - training loss: 0.7240, validation loss: 0.9166
2024-06-03 02:33:14 [INFO]: Epoch 015 - training loss: 0.7326, validation loss: 0.9152
2024-06-03 02:33:14 [INFO]: Epoch 016 - training loss: 0.7427, validation loss: 0.9140
2024-06-03 02:33:15 [INFO]: Epoch 017 - training loss: 0.7366, validation loss: 0.9130
2024-06-03 02:33:15 [INFO]: Epoch 018 - training loss: 0.7252, validation loss: 0.9121
2024-06-03 02:33:16 [INFO]: Epoch 019 - training loss: 0.7327, validation loss: 0.9114
2024-06-03 02:33:16 [INFO]: Epoch 020 - training loss: 0.7257, validation loss: 0.9108
2024-06-03 02:33:16 [INFO]: Epoch 021 - training loss: 0.7275, validation loss: 0.9103
2024-06-03 02:33:17 [INFO]: Epoch 022 - training loss: 0.7284, validation loss: 0.9099
2024-06-03 02:33:18 [INFO]: Epoch 023 - training loss: 0.7330, validation loss: 0.9095
2024-06-03 02:33:18 [INFO]: Epoch 024 - training loss: 0.7547, validation loss: 0.9091
2024-06-03 02:33:19 [INFO]: Epoch 025 - training loss: 0.7424, validation loss: 0.9089
2024-06-03 02:33:19 [INFO]: Epoch 026 - training loss: 0.7316, validation loss: 0.9087
2024-06-03 02:33:20 [INFO]: Epoch 027 - training loss: 0.7235, validation loss: 0.9085
2024-06-03 02:33:20 [INFO]: Epoch 028 - training loss: 0.7211, validation loss: 0.9083
2024-06-03 02:33:21 [INFO]: Epoch 029 - training loss: 0.7352, validation loss: 0.9081
2024-06-03 02:33:21 [INFO]: Epoch 030 - training loss: 0.7329, validation loss: 0.9080
2024-06-03 02:33:22 [INFO]: Epoch 031 - training loss: 0.7228, validation loss: 0.9078
2024-06-03 02:33:22 [INFO]: Epoch 032 - training loss: 0.7276, validation loss: 0.9077
2024-06-03 02:33:22 [INFO]: Epoch 033 - training loss: 0.7190, validation loss: 0.9076
2024-06-03 02:33:23 [INFO]: Epoch 034 - training loss: 0.7313, validation loss: 0.9075
2024-06-03 02:33:23 [INFO]: Epoch 035 - training loss: 0.7163, validation loss: 0.9075
2024-06-03 02:33:24 [INFO]: Epoch 036 - training loss: 0.7194, validation loss: 0.9074
2024-06-03 02:33:24 [INFO]: Epoch 037 - training loss: 0.7177, validation loss: 0.9073
2024-06-03 02:33:24 [INFO]: Epoch 038 - training loss: 0.7295, validation loss: 0.9073
2024-06-03 02:33:25 [INFO]: Epoch 039 - training loss: 0.7302, validation loss: 0.9072
2024-06-03 02:33:25 [INFO]: Epoch 040 - training loss: 0.7253, validation loss: 0.9071
2024-06-03 02:33:25 [INFO]: Epoch 041 - training loss: 0.7201, validation loss: 0.9071
2024-06-03 02:33:26 [INFO]: Epoch 042 - training loss: 0.7201, validation loss: 0.9071
2024-06-03 02:33:26 [INFO]: Epoch 043 - training loss: 0.7213, validation loss: 0.9070
2024-06-03 02:33:26 [INFO]: Epoch 044 - training loss: 0.7168, validation loss: 0.9070
2024-06-03 02:33:26 [INFO]: Epoch 045 - training loss: 0.7174, validation loss: 0.9069
2024-06-03 02:33:27 [INFO]: Epoch 046 - training loss: 0.7184, validation loss: 0.9069
2024-06-03 02:33:27 [INFO]: Epoch 047 - training loss: 0.7307, validation loss: 0.9069
2024-06-03 02:33:27 [INFO]: Epoch 048 - training loss: 0.7228, validation loss: 0.9068
2024-06-03 02:33:28 [INFO]: Epoch 049 - training loss: 0.7287, validation loss: 0.9068
2024-06-03 02:33:28 [INFO]: Epoch 050 - training loss: 0.7291, validation loss: 0.9068
2024-06-03 02:33:28 [INFO]: Epoch 051 - training loss: 0.7311, validation loss: 0.9067
2024-06-03 02:33:29 [INFO]: Epoch 052 - training loss: 0.7254, validation loss: 0.9067
2024-06-03 02:33:29 [INFO]: Epoch 053 - training loss: 0.7129, validation loss: 0.9067
2024-06-03 02:33:29 [INFO]: Epoch 054 - training loss: 0.7276, validation loss: 0.9067
2024-06-03 02:33:30 [INFO]: Epoch 055 - training loss: 0.7135, validation loss: 0.9067
2024-06-03 02:33:30 [INFO]: Epoch 056 - training loss: 0.7228, validation loss: 0.9066
2024-06-03 02:33:30 [INFO]: Epoch 057 - training loss: 0.7175, validation loss: 0.9066
2024-06-03 02:33:31 [INFO]: Epoch 058 - training loss: 0.7208, validation loss: 0.9066
2024-06-03 02:33:31 [INFO]: Epoch 059 - training loss: 0.7179, validation loss: 0.9066
2024-06-03 02:33:31 [INFO]: Epoch 060 - training loss: 0.7261, validation loss: 0.9066
2024-06-03 02:33:32 [INFO]: Epoch 061 - training loss: 0.7250, validation loss: 0.9065
2024-06-03 02:33:32 [INFO]: Epoch 062 - training loss: 0.7174, validation loss: 0.9065
2024-06-03 02:33:32 [INFO]: Epoch 063 - training loss: 0.7272, validation loss: 0.9065
2024-06-03 02:33:32 [INFO]: Epoch 064 - training loss: 0.7242, validation loss: 0.9065
2024-06-03 02:33:33 [INFO]: Epoch 065 - training loss: 0.7206, validation loss: 0.9065
2024-06-03 02:33:33 [INFO]: Epoch 066 - training loss: 0.7158, validation loss: 0.9065
2024-06-03 02:33:33 [INFO]: Epoch 067 - training loss: 0.7094, validation loss: 0.9065
2024-06-03 02:33:34 [INFO]: Epoch 068 - training loss: 0.7197, validation loss: 0.9065
2024-06-03 02:33:34 [INFO]: Epoch 069 - training loss: 0.7124, validation loss: 0.9064
2024-06-03 02:33:34 [INFO]: Epoch 070 - training loss: 0.7217, validation loss: 0.9064
2024-06-03 02:33:35 [INFO]: Epoch 071 - training loss: 0.7156, validation loss: 0.9064
2024-06-03 02:33:35 [INFO]: Epoch 072 - training loss: 0.7234, validation loss: 0.9064
2024-06-03 02:33:35 [INFO]: Epoch 073 - training loss: 0.7206, validation loss: 0.9064
2024-06-03 02:33:36 [INFO]: Epoch 074 - training loss: 0.7160, validation loss: 0.9064
2024-06-03 02:33:36 [INFO]: Epoch 075 - training loss: 0.7086, validation loss: 0.9064
2024-06-03 02:33:36 [INFO]: Epoch 076 - training loss: 0.7264, validation loss: 0.9064
2024-06-03 02:33:37 [INFO]: Epoch 077 - training loss: 0.7280, validation loss: 0.9064
2024-06-03 02:33:37 [INFO]: Epoch 078 - training loss: 0.7136, validation loss: 0.9063
2024-06-03 02:33:37 [INFO]: Epoch 079 - training loss: 0.7151, validation loss: 0.9063
2024-06-03 02:33:38 [INFO]: Epoch 080 - training loss: 0.7098, validation loss: 0.9063
2024-06-03 02:33:38 [INFO]: Epoch 081 - training loss: 0.7269, validation loss: 0.9063
2024-06-03 02:33:38 [INFO]: Epoch 082 - training loss: 0.7373, validation loss: 0.9063
2024-06-03 02:33:38 [INFO]: Epoch 083 - training loss: 0.7187, validation loss: 0.9063
2024-06-03 02:33:39 [INFO]: Epoch 084 - training loss: 0.7116, validation loss: 0.9063
2024-06-03 02:33:39 [INFO]: Epoch 085 - training loss: 0.7314, validation loss: 0.9063
2024-06-03 02:33:39 [INFO]: Epoch 086 - training loss: 0.7183, validation loss: 0.9063
2024-06-03 02:33:40 [INFO]: Epoch 087 - training loss: 0.7166, validation loss: 0.9063
2024-06-03 02:33:40 [INFO]: Epoch 088 - training loss: 0.7254, validation loss: 0.9063
2024-06-03 02:33:40 [INFO]: Epoch 089 - training loss: 0.7235, validation loss: 0.9063
2024-06-03 02:33:41 [INFO]: Epoch 090 - training loss: 0.7234, validation loss: 0.9063
2024-06-03 02:33:41 [INFO]: Epoch 091 - training loss: 0.7073, validation loss: 0.9063
2024-06-03 02:33:41 [INFO]: Epoch 092 - training loss: 0.7161, validation loss: 0.9063
2024-06-03 02:33:42 [INFO]: Epoch 093 - training loss: 0.7196, validation loss: 0.9063
2024-06-03 02:33:42 [INFO]: Epoch 094 - training loss: 0.7172, validation loss: 0.9063
2024-06-03 02:33:42 [INFO]: Epoch 095 - training loss: 0.7207, validation loss: 0.9062
2024-06-03 02:33:43 [INFO]: Epoch 096 - training loss: 0.7156, validation loss: 0.9062
2024-06-03 02:33:43 [INFO]: Epoch 097 - training loss: 0.7133, validation loss: 0.9062
2024-06-03 02:33:43 [INFO]: Epoch 098 - training loss: 0.7255, validation loss: 0.9062
2024-06-03 02:33:44 [INFO]: Epoch 099 - training loss: 0.7288, validation loss: 0.9062
2024-06-03 02:33:44 [INFO]: Epoch 100 - training loss: 0.7198, validation loss: 0.9062
2024-06-03 02:33:44 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 02:33:44 [INFO]: Saved the model to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_3/20240603_T023303/MRNN.pypots
2024-06-03 02:33:47 [INFO]: Successfully saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_3/imputation.pkl
2024-06-03 02:33:47 [INFO]: Round3 - MRNN on Pedestrian: MAE=0.7677, MSE=0.9915, MRE=1.0026
2024-06-03 02:33:47 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 02:33:47 [INFO]: Using the given device: cuda:0
2024-06-03 02:33:47 [INFO]: Model files will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_4/20240603_T023347
2024-06-03 02:33:47 [INFO]: Tensorboard file will be saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_4/20240603_T023347/tensorboard
2024-06-03 02:33:47 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 401,415
2024-06-03 02:33:49 [INFO]: Epoch 001 - training loss: 1.4281, validation loss: 1.3224
2024-06-03 02:33:49 [INFO]: Epoch 002 - training loss: 0.9728, validation loss: 1.2020
2024-06-03 02:33:49 [INFO]: Epoch 003 - training loss: 0.9049, validation loss: 1.0965
2024-06-03 02:33:50 [INFO]: Epoch 004 - training loss: 0.8478, validation loss: 1.0226
2024-06-03 02:33:50 [INFO]: Epoch 005 - training loss: 0.8144, validation loss: 0.9789
2024-06-03 02:33:50 [INFO]: Epoch 006 - training loss: 0.7785, validation loss: 0.9551
2024-06-03 02:33:51 [INFO]: Epoch 007 - training loss: 0.7694, validation loss: 0.9411
2024-06-03 02:33:51 [INFO]: Epoch 008 - training loss: 0.7724, validation loss: 0.9327
2024-06-03 02:33:51 [INFO]: Epoch 009 - training loss: 0.7533, validation loss: 0.9275
2024-06-03 02:33:52 [INFO]: Epoch 010 - training loss: 0.7654, validation loss: 0.9233
2024-06-03 02:33:52 [INFO]: Epoch 011 - training loss: 0.7665, validation loss: 0.9205
2024-06-03 02:33:52 [INFO]: Epoch 012 - training loss: 0.7418, validation loss: 0.9183
2024-06-03 02:33:52 [INFO]: Epoch 013 - training loss: 0.7444, validation loss: 0.9169
2024-06-03 02:33:53 [INFO]: Epoch 014 - training loss: 0.7358, validation loss: 0.9156
2024-06-03 02:33:53 [INFO]: Epoch 015 - training loss: 0.7401, validation loss: 0.9145
2024-06-03 02:33:53 [INFO]: Epoch 016 - training loss: 0.7321, validation loss: 0.9136
2024-06-03 02:33:54 [INFO]: Epoch 017 - training loss: 0.7336, validation loss: 0.9130
2024-06-03 02:33:54 [INFO]: Epoch 018 - training loss: 0.7373, validation loss: 0.9123
2024-06-03 02:33:54 [INFO]: Epoch 019 - training loss: 0.7245, validation loss: 0.9118
2024-06-03 02:33:55 [INFO]: Epoch 020 - training loss: 0.7381, validation loss: 0.9113
2024-06-03 02:33:55 [INFO]: Epoch 021 - training loss: 0.7315, validation loss: 0.9110
2024-06-03 02:33:55 [INFO]: Epoch 022 - training loss: 0.7238, validation loss: 0.9105
2024-06-03 02:33:56 [INFO]: Epoch 023 - training loss: 0.7201, validation loss: 0.9103
2024-06-03 02:33:56 [INFO]: Epoch 024 - training loss: 0.7191, validation loss: 0.9100
2024-06-03 02:33:56 [INFO]: Epoch 025 - training loss: 0.7407, validation loss: 0.9096
2024-06-03 02:33:57 [INFO]: Epoch 026 - training loss: 0.7366, validation loss: 0.9095
2024-06-03 02:33:57 [INFO]: Epoch 027 - training loss: 0.7388, validation loss: 0.9092
2024-06-03 02:33:57 [INFO]: Epoch 028 - training loss: 0.7260, validation loss: 0.9091
2024-06-03 02:33:58 [INFO]: Epoch 029 - training loss: 0.7265, validation loss: 0.9089
2024-06-03 02:33:58 [INFO]: Epoch 030 - training loss: 0.7232, validation loss: 0.9087
2024-06-03 02:33:58 [INFO]: Epoch 031 - training loss: 0.7237, validation loss: 0.9086
2024-06-03 02:33:59 [INFO]: Epoch 032 - training loss: 0.7338, validation loss: 0.9084
2024-06-03 02:33:59 [INFO]: Epoch 033 - training loss: 0.7221, validation loss: 0.9083
2024-06-03 02:33:59 [INFO]: Epoch 034 - training loss: 0.7227, validation loss: 0.9082
2024-06-03 02:33:59 [INFO]: Epoch 035 - training loss: 0.7191, validation loss: 0.9081
2024-06-03 02:34:00 [INFO]: Epoch 036 - training loss: 0.7218, validation loss: 0.9080
2024-06-03 02:34:00 [INFO]: Epoch 037 - training loss: 0.7262, validation loss: 0.9079
2024-06-03 02:34:00 [INFO]: Epoch 038 - training loss: 0.7352, validation loss: 0.9078
2024-06-03 02:34:01 [INFO]: Epoch 039 - training loss: 0.7292, validation loss: 0.9077
2024-06-03 02:34:01 [INFO]: Epoch 040 - training loss: 0.7147, validation loss: 0.9077
2024-06-03 02:34:01 [INFO]: Epoch 041 - training loss: 0.7162, validation loss: 0.9076
2024-06-03 02:34:02 [INFO]: Epoch 042 - training loss: 0.7229, validation loss: 0.9075
2024-06-03 02:34:02 [INFO]: Epoch 043 - training loss: 0.7166, validation loss: 0.9075
2024-06-03 02:34:02 [INFO]: Epoch 044 - training loss: 0.7278, validation loss: 0.9074
2024-06-03 02:34:03 [INFO]: Epoch 045 - training loss: 0.7268, validation loss: 0.9074
2024-06-03 02:34:03 [INFO]: Epoch 046 - training loss: 0.7131, validation loss: 0.9073
2024-06-03 02:34:03 [INFO]: Epoch 047 - training loss: 0.7117, validation loss: 0.9073
2024-06-03 02:34:04 [INFO]: Epoch 048 - training loss: 0.7167, validation loss: 0.9072
2024-06-03 02:34:04 [INFO]: Epoch 049 - training loss: 0.7276, validation loss: 0.9072
2024-06-03 02:34:04 [INFO]: Epoch 050 - training loss: 0.7232, validation loss: 0.9072
2024-06-03 02:34:05 [INFO]: Epoch 051 - training loss: 0.7201, validation loss: 0.9071
2024-06-03 02:34:05 [INFO]: Epoch 052 - training loss: 0.7122, validation loss: 0.9071
2024-06-03 02:34:05 [INFO]: Epoch 053 - training loss: 0.7111, validation loss: 0.9071
2024-06-03 02:34:05 [INFO]: Epoch 054 - training loss: 0.7250, validation loss: 0.9070
2024-06-03 02:34:06 [INFO]: Epoch 055 - training loss: 0.7172, validation loss: 0.9070
2024-06-03 02:34:06 [INFO]: Epoch 056 - training loss: 0.7367, validation loss: 0.9070
2024-06-03 02:34:06 [INFO]: Epoch 057 - training loss: 0.7203, validation loss: 0.9069
2024-06-03 02:34:07 [INFO]: Epoch 058 - training loss: 0.7120, validation loss: 0.9069
2024-06-03 02:34:07 [INFO]: Epoch 059 - training loss: 0.6991, validation loss: 0.9069
2024-06-03 02:34:07 [INFO]: Epoch 060 - training loss: 0.7199, validation loss: 0.9068
2024-06-03 02:34:08 [INFO]: Epoch 061 - training loss: 0.7162, validation loss: 0.9068
2024-06-03 02:34:08 [INFO]: Epoch 062 - training loss: 0.7281, validation loss: 0.9068
2024-06-03 02:34:08 [INFO]: Epoch 063 - training loss: 0.7256, validation loss: 0.9068
2024-06-03 02:34:09 [INFO]: Epoch 064 - training loss: 0.7181, validation loss: 0.9068
2024-06-03 02:34:09 [INFO]: Epoch 065 - training loss: 0.7189, validation loss: 0.9067
2024-06-03 02:34:09 [INFO]: Epoch 066 - training loss: 0.7207, validation loss: 0.9067
2024-06-03 02:34:10 [INFO]: Epoch 067 - training loss: 0.7234, validation loss: 0.9067
2024-06-03 02:34:10 [INFO]: Epoch 068 - training loss: 0.7176, validation loss: 0.9067
2024-06-03 02:34:10 [INFO]: Epoch 069 - training loss: 0.7123, validation loss: 0.9067
2024-06-03 02:34:11 [INFO]: Epoch 070 - training loss: 0.7239, validation loss: 0.9067
2024-06-03 02:34:11 [INFO]: Epoch 071 - training loss: 0.7183, validation loss: 0.9066
2024-06-03 02:34:11 [INFO]: Epoch 072 - training loss: 0.7105, validation loss: 0.9066
2024-06-03 02:34:11 [INFO]: Epoch 073 - training loss: 0.7183, validation loss: 0.9066
2024-06-03 02:34:12 [INFO]: Epoch 074 - training loss: 0.7209, validation loss: 0.9066
2024-06-03 02:34:12 [INFO]: Epoch 075 - training loss: 0.7179, validation loss: 0.9066
2024-06-03 02:34:12 [INFO]: Epoch 076 - training loss: 0.7130, validation loss: 0.9066
2024-06-03 02:34:13 [INFO]: Epoch 077 - training loss: 0.7180, validation loss: 0.9065
2024-06-03 02:34:13 [INFO]: Epoch 078 - training loss: 0.7120, validation loss: 0.9065
2024-06-03 02:34:13 [INFO]: Epoch 079 - training loss: 0.7039, validation loss: 0.9065
2024-06-03 02:34:14 [INFO]: Epoch 080 - training loss: 0.7022, validation loss: 0.9065
2024-06-03 02:34:14 [INFO]: Epoch 081 - training loss: 0.7223, validation loss: 0.9065
2024-06-03 02:34:14 [INFO]: Epoch 082 - training loss: 0.7165, validation loss: 0.9065
2024-06-03 02:34:15 [INFO]: Epoch 083 - training loss: 0.7247, validation loss: 0.9065
2024-06-03 02:34:15 [INFO]: Epoch 084 - training loss: 0.7168, validation loss: 0.9065
2024-06-03 02:34:15 [INFO]: Epoch 085 - training loss: 0.7333, validation loss: 0.9065
2024-06-03 02:34:16 [INFO]: Epoch 086 - training loss: 0.7219, validation loss: 0.9064
2024-06-03 02:34:16 [INFO]: Epoch 087 - training loss: 0.7094, validation loss: 0.9064
2024-06-03 02:34:16 [INFO]: Epoch 088 - training loss: 0.7070, validation loss: 0.9064
2024-06-03 02:34:17 [INFO]: Epoch 089 - training loss: 0.7224, validation loss: 0.9064
2024-06-03 02:34:17 [INFO]: Epoch 090 - training loss: 0.7151, validation loss: 0.9064
2024-06-03 02:34:17 [INFO]: Epoch 091 - training loss: 0.7183, validation loss: 0.9064
2024-06-03 02:34:17 [INFO]: Epoch 092 - training loss: 0.7150, validation loss: 0.9064
2024-06-03 02:34:18 [INFO]: Epoch 093 - training loss: 0.7274, validation loss: 0.9064
2024-06-03 02:34:18 [INFO]: Epoch 094 - training loss: 0.7077, validation loss: 0.9064
2024-06-03 02:34:18 [INFO]: Epoch 095 - training loss: 0.7125, validation loss: 0.9064
2024-06-03 02:34:19 [INFO]: Epoch 096 - training loss: 0.7194, validation loss: 0.9064
2024-06-03 02:34:19 [INFO]: Epoch 097 - training loss: 0.7073, validation loss: 0.9064
2024-06-03 02:34:19 [INFO]: Epoch 098 - training loss: 0.7309, validation loss: 0.9063
2024-06-03 02:34:20 [INFO]: Epoch 099 - training loss: 0.7287, validation loss: 0.9063
2024-06-03 02:34:20 [INFO]: Epoch 100 - training loss: 0.7149, validation loss: 0.9063
2024-06-03 02:34:20 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 02:34:20 [INFO]: Saved the model to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_4/20240603_T023347/MRNN.pypots
2024-06-03 02:34:24 [INFO]: Successfully saved to results_subseq_rate05/Pedestrian/MRNN_Pedestrian/round_4/imputation.pkl
2024-06-03 02:34:24 [INFO]: Round4 - MRNN on Pedestrian: MAE=0.7684, MSE=0.9914, MRE=1.0036
2024-06-03 02:34:24 [INFO]: Done! Final results:
Averaged MRNN (401,415 params) on Pedestrian: MAE=0.7680 ± 0.0004600556975887135, MSE=0.9915 ± 3.0605332757480075e-05, MRE=1.0031 ± 0.0006008537928611327, average inference time=5.48
