2024-06-03 02:53:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 02:53:54 [INFO]: Using the given device: cuda:0
2024-06-03 02:53:54 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_0/20240603_T025354
2024-06-03 02:53:54 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_0/20240603_T025354/tensorboard
2024-06-03 02:53:55 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 02:54:02 [INFO]: Epoch 001 - training loss: 1.6955, validation loss: 1.6177
2024-06-03 02:54:07 [INFO]: Epoch 002 - training loss: 1.5959, validation loss: 1.5422
2024-06-03 02:54:14 [INFO]: Epoch 003 - training loss: 1.5386, validation loss: 1.4990
2024-06-03 02:54:21 [INFO]: Epoch 004 - training loss: 1.5016, validation loss: 1.4769
2024-06-03 02:54:28 [INFO]: Epoch 005 - training loss: 1.4754, validation loss: 1.4653
2024-06-03 02:54:35 [INFO]: Epoch 006 - training loss: 1.4531, validation loss: 1.4536
2024-06-03 02:54:43 [INFO]: Epoch 007 - training loss: 1.4173, validation loss: 1.4467
2024-06-03 02:54:49 [INFO]: Epoch 008 - training loss: 1.3639, validation loss: 1.4259
2024-06-03 02:54:56 [INFO]: Epoch 009 - training loss: 1.3069, validation loss: 1.4273
2024-06-03 02:55:02 [INFO]: Epoch 010 - training loss: 1.2459, validation loss: 1.4079
2024-06-03 02:55:09 [INFO]: Epoch 011 - training loss: 1.1678, validation loss: 1.3845
2024-06-03 02:55:16 [INFO]: Epoch 012 - training loss: 1.0979, validation loss: 1.3663
2024-06-03 02:55:23 [INFO]: Epoch 013 - training loss: 1.1022, validation loss: 1.3764
2024-06-03 02:55:30 [INFO]: Epoch 014 - training loss: 1.0047, validation loss: 1.3622
2024-06-03 02:55:38 [INFO]: Epoch 015 - training loss: 0.9523, validation loss: 1.3747
2024-06-03 02:55:45 [INFO]: Epoch 016 - training loss: 0.9200, validation loss: 1.3489
2024-06-03 02:55:51 [INFO]: Epoch 017 - training loss: 0.8958, validation loss: 1.3282
2024-06-03 02:55:57 [INFO]: Epoch 018 - training loss: 0.8799, validation loss: 1.3331
2024-06-03 02:56:03 [INFO]: Epoch 019 - training loss: 0.8684, validation loss: 1.2880
2024-06-03 02:56:10 [INFO]: Epoch 020 - training loss: 0.8555, validation loss: 1.2628
2024-06-03 02:56:18 [INFO]: Epoch 021 - training loss: 0.8553, validation loss: 1.3102
2024-06-03 02:56:24 [INFO]: Epoch 022 - training loss: 0.8232, validation loss: 1.2878
2024-06-03 02:56:32 [INFO]: Epoch 023 - training loss: 0.7965, validation loss: 1.2719
2024-06-03 02:56:39 [INFO]: Epoch 024 - training loss: 0.7835, validation loss: 1.2673
2024-06-03 02:56:44 [INFO]: Epoch 025 - training loss: 0.7745, validation loss: 1.2632
2024-06-03 02:56:50 [INFO]: Epoch 026 - training loss: 0.7623, validation loss: 1.2515
2024-06-03 02:56:58 [INFO]: Epoch 027 - training loss: 0.7568, validation loss: 1.2372
2024-06-03 02:57:05 [INFO]: Epoch 028 - training loss: 0.7478, validation loss: 1.2324
2024-06-03 02:57:12 [INFO]: Epoch 029 - training loss: 0.7448, validation loss: 1.2390
2024-06-03 02:57:19 [INFO]: Epoch 030 - training loss: 0.7414, validation loss: 1.2425
2024-06-03 02:57:26 [INFO]: Epoch 031 - training loss: 0.7378, validation loss: 1.2571
2024-06-03 02:57:33 [INFO]: Epoch 032 - training loss: 0.7274, validation loss: 1.2467
2024-06-03 02:57:38 [INFO]: Epoch 033 - training loss: 0.7264, validation loss: 1.2472
2024-06-03 02:57:45 [INFO]: Epoch 034 - training loss: 0.7286, validation loss: 1.2380
2024-06-03 02:57:51 [INFO]: Epoch 035 - training loss: 0.7259, validation loss: 1.2367
2024-06-03 02:57:59 [INFO]: Epoch 036 - training loss: 0.7190, validation loss: 1.2323
2024-06-03 02:58:05 [INFO]: Epoch 037 - training loss: 0.7137, validation loss: 1.2331
2024-06-03 02:58:13 [INFO]: Epoch 038 - training loss: 0.7110, validation loss: 1.2254
2024-06-03 02:58:20 [INFO]: Epoch 039 - training loss: 0.7058, validation loss: 1.2193
2024-06-03 02:58:27 [INFO]: Epoch 040 - training loss: 0.7032, validation loss: 1.2168
2024-06-03 02:58:34 [INFO]: Epoch 041 - training loss: 0.7019, validation loss: 1.2137
2024-06-03 02:58:41 [INFO]: Epoch 042 - training loss: 0.6955, validation loss: 1.2057
2024-06-03 02:58:50 [INFO]: Epoch 043 - training loss: 0.6969, validation loss: 1.1962
2024-06-03 02:58:58 [INFO]: Epoch 044 - training loss: 0.6921, validation loss: 1.1819
2024-06-03 02:59:05 [INFO]: Epoch 045 - training loss: 0.6836, validation loss: 1.1659
2024-06-03 02:59:12 [INFO]: Epoch 046 - training loss: 0.6797, validation loss: 1.1491
2024-06-03 02:59:19 [INFO]: Epoch 047 - training loss: 0.6787, validation loss: 1.1376
2024-06-03 02:59:25 [INFO]: Epoch 048 - training loss: 0.6720, validation loss: 1.1255
2024-06-03 02:59:31 [INFO]: Epoch 049 - training loss: 0.6704, validation loss: 1.1174
2024-06-03 02:59:37 [INFO]: Epoch 050 - training loss: 0.6626, validation loss: 1.1149
2024-06-03 02:59:44 [INFO]: Epoch 051 - training loss: 0.6658, validation loss: 1.1061
2024-06-03 02:59:51 [INFO]: Epoch 052 - training loss: 0.6604, validation loss: 1.1212
2024-06-03 02:59:57 [INFO]: Epoch 053 - training loss: 0.6610, validation loss: 1.1010
2024-06-03 03:00:04 [INFO]: Epoch 054 - training loss: 0.6610, validation loss: 1.0972
2024-06-03 03:00:10 [INFO]: Epoch 055 - training loss: 0.6530, validation loss: 1.1072
2024-06-03 03:00:16 [INFO]: Epoch 056 - training loss: 0.6582, validation loss: 1.0964
2024-06-03 03:00:22 [INFO]: Epoch 057 - training loss: 0.6539, validation loss: 1.0915
2024-06-03 03:00:29 [INFO]: Epoch 058 - training loss: 0.6556, validation loss: 1.0965
2024-06-03 03:00:34 [INFO]: Epoch 059 - training loss: 0.6489, validation loss: 1.0985
2024-06-03 03:00:41 [INFO]: Epoch 060 - training loss: 0.6508, validation loss: 1.0879
2024-06-03 03:00:48 [INFO]: Epoch 061 - training loss: 0.6501, validation loss: 1.0860
2024-06-03 03:00:54 [INFO]: Epoch 062 - training loss: 0.6434, validation loss: 1.0873
2024-06-03 03:01:00 [INFO]: Epoch 063 - training loss: 0.6496, validation loss: 1.0908
2024-06-03 03:01:06 [INFO]: Epoch 064 - training loss: 0.6477, validation loss: 1.0802
2024-06-03 03:01:12 [INFO]: Epoch 065 - training loss: 0.6414, validation loss: 1.0790
2024-06-03 03:01:19 [INFO]: Epoch 066 - training loss: 0.6381, validation loss: 1.0827
2024-06-03 03:01:25 [INFO]: Epoch 067 - training loss: 0.6411, validation loss: 1.0798
2024-06-03 03:01:31 [INFO]: Epoch 068 - training loss: 0.6351, validation loss: 1.0826
2024-06-03 03:01:38 [INFO]: Epoch 069 - training loss: 0.6366, validation loss: 1.0802
2024-06-03 03:01:45 [INFO]: Epoch 070 - training loss: 0.6381, validation loss: 1.0897
2024-06-03 03:01:51 [INFO]: Epoch 071 - training loss: 0.6378, validation loss: 1.0799
2024-06-03 03:01:56 [INFO]: Epoch 072 - training loss: 0.6348, validation loss: 1.0738
2024-06-03 03:02:02 [INFO]: Epoch 073 - training loss: 0.6391, validation loss: 1.0748
2024-06-03 03:02:08 [INFO]: Epoch 074 - training loss: 0.6320, validation loss: 1.0708
2024-06-03 03:02:15 [INFO]: Epoch 075 - training loss: 0.6301, validation loss: 1.0731
2024-06-03 03:02:21 [INFO]: Epoch 076 - training loss: 0.6300, validation loss: 1.0721
2024-06-03 03:02:28 [INFO]: Epoch 077 - training loss: 0.6307, validation loss: 1.0784
2024-06-03 03:02:35 [INFO]: Epoch 078 - training loss: 0.6277, validation loss: 1.0726
2024-06-03 03:02:41 [INFO]: Epoch 079 - training loss: 0.6285, validation loss: 1.0809
2024-06-03 03:02:47 [INFO]: Epoch 080 - training loss: 0.6336, validation loss: 1.0759
2024-06-03 03:02:53 [INFO]: Epoch 081 - training loss: 0.6315, validation loss: 1.0711
2024-06-03 03:03:00 [INFO]: Epoch 082 - training loss: 0.6293, validation loss: 1.0743
2024-06-03 03:03:07 [INFO]: Epoch 083 - training loss: 0.6264, validation loss: 1.0714
2024-06-03 03:03:14 [INFO]: Epoch 084 - training loss: 0.6264, validation loss: 1.0781
2024-06-03 03:03:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:03:14 [INFO]: Finished training. The best model is from epoch#74.
2024-06-03 03:03:14 [INFO]: Saved the model to results_subseq_rate05/PeMS/Autoformer_PeMS/round_0/20240603_T025354/Autoformer.pypots
2024-06-03 03:03:16 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_0/imputation.pkl
2024-06-03 03:03:16 [INFO]: Round0 - Autoformer on PeMS: MAE=0.6596, MSE=1.4229, MRE=0.7795
2024-06-03 03:03:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 03:03:16 [INFO]: Using the given device: cuda:0
2024-06-03 03:03:16 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_1/20240603_T030316
2024-06-03 03:03:16 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_1/20240603_T030316/tensorboard
2024-06-03 03:03:16 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 03:03:22 [INFO]: Epoch 001 - training loss: 1.7205, validation loss: 1.6920
2024-06-03 03:03:29 [INFO]: Epoch 002 - training loss: 1.6294, validation loss: 1.5975
2024-06-03 03:03:34 [INFO]: Epoch 003 - training loss: 1.5666, validation loss: 1.5417
2024-06-03 03:03:40 [INFO]: Epoch 004 - training loss: 1.5202, validation loss: 1.5072
2024-06-03 03:03:46 [INFO]: Epoch 005 - training loss: 1.4921, validation loss: 1.4877
2024-06-03 03:03:53 [INFO]: Epoch 006 - training loss: 1.4568, validation loss: 1.4777
2024-06-03 03:03:58 [INFO]: Epoch 007 - training loss: 1.4361, validation loss: 1.4695
2024-06-03 03:04:04 [INFO]: Epoch 008 - training loss: 1.4055, validation loss: 1.4572
2024-06-03 03:04:10 [INFO]: Epoch 009 - training loss: 1.3537, validation loss: 1.4428
2024-06-03 03:04:15 [INFO]: Epoch 010 - training loss: 1.3098, validation loss: 1.4287
2024-06-03 03:04:20 [INFO]: Epoch 011 - training loss: 1.2735, validation loss: 1.4216
2024-06-03 03:04:25 [INFO]: Epoch 012 - training loss: 1.2437, validation loss: 1.4143
2024-06-03 03:04:30 [INFO]: Epoch 013 - training loss: 1.2199, validation loss: 1.4173
2024-06-03 03:04:35 [INFO]: Epoch 014 - training loss: 1.1967, validation loss: 1.4158
2024-06-03 03:04:40 [INFO]: Epoch 015 - training loss: 1.1598, validation loss: 1.4267
2024-06-03 03:04:45 [INFO]: Epoch 016 - training loss: 1.1403, validation loss: 1.4297
2024-06-03 03:04:50 [INFO]: Epoch 017 - training loss: 1.1166, validation loss: 1.4220
2024-06-03 03:04:56 [INFO]: Epoch 018 - training loss: 1.0985, validation loss: 1.4028
2024-06-03 03:05:01 [INFO]: Epoch 019 - training loss: 1.0788, validation loss: 1.3987
2024-06-03 03:05:06 [INFO]: Epoch 020 - training loss: 1.0539, validation loss: 1.3742
2024-06-03 03:05:11 [INFO]: Epoch 021 - training loss: 1.0304, validation loss: 1.3640
2024-06-03 03:05:15 [INFO]: Epoch 022 - training loss: 1.0176, validation loss: 1.3677
2024-06-03 03:05:20 [INFO]: Epoch 023 - training loss: 1.0030, validation loss: 1.3613
2024-06-03 03:05:25 [INFO]: Epoch 024 - training loss: 0.9912, validation loss: 1.3692
2024-06-03 03:05:30 [INFO]: Epoch 025 - training loss: 0.9817, validation loss: 1.3578
2024-06-03 03:05:35 [INFO]: Epoch 026 - training loss: 0.9709, validation loss: 1.3636
2024-06-03 03:05:40 [INFO]: Epoch 027 - training loss: 0.9640, validation loss: 1.3837
2024-06-03 03:05:45 [INFO]: Epoch 028 - training loss: 0.9522, validation loss: 1.3630
2024-06-03 03:05:51 [INFO]: Epoch 029 - training loss: 0.9420, validation loss: 1.3655
2024-06-03 03:05:56 [INFO]: Epoch 030 - training loss: 0.9300, validation loss: 1.3772
2024-06-03 03:06:00 [INFO]: Epoch 031 - training loss: 0.9297, validation loss: 1.3685
2024-06-03 03:06:05 [INFO]: Epoch 032 - training loss: 0.9106, validation loss: 1.3536
2024-06-03 03:06:10 [INFO]: Epoch 033 - training loss: 0.8923, validation loss: 1.3609
2024-06-03 03:06:15 [INFO]: Epoch 034 - training loss: 0.8800, validation loss: 1.3432
2024-06-03 03:06:20 [INFO]: Epoch 035 - training loss: 0.8709, validation loss: 1.3340
2024-06-03 03:06:25 [INFO]: Epoch 036 - training loss: 0.8599, validation loss: 1.3296
2024-06-03 03:06:30 [INFO]: Epoch 037 - training loss: 0.8579, validation loss: 1.3272
2024-06-03 03:06:35 [INFO]: Epoch 038 - training loss: 0.8405, validation loss: 1.3118
2024-06-03 03:06:41 [INFO]: Epoch 039 - training loss: 1.0259, validation loss: 1.4638
2024-06-03 03:06:45 [INFO]: Epoch 040 - training loss: 1.0472, validation loss: 1.3872
2024-06-03 03:06:50 [INFO]: Epoch 041 - training loss: 0.9452, validation loss: 1.3671
2024-06-03 03:06:55 [INFO]: Epoch 042 - training loss: 0.9017, validation loss: 1.3428
2024-06-03 03:07:00 [INFO]: Epoch 043 - training loss: 0.8804, validation loss: 1.3305
2024-06-03 03:07:05 [INFO]: Epoch 044 - training loss: 0.8595, validation loss: 1.3139
2024-06-03 03:07:10 [INFO]: Epoch 045 - training loss: 0.8478, validation loss: 1.3052
2024-06-03 03:07:15 [INFO]: Epoch 046 - training loss: 0.8356, validation loss: 1.2939
2024-06-03 03:07:20 [INFO]: Epoch 047 - training loss: 0.8282, validation loss: 1.2931
2024-06-03 03:07:25 [INFO]: Epoch 048 - training loss: 0.8216, validation loss: 1.2833
2024-06-03 03:07:30 [INFO]: Epoch 049 - training loss: 0.8179, validation loss: 1.2817
2024-06-03 03:07:35 [INFO]: Epoch 050 - training loss: 0.8110, validation loss: 1.2716
2024-06-03 03:07:39 [INFO]: Epoch 051 - training loss: 0.8094, validation loss: 1.2782
2024-06-03 03:07:45 [INFO]: Epoch 052 - training loss: 0.7997, validation loss: 1.2763
2024-06-03 03:07:49 [INFO]: Epoch 053 - training loss: 0.7908, validation loss: 1.2743
2024-06-03 03:07:54 [INFO]: Epoch 054 - training loss: 0.7844, validation loss: 1.2705
2024-06-03 03:08:00 [INFO]: Epoch 055 - training loss: 0.7814, validation loss: 1.2727
2024-06-03 03:08:05 [INFO]: Epoch 056 - training loss: 0.7776, validation loss: 1.2771
2024-06-03 03:08:10 [INFO]: Epoch 057 - training loss: 0.7733, validation loss: 1.2790
2024-06-03 03:08:15 [INFO]: Epoch 058 - training loss: 0.7677, validation loss: 1.2824
2024-06-03 03:08:19 [INFO]: Epoch 059 - training loss: 0.7695, validation loss: 1.2824
2024-06-03 03:08:23 [INFO]: Epoch 060 - training loss: 0.7615, validation loss: 1.2805
2024-06-03 03:08:28 [INFO]: Epoch 061 - training loss: 0.7646, validation loss: 1.2802
2024-06-03 03:08:33 [INFO]: Epoch 062 - training loss: 0.7609, validation loss: 1.2845
2024-06-03 03:08:38 [INFO]: Epoch 063 - training loss: 0.7535, validation loss: 1.2839
2024-06-03 03:08:43 [INFO]: Epoch 064 - training loss: 0.7533, validation loss: 1.2842
2024-06-03 03:08:43 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:08:43 [INFO]: Finished training. The best model is from epoch#54.
2024-06-03 03:08:43 [INFO]: Saved the model to results_subseq_rate05/PeMS/Autoformer_PeMS/round_1/20240603_T030316/Autoformer.pypots
2024-06-03 03:08:45 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_1/imputation.pkl
2024-06-03 03:08:45 [INFO]: Round1 - Autoformer on PeMS: MAE=0.7495, MSE=1.6141, MRE=0.8857
2024-06-03 03:08:45 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 03:08:45 [INFO]: Using the given device: cuda:0
2024-06-03 03:08:45 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_2/20240603_T030845
2024-06-03 03:08:45 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_2/20240603_T030845/tensorboard
2024-06-03 03:08:45 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 03:08:50 [INFO]: Epoch 001 - training loss: 1.7103, validation loss: 1.6528
2024-06-03 03:08:55 [INFO]: Epoch 002 - training loss: 1.6079, validation loss: 1.5412
2024-06-03 03:09:01 [INFO]: Epoch 003 - training loss: 1.5331, validation loss: 1.4891
2024-06-03 03:09:05 [INFO]: Epoch 004 - training loss: 1.4881, validation loss: 1.4564
2024-06-03 03:09:10 [INFO]: Epoch 005 - training loss: 1.4277, validation loss: 1.4350
2024-06-03 03:09:15 [INFO]: Epoch 006 - training loss: 1.3388, validation loss: 1.4149
2024-06-03 03:09:20 [INFO]: Epoch 007 - training loss: 1.2095, validation loss: 1.3852
2024-06-03 03:09:25 [INFO]: Epoch 008 - training loss: 1.0432, validation loss: 1.3334
2024-06-03 03:09:30 [INFO]: Epoch 009 - training loss: 0.9274, validation loss: 1.3082
2024-06-03 03:09:35 [INFO]: Epoch 010 - training loss: 0.8441, validation loss: 1.1961
2024-06-03 03:09:40 [INFO]: Epoch 011 - training loss: 0.8007, validation loss: 1.1006
2024-06-03 03:09:45 [INFO]: Epoch 012 - training loss: 0.7525, validation loss: 1.0674
2024-06-03 03:09:50 [INFO]: Epoch 013 - training loss: 0.7269, validation loss: 1.0445
2024-06-03 03:09:55 [INFO]: Epoch 014 - training loss: 0.7194, validation loss: 1.0340
2024-06-03 03:10:00 [INFO]: Epoch 015 - training loss: 0.7083, validation loss: 1.0170
2024-06-03 03:10:05 [INFO]: Epoch 016 - training loss: 0.7029, validation loss: 1.0078
2024-06-03 03:10:10 [INFO]: Epoch 017 - training loss: 0.6953, validation loss: 1.0014
2024-06-03 03:10:15 [INFO]: Epoch 018 - training loss: 0.6898, validation loss: 0.9972
2024-06-03 03:10:20 [INFO]: Epoch 019 - training loss: 0.6921, validation loss: 0.9993
2024-06-03 03:10:25 [INFO]: Epoch 020 - training loss: 0.6883, validation loss: 0.9876
2024-06-03 03:10:30 [INFO]: Epoch 021 - training loss: 0.6847, validation loss: 0.9866
2024-06-03 03:10:35 [INFO]: Epoch 022 - training loss: 0.6779, validation loss: 0.9809
2024-06-03 03:10:40 [INFO]: Epoch 023 - training loss: 0.6750, validation loss: 0.9842
2024-06-03 03:10:44 [INFO]: Epoch 024 - training loss: 0.6688, validation loss: 0.9787
2024-06-03 03:10:48 [INFO]: Epoch 025 - training loss: 0.6724, validation loss: 0.9717
2024-06-03 03:10:53 [INFO]: Epoch 026 - training loss: 0.6683, validation loss: 0.9828
2024-06-03 03:10:58 [INFO]: Epoch 027 - training loss: 0.6694, validation loss: 0.9698
2024-06-03 03:11:03 [INFO]: Epoch 028 - training loss: 0.6677, validation loss: 0.9621
2024-06-03 03:11:08 [INFO]: Epoch 029 - training loss: 0.6631, validation loss: 0.9722
2024-06-03 03:11:13 [INFO]: Epoch 030 - training loss: 0.6626, validation loss: 0.9725
2024-06-03 03:11:19 [INFO]: Epoch 031 - training loss: 0.6600, validation loss: 0.9683
2024-06-03 03:11:24 [INFO]: Epoch 032 - training loss: 0.6623, validation loss: 0.9563
2024-06-03 03:11:28 [INFO]: Epoch 033 - training loss: 0.6593, validation loss: 0.9738
2024-06-03 03:11:32 [INFO]: Epoch 034 - training loss: 0.6640, validation loss: 0.9594
2024-06-03 03:11:37 [INFO]: Epoch 035 - training loss: 0.6623, validation loss: 0.9588
2024-06-03 03:11:42 [INFO]: Epoch 036 - training loss: 0.6553, validation loss: 0.9533
2024-06-03 03:11:47 [INFO]: Epoch 037 - training loss: 0.6567, validation loss: 0.9552
2024-06-03 03:11:53 [INFO]: Epoch 038 - training loss: 0.6530, validation loss: 0.9566
2024-06-03 03:11:58 [INFO]: Epoch 039 - training loss: 0.6501, validation loss: 0.9580
2024-06-03 03:12:03 [INFO]: Epoch 040 - training loss: 0.6505, validation loss: 0.9545
2024-06-03 03:12:06 [INFO]: Epoch 041 - training loss: 0.6500, validation loss: 0.9553
2024-06-03 03:12:09 [INFO]: Epoch 042 - training loss: 0.6490, validation loss: 0.9594
2024-06-03 03:12:12 [INFO]: Epoch 043 - training loss: 0.6489, validation loss: 0.9499
2024-06-03 03:12:15 [INFO]: Epoch 044 - training loss: 0.6507, validation loss: 0.9561
2024-06-03 03:12:18 [INFO]: Epoch 045 - training loss: 0.6441, validation loss: 0.9619
2024-06-03 03:12:21 [INFO]: Epoch 046 - training loss: 0.6501, validation loss: 0.9488
2024-06-03 03:12:24 [INFO]: Epoch 047 - training loss: 0.6416, validation loss: 0.9541
2024-06-03 03:12:27 [INFO]: Epoch 048 - training loss: 0.6478, validation loss: 0.9512
2024-06-03 03:12:30 [INFO]: Epoch 049 - training loss: 0.6428, validation loss: 0.9541
2024-06-03 03:12:34 [INFO]: Epoch 050 - training loss: 0.6422, validation loss: 0.9519
2024-06-03 03:12:37 [INFO]: Epoch 051 - training loss: 0.6404, validation loss: 0.9580
2024-06-03 03:12:40 [INFO]: Epoch 052 - training loss: 0.6407, validation loss: 0.9476
2024-06-03 03:12:43 [INFO]: Epoch 053 - training loss: 0.6418, validation loss: 0.9437
2024-06-03 03:12:46 [INFO]: Epoch 054 - training loss: 0.6423, validation loss: 0.9493
2024-06-03 03:12:49 [INFO]: Epoch 055 - training loss: 0.6399, validation loss: 0.9577
2024-06-03 03:12:51 [INFO]: Epoch 056 - training loss: 0.6398, validation loss: 0.9517
2024-06-03 03:12:54 [INFO]: Epoch 057 - training loss: 0.6378, validation loss: 0.9475
2024-06-03 03:12:58 [INFO]: Epoch 058 - training loss: 0.6385, validation loss: 0.9453
2024-06-03 03:13:01 [INFO]: Epoch 059 - training loss: 0.6360, validation loss: 0.9477
2024-06-03 03:13:04 [INFO]: Epoch 060 - training loss: 0.6380, validation loss: 0.9601
2024-06-03 03:13:07 [INFO]: Epoch 061 - training loss: 0.6352, validation loss: 0.9497
2024-06-03 03:13:10 [INFO]: Epoch 062 - training loss: 0.6366, validation loss: 0.9496
2024-06-03 03:13:13 [INFO]: Epoch 063 - training loss: 0.6349, validation loss: 0.9559
2024-06-03 03:13:13 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:13:13 [INFO]: Finished training. The best model is from epoch#53.
2024-06-03 03:13:13 [INFO]: Saved the model to results_subseq_rate05/PeMS/Autoformer_PeMS/round_2/20240603_T030845/Autoformer.pypots
2024-06-03 03:13:14 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_2/imputation.pkl
2024-06-03 03:13:14 [INFO]: Round2 - Autoformer on PeMS: MAE=0.6119, MSE=1.2909, MRE=0.7231
2024-06-03 03:13:14 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 03:13:14 [INFO]: Using the given device: cuda:0
2024-06-03 03:13:14 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_3/20240603_T031314
2024-06-03 03:13:14 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_3/20240603_T031314/tensorboard
2024-06-03 03:13:14 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 03:13:17 [INFO]: Epoch 001 - training loss: 1.6985, validation loss: 1.6267
2024-06-03 03:13:20 [INFO]: Epoch 002 - training loss: 1.5924, validation loss: 1.5458
2024-06-03 03:13:23 [INFO]: Epoch 003 - training loss: 1.5346, validation loss: 1.5002
2024-06-03 03:13:25 [INFO]: Epoch 004 - training loss: 1.4906, validation loss: 1.4647
2024-06-03 03:13:29 [INFO]: Epoch 005 - training loss: 1.4507, validation loss: 1.4527
2024-06-03 03:13:32 [INFO]: Epoch 006 - training loss: 1.4026, validation loss: 1.4285
2024-06-03 03:13:35 [INFO]: Epoch 007 - training loss: 1.2754, validation loss: 1.4025
2024-06-03 03:13:38 [INFO]: Epoch 008 - training loss: 1.1275, validation loss: 1.3420
2024-06-03 03:13:41 [INFO]: Epoch 009 - training loss: 0.9762, validation loss: 1.3124
2024-06-03 03:13:45 [INFO]: Epoch 010 - training loss: 0.8701, validation loss: 1.2740
2024-06-03 03:13:48 [INFO]: Epoch 011 - training loss: 0.8247, validation loss: 1.1404
2024-06-03 03:13:51 [INFO]: Epoch 012 - training loss: 0.7684, validation loss: 1.0987
2024-06-03 03:13:54 [INFO]: Epoch 013 - training loss: 0.7359, validation loss: 1.0580
2024-06-03 03:13:56 [INFO]: Epoch 014 - training loss: 0.7182, validation loss: 1.0491
2024-06-03 03:13:59 [INFO]: Epoch 015 - training loss: 0.7113, validation loss: 1.0249
2024-06-03 03:14:02 [INFO]: Epoch 016 - training loss: 0.7060, validation loss: 1.0366
2024-06-03 03:14:05 [INFO]: Epoch 017 - training loss: 0.7025, validation loss: 1.0106
2024-06-03 03:14:09 [INFO]: Epoch 018 - training loss: 0.6967, validation loss: 1.0121
2024-06-03 03:14:12 [INFO]: Epoch 019 - training loss: 0.6879, validation loss: 1.0070
2024-06-03 03:14:16 [INFO]: Epoch 020 - training loss: 0.6839, validation loss: 1.0006
2024-06-03 03:14:19 [INFO]: Epoch 021 - training loss: 0.6859, validation loss: 1.0017
2024-06-03 03:14:22 [INFO]: Epoch 022 - training loss: 0.6929, validation loss: 1.0038
2024-06-03 03:14:25 [INFO]: Epoch 023 - training loss: 0.6810, validation loss: 0.9860
2024-06-03 03:14:28 [INFO]: Epoch 024 - training loss: 0.6800, validation loss: 0.9841
2024-06-03 03:14:31 [INFO]: Epoch 025 - training loss: 0.6792, validation loss: 0.9887
2024-06-03 03:14:34 [INFO]: Epoch 026 - training loss: 0.6747, validation loss: 0.9843
2024-06-03 03:14:37 [INFO]: Epoch 027 - training loss: 0.6713, validation loss: 0.9784
2024-06-03 03:14:40 [INFO]: Epoch 028 - training loss: 0.6734, validation loss: 0.9880
2024-06-03 03:14:43 [INFO]: Epoch 029 - training loss: 0.6704, validation loss: 0.9799
2024-06-03 03:14:45 [INFO]: Epoch 030 - training loss: 0.6707, validation loss: 0.9771
2024-06-03 03:14:47 [INFO]: Epoch 031 - training loss: 0.6650, validation loss: 0.9826
2024-06-03 03:14:50 [INFO]: Epoch 032 - training loss: 0.6663, validation loss: 0.9721
2024-06-03 03:14:52 [INFO]: Epoch 033 - training loss: 0.6600, validation loss: 0.9749
2024-06-03 03:14:55 [INFO]: Epoch 034 - training loss: 0.6608, validation loss: 0.9698
2024-06-03 03:14:57 [INFO]: Epoch 035 - training loss: 0.6598, validation loss: 0.9745
2024-06-03 03:15:00 [INFO]: Epoch 036 - training loss: 0.6613, validation loss: 0.9682
2024-06-03 03:15:02 [INFO]: Epoch 037 - training loss: 0.6535, validation loss: 0.9720
2024-06-03 03:15:04 [INFO]: Epoch 038 - training loss: 0.6581, validation loss: 0.9745
2024-06-03 03:15:06 [INFO]: Epoch 039 - training loss: 0.6594, validation loss: 0.9730
2024-06-03 03:15:09 [INFO]: Epoch 040 - training loss: 0.6563, validation loss: 0.9623
2024-06-03 03:15:11 [INFO]: Epoch 041 - training loss: 0.6543, validation loss: 0.9605
2024-06-03 03:15:14 [INFO]: Epoch 042 - training loss: 0.6514, validation loss: 0.9710
2024-06-03 03:15:16 [INFO]: Epoch 043 - training loss: 0.6514, validation loss: 0.9595
2024-06-03 03:15:19 [INFO]: Epoch 044 - training loss: 0.6529, validation loss: 0.9663
2024-06-03 03:15:21 [INFO]: Epoch 045 - training loss: 0.6517, validation loss: 0.9669
2024-06-03 03:15:24 [INFO]: Epoch 046 - training loss: 0.6500, validation loss: 0.9662
2024-06-03 03:15:27 [INFO]: Epoch 047 - training loss: 0.6453, validation loss: 0.9665
2024-06-03 03:15:29 [INFO]: Epoch 048 - training loss: 0.6470, validation loss: 0.9573
2024-06-03 03:15:32 [INFO]: Epoch 049 - training loss: 0.6425, validation loss: 0.9615
2024-06-03 03:15:34 [INFO]: Epoch 050 - training loss: 0.6542, validation loss: 0.9609
2024-06-03 03:15:36 [INFO]: Epoch 051 - training loss: 0.6459, validation loss: 0.9625
2024-06-03 03:15:38 [INFO]: Epoch 052 - training loss: 0.6410, validation loss: 0.9665
2024-06-03 03:15:41 [INFO]: Epoch 053 - training loss: 0.6435, validation loss: 0.9574
2024-06-03 03:15:43 [INFO]: Epoch 054 - training loss: 0.6395, validation loss: 0.9699
2024-06-03 03:15:46 [INFO]: Epoch 055 - training loss: 0.6395, validation loss: 0.9610
2024-06-03 03:15:48 [INFO]: Epoch 056 - training loss: 0.6408, validation loss: 0.9642
2024-06-03 03:15:50 [INFO]: Epoch 057 - training loss: 0.6369, validation loss: 0.9600
2024-06-03 03:15:53 [INFO]: Epoch 058 - training loss: 0.6295, validation loss: 0.9659
2024-06-03 03:15:53 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:15:53 [INFO]: Finished training. The best model is from epoch#48.
2024-06-03 03:15:53 [INFO]: Saved the model to results_subseq_rate05/PeMS/Autoformer_PeMS/round_3/20240603_T031314/Autoformer.pypots
2024-06-03 03:15:53 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_3/imputation.pkl
2024-06-03 03:15:53 [INFO]: Round3 - Autoformer on PeMS: MAE=0.6224, MSE=1.3028, MRE=0.7355
2024-06-03 03:15:53 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 03:15:53 [INFO]: Using the given device: cuda:0
2024-06-03 03:15:53 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_4/20240603_T031553
2024-06-03 03:15:53 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_4/20240603_T031553/tensorboard
2024-06-03 03:15:54 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 608,926
2024-06-03 03:15:56 [INFO]: Epoch 001 - training loss: 1.7197, validation loss: 1.6721
2024-06-03 03:15:59 [INFO]: Epoch 002 - training loss: 1.6224, validation loss: 1.5794
2024-06-03 03:16:01 [INFO]: Epoch 003 - training loss: 1.5608, validation loss: 1.5128
2024-06-03 03:16:03 [INFO]: Epoch 004 - training loss: 1.5035, validation loss: 1.4743
2024-06-03 03:16:06 [INFO]: Epoch 005 - training loss: 1.4470, validation loss: 1.4456
2024-06-03 03:16:08 [INFO]: Epoch 006 - training loss: 1.3596, validation loss: 1.4164
2024-06-03 03:16:10 [INFO]: Epoch 007 - training loss: 1.2387, validation loss: 1.3885
2024-06-03 03:16:13 [INFO]: Epoch 008 - training loss: 1.0746, validation loss: 1.3623
2024-06-03 03:16:15 [INFO]: Epoch 009 - training loss: 0.9427, validation loss: 1.3345
2024-06-03 03:16:18 [INFO]: Epoch 010 - training loss: 0.8605, validation loss: 1.2451
2024-06-03 03:16:20 [INFO]: Epoch 011 - training loss: 0.7951, validation loss: 1.1334
2024-06-03 03:16:23 [INFO]: Epoch 012 - training loss: 0.7646, validation loss: 1.1076
2024-06-03 03:16:25 [INFO]: Epoch 013 - training loss: 0.7453, validation loss: 1.0898
2024-06-03 03:16:28 [INFO]: Epoch 014 - training loss: 0.7302, validation loss: 1.0588
2024-06-03 03:16:31 [INFO]: Epoch 015 - training loss: 0.7174, validation loss: 1.0494
2024-06-03 03:16:33 [INFO]: Epoch 016 - training loss: 0.7087, validation loss: 1.0368
2024-06-03 03:16:36 [INFO]: Epoch 017 - training loss: 0.7000, validation loss: 1.0056
2024-06-03 03:16:38 [INFO]: Epoch 018 - training loss: 0.6968, validation loss: 0.9657
2024-06-03 03:16:40 [INFO]: Epoch 019 - training loss: 0.6960, validation loss: 0.9896
2024-06-03 03:16:42 [INFO]: Epoch 020 - training loss: 0.6867, validation loss: 0.9785
2024-06-03 03:16:44 [INFO]: Epoch 021 - training loss: 0.6910, validation loss: 1.0176
2024-06-03 03:16:47 [INFO]: Epoch 022 - training loss: 0.6858, validation loss: 0.9936
2024-06-03 03:16:50 [INFO]: Epoch 023 - training loss: 0.6797, validation loss: 0.9758
2024-06-03 03:16:52 [INFO]: Epoch 024 - training loss: 0.6736, validation loss: 0.9662
2024-06-03 03:16:54 [INFO]: Epoch 025 - training loss: 0.6698, validation loss: 0.9797
2024-06-03 03:16:57 [INFO]: Epoch 026 - training loss: 0.6689, validation loss: 0.9658
2024-06-03 03:17:00 [INFO]: Epoch 027 - training loss: 0.6664, validation loss: 0.9541
2024-06-03 03:17:02 [INFO]: Epoch 028 - training loss: 0.6662, validation loss: 0.9582
2024-06-03 03:17:05 [INFO]: Epoch 029 - training loss: 0.6639, validation loss: 0.9559
2024-06-03 03:17:07 [INFO]: Epoch 030 - training loss: 0.6600, validation loss: 0.9585
2024-06-03 03:17:09 [INFO]: Epoch 031 - training loss: 0.6616, validation loss: 0.9672
2024-06-03 03:17:12 [INFO]: Epoch 032 - training loss: 0.6619, validation loss: 0.9510
2024-06-03 03:17:14 [INFO]: Epoch 033 - training loss: 0.6621, validation loss: 0.9554
2024-06-03 03:17:16 [INFO]: Epoch 034 - training loss: 0.6584, validation loss: 0.9510
2024-06-03 03:17:18 [INFO]: Epoch 035 - training loss: 0.6540, validation loss: 0.9616
2024-06-03 03:17:20 [INFO]: Epoch 036 - training loss: 0.6601, validation loss: 0.9467
2024-06-03 03:17:23 [INFO]: Epoch 037 - training loss: 0.6511, validation loss: 0.9589
2024-06-03 03:17:25 [INFO]: Epoch 038 - training loss: 0.6574, validation loss: 0.9493
2024-06-03 03:17:28 [INFO]: Epoch 039 - training loss: 0.6489, validation loss: 0.9446
2024-06-03 03:17:30 [INFO]: Epoch 040 - training loss: 0.6543, validation loss: 0.9397
2024-06-03 03:17:33 [INFO]: Epoch 041 - training loss: 0.6517, validation loss: 0.9416
2024-06-03 03:17:36 [INFO]: Epoch 042 - training loss: 0.6507, validation loss: 0.9409
2024-06-03 03:17:38 [INFO]: Epoch 043 - training loss: 0.6471, validation loss: 0.9390
2024-06-03 03:17:40 [INFO]: Epoch 044 - training loss: 0.6477, validation loss: 0.9351
2024-06-03 03:17:42 [INFO]: Epoch 045 - training loss: 0.6465, validation loss: 0.9371
2024-06-03 03:17:44 [INFO]: Epoch 046 - training loss: 0.6469, validation loss: 0.9351
2024-06-03 03:17:47 [INFO]: Epoch 047 - training loss: 0.6412, validation loss: 0.9379
2024-06-03 03:17:49 [INFO]: Epoch 048 - training loss: 0.6399, validation loss: 0.9380
2024-06-03 03:17:52 [INFO]: Epoch 049 - training loss: 0.6440, validation loss: 0.9352
2024-06-03 03:17:54 [INFO]: Epoch 050 - training loss: 0.6404, validation loss: 0.9337
2024-06-03 03:17:56 [INFO]: Epoch 051 - training loss: 0.6411, validation loss: 0.9328
2024-06-03 03:17:59 [INFO]: Epoch 052 - training loss: 0.6419, validation loss: 0.9336
2024-06-03 03:18:02 [INFO]: Epoch 053 - training loss: 0.6413, validation loss: 0.9364
2024-06-03 03:18:04 [INFO]: Epoch 054 - training loss: 0.6376, validation loss: 0.9319
2024-06-03 03:18:06 [INFO]: Epoch 055 - training loss: 0.6359, validation loss: 0.9295
2024-06-03 03:18:09 [INFO]: Epoch 056 - training loss: 0.6407, validation loss: 0.9327
2024-06-03 03:18:11 [INFO]: Epoch 057 - training loss: 0.6340, validation loss: 0.9281
2024-06-03 03:18:14 [INFO]: Epoch 058 - training loss: 0.6374, validation loss: 0.9330
2024-06-03 03:18:16 [INFO]: Epoch 059 - training loss: 0.6322, validation loss: 0.9290
2024-06-03 03:18:18 [INFO]: Epoch 060 - training loss: 0.6345, validation loss: 0.9325
2024-06-03 03:18:20 [INFO]: Epoch 061 - training loss: 0.6335, validation loss: 0.9291
2024-06-03 03:18:22 [INFO]: Epoch 062 - training loss: 0.6333, validation loss: 0.9307
2024-06-03 03:18:25 [INFO]: Epoch 063 - training loss: 0.6315, validation loss: 0.9343
2024-06-03 03:18:27 [INFO]: Epoch 064 - training loss: 0.6363, validation loss: 0.9333
2024-06-03 03:18:30 [INFO]: Epoch 065 - training loss: 0.6313, validation loss: 0.9378
2024-06-03 03:18:32 [INFO]: Epoch 066 - training loss: 0.6290, validation loss: 0.9335
2024-06-03 03:18:35 [INFO]: Epoch 067 - training loss: 0.6331, validation loss: 0.9333
2024-06-03 03:18:35 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:18:35 [INFO]: Finished training. The best model is from epoch#57.
2024-06-03 03:18:35 [INFO]: Saved the model to results_subseq_rate05/PeMS/Autoformer_PeMS/round_4/20240603_T031553/Autoformer.pypots
2024-06-03 03:18:35 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Autoformer_PeMS/round_4/imputation.pkl
2024-06-03 03:18:35 [INFO]: Round4 - Autoformer on PeMS: MAE=0.5843, MSE=1.2535, MRE=0.6905
2024-06-03 03:18:35 [INFO]: Done! Final results:
Averaged Autoformer (608,926 params) on PeMS: MAE=0.6456 ± 0.057299044800872216, MSE=1.3769 ± 0.13152892304178995, MRE=0.7629 ± 0.0677117244111406, average inference time=0.18
