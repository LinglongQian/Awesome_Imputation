2024-06-03 07:12:01 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 07:12:01 [INFO]: Using the given device: cuda:0
2024-06-03 07:12:01 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_0/20240603_T071201
2024-06-03 07:12:01 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_0/20240603_T071201/tensorboard
2024-06-03 07:12:02 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 07:12:23 [INFO]: Epoch 001 - training loss: 1.1124, validation loss: 1.0770
2024-06-03 07:12:34 [INFO]: Epoch 002 - training loss: 0.7358, validation loss: 1.0580
2024-06-03 07:12:45 [INFO]: Epoch 003 - training loss: 0.6680, validation loss: 1.0417
2024-06-03 07:12:56 [INFO]: Epoch 004 - training loss: 0.6057, validation loss: 1.0174
2024-06-03 07:13:07 [INFO]: Epoch 005 - training loss: 0.5729, validation loss: 0.9039
2024-06-03 07:13:17 [INFO]: Epoch 006 - training loss: 0.5249, validation loss: 0.8489
2024-06-03 07:13:27 [INFO]: Epoch 007 - training loss: 0.4938, validation loss: 0.8342
2024-06-03 07:13:37 [INFO]: Epoch 008 - training loss: 0.4716, validation loss: 0.8061
2024-06-03 07:13:48 [INFO]: Epoch 009 - training loss: 0.4635, validation loss: 0.7699
2024-06-03 07:13:59 [INFO]: Epoch 010 - training loss: 0.4458, validation loss: 0.7747
2024-06-03 07:14:09 [INFO]: Epoch 011 - training loss: 0.4295, validation loss: 0.7510
2024-06-03 07:14:20 [INFO]: Epoch 012 - training loss: 0.4102, validation loss: 0.7369
2024-06-03 07:14:30 [INFO]: Epoch 013 - training loss: 0.4035, validation loss: 0.7005
2024-06-03 07:14:42 [INFO]: Epoch 014 - training loss: 0.3917, validation loss: 0.7116
2024-06-03 07:14:53 [INFO]: Epoch 015 - training loss: 0.3833, validation loss: 0.7001
2024-06-03 07:15:01 [INFO]: Epoch 016 - training loss: 0.3812, validation loss: 0.7151
2024-06-03 07:15:12 [INFO]: Epoch 017 - training loss: 0.3685, validation loss: 0.7092
2024-06-03 07:15:26 [INFO]: Epoch 018 - training loss: 0.3626, validation loss: 0.7083
2024-06-03 07:15:38 [INFO]: Epoch 019 - training loss: 0.3522, validation loss: 0.6931
2024-06-03 07:15:48 [INFO]: Epoch 020 - training loss: 0.3493, validation loss: 0.6950
2024-06-03 07:15:59 [INFO]: Epoch 021 - training loss: 0.3452, validation loss: 0.6973
2024-06-03 07:16:09 [INFO]: Epoch 022 - training loss: 0.3415, validation loss: 0.6939
2024-06-03 07:16:21 [INFO]: Epoch 023 - training loss: 0.3355, validation loss: 0.6808
2024-06-03 07:16:34 [INFO]: Epoch 024 - training loss: 0.3338, validation loss: 0.6808
2024-06-03 07:16:46 [INFO]: Epoch 025 - training loss: 0.3364, validation loss: 0.6768
2024-06-03 07:16:53 [INFO]: Epoch 026 - training loss: 0.3526, validation loss: 0.6813
2024-06-03 07:17:04 [INFO]: Epoch 027 - training loss: 0.3344, validation loss: 0.6860
2024-06-03 07:17:15 [INFO]: Epoch 028 - training loss: 0.3333, validation loss: 0.6833
2024-06-03 07:17:27 [INFO]: Epoch 029 - training loss: 0.3347, validation loss: 0.6809
2024-06-03 07:17:37 [INFO]: Epoch 030 - training loss: 0.3256, validation loss: 0.6697
2024-06-03 07:17:48 [INFO]: Epoch 031 - training loss: 0.3269, validation loss: 0.6736
2024-06-03 07:17:57 [INFO]: Epoch 032 - training loss: 0.3191, validation loss: 0.6862
2024-06-03 07:18:09 [INFO]: Epoch 033 - training loss: 0.3202, validation loss: 0.6688
2024-06-03 07:18:18 [INFO]: Epoch 034 - training loss: 0.3144, validation loss: 0.6651
2024-06-03 07:18:27 [INFO]: Epoch 035 - training loss: 0.3128, validation loss: 0.6631
2024-06-03 07:18:36 [INFO]: Epoch 036 - training loss: 0.3075, validation loss: 0.6562
2024-06-03 07:18:46 [INFO]: Epoch 037 - training loss: 0.3061, validation loss: 0.6701
2024-06-03 07:18:54 [INFO]: Epoch 038 - training loss: 0.3096, validation loss: 0.6575
2024-06-03 07:19:04 [INFO]: Epoch 039 - training loss: 0.3048, validation loss: 0.6721
2024-06-03 07:19:15 [INFO]: Epoch 040 - training loss: 0.3070, validation loss: 0.6617
2024-06-03 07:19:27 [INFO]: Epoch 041 - training loss: 0.3019, validation loss: 0.6537
2024-06-03 07:19:36 [INFO]: Epoch 042 - training loss: 0.3009, validation loss: 0.6587
2024-06-03 07:19:46 [INFO]: Epoch 043 - training loss: 0.3036, validation loss: 0.6558
2024-06-03 07:19:50 [INFO]: Epoch 044 - training loss: 0.3021, validation loss: 0.6693
2024-06-03 07:19:54 [INFO]: Epoch 045 - training loss: 0.3022, validation loss: 0.6562
2024-06-03 07:19:58 [INFO]: Epoch 046 - training loss: 0.3029, validation loss: 0.6571
2024-06-03 07:20:01 [INFO]: Epoch 047 - training loss: 0.3057, validation loss: 0.6636
2024-06-03 07:20:06 [INFO]: Epoch 048 - training loss: 0.3013, validation loss: 0.6648
2024-06-03 07:20:10 [INFO]: Epoch 049 - training loss: 0.3007, validation loss: 0.6705
2024-06-03 07:20:14 [INFO]: Epoch 050 - training loss: 0.3011, validation loss: 0.6584
2024-06-03 07:20:18 [INFO]: Epoch 051 - training loss: 0.2964, validation loss: 0.6709
2024-06-03 07:20:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 07:20:18 [INFO]: Finished training. The best model is from epoch#41.
2024-06-03 07:20:18 [INFO]: Saved the model to results_subseq_rate05/PeMS/Koopa_PeMS/round_0/20240603_T071201/Koopa.pypots
2024-06-03 07:20:20 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_0/imputation.pkl
2024-06-03 07:20:20 [INFO]: Round0 - Koopa on PeMS: MAE=0.5183, MSE=1.0298, MRE=0.6125
2024-06-03 07:20:20 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 07:20:20 [INFO]: Using the given device: cuda:0
2024-06-03 07:20:20 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_1/20240603_T072020
2024-06-03 07:20:20 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_1/20240603_T072020/tensorboard
2024-06-03 07:20:20 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 07:20:25 [INFO]: Epoch 001 - training loss: 1.2368, validation loss: 1.1303
2024-06-03 07:20:29 [INFO]: Epoch 002 - training loss: 0.7115, validation loss: 1.0817
2024-06-03 07:20:33 [INFO]: Epoch 003 - training loss: 0.6474, validation loss: 1.0168
2024-06-03 07:20:38 [INFO]: Epoch 004 - training loss: 0.5877, validation loss: 0.9580
2024-06-03 07:20:42 [INFO]: Epoch 005 - training loss: 0.5453, validation loss: 0.8418
2024-06-03 07:20:45 [INFO]: Epoch 006 - training loss: 0.5113, validation loss: 0.8137
2024-06-03 07:20:49 [INFO]: Epoch 007 - training loss: 0.5027, validation loss: 0.7798
2024-06-03 07:20:53 [INFO]: Epoch 008 - training loss: 0.4764, validation loss: 0.7988
2024-06-03 07:20:57 [INFO]: Epoch 009 - training loss: 0.4539, validation loss: 0.8016
2024-06-03 07:21:01 [INFO]: Epoch 010 - training loss: 0.4428, validation loss: 0.7461
2024-06-03 07:21:06 [INFO]: Epoch 011 - training loss: 0.4287, validation loss: 0.7378
2024-06-03 07:21:09 [INFO]: Epoch 012 - training loss: 0.4090, validation loss: 0.7371
2024-06-03 07:21:13 [INFO]: Epoch 013 - training loss: 0.4008, validation loss: 0.6942
2024-06-03 07:21:18 [INFO]: Epoch 014 - training loss: 0.3831, validation loss: 0.6962
2024-06-03 07:21:22 [INFO]: Epoch 015 - training loss: 0.3741, validation loss: 0.6787
2024-06-03 07:21:26 [INFO]: Epoch 016 - training loss: 0.3626, validation loss: 0.6865
2024-06-03 07:21:30 [INFO]: Epoch 017 - training loss: 0.3587, validation loss: 0.6644
2024-06-03 07:21:34 [INFO]: Epoch 018 - training loss: 0.3494, validation loss: 0.6713
2024-06-03 07:21:38 [INFO]: Epoch 019 - training loss: 0.3510, validation loss: 0.6758
2024-06-03 07:21:42 [INFO]: Epoch 020 - training loss: 0.3474, validation loss: 0.6727
2024-06-03 07:21:47 [INFO]: Epoch 021 - training loss: 0.3507, validation loss: 0.6723
2024-06-03 07:21:51 [INFO]: Epoch 022 - training loss: 0.3364, validation loss: 0.6490
2024-06-03 07:21:55 [INFO]: Epoch 023 - training loss: 0.3276, validation loss: 0.6503
2024-06-03 07:22:00 [INFO]: Epoch 024 - training loss: 0.3231, validation loss: 0.6448
2024-06-03 07:22:04 [INFO]: Epoch 025 - training loss: 0.3189, validation loss: 0.6461
2024-06-03 07:22:07 [INFO]: Epoch 026 - training loss: 0.3165, validation loss: 0.6405
2024-06-03 07:22:11 [INFO]: Epoch 027 - training loss: 0.3108, validation loss: 0.6425
2024-06-03 07:22:15 [INFO]: Epoch 028 - training loss: 0.3110, validation loss: 0.6406
2024-06-03 07:22:19 [INFO]: Epoch 029 - training loss: 0.3041, validation loss: 0.6339
2024-06-03 07:22:23 [INFO]: Epoch 030 - training loss: 0.3068, validation loss: 0.6378
2024-06-03 07:22:27 [INFO]: Epoch 031 - training loss: 0.3031, validation loss: 0.6480
2024-06-03 07:22:31 [INFO]: Epoch 032 - training loss: 0.3233, validation loss: 0.6575
2024-06-03 07:22:35 [INFO]: Epoch 033 - training loss: 0.3082, validation loss: 0.6713
2024-06-03 07:22:39 [INFO]: Epoch 034 - training loss: 0.2995, validation loss: 0.6873
2024-06-03 07:22:43 [INFO]: Epoch 035 - training loss: 0.3000, validation loss: 0.6433
2024-06-03 07:22:48 [INFO]: Epoch 036 - training loss: 0.2962, validation loss: 0.6581
2024-06-03 07:22:52 [INFO]: Epoch 037 - training loss: 0.2950, validation loss: 0.6383
2024-06-03 07:22:55 [INFO]: Epoch 038 - training loss: 0.2872, validation loss: 0.6541
2024-06-03 07:22:59 [INFO]: Epoch 039 - training loss: 0.2890, validation loss: 0.6503
2024-06-03 07:22:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 07:22:59 [INFO]: Finished training. The best model is from epoch#29.
2024-06-03 07:22:59 [INFO]: Saved the model to results_subseq_rate05/PeMS/Koopa_PeMS/round_1/20240603_T072020/Koopa.pypots
2024-06-03 07:23:00 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_1/imputation.pkl
2024-06-03 07:23:00 [INFO]: Round1 - Koopa on PeMS: MAE=0.4914, MSE=0.9870, MRE=0.5808
2024-06-03 07:23:00 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 07:23:00 [INFO]: Using the given device: cuda:0
2024-06-03 07:23:00 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_2/20240603_T072300
2024-06-03 07:23:00 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_2/20240603_T072300/tensorboard
2024-06-03 07:23:01 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 07:23:05 [INFO]: Epoch 001 - training loss: 1.0929, validation loss: 1.1167
2024-06-03 07:23:09 [INFO]: Epoch 002 - training loss: 0.7159, validation loss: 1.0898
2024-06-03 07:23:13 [INFO]: Epoch 003 - training loss: 0.6572, validation loss: 1.0226
2024-06-03 07:23:17 [INFO]: Epoch 004 - training loss: 0.6158, validation loss: 1.0305
2024-06-03 07:23:20 [INFO]: Epoch 005 - training loss: 0.5884, validation loss: 0.9877
2024-06-03 07:23:24 [INFO]: Epoch 006 - training loss: 0.5477, validation loss: 0.9230
2024-06-03 07:23:28 [INFO]: Epoch 007 - training loss: 0.5200, validation loss: 0.8979
2024-06-03 07:23:31 [INFO]: Epoch 008 - training loss: 0.4978, validation loss: 0.8444
2024-06-03 07:23:35 [INFO]: Epoch 009 - training loss: 0.4866, validation loss: 0.8600
2024-06-03 07:23:40 [INFO]: Epoch 010 - training loss: 0.4800, validation loss: 0.8329
2024-06-03 07:23:43 [INFO]: Epoch 011 - training loss: 0.4664, validation loss: 0.8237
2024-06-03 07:23:47 [INFO]: Epoch 012 - training loss: 0.4582, validation loss: 0.8083
2024-06-03 07:23:52 [INFO]: Epoch 013 - training loss: 0.4452, validation loss: 0.8186
2024-06-03 07:23:56 [INFO]: Epoch 014 - training loss: 0.4335, validation loss: 0.7780
2024-06-03 07:24:00 [INFO]: Epoch 015 - training loss: 0.4220, validation loss: 0.7806
2024-06-03 07:24:04 [INFO]: Epoch 016 - training loss: 0.4150, validation loss: 0.7802
2024-06-03 07:24:08 [INFO]: Epoch 017 - training loss: 0.4076, validation loss: 0.7825
2024-06-03 07:24:12 [INFO]: Epoch 018 - training loss: 0.3996, validation loss: 0.7773
2024-06-03 07:24:17 [INFO]: Epoch 019 - training loss: 0.3993, validation loss: 0.7680
2024-06-03 07:24:20 [INFO]: Epoch 020 - training loss: 0.4004, validation loss: 0.7885
2024-06-03 07:24:24 [INFO]: Epoch 021 - training loss: 0.3953, validation loss: 0.7683
2024-06-03 07:24:28 [INFO]: Epoch 022 - training loss: 0.4034, validation loss: 0.8145
2024-06-03 07:24:32 [INFO]: Epoch 023 - training loss: 0.3944, validation loss: 0.7644
2024-06-03 07:24:36 [INFO]: Epoch 024 - training loss: 0.3874, validation loss: 0.7767
2024-06-03 07:24:40 [INFO]: Epoch 025 - training loss: 0.3807, validation loss: 0.7485
2024-06-03 07:24:44 [INFO]: Epoch 026 - training loss: 0.3723, validation loss: 0.7457
2024-06-03 07:24:48 [INFO]: Epoch 027 - training loss: 0.3713, validation loss: 0.7281
2024-06-03 07:24:52 [INFO]: Epoch 028 - training loss: 0.3656, validation loss: 0.7300
2024-06-03 07:24:57 [INFO]: Epoch 029 - training loss: 0.3628, validation loss: 0.7309
2024-06-03 07:25:00 [INFO]: Epoch 030 - training loss: 0.3653, validation loss: 0.7412
2024-06-03 07:25:04 [INFO]: Epoch 031 - training loss: 0.3623, validation loss: 0.7157
2024-06-03 07:25:08 [INFO]: Epoch 032 - training loss: 0.3568, validation loss: 0.7076
2024-06-03 07:25:13 [INFO]: Epoch 033 - training loss: 0.3541, validation loss: 0.7184
2024-06-03 07:25:18 [INFO]: Epoch 034 - training loss: 0.3531, validation loss: 0.7019
2024-06-03 07:25:22 [INFO]: Epoch 035 - training loss: 0.3512, validation loss: 0.6953
2024-06-03 07:25:26 [INFO]: Epoch 036 - training loss: 0.3540, validation loss: 0.6969
2024-06-03 07:25:30 [INFO]: Epoch 037 - training loss: 0.3503, validation loss: 0.6927
2024-06-03 07:25:34 [INFO]: Epoch 038 - training loss: 0.3487, validation loss: 0.7015
2024-06-03 07:25:39 [INFO]: Epoch 039 - training loss: 0.3459, validation loss: 0.7034
2024-06-03 07:25:43 [INFO]: Epoch 040 - training loss: 0.3440, validation loss: 0.6917
2024-06-03 07:25:47 [INFO]: Epoch 041 - training loss: 0.3454, validation loss: 0.6867
2024-06-03 07:25:51 [INFO]: Epoch 042 - training loss: 0.3395, validation loss: 0.6886
2024-06-03 07:25:56 [INFO]: Epoch 043 - training loss: 0.3415, validation loss: 0.6979
2024-06-03 07:25:59 [INFO]: Epoch 044 - training loss: 0.3430, validation loss: 0.6901
2024-06-03 07:26:03 [INFO]: Epoch 045 - training loss: 0.3394, validation loss: 0.6910
2024-06-03 07:26:06 [INFO]: Epoch 046 - training loss: 0.3389, validation loss: 0.6945
2024-06-03 07:26:10 [INFO]: Epoch 047 - training loss: 0.3357, validation loss: 0.7039
2024-06-03 07:26:14 [INFO]: Epoch 048 - training loss: 0.3335, validation loss: 0.6851
2024-06-03 07:26:18 [INFO]: Epoch 049 - training loss: 0.3304, validation loss: 0.6836
2024-06-03 07:26:21 [INFO]: Epoch 050 - training loss: 0.3332, validation loss: 0.6804
2024-06-03 07:26:24 [INFO]: Epoch 051 - training loss: 0.3289, validation loss: 0.6834
2024-06-03 07:26:29 [INFO]: Epoch 052 - training loss: 0.3261, validation loss: 0.6801
2024-06-03 07:26:32 [INFO]: Epoch 053 - training loss: 0.3310, validation loss: 0.6816
2024-06-03 07:26:36 [INFO]: Epoch 054 - training loss: 0.3293, validation loss: 0.6784
2024-06-03 07:26:40 [INFO]: Epoch 055 - training loss: 0.3326, validation loss: 0.6902
2024-06-03 07:26:44 [INFO]: Epoch 056 - training loss: 0.3268, validation loss: 0.6828
2024-06-03 07:26:47 [INFO]: Epoch 057 - training loss: 0.3239, validation loss: 0.6802
2024-06-03 07:26:51 [INFO]: Epoch 058 - training loss: 0.3294, validation loss: 0.6835
2024-06-03 07:26:55 [INFO]: Epoch 059 - training loss: 0.3295, validation loss: 0.6833
2024-06-03 07:26:58 [INFO]: Epoch 060 - training loss: 0.3251, validation loss: 0.6890
2024-06-03 07:27:01 [INFO]: Epoch 061 - training loss: 0.3300, validation loss: 0.6914
2024-06-03 07:27:05 [INFO]: Epoch 062 - training loss: 0.3220, validation loss: 0.6842
2024-06-03 07:27:09 [INFO]: Epoch 063 - training loss: 0.3198, validation loss: 0.6727
2024-06-03 07:27:13 [INFO]: Epoch 064 - training loss: 0.3198, validation loss: 0.6764
2024-06-03 07:27:16 [INFO]: Epoch 065 - training loss: 0.3214, validation loss: 0.6927
2024-06-03 07:27:20 [INFO]: Epoch 066 - training loss: 0.3217, validation loss: 0.7070
2024-06-03 07:27:24 [INFO]: Epoch 067 - training loss: 0.3202, validation loss: 0.6739
2024-06-03 07:27:28 [INFO]: Epoch 068 - training loss: 0.3137, validation loss: 0.6761
2024-06-03 07:27:32 [INFO]: Epoch 069 - training loss: 0.3148, validation loss: 0.6765
2024-06-03 07:27:36 [INFO]: Epoch 070 - training loss: 0.3120, validation loss: 0.6920
2024-06-03 07:27:40 [INFO]: Epoch 071 - training loss: 0.3153, validation loss: 0.6856
2024-06-03 07:27:45 [INFO]: Epoch 072 - training loss: 0.3126, validation loss: 0.6875
2024-06-03 07:27:49 [INFO]: Epoch 073 - training loss: 0.3154, validation loss: 0.6791
2024-06-03 07:27:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 07:27:49 [INFO]: Finished training. The best model is from epoch#63.
2024-06-03 07:27:49 [INFO]: Saved the model to results_subseq_rate05/PeMS/Koopa_PeMS/round_2/20240603_T072300/Koopa.pypots
2024-06-03 07:27:50 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_2/imputation.pkl
2024-06-03 07:27:50 [INFO]: Round2 - Koopa on PeMS: MAE=0.5356, MSE=1.0255, MRE=0.6329
2024-06-03 07:27:50 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 07:27:50 [INFO]: Using the given device: cuda:0
2024-06-03 07:27:50 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_3/20240603_T072750
2024-06-03 07:27:50 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_3/20240603_T072750/tensorboard
2024-06-03 07:27:50 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 07:27:55 [INFO]: Epoch 001 - training loss: 1.0730, validation loss: 1.1097
2024-06-03 07:27:59 [INFO]: Epoch 002 - training loss: 0.7123, validation loss: 1.0791
2024-06-03 07:28:03 [INFO]: Epoch 003 - training loss: 0.6733, validation loss: 1.0431
2024-06-03 07:28:07 [INFO]: Epoch 004 - training loss: 0.6225, validation loss: 1.0103
2024-06-03 07:28:10 [INFO]: Epoch 005 - training loss: 0.5713, validation loss: 0.9082
2024-06-03 07:28:14 [INFO]: Epoch 006 - training loss: 0.5262, validation loss: 0.8516
2024-06-03 07:28:17 [INFO]: Epoch 007 - training loss: 0.4977, validation loss: 0.8268
2024-06-03 07:28:21 [INFO]: Epoch 008 - training loss: 0.4754, validation loss: 0.7751
2024-06-03 07:28:24 [INFO]: Epoch 009 - training loss: 0.4568, validation loss: 0.7531
2024-06-03 07:28:27 [INFO]: Epoch 010 - training loss: 0.4337, validation loss: 0.7250
2024-06-03 07:28:31 [INFO]: Epoch 011 - training loss: 0.4178, validation loss: 0.7013
2024-06-03 07:28:36 [INFO]: Epoch 012 - training loss: 0.4093, validation loss: 0.7078
2024-06-03 07:28:39 [INFO]: Epoch 013 - training loss: 0.3956, validation loss: 0.6964
2024-06-03 07:28:43 [INFO]: Epoch 014 - training loss: 0.3813, validation loss: 0.6770
2024-06-03 07:28:47 [INFO]: Epoch 015 - training loss: 0.3761, validation loss: 0.6967
2024-06-03 07:28:51 [INFO]: Epoch 016 - training loss: 0.3724, validation loss: 0.6913
2024-06-03 07:28:56 [INFO]: Epoch 017 - training loss: 0.3694, validation loss: 0.6695
2024-06-03 07:29:00 [INFO]: Epoch 018 - training loss: 0.3554, validation loss: 0.6714
2024-06-03 07:29:03 [INFO]: Epoch 019 - training loss: 0.3496, validation loss: 0.6623
2024-06-03 07:29:07 [INFO]: Epoch 020 - training loss: 0.3466, validation loss: 0.6682
2024-06-03 07:29:12 [INFO]: Epoch 021 - training loss: 0.3667, validation loss: 0.6639
2024-06-03 07:29:16 [INFO]: Epoch 022 - training loss: 0.3479, validation loss: 0.6657
2024-06-03 07:29:20 [INFO]: Epoch 023 - training loss: 0.3422, validation loss: 0.6580
2024-06-03 07:29:24 [INFO]: Epoch 024 - training loss: 0.3332, validation loss: 0.6534
2024-06-03 07:29:28 [INFO]: Epoch 025 - training loss: 0.3278, validation loss: 0.6512
2024-06-03 07:29:33 [INFO]: Epoch 026 - training loss: 0.3227, validation loss: 0.6530
2024-06-03 07:29:36 [INFO]: Epoch 027 - training loss: 0.3211, validation loss: 0.6445
2024-06-03 07:29:40 [INFO]: Epoch 028 - training loss: 0.3194, validation loss: 0.6510
2024-06-03 07:29:44 [INFO]: Epoch 029 - training loss: 0.3209, validation loss: 0.6359
2024-06-03 07:29:48 [INFO]: Epoch 030 - training loss: 0.3166, validation loss: 0.6660
2024-06-03 07:29:52 [INFO]: Epoch 031 - training loss: 0.3236, validation loss: 0.6423
2024-06-03 07:29:57 [INFO]: Epoch 032 - training loss: 0.3125, validation loss: 0.6427
2024-06-03 07:30:01 [INFO]: Epoch 033 - training loss: 0.3152, validation loss: 0.6451
2024-06-03 07:30:04 [INFO]: Epoch 034 - training loss: 0.3103, validation loss: 0.6381
2024-06-03 07:30:08 [INFO]: Epoch 035 - training loss: 0.3104, validation loss: 0.6432
2024-06-03 07:30:11 [INFO]: Epoch 036 - training loss: 0.3109, validation loss: 0.6608
2024-06-03 07:30:15 [INFO]: Epoch 037 - training loss: 0.3067, validation loss: 0.6429
2024-06-03 07:30:18 [INFO]: Epoch 038 - training loss: 0.3063, validation loss: 0.6363
2024-06-03 07:30:21 [INFO]: Epoch 039 - training loss: 0.3073, validation loss: 0.6322
2024-06-03 07:30:26 [INFO]: Epoch 040 - training loss: 0.3122, validation loss: 0.6392
2024-06-03 07:30:30 [INFO]: Epoch 041 - training loss: 0.3081, validation loss: 0.6499
2024-06-03 07:30:34 [INFO]: Epoch 042 - training loss: 0.3064, validation loss: 0.6377
2024-06-03 07:30:37 [INFO]: Epoch 043 - training loss: 0.2980, validation loss: 0.6404
2024-06-03 07:30:41 [INFO]: Epoch 044 - training loss: 0.2964, validation loss: 0.6363
2024-06-03 07:30:45 [INFO]: Epoch 045 - training loss: 0.2958, validation loss: 0.6346
2024-06-03 07:30:48 [INFO]: Epoch 046 - training loss: 0.2975, validation loss: 0.6290
2024-06-03 07:30:53 [INFO]: Epoch 047 - training loss: 0.2941, validation loss: 0.6418
2024-06-03 07:30:57 [INFO]: Epoch 048 - training loss: 0.2932, validation loss: 0.6445
2024-06-03 07:31:00 [INFO]: Epoch 049 - training loss: 0.2893, validation loss: 0.6371
2024-06-03 07:31:03 [INFO]: Epoch 050 - training loss: 0.2885, validation loss: 0.6324
2024-06-03 07:31:07 [INFO]: Epoch 051 - training loss: 0.2893, validation loss: 0.6385
2024-06-03 07:31:11 [INFO]: Epoch 052 - training loss: 0.2896, validation loss: 0.6480
2024-06-03 07:31:14 [INFO]: Epoch 053 - training loss: 0.2935, validation loss: 0.6310
2024-06-03 07:31:17 [INFO]: Epoch 054 - training loss: 0.2886, validation loss: 0.6471
2024-06-03 07:31:21 [INFO]: Epoch 055 - training loss: 0.2854, validation loss: 0.6416
2024-06-03 07:31:25 [INFO]: Epoch 056 - training loss: 0.2846, validation loss: 0.6323
2024-06-03 07:31:25 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 07:31:25 [INFO]: Finished training. The best model is from epoch#46.
2024-06-03 07:31:25 [INFO]: Saved the model to results_subseq_rate05/PeMS/Koopa_PeMS/round_3/20240603_T072750/Koopa.pypots
2024-06-03 07:31:26 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_3/imputation.pkl
2024-06-03 07:31:26 [INFO]: Round3 - Koopa on PeMS: MAE=0.4743, MSE=0.9658, MRE=0.5604
2024-06-03 07:31:26 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 07:31:26 [INFO]: Using the given device: cuda:0
2024-06-03 07:31:26 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_4/20240603_T073126
2024-06-03 07:31:26 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_4/20240603_T073126/tensorboard
2024-06-03 07:31:27 [INFO]: Koopa initialized with the given hyperparameters, the number of trainable parameters: 13,306,214
2024-06-03 07:31:31 [INFO]: Epoch 001 - training loss: 1.0813, validation loss: 1.0882
2024-06-03 07:31:34 [INFO]: Epoch 002 - training loss: 0.7079, validation loss: 1.0683
2024-06-03 07:31:38 [INFO]: Epoch 003 - training loss: 0.6436, validation loss: 1.0735
2024-06-03 07:31:42 [INFO]: Epoch 004 - training loss: 0.6036, validation loss: 1.0535
2024-06-03 07:31:45 [INFO]: Epoch 005 - training loss: 0.5711, validation loss: 0.9383
2024-06-03 07:31:49 [INFO]: Epoch 006 - training loss: 0.5299, validation loss: 0.8692
2024-06-03 07:31:54 [INFO]: Epoch 007 - training loss: 0.5080, validation loss: 0.8998
2024-06-03 07:31:58 [INFO]: Epoch 008 - training loss: 0.4931, validation loss: 0.8826
2024-06-03 07:32:02 [INFO]: Epoch 009 - training loss: 0.4660, validation loss: 0.8191
2024-06-03 07:32:06 [INFO]: Epoch 010 - training loss: 0.4547, validation loss: 0.8273
2024-06-03 07:32:10 [INFO]: Epoch 011 - training loss: 0.4423, validation loss: 0.8561
2024-06-03 07:32:14 [INFO]: Epoch 012 - training loss: 0.4354, validation loss: 0.7931
2024-06-03 07:32:19 [INFO]: Epoch 013 - training loss: 0.4223, validation loss: 0.7625
2024-06-03 07:32:22 [INFO]: Epoch 014 - training loss: 0.4105, validation loss: 0.7769
2024-06-03 07:32:26 [INFO]: Epoch 015 - training loss: 0.4013, validation loss: 0.7610
2024-06-03 07:32:30 [INFO]: Epoch 016 - training loss: 0.4009, validation loss: 0.7428
2024-06-03 07:32:34 [INFO]: Epoch 017 - training loss: 0.3922, validation loss: 0.7289
2024-06-03 07:32:38 [INFO]: Epoch 018 - training loss: 0.3819, validation loss: 0.7871
2024-06-03 07:32:41 [INFO]: Epoch 019 - training loss: 0.3754, validation loss: 0.7465
2024-06-03 07:32:45 [INFO]: Epoch 020 - training loss: 0.3690, validation loss: 0.7116
2024-06-03 07:32:49 [INFO]: Epoch 021 - training loss: 0.3650, validation loss: 0.7226
2024-06-03 07:32:54 [INFO]: Epoch 022 - training loss: 0.3591, validation loss: 0.6900
2024-06-03 07:32:58 [INFO]: Epoch 023 - training loss: 0.3546, validation loss: 0.6870
2024-06-03 07:33:02 [INFO]: Epoch 024 - training loss: 0.3491, validation loss: 0.7205
2024-06-03 07:33:05 [INFO]: Epoch 025 - training loss: 0.3529, validation loss: 0.7122
2024-06-03 07:33:09 [INFO]: Epoch 026 - training loss: 0.3481, validation loss: 0.6768
2024-06-03 07:33:13 [INFO]: Epoch 027 - training loss: 0.3414, validation loss: 0.6761
2024-06-03 07:33:17 [INFO]: Epoch 028 - training loss: 0.3361, validation loss: 0.6779
2024-06-03 07:33:21 [INFO]: Epoch 029 - training loss: 0.3319, validation loss: 0.6714
2024-06-03 07:33:25 [INFO]: Epoch 030 - training loss: 0.3292, validation loss: 0.6740
2024-06-03 07:33:28 [INFO]: Epoch 031 - training loss: 0.3279, validation loss: 0.6675
2024-06-03 07:33:33 [INFO]: Epoch 032 - training loss: 0.3210, validation loss: 0.6647
2024-06-03 07:33:38 [INFO]: Epoch 033 - training loss: 0.3212, validation loss: 0.6658
2024-06-03 07:33:43 [INFO]: Epoch 034 - training loss: 0.3270, validation loss: 0.6630
2024-06-03 07:33:48 [INFO]: Epoch 035 - training loss: 0.3266, validation loss: 0.6820
2024-06-03 07:33:53 [INFO]: Epoch 036 - training loss: 0.3233, validation loss: 0.6582
2024-06-03 07:33:58 [INFO]: Epoch 037 - training loss: 0.3164, validation loss: 0.6632
2024-06-03 07:34:02 [INFO]: Epoch 038 - training loss: 0.3138, validation loss: 0.6722
2024-06-03 07:34:06 [INFO]: Epoch 039 - training loss: 0.3205, validation loss: 0.6622
2024-06-03 07:34:10 [INFO]: Epoch 040 - training loss: 0.3141, validation loss: 0.6631
2024-06-03 07:34:14 [INFO]: Epoch 041 - training loss: 0.3112, validation loss: 0.6687
2024-06-03 07:34:19 [INFO]: Epoch 042 - training loss: 0.3057, validation loss: 0.6595
2024-06-03 07:34:23 [INFO]: Epoch 043 - training loss: 0.3048, validation loss: 0.6634
2024-06-03 07:34:27 [INFO]: Epoch 044 - training loss: 0.3004, validation loss: 0.6484
2024-06-03 07:34:32 [INFO]: Epoch 045 - training loss: 0.3026, validation loss: 0.6592
2024-06-03 07:34:35 [INFO]: Epoch 046 - training loss: 0.3075, validation loss: 0.6628
2024-06-03 07:34:40 [INFO]: Epoch 047 - training loss: 0.3003, validation loss: 0.6551
2024-06-03 07:34:44 [INFO]: Epoch 048 - training loss: 0.2992, validation loss: 0.6490
2024-06-03 07:34:48 [INFO]: Epoch 049 - training loss: 0.2966, validation loss: 0.6528
2024-06-03 07:34:52 [INFO]: Epoch 050 - training loss: 0.2952, validation loss: 0.6498
2024-06-03 07:34:56 [INFO]: Epoch 051 - training loss: 0.2932, validation loss: 0.6426
2024-06-03 07:35:01 [INFO]: Epoch 052 - training loss: 0.2945, validation loss: 0.6463
2024-06-03 07:35:05 [INFO]: Epoch 053 - training loss: 0.2959, validation loss: 0.6454
2024-06-03 07:35:10 [INFO]: Epoch 054 - training loss: 0.3041, validation loss: 0.6720
2024-06-03 07:35:11 [INFO]: Epoch 055 - training loss: 4.6391, validation loss: 12.7645
2024-06-03 07:35:13 [INFO]: Epoch 056 - training loss: 4.5599, validation loss: 7.3357
2024-06-03 07:35:15 [INFO]: Epoch 057 - training loss: 3.3940, validation loss: 4.3504
2024-06-03 07:35:17 [INFO]: Epoch 058 - training loss: 2.5363, validation loss: 2.8554
2024-06-03 07:35:19 [INFO]: Epoch 059 - training loss: 2.0098, validation loss: 2.1040
2024-06-03 07:35:21 [INFO]: Epoch 060 - training loss: 1.7303, validation loss: 1.7461
2024-06-03 07:35:23 [INFO]: Epoch 061 - training loss: 1.5766, validation loss: 1.5511
2024-06-03 07:35:23 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 07:35:23 [INFO]: Finished training. The best model is from epoch#51.
2024-06-03 07:35:24 [INFO]: Saved the model to results_subseq_rate05/PeMS/Koopa_PeMS/round_4/20240603_T073126/Koopa.pypots
2024-06-03 07:35:24 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Koopa_PeMS/round_4/imputation.pkl
2024-06-03 07:35:24 [INFO]: Round4 - Koopa on PeMS: MAE=0.9627, MSE=1.9678, MRE=1.1376
2024-06-03 07:35:24 [INFO]: Done! Final results:
Averaged Koopa (13,306,214 params) on PeMS: MAE=0.5964 ± 0.1843249172810897, MSE=1.1952 ± 0.3870383262432867, MRE=0.7048 ± 0.21782139727490618, average inference time=0.22
