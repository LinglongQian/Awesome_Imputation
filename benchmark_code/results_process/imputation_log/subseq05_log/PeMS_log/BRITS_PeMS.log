2024-06-03 02:53:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 02:53:54 [INFO]: Using the given device: cuda:0
2024-06-03 02:53:54 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_0/20240603_T025354
2024-06-03 02:53:54 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_0/20240603_T025354/tensorboard
2024-06-03 02:53:56 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 02:55:02 [INFO]: Epoch 001 - training loss: 1.0351, validation loss: 0.7931
2024-06-03 02:56:09 [INFO]: Epoch 002 - training loss: 0.7148, validation loss: 0.6702
2024-06-03 02:57:18 [INFO]: Epoch 003 - training loss: 0.6209, validation loss: 0.6152
2024-06-03 02:58:25 [INFO]: Epoch 004 - training loss: 0.5769, validation loss: 0.5886
2024-06-03 02:59:26 [INFO]: Epoch 005 - training loss: 0.5430, validation loss: 0.5697
2024-06-03 03:00:28 [INFO]: Epoch 006 - training loss: 0.5261, validation loss: 0.5622
2024-06-03 03:01:30 [INFO]: Epoch 007 - training loss: 0.5043, validation loss: 0.5556
2024-06-03 03:02:34 [INFO]: Epoch 008 - training loss: 0.4923, validation loss: 0.5513
2024-06-03 03:03:34 [INFO]: Epoch 009 - training loss: 0.4763, validation loss: 0.5437
2024-06-03 03:04:25 [INFO]: Epoch 010 - training loss: 0.4642, validation loss: 0.5455
2024-06-03 03:05:12 [INFO]: Epoch 011 - training loss: 0.4573, validation loss: 0.5381
2024-06-03 03:05:59 [INFO]: Epoch 012 - training loss: 0.4518, validation loss: 0.5390
2024-06-03 03:06:48 [INFO]: Epoch 013 - training loss: 0.4419, validation loss: 0.5386
2024-06-03 03:07:35 [INFO]: Epoch 014 - training loss: 0.4336, validation loss: 0.5334
2024-06-03 03:08:22 [INFO]: Epoch 015 - training loss: 0.4266, validation loss: 0.5338
2024-06-03 03:09:10 [INFO]: Epoch 016 - training loss: 0.4236, validation loss: 0.5298
2024-06-03 03:09:57 [INFO]: Epoch 017 - training loss: 0.4152, validation loss: 0.5288
2024-06-03 03:10:46 [INFO]: Epoch 018 - training loss: 0.4137, validation loss: 0.5259
2024-06-03 03:11:32 [INFO]: Epoch 019 - training loss: 0.4039, validation loss: 0.5279
2024-06-03 03:12:11 [INFO]: Epoch 020 - training loss: 0.3990, validation loss: 0.5286
2024-06-03 03:12:36 [INFO]: Epoch 021 - training loss: 0.3962, validation loss: 0.5247
2024-06-03 03:13:03 [INFO]: Epoch 022 - training loss: 0.3935, validation loss: 0.5233
2024-06-03 03:13:28 [INFO]: Epoch 023 - training loss: 0.3882, validation loss: 0.5213
2024-06-03 03:13:57 [INFO]: Epoch 024 - training loss: 0.3844, validation loss: 0.5220
2024-06-03 03:14:27 [INFO]: Epoch 025 - training loss: 0.3804, validation loss: 0.5181
2024-06-03 03:14:50 [INFO]: Epoch 026 - training loss: 0.3733, validation loss: 0.5198
2024-06-03 03:15:09 [INFO]: Epoch 027 - training loss: 0.3735, validation loss: 0.5164
2024-06-03 03:15:33 [INFO]: Epoch 028 - training loss: 0.3686, validation loss: 0.5159
2024-06-03 03:15:54 [INFO]: Epoch 029 - training loss: 0.3671, validation loss: 0.5157
2024-06-03 03:16:15 [INFO]: Epoch 030 - training loss: 0.3650, validation loss: 0.5137
2024-06-03 03:16:38 [INFO]: Epoch 031 - training loss: 0.3633, validation loss: 0.5125
2024-06-03 03:17:00 [INFO]: Epoch 032 - training loss: 0.3566, validation loss: 0.5131
2024-06-03 03:17:19 [INFO]: Epoch 033 - training loss: 0.3581, validation loss: 0.5135
2024-06-03 03:17:43 [INFO]: Epoch 034 - training loss: 0.3545, validation loss: 0.5126
2024-06-03 03:18:05 [INFO]: Epoch 035 - training loss: 0.3526, validation loss: 0.5120
2024-06-03 03:18:26 [INFO]: Epoch 036 - training loss: 0.3472, validation loss: 0.5116
2024-06-03 03:18:47 [INFO]: Epoch 037 - training loss: 0.3448, validation loss: 0.5117
2024-06-03 03:19:04 [INFO]: Epoch 038 - training loss: 0.3409, validation loss: 0.5105
2024-06-03 03:19:20 [INFO]: Epoch 039 - training loss: 0.3388, validation loss: 0.5123
2024-06-03 03:19:40 [INFO]: Epoch 040 - training loss: 0.3390, validation loss: 0.5079
2024-06-03 03:19:55 [INFO]: Epoch 041 - training loss: 0.3332, validation loss: 0.5116
2024-06-03 03:20:15 [INFO]: Epoch 042 - training loss: 0.3368, validation loss: 0.5037
2024-06-03 03:20:31 [INFO]: Epoch 043 - training loss: 0.3346, validation loss: 0.5085
2024-06-03 03:20:48 [INFO]: Epoch 044 - training loss: 0.3337, validation loss: 0.5080
2024-06-03 03:21:06 [INFO]: Epoch 045 - training loss: 0.3277, validation loss: 0.5063
2024-06-03 03:21:21 [INFO]: Epoch 046 - training loss: 0.3268, validation loss: 0.5065
2024-06-03 03:21:41 [INFO]: Epoch 047 - training loss: 0.3227, validation loss: 0.5073
2024-06-03 03:21:57 [INFO]: Epoch 048 - training loss: 0.3234, validation loss: 0.5059
2024-06-03 03:22:17 [INFO]: Epoch 049 - training loss: 0.3221, validation loss: 0.5047
2024-06-03 03:22:33 [INFO]: Epoch 050 - training loss: 0.3208, validation loss: 0.5037
2024-06-03 03:22:49 [INFO]: Epoch 051 - training loss: 0.3200, validation loss: 0.5058
2024-06-03 03:23:08 [INFO]: Epoch 052 - training loss: 0.3198, validation loss: 0.5034
2024-06-03 03:23:23 [INFO]: Epoch 053 - training loss: 0.3146, validation loss: 0.5038
2024-06-03 03:23:44 [INFO]: Epoch 054 - training loss: 0.3165, validation loss: 0.5024
2024-06-03 03:23:59 [INFO]: Epoch 055 - training loss: 0.3120, validation loss: 0.5031
2024-06-03 03:24:18 [INFO]: Epoch 056 - training loss: 0.3113, validation loss: 0.5011
2024-06-03 03:24:35 [INFO]: Epoch 057 - training loss: 0.3078, validation loss: 0.5023
2024-06-03 03:24:51 [INFO]: Epoch 058 - training loss: 0.3085, validation loss: 0.5029
2024-06-03 03:25:10 [INFO]: Epoch 059 - training loss: 0.3050, validation loss: 0.5037
2024-06-03 03:25:25 [INFO]: Epoch 060 - training loss: 0.3048, validation loss: 0.5009
2024-06-03 03:25:46 [INFO]: Epoch 061 - training loss: 0.3049, validation loss: 0.5007
2024-06-03 03:26:01 [INFO]: Epoch 062 - training loss: 0.3012, validation loss: 0.5010
2024-06-03 03:26:19 [INFO]: Epoch 063 - training loss: 0.3021, validation loss: 0.5039
2024-06-03 03:26:37 [INFO]: Epoch 064 - training loss: 0.3006, validation loss: 0.5011
2024-06-03 03:26:52 [INFO]: Epoch 065 - training loss: 0.3003, validation loss: 0.5007
2024-06-03 03:27:12 [INFO]: Epoch 066 - training loss: 0.2974, validation loss: 0.5000
2024-06-03 03:27:27 [INFO]: Epoch 067 - training loss: 0.2948, validation loss: 0.5007
2024-06-03 03:27:48 [INFO]: Epoch 068 - training loss: 0.2903, validation loss: 0.5008
2024-06-03 03:28:03 [INFO]: Epoch 069 - training loss: 0.2953, validation loss: 0.4972
2024-06-03 03:28:21 [INFO]: Epoch 070 - training loss: 0.2901, validation loss: 0.4990
2024-06-03 03:28:39 [INFO]: Epoch 071 - training loss: 0.2906, validation loss: 0.4997
2024-06-03 03:28:54 [INFO]: Epoch 072 - training loss: 0.2899, validation loss: 0.4968
2024-06-03 03:29:14 [INFO]: Epoch 073 - training loss: 0.2896, validation loss: 0.4976
2024-06-03 03:29:29 [INFO]: Epoch 074 - training loss: 0.2929, validation loss: 0.4986
2024-06-03 03:29:49 [INFO]: Epoch 075 - training loss: 0.2886, validation loss: 0.4971
2024-06-03 03:30:05 [INFO]: Epoch 076 - training loss: 0.2893, validation loss: 0.4971
2024-06-03 03:30:22 [INFO]: Epoch 077 - training loss: 0.2881, validation loss: 0.4954
2024-06-03 03:30:41 [INFO]: Epoch 078 - training loss: 0.2862, validation loss: 0.4978
2024-06-03 03:30:55 [INFO]: Epoch 079 - training loss: 0.2866, validation loss: 0.4956
2024-06-03 03:31:16 [INFO]: Epoch 080 - training loss: 0.2833, validation loss: 0.4984
2024-06-03 03:31:31 [INFO]: Epoch 081 - training loss: 0.2829, validation loss: 0.4970
2024-06-03 03:31:50 [INFO]: Epoch 082 - training loss: 0.2791, validation loss: 0.4963
2024-06-03 03:32:06 [INFO]: Epoch 083 - training loss: 0.2789, validation loss: 0.4964
2024-06-03 03:32:23 [INFO]: Epoch 084 - training loss: 0.2799, validation loss: 0.4956
2024-06-03 03:32:42 [INFO]: Epoch 085 - training loss: 0.2763, validation loss: 0.4945
2024-06-03 03:32:56 [INFO]: Epoch 086 - training loss: 0.2739, validation loss: 0.4986
2024-06-03 03:33:17 [INFO]: Epoch 087 - training loss: 0.2764, validation loss: 0.4954
2024-06-03 03:33:33 [INFO]: Epoch 088 - training loss: 0.2742, validation loss: 0.4969
2024-06-03 03:33:52 [INFO]: Epoch 089 - training loss: 0.2730, validation loss: 0.4972
2024-06-03 03:34:09 [INFO]: Epoch 090 - training loss: 0.2728, validation loss: 0.4960
2024-06-03 03:34:24 [INFO]: Epoch 091 - training loss: 0.2740, validation loss: 0.4968
2024-06-03 03:34:44 [INFO]: Epoch 092 - training loss: 0.2697, validation loss: 0.4924
2024-06-03 03:34:58 [INFO]: Epoch 093 - training loss: 0.2710, validation loss: 0.4952
2024-06-03 03:35:19 [INFO]: Epoch 094 - training loss: 0.2730, validation loss: 0.4938
2024-06-03 03:35:35 [INFO]: Epoch 095 - training loss: 0.2686, validation loss: 0.4945
2024-06-03 03:35:53 [INFO]: Epoch 096 - training loss: 0.2686, validation loss: 0.4939
2024-06-03 03:36:10 [INFO]: Epoch 097 - training loss: 0.2702, validation loss: 0.4922
2024-06-03 03:36:25 [INFO]: Epoch 098 - training loss: 0.2695, validation loss: 0.4946
2024-06-03 03:36:45 [INFO]: Epoch 099 - training loss: 0.2678, validation loss: 0.4945
2024-06-03 03:37:00 [INFO]: Epoch 100 - training loss: 0.2658, validation loss: 0.4931
2024-06-03 03:37:00 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 03:37:00 [INFO]: Saved the model to results_subseq_rate05/PeMS/BRITS_PeMS/round_0/20240603_T025354/BRITS.pypots
2024-06-03 03:37:23 [INFO]: Successfully saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_0/imputation.pkl
2024-06-03 03:37:23 [INFO]: Round0 - BRITS on PeMS: MAE=0.3334, MSE=0.7457, MRE=0.3940
2024-06-03 03:37:23 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 03:37:23 [INFO]: Using the given device: cuda:0
2024-06-03 03:37:23 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_1/20240603_T033723
2024-06-03 03:37:23 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_1/20240603_T033723/tensorboard
2024-06-03 03:37:23 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 03:37:42 [INFO]: Epoch 001 - training loss: 1.0342, validation loss: 0.7884
2024-06-03 03:37:57 [INFO]: Epoch 002 - training loss: 0.7134, validation loss: 0.6686
2024-06-03 03:38:18 [INFO]: Epoch 003 - training loss: 0.6265, validation loss: 0.6067
2024-06-03 03:38:33 [INFO]: Epoch 004 - training loss: 0.5792, validation loss: 0.5823
2024-06-03 03:38:52 [INFO]: Epoch 005 - training loss: 0.5497, validation loss: 0.5735
2024-06-03 03:39:08 [INFO]: Epoch 006 - training loss: 0.5220, validation loss: 0.5587
2024-06-03 03:39:25 [INFO]: Epoch 007 - training loss: 0.5063, validation loss: 0.5528
2024-06-03 03:39:44 [INFO]: Epoch 008 - training loss: 0.4908, validation loss: 0.5533
2024-06-03 03:39:59 [INFO]: Epoch 009 - training loss: 0.4785, validation loss: 0.5456
2024-06-03 03:40:20 [INFO]: Epoch 010 - training loss: 0.4627, validation loss: 0.5450
2024-06-03 03:40:35 [INFO]: Epoch 011 - training loss: 0.4585, validation loss: 0.5406
2024-06-03 03:40:54 [INFO]: Epoch 012 - training loss: 0.4464, validation loss: 0.5417
2024-06-03 03:41:11 [INFO]: Epoch 013 - training loss: 0.4409, validation loss: 0.5363
2024-06-03 03:41:27 [INFO]: Epoch 014 - training loss: 0.4350, validation loss: 0.5350
2024-06-03 03:41:47 [INFO]: Epoch 015 - training loss: 0.4261, validation loss: 0.5348
2024-06-03 03:42:02 [INFO]: Epoch 016 - training loss: 0.4245, validation loss: 0.5328
2024-06-03 03:42:23 [INFO]: Epoch 017 - training loss: 0.4152, validation loss: 0.5276
2024-06-03 03:42:39 [INFO]: Epoch 018 - training loss: 0.4121, validation loss: 0.5278
2024-06-03 03:43:00 [INFO]: Epoch 019 - training loss: 0.4071, validation loss: 0.5286
2024-06-03 03:43:21 [INFO]: Epoch 020 - training loss: 0.3971, validation loss: 0.5259
2024-06-03 03:43:43 [INFO]: Epoch 021 - training loss: 0.3914, validation loss: 0.5281
2024-06-03 03:44:04 [INFO]: Epoch 022 - training loss: 0.3908, validation loss: 0.5224
2024-06-03 03:44:26 [INFO]: Epoch 023 - training loss: 0.3883, validation loss: 0.5218
2024-06-03 03:44:48 [INFO]: Epoch 024 - training loss: 0.3802, validation loss: 0.5207
2024-06-03 03:45:09 [INFO]: Epoch 025 - training loss: 0.3805, validation loss: 0.5192
2024-06-03 03:45:31 [INFO]: Epoch 026 - training loss: 0.3751, validation loss: 0.5191
2024-06-03 03:45:53 [INFO]: Epoch 027 - training loss: 0.3706, validation loss: 0.5189
2024-06-03 03:46:14 [INFO]: Epoch 028 - training loss: 0.3723, validation loss: 0.5175
2024-06-03 03:46:36 [INFO]: Epoch 029 - training loss: 0.3690, validation loss: 0.5190
2024-06-03 03:46:58 [INFO]: Epoch 030 - training loss: 0.3612, validation loss: 0.5159
2024-06-03 03:47:19 [INFO]: Epoch 031 - training loss: 0.3643, validation loss: 0.5159
2024-06-03 03:47:41 [INFO]: Epoch 032 - training loss: 0.3582, validation loss: 0.5159
2024-06-03 03:48:01 [INFO]: Epoch 033 - training loss: 0.3554, validation loss: 0.5149
2024-06-03 03:48:23 [INFO]: Epoch 034 - training loss: 0.3487, validation loss: 0.5147
2024-06-03 03:48:44 [INFO]: Epoch 035 - training loss: 0.3491, validation loss: 0.5136
2024-06-03 03:49:06 [INFO]: Epoch 036 - training loss: 0.3474, validation loss: 0.5138
2024-06-03 03:49:27 [INFO]: Epoch 037 - training loss: 0.3462, validation loss: 0.5128
2024-06-03 03:49:49 [INFO]: Epoch 038 - training loss: 0.3421, validation loss: 0.5087
2024-06-03 03:50:11 [INFO]: Epoch 039 - training loss: 0.3390, validation loss: 0.5115
2024-06-03 03:50:32 [INFO]: Epoch 040 - training loss: 0.3374, validation loss: 0.5107
2024-06-03 03:50:54 [INFO]: Epoch 041 - training loss: 0.3358, validation loss: 0.5095
2024-06-03 03:51:16 [INFO]: Epoch 042 - training loss: 0.3302, validation loss: 0.5091
2024-06-03 03:51:37 [INFO]: Epoch 043 - training loss: 0.3292, validation loss: 0.5081
2024-06-03 03:51:59 [INFO]: Epoch 044 - training loss: 0.3303, validation loss: 0.5099
2024-06-03 03:52:21 [INFO]: Epoch 045 - training loss: 0.3277, validation loss: 0.5104
2024-06-03 03:52:42 [INFO]: Epoch 046 - training loss: 0.3269, validation loss: 0.5074
2024-06-03 03:53:04 [INFO]: Epoch 047 - training loss: 0.3264, validation loss: 0.5068
2024-06-03 03:53:26 [INFO]: Epoch 048 - training loss: 0.3225, validation loss: 0.5086
2024-06-03 03:53:47 [INFO]: Epoch 049 - training loss: 0.3182, validation loss: 0.5083
2024-06-03 03:54:09 [INFO]: Epoch 050 - training loss: 0.3183, validation loss: 0.5066
2024-06-03 03:54:31 [INFO]: Epoch 051 - training loss: 0.3175, validation loss: 0.5087
2024-06-03 03:54:52 [INFO]: Epoch 052 - training loss: 0.3165, validation loss: 0.5066
2024-06-03 03:55:14 [INFO]: Epoch 053 - training loss: 0.3170, validation loss: 0.5050
2024-06-03 03:55:36 [INFO]: Epoch 054 - training loss: 0.3121, validation loss: 0.5040
2024-06-03 03:55:57 [INFO]: Epoch 055 - training loss: 0.3100, validation loss: 0.5036
2024-06-03 03:56:19 [INFO]: Epoch 056 - training loss: 0.3121, validation loss: 0.5041
2024-06-03 03:56:40 [INFO]: Epoch 057 - training loss: 0.3075, validation loss: 0.5047
2024-06-03 03:57:02 [INFO]: Epoch 058 - training loss: 0.3073, validation loss: 0.5043
2024-06-03 03:57:24 [INFO]: Epoch 059 - training loss: 0.3079, validation loss: 0.5040
2024-06-03 03:57:45 [INFO]: Epoch 060 - training loss: 0.3044, validation loss: 0.5032
2024-06-03 03:58:07 [INFO]: Epoch 061 - training loss: 0.3015, validation loss: 0.5012
2024-06-03 03:58:29 [INFO]: Epoch 062 - training loss: 0.3025, validation loss: 0.5040
2024-06-03 03:58:50 [INFO]: Epoch 063 - training loss: 0.3008, validation loss: 0.5025
2024-06-03 03:59:12 [INFO]: Epoch 064 - training loss: 0.3002, validation loss: 0.4994
2024-06-03 03:59:34 [INFO]: Epoch 065 - training loss: 0.3001, validation loss: 0.4984
2024-06-03 03:59:55 [INFO]: Epoch 066 - training loss: 0.3021, validation loss: 0.4966
2024-06-03 04:00:17 [INFO]: Epoch 067 - training loss: 0.2975, validation loss: 0.5030
2024-06-03 04:00:39 [INFO]: Epoch 068 - training loss: 0.2997, validation loss: 0.4983
2024-06-03 04:01:00 [INFO]: Epoch 069 - training loss: 0.2987, validation loss: 0.4983
2024-06-03 04:01:22 [INFO]: Epoch 070 - training loss: 0.2944, validation loss: 0.4997
2024-06-03 04:01:44 [INFO]: Epoch 071 - training loss: 0.2910, validation loss: 0.5013
2024-06-03 04:02:05 [INFO]: Epoch 072 - training loss: 0.2888, validation loss: 0.4980
2024-06-03 04:02:27 [INFO]: Epoch 073 - training loss: 0.2874, validation loss: 0.4987
2024-06-03 04:02:48 [INFO]: Epoch 074 - training loss: 0.2841, validation loss: 0.5004
2024-06-03 04:03:10 [INFO]: Epoch 075 - training loss: 0.2873, validation loss: 0.4964
2024-06-03 04:03:28 [INFO]: Epoch 076 - training loss: 0.2851, validation loss: 0.4986
2024-06-03 04:03:50 [INFO]: Epoch 077 - training loss: 0.2838, validation loss: 0.4987
2024-06-03 04:04:12 [INFO]: Epoch 078 - training loss: 0.2823, validation loss: 0.4987
2024-06-03 04:04:33 [INFO]: Epoch 079 - training loss: 0.2828, validation loss: 0.4991
2024-06-03 04:04:55 [INFO]: Epoch 080 - training loss: 0.2775, validation loss: 0.4999
2024-06-03 04:05:16 [INFO]: Epoch 081 - training loss: 0.2839, validation loss: 0.4955
2024-06-03 04:05:38 [INFO]: Epoch 082 - training loss: 0.2797, validation loss: 0.4983
2024-06-03 04:06:00 [INFO]: Epoch 083 - training loss: 0.2826, validation loss: 0.4964
2024-06-03 04:06:21 [INFO]: Epoch 084 - training loss: 0.2793, validation loss: 0.4975
2024-06-03 04:06:43 [INFO]: Epoch 085 - training loss: 0.2759, validation loss: 0.4984
2024-06-03 04:07:05 [INFO]: Epoch 086 - training loss: 0.2753, validation loss: 0.4952
2024-06-03 04:07:19 [INFO]: Epoch 087 - training loss: 0.2726, validation loss: 0.4954
2024-06-03 04:07:38 [INFO]: Epoch 088 - training loss: 0.2746, validation loss: 0.4964
2024-06-03 04:07:53 [INFO]: Epoch 089 - training loss: 0.2733, validation loss: 0.4955
2024-06-03 04:08:14 [INFO]: Epoch 090 - training loss: 0.2744, validation loss: 0.4949
2024-06-03 04:08:29 [INFO]: Epoch 091 - training loss: 0.2722, validation loss: 0.4955
2024-06-03 04:08:47 [INFO]: Epoch 092 - training loss: 0.2685, validation loss: 0.4964
2024-06-03 04:09:04 [INFO]: Epoch 093 - training loss: 0.2698, validation loss: 0.4956
2024-06-03 04:09:20 [INFO]: Epoch 094 - training loss: 0.2687, validation loss: 0.4939
2024-06-03 04:09:40 [INFO]: Epoch 095 - training loss: 0.2657, validation loss: 0.4956
2024-06-03 04:09:55 [INFO]: Epoch 096 - training loss: 0.2679, validation loss: 0.4923
2024-06-03 04:10:16 [INFO]: Epoch 097 - training loss: 0.2671, validation loss: 0.4943
2024-06-03 04:10:31 [INFO]: Epoch 098 - training loss: 0.2641, validation loss: 0.4944
2024-06-03 04:10:49 [INFO]: Epoch 099 - training loss: 0.2670, validation loss: 0.4939
2024-06-03 04:11:06 [INFO]: Epoch 100 - training loss: 0.2669, validation loss: 0.4943
2024-06-03 04:11:06 [INFO]: Finished training. The best model is from epoch#96.
2024-06-03 04:11:06 [INFO]: Saved the model to results_subseq_rate05/PeMS/BRITS_PeMS/round_1/20240603_T033723/BRITS.pypots
2024-06-03 04:11:24 [INFO]: Successfully saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_1/imputation.pkl
2024-06-03 04:11:24 [INFO]: Round1 - BRITS on PeMS: MAE=0.3337, MSE=0.7460, MRE=0.3944
2024-06-03 04:11:24 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 04:11:24 [INFO]: Using the given device: cuda:0
2024-06-03 04:11:24 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_2/20240603_T041124
2024-06-03 04:11:24 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_2/20240603_T041124/tensorboard
2024-06-03 04:11:24 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 04:11:46 [INFO]: Epoch 001 - training loss: 1.0346, validation loss: 0.7931
2024-06-03 04:12:01 [INFO]: Epoch 002 - training loss: 0.7141, validation loss: 0.6621
2024-06-03 04:12:18 [INFO]: Epoch 003 - training loss: 0.6266, validation loss: 0.6038
2024-06-03 04:12:36 [INFO]: Epoch 004 - training loss: 0.5761, validation loss: 0.5846
2024-06-03 04:12:51 [INFO]: Epoch 005 - training loss: 0.5468, validation loss: 0.5684
2024-06-03 04:13:12 [INFO]: Epoch 006 - training loss: 0.5215, validation loss: 0.5635
2024-06-03 04:13:28 [INFO]: Epoch 007 - training loss: 0.5050, validation loss: 0.5601
2024-06-03 04:13:47 [INFO]: Epoch 008 - training loss: 0.4942, validation loss: 0.5501
2024-06-03 04:14:03 [INFO]: Epoch 009 - training loss: 0.4741, validation loss: 0.5469
2024-06-03 04:14:20 [INFO]: Epoch 010 - training loss: 0.4666, validation loss: 0.5410
2024-06-03 04:14:39 [INFO]: Epoch 011 - training loss: 0.4581, validation loss: 0.5392
2024-06-03 04:14:54 [INFO]: Epoch 012 - training loss: 0.4478, validation loss: 0.5427
2024-06-03 04:15:15 [INFO]: Epoch 013 - training loss: 0.4431, validation loss: 0.5371
2024-06-03 04:15:30 [INFO]: Epoch 014 - training loss: 0.4361, validation loss: 0.5391
2024-06-03 04:15:48 [INFO]: Epoch 015 - training loss: 0.4277, validation loss: 0.5371
2024-06-03 04:16:05 [INFO]: Epoch 016 - training loss: 0.4263, validation loss: 0.5311
2024-06-03 04:16:21 [INFO]: Epoch 017 - training loss: 0.4147, validation loss: 0.5325
2024-06-03 04:16:40 [INFO]: Epoch 018 - training loss: 0.4087, validation loss: 0.5305
2024-06-03 04:16:55 [INFO]: Epoch 019 - training loss: 0.4045, validation loss: 0.5270
2024-06-03 04:17:16 [INFO]: Epoch 020 - training loss: 0.4002, validation loss: 0.5282
2024-06-03 04:17:31 [INFO]: Epoch 021 - training loss: 0.3977, validation loss: 0.5246
2024-06-03 04:17:49 [INFO]: Epoch 022 - training loss: 0.3921, validation loss: 0.5217
2024-06-03 04:18:06 [INFO]: Epoch 023 - training loss: 0.3909, validation loss: 0.5220
2024-06-03 04:18:22 [INFO]: Epoch 024 - training loss: 0.3861, validation loss: 0.5213
2024-06-03 04:18:42 [INFO]: Epoch 025 - training loss: 0.3826, validation loss: 0.5213
2024-06-03 04:18:58 [INFO]: Epoch 026 - training loss: 0.3749, validation loss: 0.5196
2024-06-03 04:19:18 [INFO]: Epoch 027 - training loss: 0.3696, validation loss: 0.5197
2024-06-03 04:19:34 [INFO]: Epoch 028 - training loss: 0.3741, validation loss: 0.5157
2024-06-03 04:19:51 [INFO]: Epoch 029 - training loss: 0.3646, validation loss: 0.5203
2024-06-03 04:20:09 [INFO]: Epoch 030 - training loss: 0.3637, validation loss: 0.5168
2024-06-03 04:20:24 [INFO]: Epoch 031 - training loss: 0.3626, validation loss: 0.5150
2024-06-03 04:20:45 [INFO]: Epoch 032 - training loss: 0.3599, validation loss: 0.5137
2024-06-03 04:21:00 [INFO]: Epoch 033 - training loss: 0.3564, validation loss: 0.5139
2024-06-03 04:21:19 [INFO]: Epoch 034 - training loss: 0.3534, validation loss: 0.5152
2024-06-03 04:21:35 [INFO]: Epoch 035 - training loss: 0.3491, validation loss: 0.5146
2024-06-03 04:21:51 [INFO]: Epoch 036 - training loss: 0.3456, validation loss: 0.5119
2024-06-03 04:22:10 [INFO]: Epoch 037 - training loss: 0.3465, validation loss: 0.5098
2024-06-03 04:22:25 [INFO]: Epoch 038 - training loss: 0.3428, validation loss: 0.5133
2024-06-03 04:22:47 [INFO]: Epoch 039 - training loss: 0.3415, validation loss: 0.5100
2024-06-03 04:23:01 [INFO]: Epoch 040 - training loss: 0.3373, validation loss: 0.5114
2024-06-03 04:23:22 [INFO]: Epoch 041 - training loss: 0.3392, validation loss: 0.5124
2024-06-03 04:23:43 [INFO]: Epoch 042 - training loss: 0.3335, validation loss: 0.5077
2024-06-03 04:24:05 [INFO]: Epoch 043 - training loss: 0.3306, validation loss: 0.5114
2024-06-03 04:24:27 [INFO]: Epoch 044 - training loss: 0.3317, validation loss: 0.5082
2024-06-03 04:24:48 [INFO]: Epoch 045 - training loss: 0.3259, validation loss: 0.5087
2024-06-03 04:25:10 [INFO]: Epoch 046 - training loss: 0.3244, validation loss: 0.5072
2024-06-03 04:25:31 [INFO]: Epoch 047 - training loss: 0.3232, validation loss: 0.5073
2024-06-03 04:25:53 [INFO]: Epoch 048 - training loss: 0.3256, validation loss: 0.5068
2024-06-03 04:26:15 [INFO]: Epoch 049 - training loss: 0.3208, validation loss: 0.5063
2024-06-03 04:26:36 [INFO]: Epoch 050 - training loss: 0.3185, validation loss: 0.5064
2024-06-03 04:26:58 [INFO]: Epoch 051 - training loss: 0.3173, validation loss: 0.5050
2024-06-03 04:27:20 [INFO]: Epoch 052 - training loss: 0.3151, validation loss: 0.5073
2024-06-03 04:27:41 [INFO]: Epoch 053 - training loss: 0.3174, validation loss: 0.5075
2024-06-03 04:28:02 [INFO]: Epoch 054 - training loss: 0.3133, validation loss: 0.5060
2024-06-03 04:28:21 [INFO]: Epoch 055 - training loss: 0.3112, validation loss: 0.5051
2024-06-03 04:28:43 [INFO]: Epoch 056 - training loss: 0.3095, validation loss: 0.5034
2024-06-03 04:29:04 [INFO]: Epoch 057 - training loss: 0.3104, validation loss: 0.5027
2024-06-03 04:29:26 [INFO]: Epoch 058 - training loss: 0.3081, validation loss: 0.5021
2024-06-03 04:29:47 [INFO]: Epoch 059 - training loss: 0.3021, validation loss: 0.5046
2024-06-03 04:30:09 [INFO]: Epoch 060 - training loss: 0.3074, validation loss: 0.5008
2024-06-03 04:30:31 [INFO]: Epoch 061 - training loss: 0.3025, validation loss: 0.5032
2024-06-03 04:30:52 [INFO]: Epoch 062 - training loss: 0.3023, validation loss: 0.5046
2024-06-03 04:31:14 [INFO]: Epoch 063 - training loss: 0.3032, validation loss: 0.5024
2024-06-03 04:31:36 [INFO]: Epoch 064 - training loss: 0.3002, validation loss: 0.5017
2024-06-03 04:31:57 [INFO]: Epoch 065 - training loss: 0.2973, validation loss: 0.5017
2024-06-03 04:32:19 [INFO]: Epoch 066 - training loss: 0.2938, validation loss: 0.5006
2024-06-03 04:32:41 [INFO]: Epoch 067 - training loss: 0.2954, validation loss: 0.4976
2024-06-03 04:33:02 [INFO]: Epoch 068 - training loss: 0.2969, validation loss: 0.5014
2024-06-03 04:33:24 [INFO]: Epoch 069 - training loss: 0.2943, validation loss: 0.4998
2024-06-03 04:33:45 [INFO]: Epoch 070 - training loss: 0.2934, validation loss: 0.4988
2024-06-03 04:34:07 [INFO]: Epoch 071 - training loss: 0.2908, validation loss: 0.5019
2024-06-03 04:34:29 [INFO]: Epoch 072 - training loss: 0.2909, validation loss: 0.4995
2024-06-03 04:34:50 [INFO]: Epoch 073 - training loss: 0.2882, validation loss: 0.4989
2024-06-03 04:35:12 [INFO]: Epoch 074 - training loss: 0.2869, validation loss: 0.5001
2024-06-03 04:35:34 [INFO]: Epoch 075 - training loss: 0.2855, validation loss: 0.4982
2024-06-03 04:35:55 [INFO]: Epoch 076 - training loss: 0.2834, validation loss: 0.5016
2024-06-03 04:36:17 [INFO]: Epoch 077 - training loss: 0.2889, validation loss: 0.4996
2024-06-03 04:36:17 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 04:36:17 [INFO]: Finished training. The best model is from epoch#67.
2024-06-03 04:36:17 [INFO]: Saved the model to results_subseq_rate05/PeMS/BRITS_PeMS/round_2/20240603_T041124/BRITS.pypots
2024-06-03 04:36:44 [INFO]: Successfully saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_2/imputation.pkl
2024-06-03 04:36:44 [INFO]: Round2 - BRITS on PeMS: MAE=0.3347, MSE=0.7510, MRE=0.3955
2024-06-03 04:36:44 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 04:36:44 [INFO]: Using the given device: cuda:0
2024-06-03 04:36:44 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_3/20240603_T043644
2024-06-03 04:36:44 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_3/20240603_T043644/tensorboard
2024-06-03 04:36:44 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 04:37:06 [INFO]: Epoch 001 - training loss: 1.0388, validation loss: 0.7922
2024-06-03 04:37:28 [INFO]: Epoch 002 - training loss: 0.7148, validation loss: 0.6675
2024-06-03 04:37:50 [INFO]: Epoch 003 - training loss: 0.6275, validation loss: 0.6058
2024-06-03 04:38:11 [INFO]: Epoch 004 - training loss: 0.5766, validation loss: 0.5800
2024-06-03 04:38:33 [INFO]: Epoch 005 - training loss: 0.5442, validation loss: 0.5665
2024-06-03 04:38:54 [INFO]: Epoch 006 - training loss: 0.5212, validation loss: 0.5597
2024-06-03 04:39:16 [INFO]: Epoch 007 - training loss: 0.5056, validation loss: 0.5551
2024-06-03 04:39:38 [INFO]: Epoch 008 - training loss: 0.4912, validation loss: 0.5534
2024-06-03 04:39:59 [INFO]: Epoch 009 - training loss: 0.4814, validation loss: 0.5425
2024-06-03 04:40:21 [INFO]: Epoch 010 - training loss: 0.4683, validation loss: 0.5437
2024-06-03 04:40:43 [INFO]: Epoch 011 - training loss: 0.4570, validation loss: 0.5421
2024-06-03 04:41:04 [INFO]: Epoch 012 - training loss: 0.4475, validation loss: 0.5375
2024-06-03 04:41:26 [INFO]: Epoch 013 - training loss: 0.4395, validation loss: 0.5346
2024-06-03 04:41:48 [INFO]: Epoch 014 - training loss: 0.4359, validation loss: 0.5352
2024-06-03 04:42:09 [INFO]: Epoch 015 - training loss: 0.4279, validation loss: 0.5307
2024-06-03 04:42:31 [INFO]: Epoch 016 - training loss: 0.4217, validation loss: 0.5291
2024-06-03 04:42:52 [INFO]: Epoch 017 - training loss: 0.4162, validation loss: 0.5284
2024-06-03 04:43:14 [INFO]: Epoch 018 - training loss: 0.4105, validation loss: 0.5272
2024-06-03 04:43:31 [INFO]: Epoch 019 - training loss: 0.4090, validation loss: 0.5263
2024-06-03 04:43:52 [INFO]: Epoch 020 - training loss: 0.3985, validation loss: 0.5256
2024-06-03 04:44:14 [INFO]: Epoch 021 - training loss: 0.3931, validation loss: 0.5252
2024-06-03 04:44:35 [INFO]: Epoch 022 - training loss: 0.3929, validation loss: 0.5230
2024-06-03 04:44:57 [INFO]: Epoch 023 - training loss: 0.3864, validation loss: 0.5215
2024-06-03 04:45:19 [INFO]: Epoch 024 - training loss: 0.3829, validation loss: 0.5193
2024-06-03 04:45:40 [INFO]: Epoch 025 - training loss: 0.3787, validation loss: 0.5206
2024-06-03 04:46:02 [INFO]: Epoch 026 - training loss: 0.3787, validation loss: 0.5164
2024-06-03 04:46:24 [INFO]: Epoch 027 - training loss: 0.3723, validation loss: 0.5192
2024-06-03 04:46:45 [INFO]: Epoch 028 - training loss: 0.3704, validation loss: 0.5177
2024-06-03 04:47:07 [INFO]: Epoch 029 - training loss: 0.3640, validation loss: 0.5150
2024-06-03 04:47:28 [INFO]: Epoch 030 - training loss: 0.3651, validation loss: 0.5153
2024-06-03 04:47:41 [INFO]: Epoch 031 - training loss: 0.3587, validation loss: 0.5131
2024-06-03 04:48:02 [INFO]: Epoch 032 - training loss: 0.3598, validation loss: 0.5125
2024-06-03 04:48:17 [INFO]: Epoch 033 - training loss: 0.3560, validation loss: 0.5091
2024-06-03 04:48:37 [INFO]: Epoch 034 - training loss: 0.3542, validation loss: 0.5108
2024-06-03 04:48:53 [INFO]: Epoch 035 - training loss: 0.3510, validation loss: 0.5118
2024-06-03 04:49:10 [INFO]: Epoch 036 - training loss: 0.3477, validation loss: 0.5121
2024-06-03 04:49:28 [INFO]: Epoch 037 - training loss: 0.3468, validation loss: 0.5105
2024-06-03 04:49:42 [INFO]: Epoch 038 - training loss: 0.3441, validation loss: 0.5091
2024-06-03 04:50:03 [INFO]: Epoch 039 - training loss: 0.3414, validation loss: 0.5089
2024-06-03 04:50:18 [INFO]: Epoch 040 - training loss: 0.3396, validation loss: 0.5083
2024-06-03 04:50:38 [INFO]: Epoch 041 - training loss: 0.3395, validation loss: 0.5070
2024-06-03 04:50:54 [INFO]: Epoch 042 - training loss: 0.3347, validation loss: 0.5073
2024-06-03 04:51:11 [INFO]: Epoch 043 - training loss: 0.3324, validation loss: 0.5066
2024-06-03 04:51:29 [INFO]: Epoch 044 - training loss: 0.3311, validation loss: 0.5074
2024-06-03 04:51:43 [INFO]: Epoch 045 - training loss: 0.3258, validation loss: 0.5070
2024-06-03 04:52:05 [INFO]: Epoch 046 - training loss: 0.3239, validation loss: 0.5042
2024-06-03 04:52:20 [INFO]: Epoch 047 - training loss: 0.3240, validation loss: 0.5058
2024-06-03 04:52:39 [INFO]: Epoch 048 - training loss: 0.3239, validation loss: 0.5048
2024-06-03 04:52:56 [INFO]: Epoch 049 - training loss: 0.3206, validation loss: 0.5059
2024-06-03 04:53:12 [INFO]: Epoch 050 - training loss: 0.3172, validation loss: 0.5043
2024-06-03 04:53:32 [INFO]: Epoch 051 - training loss: 0.3167, validation loss: 0.5050
2024-06-03 04:53:47 [INFO]: Epoch 052 - training loss: 0.3165, validation loss: 0.5072
2024-06-03 04:54:08 [INFO]: Epoch 053 - training loss: 0.3154, validation loss: 0.5020
2024-06-03 04:54:23 [INFO]: Epoch 054 - training loss: 0.3134, validation loss: 0.5009
2024-06-03 04:54:41 [INFO]: Epoch 055 - training loss: 0.3111, validation loss: 0.5055
2024-06-03 04:54:58 [INFO]: Epoch 056 - training loss: 0.3118, validation loss: 0.5023
2024-06-03 04:55:13 [INFO]: Epoch 057 - training loss: 0.3099, validation loss: 0.5023
2024-06-03 04:55:34 [INFO]: Epoch 058 - training loss: 0.3057, validation loss: 0.5046
2024-06-03 04:55:49 [INFO]: Epoch 059 - training loss: 0.3080, validation loss: 0.5003
2024-06-03 04:56:09 [INFO]: Epoch 060 - training loss: 0.3048, validation loss: 0.5007
2024-06-03 04:56:25 [INFO]: Epoch 061 - training loss: 0.3064, validation loss: 0.5016
2024-06-03 04:56:42 [INFO]: Epoch 062 - training loss: 0.3018, validation loss: 0.5009
2024-06-03 04:57:01 [INFO]: Epoch 063 - training loss: 0.2998, validation loss: 0.5007
2024-06-03 04:57:15 [INFO]: Epoch 064 - training loss: 0.2986, validation loss: 0.4990
2024-06-03 04:57:36 [INFO]: Epoch 065 - training loss: 0.2982, validation loss: 0.4989
2024-06-03 04:57:51 [INFO]: Epoch 066 - training loss: 0.2968, validation loss: 0.5042
2024-06-03 04:58:11 [INFO]: Epoch 067 - training loss: 0.2963, validation loss: 0.4976
2024-06-03 04:58:28 [INFO]: Epoch 068 - training loss: 0.2953, validation loss: 0.5025
2024-06-03 04:58:44 [INFO]: Epoch 069 - training loss: 0.2945, validation loss: 0.4978
2024-06-03 04:59:04 [INFO]: Epoch 070 - training loss: 0.2936, validation loss: 0.4980
2024-06-03 04:59:18 [INFO]: Epoch 071 - training loss: 0.2907, validation loss: 0.4971
2024-06-03 04:59:39 [INFO]: Epoch 072 - training loss: 0.2908, validation loss: 0.4973
2024-06-03 04:59:54 [INFO]: Epoch 073 - training loss: 0.2895, validation loss: 0.4978
2024-06-03 05:00:12 [INFO]: Epoch 074 - training loss: 0.2869, validation loss: 0.4971
2024-06-03 05:00:29 [INFO]: Epoch 075 - training loss: 0.2838, validation loss: 0.4988
2024-06-03 05:00:45 [INFO]: Epoch 076 - training loss: 0.2848, validation loss: 0.4969
2024-06-03 05:01:05 [INFO]: Epoch 077 - training loss: 0.2822, validation loss: 0.4990
2024-06-03 05:01:20 [INFO]: Epoch 078 - training loss: 0.2838, validation loss: 0.4950
2024-06-03 05:01:41 [INFO]: Epoch 079 - training loss: 0.2799, validation loss: 0.4976
2024-06-03 05:01:56 [INFO]: Epoch 080 - training loss: 0.2820, validation loss: 0.4962
2024-06-03 05:02:14 [INFO]: Epoch 081 - training loss: 0.2811, validation loss: 0.4969
2024-06-03 05:02:32 [INFO]: Epoch 082 - training loss: 0.2823, validation loss: 0.4935
2024-06-03 05:02:47 [INFO]: Epoch 083 - training loss: 0.2798, validation loss: 0.4940
2024-06-03 05:03:08 [INFO]: Epoch 084 - training loss: 0.2757, validation loss: 0.4957
2024-06-03 05:03:23 [INFO]: Epoch 085 - training loss: 0.2766, validation loss: 0.4970
2024-06-03 05:03:42 [INFO]: Epoch 086 - training loss: 0.2768, validation loss: 0.4963
2024-06-03 05:03:59 [INFO]: Epoch 087 - training loss: 0.2751, validation loss: 0.4946
2024-06-03 05:04:15 [INFO]: Epoch 088 - training loss: 0.2739, validation loss: 0.4952
2024-06-03 05:04:34 [INFO]: Epoch 089 - training loss: 0.2740, validation loss: 0.4946
2024-06-03 05:04:49 [INFO]: Epoch 090 - training loss: 0.2716, validation loss: 0.4940
2024-06-03 05:05:10 [INFO]: Epoch 091 - training loss: 0.2696, validation loss: 0.4946
2024-06-03 05:05:25 [INFO]: Epoch 092 - training loss: 0.2715, validation loss: 0.4951
2024-06-03 05:05:25 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 05:05:25 [INFO]: Finished training. The best model is from epoch#82.
2024-06-03 05:05:25 [INFO]: Saved the model to results_subseq_rate05/PeMS/BRITS_PeMS/round_3/20240603_T043644/BRITS.pypots
2024-06-03 05:05:45 [INFO]: Successfully saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_3/imputation.pkl
2024-06-03 05:05:45 [INFO]: Round3 - BRITS on PeMS: MAE=0.3333, MSE=0.7471, MRE=0.3939
2024-06-03 05:05:45 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 05:05:45 [INFO]: Using the given device: cuda:0
2024-06-03 05:05:45 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_4/20240603_T050545
2024-06-03 05:05:45 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_4/20240603_T050545/tensorboard
2024-06-03 05:05:45 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 32,012,048
2024-06-03 05:06:06 [INFO]: Epoch 001 - training loss: 1.0315, validation loss: 0.7922
2024-06-03 05:06:22 [INFO]: Epoch 002 - training loss: 0.7163, validation loss: 0.6605
2024-06-03 05:06:42 [INFO]: Epoch 003 - training loss: 0.6286, validation loss: 0.6074
2024-06-03 05:06:58 [INFO]: Epoch 004 - training loss: 0.5736, validation loss: 0.5793
2024-06-03 05:07:14 [INFO]: Epoch 005 - training loss: 0.5367, validation loss: 0.5692
2024-06-03 05:07:33 [INFO]: Epoch 006 - training loss: 0.5227, validation loss: 0.5621
2024-06-03 05:07:47 [INFO]: Epoch 007 - training loss: 0.4992, validation loss: 0.5530
2024-06-03 05:08:08 [INFO]: Epoch 008 - training loss: 0.4869, validation loss: 0.5494
2024-06-03 05:08:24 [INFO]: Epoch 009 - training loss: 0.4734, validation loss: 0.5436
2024-06-03 05:08:43 [INFO]: Epoch 010 - training loss: 0.4651, validation loss: 0.5428
2024-06-03 05:08:59 [INFO]: Epoch 011 - training loss: 0.4550, validation loss: 0.5413
2024-06-03 05:09:16 [INFO]: Epoch 012 - training loss: 0.4448, validation loss: 0.5407
2024-06-03 05:09:34 [INFO]: Epoch 013 - training loss: 0.4403, validation loss: 0.5362
2024-06-03 05:09:49 [INFO]: Epoch 014 - training loss: 0.4313, validation loss: 0.5309
2024-06-03 05:10:10 [INFO]: Epoch 015 - training loss: 0.4260, validation loss: 0.5386
2024-06-03 05:10:25 [INFO]: Epoch 016 - training loss: 0.4183, validation loss: 0.5348
2024-06-03 05:10:44 [INFO]: Epoch 017 - training loss: 0.4148, validation loss: 0.5296
2024-06-03 05:11:00 [INFO]: Epoch 018 - training loss: 0.4076, validation loss: 0.5289
2024-06-03 05:11:16 [INFO]: Epoch 019 - training loss: 0.4017, validation loss: 0.5269
2024-06-03 05:11:36 [INFO]: Epoch 020 - training loss: 0.3961, validation loss: 0.5234
2024-06-03 05:11:51 [INFO]: Epoch 021 - training loss: 0.3937, validation loss: 0.5259
2024-06-03 05:12:12 [INFO]: Epoch 022 - training loss: 0.3881, validation loss: 0.5248
2024-06-03 05:12:27 [INFO]: Epoch 023 - training loss: 0.3867, validation loss: 0.5211
2024-06-03 05:12:45 [INFO]: Epoch 024 - training loss: 0.3817, validation loss: 0.5232
2024-06-03 05:13:03 [INFO]: Epoch 025 - training loss: 0.3807, validation loss: 0.5224
2024-06-03 05:13:18 [INFO]: Epoch 026 - training loss: 0.3758, validation loss: 0.5174
2024-06-03 05:13:38 [INFO]: Epoch 027 - training loss: 0.3719, validation loss: 0.5169
2024-06-03 05:13:54 [INFO]: Epoch 028 - training loss: 0.3659, validation loss: 0.5164
2024-06-03 05:14:14 [INFO]: Epoch 029 - training loss: 0.3657, validation loss: 0.5162
2024-06-03 05:14:30 [INFO]: Epoch 030 - training loss: 0.3640, validation loss: 0.5163
2024-06-03 05:14:47 [INFO]: Epoch 031 - training loss: 0.3596, validation loss: 0.5132
2024-06-03 05:15:05 [INFO]: Epoch 032 - training loss: 0.3547, validation loss: 0.5192
2024-06-03 05:15:20 [INFO]: Epoch 033 - training loss: 0.3556, validation loss: 0.5151
2024-06-03 05:15:41 [INFO]: Epoch 034 - training loss: 0.3516, validation loss: 0.5129
2024-06-03 05:15:57 [INFO]: Epoch 035 - training loss: 0.3485, validation loss: 0.5131
2024-06-03 05:16:16 [INFO]: Epoch 036 - training loss: 0.3458, validation loss: 0.5122
2024-06-03 05:16:33 [INFO]: Epoch 037 - training loss: 0.3465, validation loss: 0.5101
2024-06-03 05:16:54 [INFO]: Epoch 038 - training loss: 0.3429, validation loss: 0.5096
2024-06-03 05:17:15 [INFO]: Epoch 039 - training loss: 0.3364, validation loss: 0.5123
2024-06-03 05:17:37 [INFO]: Epoch 040 - training loss: 0.3339, validation loss: 0.5125
2024-06-03 05:17:58 [INFO]: Epoch 041 - training loss: 0.3364, validation loss: 0.5111
2024-06-03 05:18:20 [INFO]: Epoch 042 - training loss: 0.3332, validation loss: 0.5074
2024-06-03 05:18:42 [INFO]: Epoch 043 - training loss: 0.3292, validation loss: 0.5086
2024-06-03 05:19:04 [INFO]: Epoch 044 - training loss: 0.3291, validation loss: 0.5067
2024-06-03 05:19:25 [INFO]: Epoch 045 - training loss: 0.3238, validation loss: 0.5093
2024-06-03 05:19:47 [INFO]: Epoch 046 - training loss: 0.3236, validation loss: 0.5073
2024-06-03 05:20:09 [INFO]: Epoch 047 - training loss: 0.3218, validation loss: 0.5091
2024-06-03 05:20:30 [INFO]: Epoch 048 - training loss: 0.3198, validation loss: 0.5066
2024-06-03 05:20:52 [INFO]: Epoch 049 - training loss: 0.3195, validation loss: 0.5079
2024-06-03 05:21:13 [INFO]: Epoch 050 - training loss: 0.3181, validation loss: 0.5079
2024-06-03 05:21:35 [INFO]: Epoch 051 - training loss: 0.3184, validation loss: 0.5097
2024-06-03 05:21:55 [INFO]: Epoch 052 - training loss: 0.3155, validation loss: 0.5071
2024-06-03 05:22:17 [INFO]: Epoch 053 - training loss: 0.3146, validation loss: 0.5068
2024-06-03 05:22:39 [INFO]: Epoch 054 - training loss: 0.3110, validation loss: 0.5044
2024-06-03 05:23:00 [INFO]: Epoch 055 - training loss: 0.3077, validation loss: 0.5071
2024-06-03 05:23:22 [INFO]: Epoch 056 - training loss: 0.3097, validation loss: 0.5046
2024-06-03 05:23:44 [INFO]: Epoch 057 - training loss: 0.3088, validation loss: 0.5024
2024-06-03 05:24:05 [INFO]: Epoch 058 - training loss: 0.3091, validation loss: 0.5039
2024-06-03 05:24:27 [INFO]: Epoch 059 - training loss: 0.3032, validation loss: 0.5046
2024-06-03 05:24:49 [INFO]: Epoch 060 - training loss: 0.3040, validation loss: 0.5029
2024-06-03 05:25:10 [INFO]: Epoch 061 - training loss: 0.2992, validation loss: 0.5056
2024-06-03 05:25:32 [INFO]: Epoch 062 - training loss: 0.3032, validation loss: 0.5019
2024-06-03 05:25:54 [INFO]: Epoch 063 - training loss: 0.3009, validation loss: 0.5013
2024-06-03 05:26:15 [INFO]: Epoch 064 - training loss: 0.2972, validation loss: 0.5051
2024-06-03 05:26:37 [INFO]: Epoch 065 - training loss: 0.2964, validation loss: 0.5012
2024-06-03 05:26:59 [INFO]: Epoch 066 - training loss: 0.2937, validation loss: 0.5023
2024-06-03 05:27:20 [INFO]: Epoch 067 - training loss: 0.2939, validation loss: 0.5023
2024-06-03 05:27:42 [INFO]: Epoch 068 - training loss: 0.2935, validation loss: 0.5024
2024-06-03 05:28:04 [INFO]: Epoch 069 - training loss: 0.2911, validation loss: 0.5013
2024-06-03 05:28:25 [INFO]: Epoch 070 - training loss: 0.2909, validation loss: 0.5004
2024-06-03 05:28:47 [INFO]: Epoch 071 - training loss: 0.2887, validation loss: 0.5034
2024-06-03 05:29:09 [INFO]: Epoch 072 - training loss: 0.2889, validation loss: 0.5001
2024-06-03 05:29:30 [INFO]: Epoch 073 - training loss: 0.2868, validation loss: 0.5018
2024-06-03 05:29:52 [INFO]: Epoch 074 - training loss: 0.2851, validation loss: 0.5014
2024-06-03 05:30:14 [INFO]: Epoch 075 - training loss: 0.2865, validation loss: 0.4989
2024-06-03 05:30:35 [INFO]: Epoch 076 - training loss: 0.2872, validation loss: 0.4990
2024-06-03 05:30:57 [INFO]: Epoch 077 - training loss: 0.2831, validation loss: 0.4987
2024-06-03 05:31:19 [INFO]: Epoch 078 - training loss: 0.2824, validation loss: 0.4977
2024-06-03 05:31:40 [INFO]: Epoch 079 - training loss: 0.2840, validation loss: 0.4985
2024-06-03 05:32:02 [INFO]: Epoch 080 - training loss: 0.2814, validation loss: 0.4987
2024-06-03 05:32:23 [INFO]: Epoch 081 - training loss: 0.2810, validation loss: 0.4990
2024-06-03 05:32:45 [INFO]: Epoch 082 - training loss: 0.2783, validation loss: 0.4969
2024-06-03 05:33:07 [INFO]: Epoch 083 - training loss: 0.2786, validation loss: 0.4985
2024-06-03 05:33:28 [INFO]: Epoch 084 - training loss: 0.2785, validation loss: 0.4977
2024-06-03 05:33:50 [INFO]: Epoch 085 - training loss: 0.2766, validation loss: 0.4968
2024-06-03 05:34:12 [INFO]: Epoch 086 - training loss: 0.2786, validation loss: 0.4980
2024-06-03 05:34:33 [INFO]: Epoch 087 - training loss: 0.2743, validation loss: 0.4982
2024-06-03 05:34:55 [INFO]: Epoch 088 - training loss: 0.2747, validation loss: 0.4966
2024-06-03 05:35:17 [INFO]: Epoch 089 - training loss: 0.2738, validation loss: 0.4980
2024-06-03 05:35:38 [INFO]: Epoch 090 - training loss: 0.2731, validation loss: 0.4964
2024-06-03 05:36:00 [INFO]: Epoch 091 - training loss: 0.2700, validation loss: 0.4979
2024-06-03 05:36:22 [INFO]: Epoch 092 - training loss: 0.2699, validation loss: 0.4981
2024-06-03 05:36:43 [INFO]: Epoch 093 - training loss: 0.2697, validation loss: 0.4968
2024-06-03 05:37:00 [INFO]: Epoch 094 - training loss: 0.2689, validation loss: 0.4957
2024-06-03 05:37:21 [INFO]: Epoch 095 - training loss: 0.2657, validation loss: 0.4965
2024-06-03 05:37:43 [INFO]: Epoch 096 - training loss: 0.2638, validation loss: 0.4960
2024-06-03 05:38:04 [INFO]: Epoch 097 - training loss: 0.2623, validation loss: 0.4971
2024-06-03 05:38:26 [INFO]: Epoch 098 - training loss: 0.2661, validation loss: 0.4970
2024-06-03 05:38:48 [INFO]: Epoch 099 - training loss: 0.2628, validation loss: 0.4982
2024-06-03 05:39:09 [INFO]: Epoch 100 - training loss: 0.2626, validation loss: 0.4951
2024-06-03 05:39:09 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 05:39:10 [INFO]: Saved the model to results_subseq_rate05/PeMS/BRITS_PeMS/round_4/20240603_T050545/BRITS.pypots
2024-06-03 05:39:36 [INFO]: Successfully saved to results_subseq_rate05/PeMS/BRITS_PeMS/round_4/imputation.pkl
2024-06-03 05:39:36 [INFO]: Round4 - BRITS on PeMS: MAE=0.3329, MSE=0.7455, MRE=0.3934
2024-06-03 05:39:36 [INFO]: Done! Final results:
Averaged BRITS (32,012,048 params) on PeMS: MAE=0.3336 ± 0.0006053047396994399, MSE=0.7470 ± 0.0020711159851787854, MRE=0.3942 ± 0.0007153038564905199, average inference time=5.45
