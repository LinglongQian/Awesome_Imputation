2024-06-03 02:53:54 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 02:53:54 [INFO]: Using the given device: cuda:0
2024-06-03 02:53:54 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_0/20240603_T025354
2024-06-03 02:53:54 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_0/20240603_T025354/tensorboard
2024-06-03 02:53:55 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 02:53:59 [INFO]: Epoch 001 - training loss: 3.1249, validation loss: 1.6322
2024-06-03 02:54:01 [INFO]: Epoch 002 - training loss: 0.9679, validation loss: 0.9998
2024-06-03 02:54:04 [INFO]: Epoch 003 - training loss: 0.6528, validation loss: 0.8578
2024-06-03 02:54:06 [INFO]: Epoch 004 - training loss: 0.5475, validation loss: 0.7641
2024-06-03 02:54:09 [INFO]: Epoch 005 - training loss: 0.4917, validation loss: 0.7171
2024-06-03 02:54:11 [INFO]: Epoch 006 - training loss: 0.4576, validation loss: 0.6941
2024-06-03 02:54:14 [INFO]: Epoch 007 - training loss: 0.4461, validation loss: 0.6725
2024-06-03 02:54:17 [INFO]: Epoch 008 - training loss: 0.4405, validation loss: 0.6905
2024-06-03 02:54:20 [INFO]: Epoch 009 - training loss: 0.4108, validation loss: 0.6354
2024-06-03 02:54:23 [INFO]: Epoch 010 - training loss: 0.3931, validation loss: 0.6323
2024-06-03 02:54:26 [INFO]: Epoch 011 - training loss: 0.3764, validation loss: 0.6350
2024-06-03 02:54:29 [INFO]: Epoch 012 - training loss: 0.3644, validation loss: 0.6225
2024-06-03 02:54:32 [INFO]: Epoch 013 - training loss: 0.3523, validation loss: 0.6129
2024-06-03 02:54:35 [INFO]: Epoch 014 - training loss: 0.3469, validation loss: 0.6041
2024-06-03 02:54:38 [INFO]: Epoch 015 - training loss: 0.3423, validation loss: 0.6266
2024-06-03 02:54:41 [INFO]: Epoch 016 - training loss: 0.3398, validation loss: 0.6050
2024-06-03 02:54:43 [INFO]: Epoch 017 - training loss: 0.3288, validation loss: 0.6128
2024-06-03 02:54:46 [INFO]: Epoch 018 - training loss: 0.3291, validation loss: 0.6060
2024-06-03 02:54:49 [INFO]: Epoch 019 - training loss: 0.3194, validation loss: 0.5989
2024-06-03 02:54:52 [INFO]: Epoch 020 - training loss: 0.3103, validation loss: 0.6019
2024-06-03 02:54:55 [INFO]: Epoch 021 - training loss: 0.3048, validation loss: 0.6012
2024-06-03 02:54:58 [INFO]: Epoch 022 - training loss: 0.2981, validation loss: 0.5960
2024-06-03 02:55:00 [INFO]: Epoch 023 - training loss: 0.2931, validation loss: 0.5966
2024-06-03 02:55:03 [INFO]: Epoch 024 - training loss: 0.2946, validation loss: 0.5887
2024-06-03 02:55:06 [INFO]: Epoch 025 - training loss: 0.2901, validation loss: 0.5972
2024-06-03 02:55:09 [INFO]: Epoch 026 - training loss: 0.2882, validation loss: 0.5983
2024-06-03 02:55:12 [INFO]: Epoch 027 - training loss: 0.2832, validation loss: 0.5908
2024-06-03 02:55:15 [INFO]: Epoch 028 - training loss: 0.2827, validation loss: 0.5937
2024-06-03 02:55:18 [INFO]: Epoch 029 - training loss: 0.2802, validation loss: 0.5944
2024-06-03 02:55:21 [INFO]: Epoch 030 - training loss: 0.2767, validation loss: 0.6006
2024-06-03 02:55:24 [INFO]: Epoch 031 - training loss: 0.2761, validation loss: 0.5958
2024-06-03 02:55:27 [INFO]: Epoch 032 - training loss: 0.2732, validation loss: 0.5861
2024-06-03 02:55:30 [INFO]: Epoch 033 - training loss: 0.2714, validation loss: 0.5918
2024-06-03 02:55:33 [INFO]: Epoch 034 - training loss: 0.2696, validation loss: 0.5887
2024-06-03 02:55:35 [INFO]: Epoch 035 - training loss: 0.2670, validation loss: 0.5962
2024-06-03 02:55:38 [INFO]: Epoch 036 - training loss: 0.2737, validation loss: 0.5961
2024-06-03 02:55:41 [INFO]: Epoch 037 - training loss: 0.2698, validation loss: 0.5995
2024-06-03 02:55:44 [INFO]: Epoch 038 - training loss: 0.2683, validation loss: 0.5867
2024-06-03 02:55:47 [INFO]: Epoch 039 - training loss: 0.2682, validation loss: 0.5919
2024-06-03 02:55:49 [INFO]: Epoch 040 - training loss: 0.2601, validation loss: 0.5884
2024-06-03 02:55:52 [INFO]: Epoch 041 - training loss: 0.2612, validation loss: 0.5853
2024-06-03 02:55:54 [INFO]: Epoch 042 - training loss: 0.2625, validation loss: 0.5964
2024-06-03 02:55:57 [INFO]: Epoch 043 - training loss: 0.2592, validation loss: 0.5876
2024-06-03 02:56:00 [INFO]: Epoch 044 - training loss: 0.2567, validation loss: 0.5965
2024-06-03 02:56:02 [INFO]: Epoch 045 - training loss: 0.2579, validation loss: 0.5864
2024-06-03 02:56:05 [INFO]: Epoch 046 - training loss: 0.2548, validation loss: 0.5910
2024-06-03 02:56:08 [INFO]: Epoch 047 - training loss: 0.2537, validation loss: 0.5893
2024-06-03 02:56:11 [INFO]: Epoch 048 - training loss: 0.2511, validation loss: 0.5858
2024-06-03 02:56:14 [INFO]: Epoch 049 - training loss: 0.2490, validation loss: 0.5848
2024-06-03 02:56:17 [INFO]: Epoch 050 - training loss: 0.2521, validation loss: 0.5847
2024-06-03 02:56:19 [INFO]: Epoch 051 - training loss: 0.2539, validation loss: 0.5900
2024-06-03 02:56:22 [INFO]: Epoch 052 - training loss: 0.2499, validation loss: 0.5896
2024-06-03 02:56:25 [INFO]: Epoch 053 - training loss: 0.2484, validation loss: 0.5870
2024-06-03 02:56:28 [INFO]: Epoch 054 - training loss: 0.2478, validation loss: 0.5850
2024-06-03 02:56:31 [INFO]: Epoch 055 - training loss: 0.2468, validation loss: 0.5829
2024-06-03 02:56:34 [INFO]: Epoch 056 - training loss: 0.2470, validation loss: 0.5860
2024-06-03 02:56:37 [INFO]: Epoch 057 - training loss: 0.2483, validation loss: 0.5839
2024-06-03 02:56:39 [INFO]: Epoch 058 - training loss: 0.2521, validation loss: 0.5901
2024-06-03 02:56:42 [INFO]: Epoch 059 - training loss: 0.2503, validation loss: 0.5838
2024-06-03 02:56:44 [INFO]: Epoch 060 - training loss: 0.2506, validation loss: 0.5920
2024-06-03 02:56:47 [INFO]: Epoch 061 - training loss: 0.2463, validation loss: 0.5940
2024-06-03 02:56:50 [INFO]: Epoch 062 - training loss: 0.2433, validation loss: 0.5861
2024-06-03 02:56:52 [INFO]: Epoch 063 - training loss: 0.2454, validation loss: 0.5866
2024-06-03 02:56:55 [INFO]: Epoch 064 - training loss: 0.2434, validation loss: 0.5862
2024-06-03 02:56:58 [INFO]: Epoch 065 - training loss: 0.2444, validation loss: 0.5883
2024-06-03 02:56:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:56:58 [INFO]: Finished training. The best model is from epoch#55.
2024-06-03 02:56:58 [INFO]: Saved the model to results_subseq_rate05/PeMS/DLinear_PeMS/round_0/20240603_T025354/DLinear.pypots
2024-06-03 02:56:59 [INFO]: Successfully saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_0/imputation.pkl
2024-06-03 02:56:59 [INFO]: Round0 - DLinear on PeMS: MAE=0.4514, MSE=0.8572, MRE=0.5334
2024-06-03 02:56:59 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 02:56:59 [INFO]: Using the given device: cuda:0
2024-06-03 02:56:59 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_1/20240603_T025659
2024-06-03 02:56:59 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_1/20240603_T025659/tensorboard
2024-06-03 02:56:59 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 02:57:02 [INFO]: Epoch 001 - training loss: 3.2670, validation loss: 1.4292
2024-06-03 02:57:05 [INFO]: Epoch 002 - training loss: 1.0324, validation loss: 1.0236
2024-06-03 02:57:08 [INFO]: Epoch 003 - training loss: 0.7050, validation loss: 0.8843
2024-06-03 02:57:11 [INFO]: Epoch 004 - training loss: 0.5771, validation loss: 0.8299
2024-06-03 02:57:14 [INFO]: Epoch 005 - training loss: 0.5070, validation loss: 0.7517
2024-06-03 02:57:16 [INFO]: Epoch 006 - training loss: 0.4583, validation loss: 0.6947
2024-06-03 02:57:19 [INFO]: Epoch 007 - training loss: 0.4390, validation loss: 0.6854
2024-06-03 02:57:22 [INFO]: Epoch 008 - training loss: 0.4220, validation loss: 0.6542
2024-06-03 02:57:25 [INFO]: Epoch 009 - training loss: 0.4068, validation loss: 0.6424
2024-06-03 02:57:27 [INFO]: Epoch 010 - training loss: 0.3942, validation loss: 0.6499
2024-06-03 02:57:30 [INFO]: Epoch 011 - training loss: 0.3808, validation loss: 0.6411
2024-06-03 02:57:33 [INFO]: Epoch 012 - training loss: 0.3740, validation loss: 0.6226
2024-06-03 02:57:35 [INFO]: Epoch 013 - training loss: 0.3651, validation loss: 0.6297
2024-06-03 02:57:38 [INFO]: Epoch 014 - training loss: 0.3547, validation loss: 0.6249
2024-06-03 02:57:40 [INFO]: Epoch 015 - training loss: 0.3551, validation loss: 0.6230
2024-06-03 02:57:43 [INFO]: Epoch 016 - training loss: 0.3481, validation loss: 0.6199
2024-06-03 02:57:46 [INFO]: Epoch 017 - training loss: 0.3341, validation loss: 0.6187
2024-06-03 02:57:49 [INFO]: Epoch 018 - training loss: 0.3246, validation loss: 0.6027
2024-06-03 02:57:51 [INFO]: Epoch 019 - training loss: 0.3218, validation loss: 0.6160
2024-06-03 02:57:54 [INFO]: Epoch 020 - training loss: 0.3162, validation loss: 0.6042
2024-06-03 02:57:57 [INFO]: Epoch 021 - training loss: 0.3101, validation loss: 0.6132
2024-06-03 02:58:00 [INFO]: Epoch 022 - training loss: 0.3110, validation loss: 0.6028
2024-06-03 02:58:03 [INFO]: Epoch 023 - training loss: 0.3051, validation loss: 0.5986
2024-06-03 02:58:06 [INFO]: Epoch 024 - training loss: 0.2943, validation loss: 0.6053
2024-06-03 02:58:09 [INFO]: Epoch 025 - training loss: 0.2909, validation loss: 0.5946
2024-06-03 02:58:12 [INFO]: Epoch 026 - training loss: 0.2941, validation loss: 0.6129
2024-06-03 02:58:15 [INFO]: Epoch 027 - training loss: 0.2952, validation loss: 0.6033
2024-06-03 02:58:17 [INFO]: Epoch 028 - training loss: 0.2840, validation loss: 0.5955
2024-06-03 02:58:20 [INFO]: Epoch 029 - training loss: 0.2787, validation loss: 0.6023
2024-06-03 02:58:23 [INFO]: Epoch 030 - training loss: 0.2800, validation loss: 0.5962
2024-06-03 02:58:26 [INFO]: Epoch 031 - training loss: 0.2753, validation loss: 0.6000
2024-06-03 02:58:28 [INFO]: Epoch 032 - training loss: 0.2762, validation loss: 0.5919
2024-06-03 02:58:31 [INFO]: Epoch 033 - training loss: 0.2751, validation loss: 0.5921
2024-06-03 02:58:34 [INFO]: Epoch 034 - training loss: 0.2732, validation loss: 0.5961
2024-06-03 02:58:36 [INFO]: Epoch 035 - training loss: 0.2701, validation loss: 0.5907
2024-06-03 02:58:39 [INFO]: Epoch 036 - training loss: 0.2684, validation loss: 0.5865
2024-06-03 02:58:42 [INFO]: Epoch 037 - training loss: 0.2709, validation loss: 0.5936
2024-06-03 02:58:45 [INFO]: Epoch 038 - training loss: 0.2672, validation loss: 0.5893
2024-06-03 02:58:48 [INFO]: Epoch 039 - training loss: 0.2652, validation loss: 0.5962
2024-06-03 02:58:51 [INFO]: Epoch 040 - training loss: 0.2655, validation loss: 0.5853
2024-06-03 02:58:54 [INFO]: Epoch 041 - training loss: 0.2627, validation loss: 0.5882
2024-06-03 02:58:57 [INFO]: Epoch 042 - training loss: 0.2625, validation loss: 0.5876
2024-06-03 02:59:00 [INFO]: Epoch 043 - training loss: 0.2627, validation loss: 0.5831
2024-06-03 02:59:03 [INFO]: Epoch 044 - training loss: 0.2674, validation loss: 0.5842
2024-06-03 02:59:05 [INFO]: Epoch 045 - training loss: 0.2593, validation loss: 0.5901
2024-06-03 02:59:08 [INFO]: Epoch 046 - training loss: 0.2567, validation loss: 0.5845
2024-06-03 02:59:11 [INFO]: Epoch 047 - training loss: 0.2560, validation loss: 0.5898
2024-06-03 02:59:14 [INFO]: Epoch 048 - training loss: 0.2577, validation loss: 0.5860
2024-06-03 02:59:17 [INFO]: Epoch 049 - training loss: 0.2534, validation loss: 0.5885
2024-06-03 02:59:19 [INFO]: Epoch 050 - training loss: 0.2532, validation loss: 0.5900
2024-06-03 02:59:22 [INFO]: Epoch 051 - training loss: 0.2533, validation loss: 0.5895
2024-06-03 02:59:24 [INFO]: Epoch 052 - training loss: 0.2531, validation loss: 0.5830
2024-06-03 02:59:27 [INFO]: Epoch 053 - training loss: 0.2506, validation loss: 0.5802
2024-06-03 02:59:29 [INFO]: Epoch 054 - training loss: 0.2520, validation loss: 0.5822
2024-06-03 02:59:32 [INFO]: Epoch 055 - training loss: 0.2477, validation loss: 0.5854
2024-06-03 02:59:34 [INFO]: Epoch 056 - training loss: 0.2479, validation loss: 0.5900
2024-06-03 02:59:37 [INFO]: Epoch 057 - training loss: 0.2502, validation loss: 0.5848
2024-06-03 02:59:39 [INFO]: Epoch 058 - training loss: 0.2511, validation loss: 0.5826
2024-06-03 02:59:42 [INFO]: Epoch 059 - training loss: 0.2505, validation loss: 0.5816
2024-06-03 02:59:45 [INFO]: Epoch 060 - training loss: 0.2494, validation loss: 0.5822
2024-06-03 02:59:47 [INFO]: Epoch 061 - training loss: 0.2505, validation loss: 0.5812
2024-06-03 02:59:50 [INFO]: Epoch 062 - training loss: 0.2555, validation loss: 0.5845
2024-06-03 02:59:52 [INFO]: Epoch 063 - training loss: 0.2548, validation loss: 0.5818
2024-06-03 02:59:52 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 02:59:52 [INFO]: Finished training. The best model is from epoch#53.
2024-06-03 02:59:52 [INFO]: Saved the model to results_subseq_rate05/PeMS/DLinear_PeMS/round_1/20240603_T025659/DLinear.pypots
2024-06-03 02:59:53 [INFO]: Successfully saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_1/imputation.pkl
2024-06-03 02:59:53 [INFO]: Round1 - DLinear on PeMS: MAE=0.4485, MSE=0.8586, MRE=0.5300
2024-06-03 02:59:53 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 02:59:53 [INFO]: Using the given device: cuda:0
2024-06-03 02:59:53 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_2/20240603_T025953
2024-06-03 02:59:53 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_2/20240603_T025953/tensorboard
2024-06-03 02:59:53 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 02:59:56 [INFO]: Epoch 001 - training loss: 3.4254, validation loss: 1.7959
2024-06-03 02:59:59 [INFO]: Epoch 002 - training loss: 0.9971, validation loss: 1.0397
2024-06-03 03:00:01 [INFO]: Epoch 003 - training loss: 0.6738, validation loss: 0.8808
2024-06-03 03:00:04 [INFO]: Epoch 004 - training loss: 0.5554, validation loss: 0.7678
2024-06-03 03:00:06 [INFO]: Epoch 005 - training loss: 0.4975, validation loss: 0.7271
2024-06-03 03:00:09 [INFO]: Epoch 006 - training loss: 0.4603, validation loss: 0.6851
2024-06-03 03:00:11 [INFO]: Epoch 007 - training loss: 0.4318, validation loss: 0.6605
2024-06-03 03:00:13 [INFO]: Epoch 008 - training loss: 0.4177, validation loss: 0.6377
2024-06-03 03:00:16 [INFO]: Epoch 009 - training loss: 0.4020, validation loss: 0.6200
2024-06-03 03:00:18 [INFO]: Epoch 010 - training loss: 0.3839, validation loss: 0.6181
2024-06-03 03:00:21 [INFO]: Epoch 011 - training loss: 0.3733, validation loss: 0.6087
2024-06-03 03:00:23 [INFO]: Epoch 012 - training loss: 0.3660, validation loss: 0.6138
2024-06-03 03:00:26 [INFO]: Epoch 013 - training loss: 0.3610, validation loss: 0.6018
2024-06-03 03:00:28 [INFO]: Epoch 014 - training loss: 0.3556, validation loss: 0.5937
2024-06-03 03:00:31 [INFO]: Epoch 015 - training loss: 0.3545, validation loss: 0.6114
2024-06-03 03:00:33 [INFO]: Epoch 016 - training loss: 0.3413, validation loss: 0.6011
2024-06-03 03:00:36 [INFO]: Epoch 017 - training loss: 0.3311, validation loss: 0.5864
2024-06-03 03:00:39 [INFO]: Epoch 018 - training loss: 0.3238, validation loss: 0.5888
2024-06-03 03:00:41 [INFO]: Epoch 019 - training loss: 0.3156, validation loss: 0.5854
2024-06-03 03:00:44 [INFO]: Epoch 020 - training loss: 0.3087, validation loss: 0.5816
2024-06-03 03:00:46 [INFO]: Epoch 021 - training loss: 0.3035, validation loss: 0.5796
2024-06-03 03:00:49 [INFO]: Epoch 022 - training loss: 0.3033, validation loss: 0.5835
2024-06-03 03:00:51 [INFO]: Epoch 023 - training loss: 0.2973, validation loss: 0.5828
2024-06-03 03:00:54 [INFO]: Epoch 024 - training loss: 0.2939, validation loss: 0.5868
2024-06-03 03:00:56 [INFO]: Epoch 025 - training loss: 0.2934, validation loss: 0.5879
2024-06-03 03:00:59 [INFO]: Epoch 026 - training loss: 0.2868, validation loss: 0.5834
2024-06-03 03:01:01 [INFO]: Epoch 027 - training loss: 0.2859, validation loss: 0.5910
2024-06-03 03:01:03 [INFO]: Epoch 028 - training loss: 0.2805, validation loss: 0.5910
2024-06-03 03:01:05 [INFO]: Epoch 029 - training loss: 0.2825, validation loss: 0.5871
2024-06-03 03:01:08 [INFO]: Epoch 030 - training loss: 0.2783, validation loss: 0.5890
2024-06-03 03:01:10 [INFO]: Epoch 031 - training loss: 0.2741, validation loss: 0.5885
2024-06-03 03:01:10 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:01:10 [INFO]: Finished training. The best model is from epoch#21.
2024-06-03 03:01:10 [INFO]: Saved the model to results_subseq_rate05/PeMS/DLinear_PeMS/round_2/20240603_T025953/DLinear.pypots
2024-06-03 03:01:11 [INFO]: Successfully saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_2/imputation.pkl
2024-06-03 03:01:11 [INFO]: Round2 - DLinear on PeMS: MAE=0.4520, MSE=0.8721, MRE=0.5342
2024-06-03 03:01:11 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 03:01:11 [INFO]: Using the given device: cuda:0
2024-06-03 03:01:11 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_3/20240603_T030111
2024-06-03 03:01:11 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_3/20240603_T030111/tensorboard
2024-06-03 03:01:11 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 03:01:14 [INFO]: Epoch 001 - training loss: 3.2801, validation loss: 1.7010
2024-06-03 03:01:17 [INFO]: Epoch 002 - training loss: 1.0444, validation loss: 1.2284
2024-06-03 03:01:19 [INFO]: Epoch 003 - training loss: 0.7270, validation loss: 0.9925
2024-06-03 03:01:22 [INFO]: Epoch 004 - training loss: 0.5816, validation loss: 0.8258
2024-06-03 03:01:24 [INFO]: Epoch 005 - training loss: 0.5205, validation loss: 0.7825
2024-06-03 03:01:27 [INFO]: Epoch 006 - training loss: 0.4827, validation loss: 0.7869
2024-06-03 03:01:29 [INFO]: Epoch 007 - training loss: 0.4552, validation loss: 0.7343
2024-06-03 03:01:31 [INFO]: Epoch 008 - training loss: 0.4357, validation loss: 0.7245
2024-06-03 03:01:34 [INFO]: Epoch 009 - training loss: 0.4174, validation loss: 0.6705
2024-06-03 03:01:36 [INFO]: Epoch 010 - training loss: 0.4010, validation loss: 0.6506
2024-06-03 03:01:39 [INFO]: Epoch 011 - training loss: 0.3858, validation loss: 0.6247
2024-06-03 03:01:41 [INFO]: Epoch 012 - training loss: 0.3826, validation loss: 0.6179
2024-06-03 03:01:44 [INFO]: Epoch 013 - training loss: 0.3669, validation loss: 0.6150
2024-06-03 03:01:46 [INFO]: Epoch 014 - training loss: 0.3598, validation loss: 0.6055
2024-06-03 03:01:49 [INFO]: Epoch 015 - training loss: 0.3509, validation loss: 0.6055
2024-06-03 03:01:52 [INFO]: Epoch 016 - training loss: 0.3448, validation loss: 0.5957
2024-06-03 03:01:54 [INFO]: Epoch 017 - training loss: 0.3544, validation loss: 0.5915
2024-06-03 03:01:56 [INFO]: Epoch 018 - training loss: 0.3409, validation loss: 0.5951
2024-06-03 03:01:58 [INFO]: Epoch 019 - training loss: 0.3290, validation loss: 0.5865
2024-06-03 03:02:01 [INFO]: Epoch 020 - training loss: 0.3273, validation loss: 0.6058
2024-06-03 03:02:03 [INFO]: Epoch 021 - training loss: 0.3218, validation loss: 0.5853
2024-06-03 03:02:06 [INFO]: Epoch 022 - training loss: 0.3137, validation loss: 0.5909
2024-06-03 03:02:08 [INFO]: Epoch 023 - training loss: 0.3121, validation loss: 0.5819
2024-06-03 03:02:11 [INFO]: Epoch 024 - training loss: 0.3100, validation loss: 0.5812
2024-06-03 03:02:13 [INFO]: Epoch 025 - training loss: 0.3028, validation loss: 0.5785
2024-06-03 03:02:16 [INFO]: Epoch 026 - training loss: 0.3054, validation loss: 0.5820
2024-06-03 03:02:19 [INFO]: Epoch 027 - training loss: 0.2998, validation loss: 0.5815
2024-06-03 03:02:21 [INFO]: Epoch 028 - training loss: 0.2931, validation loss: 0.5817
2024-06-03 03:02:24 [INFO]: Epoch 029 - training loss: 0.2869, validation loss: 0.5889
2024-06-03 03:02:26 [INFO]: Epoch 030 - training loss: 0.2889, validation loss: 0.5791
2024-06-03 03:02:29 [INFO]: Epoch 031 - training loss: 0.2789, validation loss: 0.5840
2024-06-03 03:02:31 [INFO]: Epoch 032 - training loss: 0.2790, validation loss: 0.5783
2024-06-03 03:02:34 [INFO]: Epoch 033 - training loss: 0.2750, validation loss: 0.5872
2024-06-03 03:02:37 [INFO]: Epoch 034 - training loss: 0.2715, validation loss: 0.5823
2024-06-03 03:02:39 [INFO]: Epoch 035 - training loss: 0.2726, validation loss: 0.5810
2024-06-03 03:02:42 [INFO]: Epoch 036 - training loss: 0.2722, validation loss: 0.5777
2024-06-03 03:02:44 [INFO]: Epoch 037 - training loss: 0.2718, validation loss: 0.5845
2024-06-03 03:02:46 [INFO]: Epoch 038 - training loss: 0.2743, validation loss: 0.5829
2024-06-03 03:02:48 [INFO]: Epoch 039 - training loss: 0.2731, validation loss: 0.5848
2024-06-03 03:02:51 [INFO]: Epoch 040 - training loss: 0.2712, validation loss: 0.5883
2024-06-03 03:02:53 [INFO]: Epoch 041 - training loss: 0.2677, validation loss: 0.5802
2024-06-03 03:02:56 [INFO]: Epoch 042 - training loss: 0.2677, validation loss: 0.5830
2024-06-03 03:02:58 [INFO]: Epoch 043 - training loss: 0.2620, validation loss: 0.5833
2024-06-03 03:03:01 [INFO]: Epoch 044 - training loss: 0.2588, validation loss: 0.5815
2024-06-03 03:03:03 [INFO]: Epoch 045 - training loss: 0.2634, validation loss: 0.5866
2024-06-03 03:03:06 [INFO]: Epoch 046 - training loss: 0.2616, validation loss: 0.5897
2024-06-03 03:03:06 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:03:06 [INFO]: Finished training. The best model is from epoch#36.
2024-06-03 03:03:06 [INFO]: Saved the model to results_subseq_rate05/PeMS/DLinear_PeMS/round_3/20240603_T030111/DLinear.pypots
2024-06-03 03:03:07 [INFO]: Successfully saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_3/imputation.pkl
2024-06-03 03:03:07 [INFO]: Round3 - DLinear on PeMS: MAE=0.4625, MSE=0.8807, MRE=0.5466
2024-06-03 03:03:07 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 03:03:07 [INFO]: Using the given device: cuda:0
2024-06-03 03:03:07 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_4/20240603_T030307
2024-06-03 03:03:07 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_4/20240603_T030307/tensorboard
2024-06-03 03:03:07 [INFO]: DLinear initialized with the given hyperparameters, the number of trainable parameters: 5,301,100
2024-06-03 03:03:10 [INFO]: Epoch 001 - training loss: 3.7009, validation loss: 1.6795
2024-06-03 03:03:12 [INFO]: Epoch 002 - training loss: 0.9927, validation loss: 1.0420
2024-06-03 03:03:15 [INFO]: Epoch 003 - training loss: 0.6622, validation loss: 0.8202
2024-06-03 03:03:17 [INFO]: Epoch 004 - training loss: 0.5458, validation loss: 0.7898
2024-06-03 03:03:20 [INFO]: Epoch 005 - training loss: 0.4928, validation loss: 0.7545
2024-06-03 03:03:22 [INFO]: Epoch 006 - training loss: 0.4594, validation loss: 0.7324
2024-06-03 03:03:25 [INFO]: Epoch 007 - training loss: 0.4378, validation loss: 0.6825
2024-06-03 03:03:27 [INFO]: Epoch 008 - training loss: 0.4136, validation loss: 0.6654
2024-06-03 03:03:30 [INFO]: Epoch 009 - training loss: 0.3990, validation loss: 0.6428
2024-06-03 03:03:32 [INFO]: Epoch 010 - training loss: 0.3883, validation loss: 0.6314
2024-06-03 03:03:34 [INFO]: Epoch 011 - training loss: 0.3855, validation loss: 0.6236
2024-06-03 03:03:37 [INFO]: Epoch 012 - training loss: 0.3750, validation loss: 0.6177
2024-06-03 03:03:39 [INFO]: Epoch 013 - training loss: 0.3677, validation loss: 0.6152
2024-06-03 03:03:42 [INFO]: Epoch 014 - training loss: 0.3633, validation loss: 0.6243
2024-06-03 03:03:44 [INFO]: Epoch 015 - training loss: 0.3581, validation loss: 0.6220
2024-06-03 03:03:47 [INFO]: Epoch 016 - training loss: 0.3515, validation loss: 0.6230
2024-06-03 03:03:49 [INFO]: Epoch 017 - training loss: 0.3391, validation loss: 0.6183
2024-06-03 03:03:52 [INFO]: Epoch 018 - training loss: 0.3354, validation loss: 0.6102
2024-06-03 03:03:54 [INFO]: Epoch 019 - training loss: 0.3397, validation loss: 0.6363
2024-06-03 03:03:56 [INFO]: Epoch 020 - training loss: 0.3293, validation loss: 0.6210
2024-06-03 03:03:58 [INFO]: Epoch 021 - training loss: 0.3180, validation loss: 0.6182
2024-06-03 03:04:00 [INFO]: Epoch 022 - training loss: 0.3097, validation loss: 0.6157
2024-06-03 03:04:03 [INFO]: Epoch 023 - training loss: 0.3061, validation loss: 0.6220
2024-06-03 03:04:05 [INFO]: Epoch 024 - training loss: 0.2995, validation loss: 0.6187
2024-06-03 03:04:07 [INFO]: Epoch 025 - training loss: 0.2959, validation loss: 0.6281
2024-06-03 03:04:09 [INFO]: Epoch 026 - training loss: 0.2955, validation loss: 0.6120
2024-06-03 03:04:11 [INFO]: Epoch 027 - training loss: 0.2950, validation loss: 0.6213
2024-06-03 03:04:13 [INFO]: Epoch 028 - training loss: 0.2912, validation loss: 0.6192
2024-06-03 03:04:13 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:04:13 [INFO]: Finished training. The best model is from epoch#18.
2024-06-03 03:04:13 [INFO]: Saved the model to results_subseq_rate05/PeMS/DLinear_PeMS/round_4/20240603_T030307/DLinear.pypots
2024-06-03 03:04:14 [INFO]: Successfully saved to results_subseq_rate05/PeMS/DLinear_PeMS/round_4/imputation.pkl
2024-06-03 03:04:14 [INFO]: Round4 - DLinear on PeMS: MAE=0.4805, MSE=0.9118, MRE=0.5678
2024-06-03 03:04:14 [INFO]: Done! Final results:
Averaged DLinear (5,301,100 params) on PeMS: MAE=0.4590 ± 0.01176795743419826, MSE=0.8761 ± 0.019881636277174194, MRE=0.5424 ± 0.013906491695204241, average inference time=0.11
