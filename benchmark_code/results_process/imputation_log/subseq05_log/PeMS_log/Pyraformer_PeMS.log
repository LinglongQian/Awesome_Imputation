2024-06-03 07:57:39 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 07:57:39 [INFO]: Using the given device: cuda:0
2024-06-03 07:57:40 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_0/20240603_T075740
2024-06-03 07:57:40 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_0/20240603_T075740/tensorboard
2024-06-03 07:57:41 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 07:57:48 [INFO]: Epoch 001 - training loss: 0.9234, validation loss: 0.7590
2024-06-03 07:57:49 [INFO]: Epoch 002 - training loss: 0.5541, validation loss: 0.6321
2024-06-03 07:57:51 [INFO]: Epoch 003 - training loss: 0.4729, validation loss: 0.6143
2024-06-03 07:57:53 [INFO]: Epoch 004 - training loss: 0.4395, validation loss: 0.5884
2024-06-03 07:57:55 [INFO]: Epoch 005 - training loss: 0.4216, validation loss: 0.5805
2024-06-03 07:57:58 [INFO]: Epoch 006 - training loss: 0.4055, validation loss: 0.5743
2024-06-03 07:58:00 [INFO]: Epoch 007 - training loss: 0.3959, validation loss: 0.5693
2024-06-03 07:58:02 [INFO]: Epoch 008 - training loss: 0.3836, validation loss: 0.5653
2024-06-03 07:58:04 [INFO]: Epoch 009 - training loss: 0.3730, validation loss: 0.5614
2024-06-03 07:58:06 [INFO]: Epoch 010 - training loss: 0.3598, validation loss: 0.5608
2024-06-03 07:58:08 [INFO]: Epoch 011 - training loss: 0.3568, validation loss: 0.5509
2024-06-03 07:58:10 [INFO]: Epoch 012 - training loss: 0.3561, validation loss: 0.5539
2024-06-03 07:58:13 [INFO]: Epoch 013 - training loss: 0.3522, validation loss: 0.5498
2024-06-03 07:58:15 [INFO]: Epoch 014 - training loss: 0.3404, validation loss: 0.5436
2024-06-03 07:58:17 [INFO]: Epoch 015 - training loss: 0.3312, validation loss: 0.5440
2024-06-03 07:58:19 [INFO]: Epoch 016 - training loss: 0.3263, validation loss: 0.5413
2024-06-03 07:58:21 [INFO]: Epoch 017 - training loss: 0.3259, validation loss: 0.5405
2024-06-03 07:58:23 [INFO]: Epoch 018 - training loss: 0.3236, validation loss: 0.5406
2024-06-03 07:58:26 [INFO]: Epoch 019 - training loss: 0.3158, validation loss: 0.5390
2024-06-03 07:58:29 [INFO]: Epoch 020 - training loss: 0.3219, validation loss: 0.5384
2024-06-03 07:58:32 [INFO]: Epoch 021 - training loss: 0.3145, validation loss: 0.5348
2024-06-03 07:58:35 [INFO]: Epoch 022 - training loss: 0.3092, validation loss: 0.5365
2024-06-03 07:58:38 [INFO]: Epoch 023 - training loss: 0.3035, validation loss: 0.5322
2024-06-03 07:58:41 [INFO]: Epoch 024 - training loss: 0.3037, validation loss: 0.5323
2024-06-03 07:58:44 [INFO]: Epoch 025 - training loss: 0.3032, validation loss: 0.5329
2024-06-03 07:58:47 [INFO]: Epoch 026 - training loss: 0.2926, validation loss: 0.5286
2024-06-03 07:58:50 [INFO]: Epoch 027 - training loss: 0.2893, validation loss: 0.5303
2024-06-03 07:58:53 [INFO]: Epoch 028 - training loss: 0.2914, validation loss: 0.5269
2024-06-03 07:58:56 [INFO]: Epoch 029 - training loss: 0.2836, validation loss: 0.5237
2024-06-03 07:58:59 [INFO]: Epoch 030 - training loss: 0.2813, validation loss: 0.5261
2024-06-03 07:59:02 [INFO]: Epoch 031 - training loss: 0.2799, validation loss: 0.5334
2024-06-03 07:59:05 [INFO]: Epoch 032 - training loss: 0.2856, validation loss: 0.5294
2024-06-03 07:59:08 [INFO]: Epoch 033 - training loss: 0.2801, validation loss: 0.5294
2024-06-03 07:59:11 [INFO]: Epoch 034 - training loss: 0.2770, validation loss: 0.5266
2024-06-03 07:59:15 [INFO]: Epoch 035 - training loss: 0.2794, validation loss: 0.5309
2024-06-03 07:59:18 [INFO]: Epoch 036 - training loss: 0.2722, validation loss: 0.5229
2024-06-03 07:59:21 [INFO]: Epoch 037 - training loss: 0.2665, validation loss: 0.5257
2024-06-03 07:59:24 [INFO]: Epoch 038 - training loss: 0.2678, validation loss: 0.5247
2024-06-03 07:59:27 [INFO]: Epoch 039 - training loss: 0.2697, validation loss: 0.5266
2024-06-03 07:59:30 [INFO]: Epoch 040 - training loss: 0.2717, validation loss: 0.5325
2024-06-03 07:59:33 [INFO]: Epoch 041 - training loss: 0.2683, validation loss: 0.5243
2024-06-03 07:59:36 [INFO]: Epoch 042 - training loss: 0.2608, validation loss: 0.5244
2024-06-03 07:59:40 [INFO]: Epoch 043 - training loss: 0.2550, validation loss: 0.5195
2024-06-03 07:59:43 [INFO]: Epoch 044 - training loss: 0.2576, validation loss: 0.5271
2024-06-03 07:59:46 [INFO]: Epoch 045 - training loss: 0.2560, validation loss: 0.5209
2024-06-03 07:59:49 [INFO]: Epoch 046 - training loss: 0.2546, validation loss: 0.5232
2024-06-03 07:59:52 [INFO]: Epoch 047 - training loss: 0.2560, validation loss: 0.5246
2024-06-03 07:59:55 [INFO]: Epoch 048 - training loss: 0.2516, validation loss: 0.5192
2024-06-03 07:59:58 [INFO]: Epoch 049 - training loss: 0.2541, validation loss: 0.5210
2024-06-03 08:00:02 [INFO]: Epoch 050 - training loss: 0.2510, validation loss: 0.5219
2024-06-03 08:00:05 [INFO]: Epoch 051 - training loss: 0.2506, validation loss: 0.5203
2024-06-03 08:00:08 [INFO]: Epoch 052 - training loss: 0.2502, validation loss: 0.5140
2024-06-03 08:00:11 [INFO]: Epoch 053 - training loss: 0.2552, validation loss: 0.5170
2024-06-03 08:00:13 [INFO]: Epoch 054 - training loss: 0.2522, validation loss: 0.5244
2024-06-03 08:00:16 [INFO]: Epoch 055 - training loss: 0.2493, validation loss: 0.5194
2024-06-03 08:00:19 [INFO]: Epoch 056 - training loss: 0.2460, validation loss: 0.5195
2024-06-03 08:00:23 [INFO]: Epoch 057 - training loss: 0.2421, validation loss: 0.5168
2024-06-03 08:00:26 [INFO]: Epoch 058 - training loss: 0.2432, validation loss: 0.5215
2024-06-03 08:00:29 [INFO]: Epoch 059 - training loss: 0.2497, validation loss: 0.5127
2024-06-03 08:00:32 [INFO]: Epoch 060 - training loss: 0.2431, validation loss: 0.5194
2024-06-03 08:00:35 [INFO]: Epoch 061 - training loss: 0.2392, validation loss: 0.5225
2024-06-03 08:00:39 [INFO]: Epoch 062 - training loss: 0.2383, validation loss: 0.5193
2024-06-03 08:00:42 [INFO]: Epoch 063 - training loss: 0.2404, validation loss: 0.5173
2024-06-03 08:00:45 [INFO]: Epoch 064 - training loss: 0.2390, validation loss: 0.5191
2024-06-03 08:00:48 [INFO]: Epoch 065 - training loss: 0.2343, validation loss: 0.5180
2024-06-03 08:00:51 [INFO]: Epoch 066 - training loss: 0.2318, validation loss: 0.5146
2024-06-03 08:00:54 [INFO]: Epoch 067 - training loss: 0.2339, validation loss: 0.5186
2024-06-03 08:00:58 [INFO]: Epoch 068 - training loss: 0.2343, validation loss: 0.5184
2024-06-03 08:01:01 [INFO]: Epoch 069 - training loss: 0.2324, validation loss: 0.5168
2024-06-03 08:01:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 08:01:01 [INFO]: Finished training. The best model is from epoch#59.
2024-06-03 08:01:01 [INFO]: Saved the model to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_0/20240603_T075740/Pyraformer.pypots
2024-06-03 08:01:03 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_0/imputation.pkl
2024-06-03 08:01:03 [INFO]: Round0 - Pyraformer on PeMS: MAE=0.3511, MSE=0.7627, MRE=0.4150
2024-06-03 08:01:03 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 08:01:03 [INFO]: Using the given device: cuda:0
2024-06-03 08:01:03 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_1/20240603_T080103
2024-06-03 08:01:03 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_1/20240603_T080103/tensorboard
2024-06-03 08:01:03 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 08:01:06 [INFO]: Epoch 001 - training loss: 0.9456, validation loss: 0.7666
2024-06-03 08:01:09 [INFO]: Epoch 002 - training loss: 0.5657, validation loss: 0.6454
2024-06-03 08:01:12 [INFO]: Epoch 003 - training loss: 0.4787, validation loss: 0.5998
2024-06-03 08:01:15 [INFO]: Epoch 004 - training loss: 0.4412, validation loss: 0.5871
2024-06-03 08:01:18 [INFO]: Epoch 005 - training loss: 0.4288, validation loss: 0.5773
2024-06-03 08:01:21 [INFO]: Epoch 006 - training loss: 0.4104, validation loss: 0.5808
2024-06-03 08:01:24 [INFO]: Epoch 007 - training loss: 0.3922, validation loss: 0.5779
2024-06-03 08:01:27 [INFO]: Epoch 008 - training loss: 0.3749, validation loss: 0.5751
2024-06-03 08:01:31 [INFO]: Epoch 009 - training loss: 0.3752, validation loss: 0.5627
2024-06-03 08:01:34 [INFO]: Epoch 010 - training loss: 0.3627, validation loss: 0.5628
2024-06-03 08:01:37 [INFO]: Epoch 011 - training loss: 0.3574, validation loss: 0.5564
2024-06-03 08:01:40 [INFO]: Epoch 012 - training loss: 0.3572, validation loss: 0.5657
2024-06-03 08:01:43 [INFO]: Epoch 013 - training loss: 0.3596, validation loss: 0.5440
2024-06-03 08:01:46 [INFO]: Epoch 014 - training loss: 0.3422, validation loss: 0.5434
2024-06-03 08:01:49 [INFO]: Epoch 015 - training loss: 0.3356, validation loss: 0.5446
2024-06-03 08:01:52 [INFO]: Epoch 016 - training loss: 0.3262, validation loss: 0.5456
2024-06-03 08:01:55 [INFO]: Epoch 017 - training loss: 0.3278, validation loss: 0.5392
2024-06-03 08:01:58 [INFO]: Epoch 018 - training loss: 0.3169, validation loss: 0.5345
2024-06-03 08:02:01 [INFO]: Epoch 019 - training loss: 0.3155, validation loss: 0.5359
2024-06-03 08:02:04 [INFO]: Epoch 020 - training loss: 0.3116, validation loss: 0.5351
2024-06-03 08:02:07 [INFO]: Epoch 021 - training loss: 0.3088, validation loss: 0.5365
2024-06-03 08:02:10 [INFO]: Epoch 022 - training loss: 0.3071, validation loss: 0.5385
2024-06-03 08:02:14 [INFO]: Epoch 023 - training loss: 0.3060, validation loss: 0.5315
2024-06-03 08:02:17 [INFO]: Epoch 024 - training loss: 0.3006, validation loss: 0.5305
2024-06-03 08:02:20 [INFO]: Epoch 025 - training loss: 0.3007, validation loss: 0.5303
2024-06-03 08:02:23 [INFO]: Epoch 026 - training loss: 0.2950, validation loss: 0.5398
2024-06-03 08:02:26 [INFO]: Epoch 027 - training loss: 0.2979, validation loss: 0.5286
2024-06-03 08:02:29 [INFO]: Epoch 028 - training loss: 0.2914, validation loss: 0.5292
2024-06-03 08:02:32 [INFO]: Epoch 029 - training loss: 0.2894, validation loss: 0.5285
2024-06-03 08:02:35 [INFO]: Epoch 030 - training loss: 0.2831, validation loss: 0.5249
2024-06-03 08:02:39 [INFO]: Epoch 031 - training loss: 0.2809, validation loss: 0.5295
2024-06-03 08:02:42 [INFO]: Epoch 032 - training loss: 0.2804, validation loss: 0.5250
2024-06-03 08:02:45 [INFO]: Epoch 033 - training loss: 0.2806, validation loss: 0.5268
2024-06-03 08:02:48 [INFO]: Epoch 034 - training loss: 0.2731, validation loss: 0.5250
2024-06-03 08:02:52 [INFO]: Epoch 035 - training loss: 0.2687, validation loss: 0.5269
2024-06-03 08:02:55 [INFO]: Epoch 036 - training loss: 0.2698, validation loss: 0.5247
2024-06-03 08:02:58 [INFO]: Epoch 037 - training loss: 0.2708, validation loss: 0.5292
2024-06-03 08:03:01 [INFO]: Epoch 038 - training loss: 0.2692, validation loss: 0.5240
2024-06-03 08:03:04 [INFO]: Epoch 039 - training loss: 0.2646, validation loss: 0.5235
2024-06-03 08:03:07 [INFO]: Epoch 040 - training loss: 0.2685, validation loss: 0.5262
2024-06-03 08:03:11 [INFO]: Epoch 041 - training loss: 0.2685, validation loss: 0.5207
2024-06-03 08:03:14 [INFO]: Epoch 042 - training loss: 0.2607, validation loss: 0.5225
2024-06-03 08:03:17 [INFO]: Epoch 043 - training loss: 0.2647, validation loss: 0.5205
2024-06-03 08:03:20 [INFO]: Epoch 044 - training loss: 0.2617, validation loss: 0.5209
2024-06-03 08:03:23 [INFO]: Epoch 045 - training loss: 0.2608, validation loss: 0.5222
2024-06-03 08:03:26 [INFO]: Epoch 046 - training loss: 0.2603, validation loss: 0.5189
2024-06-03 08:03:29 [INFO]: Epoch 047 - training loss: 0.2615, validation loss: 0.5171
2024-06-03 08:03:32 [INFO]: Epoch 048 - training loss: 0.2546, validation loss: 0.5180
2024-06-03 08:03:35 [INFO]: Epoch 049 - training loss: 0.2503, validation loss: 0.5214
2024-06-03 08:03:38 [INFO]: Epoch 050 - training loss: 0.2492, validation loss: 0.5148
2024-06-03 08:03:41 [INFO]: Epoch 051 - training loss: 0.2488, validation loss: 0.5182
2024-06-03 08:03:44 [INFO]: Epoch 052 - training loss: 0.2470, validation loss: 0.5179
2024-06-03 08:03:48 [INFO]: Epoch 053 - training loss: 0.2481, validation loss: 0.5154
2024-06-03 08:03:51 [INFO]: Epoch 054 - training loss: 0.2491, validation loss: 0.5156
2024-06-03 08:03:54 [INFO]: Epoch 055 - training loss: 0.2458, validation loss: 0.5219
2024-06-03 08:03:58 [INFO]: Epoch 056 - training loss: 0.2514, validation loss: 0.5164
2024-06-03 08:04:01 [INFO]: Epoch 057 - training loss: 0.2433, validation loss: 0.5155
2024-06-03 08:04:04 [INFO]: Epoch 058 - training loss: 0.2436, validation loss: 0.5150
2024-06-03 08:04:07 [INFO]: Epoch 059 - training loss: 0.2469, validation loss: 0.5218
2024-06-03 08:04:10 [INFO]: Epoch 060 - training loss: 0.2418, validation loss: 0.5196
2024-06-03 08:04:10 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 08:04:10 [INFO]: Finished training. The best model is from epoch#50.
2024-06-03 08:04:11 [INFO]: Saved the model to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_1/20240603_T080103/Pyraformer.pypots
2024-06-03 08:04:12 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_1/imputation.pkl
2024-06-03 08:04:12 [INFO]: Round1 - Pyraformer on PeMS: MAE=0.3525, MSE=0.7633, MRE=0.4166
2024-06-03 08:04:12 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 08:04:12 [INFO]: Using the given device: cuda:0
2024-06-03 08:04:12 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_2/20240603_T080412
2024-06-03 08:04:12 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_2/20240603_T080412/tensorboard
2024-06-03 08:04:12 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 08:04:16 [INFO]: Epoch 001 - training loss: 0.9362, validation loss: 0.7705
2024-06-03 08:04:19 [INFO]: Epoch 002 - training loss: 0.5498, validation loss: 0.6429
2024-06-03 08:04:22 [INFO]: Epoch 003 - training loss: 0.4813, validation loss: 0.6102
2024-06-03 08:04:25 [INFO]: Epoch 004 - training loss: 0.4444, validation loss: 0.5963
2024-06-03 08:04:28 [INFO]: Epoch 005 - training loss: 0.4259, validation loss: 0.5882
2024-06-03 08:04:32 [INFO]: Epoch 006 - training loss: 0.4066, validation loss: 0.5790
2024-06-03 08:04:35 [INFO]: Epoch 007 - training loss: 0.3983, validation loss: 0.5716
2024-06-03 08:04:38 [INFO]: Epoch 008 - training loss: 0.3872, validation loss: 0.5688
2024-06-03 08:04:41 [INFO]: Epoch 009 - training loss: 0.3776, validation loss: 0.5571
2024-06-03 08:04:44 [INFO]: Epoch 010 - training loss: 0.3753, validation loss: 0.5625
2024-06-03 08:04:47 [INFO]: Epoch 011 - training loss: 0.3616, validation loss: 0.5506
2024-06-03 08:04:50 [INFO]: Epoch 012 - training loss: 0.3508, validation loss: 0.5533
2024-06-03 08:04:53 [INFO]: Epoch 013 - training loss: 0.3475, validation loss: 0.5480
2024-06-03 08:04:56 [INFO]: Epoch 014 - training loss: 0.3435, validation loss: 0.5450
2024-06-03 08:04:59 [INFO]: Epoch 015 - training loss: 0.3376, validation loss: 0.5427
2024-06-03 08:05:02 [INFO]: Epoch 016 - training loss: 0.3355, validation loss: 0.5418
2024-06-03 08:05:05 [INFO]: Epoch 017 - training loss: 0.3270, validation loss: 0.5409
2024-06-03 08:05:09 [INFO]: Epoch 018 - training loss: 0.3192, validation loss: 0.5412
2024-06-03 08:05:12 [INFO]: Epoch 019 - training loss: 0.3169, validation loss: 0.5356
2024-06-03 08:05:15 [INFO]: Epoch 020 - training loss: 0.3122, validation loss: 0.5391
2024-06-03 08:05:18 [INFO]: Epoch 021 - training loss: 0.3060, validation loss: 0.5299
2024-06-03 08:05:21 [INFO]: Epoch 022 - training loss: 0.3001, validation loss: 0.5306
2024-06-03 08:05:23 [INFO]: Epoch 023 - training loss: 0.2996, validation loss: 0.5305
2024-06-03 08:05:26 [INFO]: Epoch 024 - training loss: 0.2970, validation loss: 0.5317
2024-06-03 08:05:29 [INFO]: Epoch 025 - training loss: 0.2911, validation loss: 0.5309
2024-06-03 08:05:31 [INFO]: Epoch 026 - training loss: 0.2891, validation loss: 0.5332
2024-06-03 08:05:34 [INFO]: Epoch 027 - training loss: 0.2905, validation loss: 0.5283
2024-06-03 08:05:36 [INFO]: Epoch 028 - training loss: 0.2880, validation loss: 0.5293
2024-06-03 08:05:39 [INFO]: Epoch 029 - training loss: 0.2855, validation loss: 0.5261
2024-06-03 08:05:41 [INFO]: Epoch 030 - training loss: 0.2801, validation loss: 0.5242
2024-06-03 08:05:44 [INFO]: Epoch 031 - training loss: 0.2786, validation loss: 0.5259
2024-06-03 08:05:46 [INFO]: Epoch 032 - training loss: 0.2783, validation loss: 0.5218
2024-06-03 08:05:49 [INFO]: Epoch 033 - training loss: 0.2790, validation loss: 0.5220
2024-06-03 08:05:52 [INFO]: Epoch 034 - training loss: 0.2735, validation loss: 0.5228
2024-06-03 08:05:54 [INFO]: Epoch 035 - training loss: 0.2676, validation loss: 0.5233
2024-06-03 08:05:57 [INFO]: Epoch 036 - training loss: 0.2674, validation loss: 0.5244
2024-06-03 08:05:59 [INFO]: Epoch 037 - training loss: 0.2716, validation loss: 0.5203
2024-06-03 08:06:02 [INFO]: Epoch 038 - training loss: 0.2731, validation loss: 0.5305
2024-06-03 08:06:05 [INFO]: Epoch 039 - training loss: 0.2669, validation loss: 0.5247
2024-06-03 08:06:07 [INFO]: Epoch 040 - training loss: 0.2636, validation loss: 0.5212
2024-06-03 08:06:10 [INFO]: Epoch 041 - training loss: 0.2597, validation loss: 0.5234
2024-06-03 08:06:12 [INFO]: Epoch 042 - training loss: 0.2583, validation loss: 0.5211
2024-06-03 08:06:15 [INFO]: Epoch 043 - training loss: 0.2578, validation loss: 0.5182
2024-06-03 08:06:18 [INFO]: Epoch 044 - training loss: 0.2567, validation loss: 0.5209
2024-06-03 08:06:20 [INFO]: Epoch 045 - training loss: 0.2549, validation loss: 0.5144
2024-06-03 08:06:23 [INFO]: Epoch 046 - training loss: 0.2534, validation loss: 0.5179
2024-06-03 08:06:26 [INFO]: Epoch 047 - training loss: 0.2491, validation loss: 0.5221
2024-06-03 08:06:29 [INFO]: Epoch 048 - training loss: 0.2519, validation loss: 0.5203
2024-06-03 08:06:32 [INFO]: Epoch 049 - training loss: 0.2509, validation loss: 0.5171
2024-06-03 08:06:34 [INFO]: Epoch 050 - training loss: 0.2462, validation loss: 0.5174
2024-06-03 08:06:37 [INFO]: Epoch 051 - training loss: 0.2454, validation loss: 0.5151
2024-06-03 08:06:39 [INFO]: Epoch 052 - training loss: 0.2453, validation loss: 0.5160
2024-06-03 08:06:42 [INFO]: Epoch 053 - training loss: 0.2520, validation loss: 0.5174
2024-06-03 08:06:44 [INFO]: Epoch 054 - training loss: 0.2538, validation loss: 0.5179
2024-06-03 08:06:47 [INFO]: Epoch 055 - training loss: 0.2476, validation loss: 0.5155
2024-06-03 08:06:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 08:06:47 [INFO]: Finished training. The best model is from epoch#45.
2024-06-03 08:06:47 [INFO]: Saved the model to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_2/20240603_T080412/Pyraformer.pypots
2024-06-03 08:06:48 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_2/imputation.pkl
2024-06-03 08:06:48 [INFO]: Round2 - Pyraformer on PeMS: MAE=0.3490, MSE=0.7588, MRE=0.4125
2024-06-03 08:06:48 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 08:06:48 [INFO]: Using the given device: cuda:0
2024-06-03 08:06:48 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_3/20240603_T080648
2024-06-03 08:06:48 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_3/20240603_T080648/tensorboard
2024-06-03 08:06:49 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 08:06:51 [INFO]: Epoch 001 - training loss: 0.9583, validation loss: 0.7766
2024-06-03 08:06:54 [INFO]: Epoch 002 - training loss: 0.5600, validation loss: 0.6334
2024-06-03 08:06:57 [INFO]: Epoch 003 - training loss: 0.4847, validation loss: 0.6095
2024-06-03 08:06:59 [INFO]: Epoch 004 - training loss: 0.4407, validation loss: 0.5929
2024-06-03 08:07:02 [INFO]: Epoch 005 - training loss: 0.4319, validation loss: 0.5825
2024-06-03 08:07:04 [INFO]: Epoch 006 - training loss: 0.4122, validation loss: 0.5767
2024-06-03 08:07:07 [INFO]: Epoch 007 - training loss: 0.3966, validation loss: 0.5671
2024-06-03 08:07:10 [INFO]: Epoch 008 - training loss: 0.3822, validation loss: 0.5662
2024-06-03 08:07:12 [INFO]: Epoch 009 - training loss: 0.3688, validation loss: 0.5759
2024-06-03 08:07:15 [INFO]: Epoch 010 - training loss: 0.3789, validation loss: 0.5608
2024-06-03 08:07:18 [INFO]: Epoch 011 - training loss: 0.3573, validation loss: 0.5578
2024-06-03 08:07:20 [INFO]: Epoch 012 - training loss: 0.3528, validation loss: 0.5519
2024-06-03 08:07:22 [INFO]: Epoch 013 - training loss: 0.3443, validation loss: 0.5478
2024-06-03 08:07:24 [INFO]: Epoch 014 - training loss: 0.3385, validation loss: 0.5537
2024-06-03 08:07:25 [INFO]: Epoch 015 - training loss: 0.3348, validation loss: 0.5454
2024-06-03 08:07:27 [INFO]: Epoch 016 - training loss: 0.3318, validation loss: 0.5447
2024-06-03 08:07:28 [INFO]: Epoch 017 - training loss: 0.3282, validation loss: 0.5410
2024-06-03 08:07:29 [INFO]: Epoch 018 - training loss: 0.3208, validation loss: 0.5435
2024-06-03 08:07:31 [INFO]: Epoch 019 - training loss: 0.3139, validation loss: 0.5445
2024-06-03 08:07:32 [INFO]: Epoch 020 - training loss: 0.3141, validation loss: 0.5412
2024-06-03 08:07:34 [INFO]: Epoch 021 - training loss: 0.3105, validation loss: 0.5383
2024-06-03 08:07:35 [INFO]: Epoch 022 - training loss: 0.3044, validation loss: 0.5382
2024-06-03 08:07:36 [INFO]: Epoch 023 - training loss: 0.2956, validation loss: 0.5354
2024-06-03 08:07:38 [INFO]: Epoch 024 - training loss: 0.3007, validation loss: 0.5338
2024-06-03 08:07:39 [INFO]: Epoch 025 - training loss: 0.2960, validation loss: 0.5319
2024-06-03 08:07:41 [INFO]: Epoch 026 - training loss: 0.2925, validation loss: 0.5328
2024-06-03 08:07:42 [INFO]: Epoch 027 - training loss: 0.2884, validation loss: 0.5393
2024-06-03 08:07:44 [INFO]: Epoch 028 - training loss: 0.2862, validation loss: 0.5400
2024-06-03 08:07:45 [INFO]: Epoch 029 - training loss: 0.2867, validation loss: 0.5312
2024-06-03 08:07:47 [INFO]: Epoch 030 - training loss: 0.2916, validation loss: 0.5288
2024-06-03 08:07:49 [INFO]: Epoch 031 - training loss: 0.2847, validation loss: 0.5270
2024-06-03 08:07:50 [INFO]: Epoch 032 - training loss: 0.2783, validation loss: 0.5274
2024-06-03 08:07:52 [INFO]: Epoch 033 - training loss: 0.2748, validation loss: 0.5436
2024-06-03 08:07:53 [INFO]: Epoch 034 - training loss: 0.2725, validation loss: 0.5290
2024-06-03 08:07:55 [INFO]: Epoch 035 - training loss: 0.2790, validation loss: 0.5307
2024-06-03 08:07:56 [INFO]: Epoch 036 - training loss: 0.2722, validation loss: 0.5281
2024-06-03 08:07:58 [INFO]: Epoch 037 - training loss: 0.2711, validation loss: 0.5304
2024-06-03 08:07:59 [INFO]: Epoch 038 - training loss: 0.2683, validation loss: 0.5267
2024-06-03 08:08:01 [INFO]: Epoch 039 - training loss: 0.2639, validation loss: 0.5260
2024-06-03 08:08:02 [INFO]: Epoch 040 - training loss: 0.2639, validation loss: 0.5229
2024-06-03 08:08:03 [INFO]: Epoch 041 - training loss: 0.2668, validation loss: 0.5286
2024-06-03 08:08:05 [INFO]: Epoch 042 - training loss: 0.2652, validation loss: 0.5276
2024-06-03 08:08:06 [INFO]: Epoch 043 - training loss: 0.2580, validation loss: 0.5223
2024-06-03 08:08:07 [INFO]: Epoch 044 - training loss: 0.2591, validation loss: 0.5274
2024-06-03 08:08:09 [INFO]: Epoch 045 - training loss: 0.2574, validation loss: 0.5250
2024-06-03 08:08:10 [INFO]: Epoch 046 - training loss: 0.2542, validation loss: 0.5256
2024-06-03 08:08:12 [INFO]: Epoch 047 - training loss: 0.2579, validation loss: 0.5246
2024-06-03 08:08:13 [INFO]: Epoch 048 - training loss: 0.2541, validation loss: 0.5223
2024-06-03 08:08:15 [INFO]: Epoch 049 - training loss: 0.2527, validation loss: 0.5289
2024-06-03 08:08:16 [INFO]: Epoch 050 - training loss: 0.2497, validation loss: 0.5221
2024-06-03 08:08:18 [INFO]: Epoch 051 - training loss: 0.2523, validation loss: 0.5251
2024-06-03 08:08:19 [INFO]: Epoch 052 - training loss: 0.2479, validation loss: 0.5201
2024-06-03 08:08:20 [INFO]: Epoch 053 - training loss: 0.2477, validation loss: 0.5200
2024-06-03 08:08:23 [INFO]: Epoch 054 - training loss: 0.2488, validation loss: 0.5189
2024-06-03 08:08:25 [INFO]: Epoch 055 - training loss: 0.2468, validation loss: 0.5224
2024-06-03 08:08:28 [INFO]: Epoch 056 - training loss: 0.2470, validation loss: 0.5218
2024-06-03 08:08:30 [INFO]: Epoch 057 - training loss: 0.2435, validation loss: 0.5170
2024-06-03 08:08:32 [INFO]: Epoch 058 - training loss: 0.2418, validation loss: 0.5136
2024-06-03 08:08:35 [INFO]: Epoch 059 - training loss: 0.2395, validation loss: 0.5219
2024-06-03 08:08:37 [INFO]: Epoch 060 - training loss: 0.2358, validation loss: 0.5190
2024-06-03 08:08:39 [INFO]: Epoch 061 - training loss: 0.2355, validation loss: 0.5211
2024-06-03 08:08:42 [INFO]: Epoch 062 - training loss: 0.2380, validation loss: 0.5180
2024-06-03 08:08:44 [INFO]: Epoch 063 - training loss: 0.2389, validation loss: 0.5214
2024-06-03 08:08:46 [INFO]: Epoch 064 - training loss: 0.2390, validation loss: 0.5179
2024-06-03 08:08:49 [INFO]: Epoch 065 - training loss: 0.2436, validation loss: 0.5237
2024-06-03 08:08:51 [INFO]: Epoch 066 - training loss: 0.2418, validation loss: 0.5179
2024-06-03 08:08:53 [INFO]: Epoch 067 - training loss: 0.2391, validation loss: 0.5139
2024-06-03 08:08:55 [INFO]: Epoch 068 - training loss: 0.2362, validation loss: 0.5163
2024-06-03 08:08:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 08:08:55 [INFO]: Finished training. The best model is from epoch#58.
2024-06-03 08:08:56 [INFO]: Saved the model to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_3/20240603_T080648/Pyraformer.pypots
2024-06-03 08:08:57 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_3/imputation.pkl
2024-06-03 08:08:57 [INFO]: Round3 - Pyraformer on PeMS: MAE=0.3512, MSE=0.7621, MRE=0.4150
2024-06-03 08:08:57 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 08:08:57 [INFO]: Using the given device: cuda:0
2024-06-03 08:08:57 [INFO]: Model files will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_4/20240603_T080857
2024-06-03 08:08:57 [INFO]: Tensorboard file will be saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_4/20240603_T080857/tensorboard
2024-06-03 08:08:57 [INFO]: Pyraformer initialized with the given hyperparameters, the number of trainable parameters: 4,048,606
2024-06-03 08:08:59 [INFO]: Epoch 001 - training loss: 0.9217, validation loss: 0.7543
2024-06-03 08:09:01 [INFO]: Epoch 002 - training loss: 0.5527, validation loss: 0.6384
2024-06-03 08:09:04 [INFO]: Epoch 003 - training loss: 0.4772, validation loss: 0.5997
2024-06-03 08:09:06 [INFO]: Epoch 004 - training loss: 0.4427, validation loss: 0.5849
2024-06-03 08:09:08 [INFO]: Epoch 005 - training loss: 0.4120, validation loss: 0.5884
2024-06-03 08:09:11 [INFO]: Epoch 006 - training loss: 0.4085, validation loss: 0.5745
2024-06-03 08:09:13 [INFO]: Epoch 007 - training loss: 0.3883, validation loss: 0.5659
2024-06-03 08:09:15 [INFO]: Epoch 008 - training loss: 0.3804, validation loss: 0.5648
2024-06-03 08:09:18 [INFO]: Epoch 009 - training loss: 0.3828, validation loss: 0.5703
2024-06-03 08:09:20 [INFO]: Epoch 010 - training loss: 0.3657, validation loss: 0.5590
2024-06-03 08:09:22 [INFO]: Epoch 011 - training loss: 0.3560, validation loss: 0.5589
2024-06-03 08:09:24 [INFO]: Epoch 012 - training loss: 0.3497, validation loss: 0.5536
2024-06-03 08:09:27 [INFO]: Epoch 013 - training loss: 0.3467, validation loss: 0.5508
2024-06-03 08:09:29 [INFO]: Epoch 014 - training loss: 0.3379, validation loss: 0.5437
2024-06-03 08:09:31 [INFO]: Epoch 015 - training loss: 0.3338, validation loss: 0.5466
2024-06-03 08:09:34 [INFO]: Epoch 016 - training loss: 0.3319, validation loss: 0.5439
2024-06-03 08:09:36 [INFO]: Epoch 017 - training loss: 0.3276, validation loss: 0.5461
2024-06-03 08:09:38 [INFO]: Epoch 018 - training loss: 0.3184, validation loss: 0.5368
2024-06-03 08:09:41 [INFO]: Epoch 019 - training loss: 0.3122, validation loss: 0.5365
2024-06-03 08:09:43 [INFO]: Epoch 020 - training loss: 0.3108, validation loss: 0.5409
2024-06-03 08:09:45 [INFO]: Epoch 021 - training loss: 0.3047, validation loss: 0.5370
2024-06-03 08:09:48 [INFO]: Epoch 022 - training loss: 0.3053, validation loss: 0.5368
2024-06-03 08:09:50 [INFO]: Epoch 023 - training loss: 0.3004, validation loss: 0.5331
2024-06-03 08:09:52 [INFO]: Epoch 024 - training loss: 0.2944, validation loss: 0.5304
2024-06-03 08:09:55 [INFO]: Epoch 025 - training loss: 0.2973, validation loss: 0.5290
2024-06-03 08:09:57 [INFO]: Epoch 026 - training loss: 0.2934, validation loss: 0.5275
2024-06-03 08:09:59 [INFO]: Epoch 027 - training loss: 0.2876, validation loss: 0.5292
2024-06-03 08:10:01 [INFO]: Epoch 028 - training loss: 0.2842, validation loss: 0.5274
2024-06-03 08:10:04 [INFO]: Epoch 029 - training loss: 0.2859, validation loss: 0.5312
2024-06-03 08:10:06 [INFO]: Epoch 030 - training loss: 0.2801, validation loss: 0.5355
2024-06-03 08:10:08 [INFO]: Epoch 031 - training loss: 0.2832, validation loss: 0.5282
2024-06-03 08:10:10 [INFO]: Epoch 032 - training loss: 0.2777, validation loss: 0.5295
2024-06-03 08:10:13 [INFO]: Epoch 033 - training loss: 0.2792, validation loss: 0.5295
2024-06-03 08:10:15 [INFO]: Epoch 034 - training loss: 0.2790, validation loss: 0.5278
2024-06-03 08:10:18 [INFO]: Epoch 035 - training loss: 0.2703, validation loss: 0.5233
2024-06-03 08:10:20 [INFO]: Epoch 036 - training loss: 0.2697, validation loss: 0.5272
2024-06-03 08:10:22 [INFO]: Epoch 037 - training loss: 0.2664, validation loss: 0.5240
2024-06-03 08:10:25 [INFO]: Epoch 038 - training loss: 0.2656, validation loss: 0.5252
2024-06-03 08:10:27 [INFO]: Epoch 039 - training loss: 0.2674, validation loss: 0.5274
2024-06-03 08:10:30 [INFO]: Epoch 040 - training loss: 0.2612, validation loss: 0.5217
2024-06-03 08:10:32 [INFO]: Epoch 041 - training loss: 0.2619, validation loss: 0.5243
2024-06-03 08:10:34 [INFO]: Epoch 042 - training loss: 0.2679, validation loss: 0.5259
2024-06-03 08:10:37 [INFO]: Epoch 043 - training loss: 0.2693, validation loss: 0.5247
2024-06-03 08:10:39 [INFO]: Epoch 044 - training loss: 0.2633, validation loss: 0.5252
2024-06-03 08:10:41 [INFO]: Epoch 045 - training loss: 0.2596, validation loss: 0.5269
2024-06-03 08:10:44 [INFO]: Epoch 046 - training loss: 0.2555, validation loss: 0.5265
2024-06-03 08:10:46 [INFO]: Epoch 047 - training loss: 0.2517, validation loss: 0.5235
2024-06-03 08:10:48 [INFO]: Epoch 048 - training loss: 0.2515, validation loss: 0.5225
2024-06-03 08:10:51 [INFO]: Epoch 049 - training loss: 0.2471, validation loss: 0.5223
2024-06-03 08:10:53 [INFO]: Epoch 050 - training loss: 0.2502, validation loss: 0.5263
2024-06-03 08:10:53 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 08:10:53 [INFO]: Finished training. The best model is from epoch#40.
2024-06-03 08:10:53 [INFO]: Saved the model to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_4/20240603_T080857/Pyraformer.pypots
2024-06-03 08:10:54 [INFO]: Successfully saved to results_subseq_rate05/PeMS/Pyraformer_PeMS/round_4/imputation.pkl
2024-06-03 08:10:54 [INFO]: Round4 - Pyraformer on PeMS: MAE=0.3545, MSE=0.7744, MRE=0.4189
2024-06-03 08:10:54 [INFO]: Done! Final results:
Averaged Pyraformer (4,048,606 params) on PeMS: MAE=0.3517 ± 0.0017925080235011235, MSE=0.7643 ± 0.005325297620894178, MRE=0.4156 ± 0.0021182518786110227, average inference time=0.21
