2024-06-03 03:48:37 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 03:48:37 [INFO]: Using the given device: cuda:0
2024-06-03 03:48:38 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_0/20240603_T034838
2024-06-03 03:48:38 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_0/20240603_T034838/tensorboard
2024-06-03 03:48:39 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 03:48:44 [INFO]: Epoch 001 - training loss: 1.0681, validation loss: 1.0808
2024-06-03 03:48:45 [INFO]: Epoch 002 - training loss: 0.6601, validation loss: 1.0783
2024-06-03 03:48:46 [INFO]: Epoch 003 - training loss: 0.6147, validation loss: 1.0643
2024-06-03 03:48:47 [INFO]: Epoch 004 - training loss: 0.5835, validation loss: 1.0605
2024-06-03 03:48:49 [INFO]: Epoch 005 - training loss: 0.5664, validation loss: 1.0516
2024-06-03 03:48:50 [INFO]: Epoch 006 - training loss: 0.5632, validation loss: 1.0520
2024-06-03 03:48:51 [INFO]: Epoch 007 - training loss: 0.5621, validation loss: 1.0605
2024-06-03 03:48:52 [INFO]: Epoch 008 - training loss: 0.5781, validation loss: 1.0530
2024-06-03 03:48:53 [INFO]: Epoch 009 - training loss: 0.5587, validation loss: 1.0436
2024-06-03 03:48:55 [INFO]: Epoch 010 - training loss: 0.5553, validation loss: 1.0495
2024-06-03 03:48:56 [INFO]: Epoch 011 - training loss: 0.5533, validation loss: 1.0558
2024-06-03 03:48:57 [INFO]: Epoch 012 - training loss: 0.5527, validation loss: 1.0433
2024-06-03 03:48:59 [INFO]: Epoch 013 - training loss: 0.5431, validation loss: 1.0499
2024-06-03 03:49:00 [INFO]: Epoch 014 - training loss: 0.5492, validation loss: 1.0486
2024-06-03 03:49:01 [INFO]: Epoch 015 - training loss: 0.5414, validation loss: 1.0534
2024-06-03 03:49:03 [INFO]: Epoch 016 - training loss: 0.5452, validation loss: 1.0379
2024-06-03 03:49:04 [INFO]: Epoch 017 - training loss: 0.5424, validation loss: 1.0479
2024-06-03 03:49:05 [INFO]: Epoch 018 - training loss: 0.5391, validation loss: 1.0479
2024-06-03 03:49:06 [INFO]: Epoch 019 - training loss: 0.5336, validation loss: 1.0365
2024-06-03 03:49:08 [INFO]: Epoch 020 - training loss: 0.5386, validation loss: 1.0502
2024-06-03 03:49:09 [INFO]: Epoch 021 - training loss: 0.5326, validation loss: 1.0392
2024-06-03 03:49:10 [INFO]: Epoch 022 - training loss: 0.5276, validation loss: 1.0424
2024-06-03 03:49:11 [INFO]: Epoch 023 - training loss: 0.5307, validation loss: 1.0419
2024-06-03 03:49:13 [INFO]: Epoch 024 - training loss: 0.5242, validation loss: 1.0400
2024-06-03 03:49:14 [INFO]: Epoch 025 - training loss: 0.5267, validation loss: 1.0402
2024-06-03 03:49:15 [INFO]: Epoch 026 - training loss: 0.5256, validation loss: 1.0363
2024-06-03 03:49:17 [INFO]: Epoch 027 - training loss: 0.5253, validation loss: 1.0387
2024-06-03 03:49:18 [INFO]: Epoch 028 - training loss: 0.5321, validation loss: 1.0438
2024-06-03 03:49:19 [INFO]: Epoch 029 - training loss: 0.5312, validation loss: 1.0314
2024-06-03 03:49:21 [INFO]: Epoch 030 - training loss: 0.5190, validation loss: 1.0316
2024-06-03 03:49:22 [INFO]: Epoch 031 - training loss: 0.5356, validation loss: 1.0358
2024-06-03 03:49:23 [INFO]: Epoch 032 - training loss: 0.5341, validation loss: 1.0373
2024-06-03 03:49:24 [INFO]: Epoch 033 - training loss: 0.5279, validation loss: 1.0228
2024-06-03 03:49:26 [INFO]: Epoch 034 - training loss: 0.5347, validation loss: 1.0317
2024-06-03 03:49:27 [INFO]: Epoch 035 - training loss: 0.5149, validation loss: 1.0284
2024-06-03 03:49:28 [INFO]: Epoch 036 - training loss: 0.5199, validation loss: 1.0105
2024-06-03 03:49:29 [INFO]: Epoch 037 - training loss: 0.5173, validation loss: 1.0422
2024-06-03 03:49:30 [INFO]: Epoch 038 - training loss: 0.5175, validation loss: 1.0156
2024-06-03 03:49:31 [INFO]: Epoch 039 - training loss: 0.5231, validation loss: 1.0234
2024-06-03 03:49:32 [INFO]: Epoch 040 - training loss: 0.5110, validation loss: 1.0267
2024-06-03 03:49:33 [INFO]: Epoch 041 - training loss: 0.5265, validation loss: 1.0258
2024-06-03 03:49:35 [INFO]: Epoch 042 - training loss: 0.5122, validation loss: 1.0181
2024-06-03 03:49:36 [INFO]: Epoch 043 - training loss: 0.5158, validation loss: 1.0197
2024-06-03 03:49:37 [INFO]: Epoch 044 - training loss: 0.5042, validation loss: 1.0161
2024-06-03 03:49:39 [INFO]: Epoch 045 - training loss: 0.5120, validation loss: 1.0291
2024-06-03 03:49:40 [INFO]: Epoch 046 - training loss: 0.5097, validation loss: 1.0192
2024-06-03 03:49:40 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:49:40 [INFO]: Finished training. The best model is from epoch#36.
2024-06-03 03:49:40 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_0/20240603_T034838/MICN.pypots
2024-06-03 03:49:41 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_0/imputation.pkl
2024-06-03 03:49:41 [INFO]: Round0 - MICN on ETT_h1: MAE=0.8562, MSE=1.3556, MRE=0.9644
2024-06-03 03:49:41 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 03:49:41 [INFO]: Using the given device: cuda:0
2024-06-03 03:49:41 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_1/20240603_T034941
2024-06-03 03:49:41 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_1/20240603_T034941/tensorboard
2024-06-03 03:49:41 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 03:49:42 [INFO]: Epoch 001 - training loss: 1.0343, validation loss: 1.1500
2024-06-03 03:49:43 [INFO]: Epoch 002 - training loss: 0.6431, validation loss: 1.0506
2024-06-03 03:49:45 [INFO]: Epoch 003 - training loss: 0.6079, validation loss: 1.0686
2024-06-03 03:49:46 [INFO]: Epoch 004 - training loss: 0.5872, validation loss: 1.0552
2024-06-03 03:49:47 [INFO]: Epoch 005 - training loss: 0.5742, validation loss: 1.0506
2024-06-03 03:49:49 [INFO]: Epoch 006 - training loss: 0.5646, validation loss: 1.0508
2024-06-03 03:49:50 [INFO]: Epoch 007 - training loss: 0.5673, validation loss: 1.0432
2024-06-03 03:49:51 [INFO]: Epoch 008 - training loss: 0.5576, validation loss: 1.0473
2024-06-03 03:49:52 [INFO]: Epoch 009 - training loss: 0.5560, validation loss: 1.0522
2024-06-03 03:49:53 [INFO]: Epoch 010 - training loss: 0.5556, validation loss: 1.0440
2024-06-03 03:49:55 [INFO]: Epoch 011 - training loss: 0.5574, validation loss: 1.0518
2024-06-03 03:49:56 [INFO]: Epoch 012 - training loss: 0.5591, validation loss: 1.0420
2024-06-03 03:49:58 [INFO]: Epoch 013 - training loss: 0.5445, validation loss: 1.0488
2024-06-03 03:49:59 [INFO]: Epoch 014 - training loss: 0.5459, validation loss: 1.0370
2024-06-03 03:50:00 [INFO]: Epoch 015 - training loss: 0.5386, validation loss: 1.0473
2024-06-03 03:50:02 [INFO]: Epoch 016 - training loss: 0.5441, validation loss: 1.0311
2024-06-03 03:50:03 [INFO]: Epoch 017 - training loss: 0.5356, validation loss: 1.0443
2024-06-03 03:50:04 [INFO]: Epoch 018 - training loss: 0.5433, validation loss: 1.0383
2024-06-03 03:50:05 [INFO]: Epoch 019 - training loss: 0.5392, validation loss: 1.0391
2024-06-03 03:50:07 [INFO]: Epoch 020 - training loss: 0.5387, validation loss: 1.0392
2024-06-03 03:50:08 [INFO]: Epoch 021 - training loss: 0.5332, validation loss: 1.0371
2024-06-03 03:50:09 [INFO]: Epoch 022 - training loss: 0.5305, validation loss: 1.0329
2024-06-03 03:50:10 [INFO]: Epoch 023 - training loss: 0.5376, validation loss: 1.0390
2024-06-03 03:50:12 [INFO]: Epoch 024 - training loss: 0.5378, validation loss: 1.0292
2024-06-03 03:50:13 [INFO]: Epoch 025 - training loss: 0.5291, validation loss: 1.0281
2024-06-03 03:50:14 [INFO]: Epoch 026 - training loss: 0.5377, validation loss: 1.0343
2024-06-03 03:50:16 [INFO]: Epoch 027 - training loss: 0.5211, validation loss: 1.0265
2024-06-03 03:50:17 [INFO]: Epoch 028 - training loss: 0.5264, validation loss: 1.0237
2024-06-03 03:50:18 [INFO]: Epoch 029 - training loss: 0.5278, validation loss: 1.0231
2024-06-03 03:50:20 [INFO]: Epoch 030 - training loss: 0.5193, validation loss: 1.0229
2024-06-03 03:50:21 [INFO]: Epoch 031 - training loss: 0.5376, validation loss: 1.0305
2024-06-03 03:50:22 [INFO]: Epoch 032 - training loss: 0.5386, validation loss: 1.0252
2024-06-03 03:50:24 [INFO]: Epoch 033 - training loss: 0.5255, validation loss: 1.0199
2024-06-03 03:50:25 [INFO]: Epoch 034 - training loss: 0.5135, validation loss: 1.0232
2024-06-03 03:50:26 [INFO]: Epoch 035 - training loss: 0.5196, validation loss: 1.0211
2024-06-03 03:50:27 [INFO]: Epoch 036 - training loss: 0.5198, validation loss: 1.0187
2024-06-03 03:50:28 [INFO]: Epoch 037 - training loss: 0.5299, validation loss: 1.0161
2024-06-03 03:50:29 [INFO]: Epoch 038 - training loss: 0.5177, validation loss: 1.0203
2024-06-03 03:50:31 [INFO]: Epoch 039 - training loss: 0.5231, validation loss: 1.0199
2024-06-03 03:50:32 [INFO]: Epoch 040 - training loss: 0.5182, validation loss: 1.0111
2024-06-03 03:50:33 [INFO]: Epoch 041 - training loss: 0.5127, validation loss: 1.0124
2024-06-03 03:50:34 [INFO]: Epoch 042 - training loss: 0.5147, validation loss: 1.0134
2024-06-03 03:50:35 [INFO]: Epoch 043 - training loss: 0.5125, validation loss: 1.0076
2024-06-03 03:50:36 [INFO]: Epoch 044 - training loss: 0.5036, validation loss: 1.0160
2024-06-03 03:50:37 [INFO]: Epoch 045 - training loss: 0.5198, validation loss: 1.0063
2024-06-03 03:50:38 [INFO]: Epoch 046 - training loss: 0.5136, validation loss: 1.0151
2024-06-03 03:50:39 [INFO]: Epoch 047 - training loss: 0.5081, validation loss: 1.0093
2024-06-03 03:50:40 [INFO]: Epoch 048 - training loss: 0.5172, validation loss: 1.0085
2024-06-03 03:50:42 [INFO]: Epoch 049 - training loss: 0.5146, validation loss: 1.0104
2024-06-03 03:50:43 [INFO]: Epoch 050 - training loss: 0.4959, validation loss: 1.0087
2024-06-03 03:50:44 [INFO]: Epoch 051 - training loss: 0.5090, validation loss: 1.0035
2024-06-03 03:50:45 [INFO]: Epoch 052 - training loss: 0.5089, validation loss: 1.0069
2024-06-03 03:50:46 [INFO]: Epoch 053 - training loss: 0.5089, validation loss: 1.0134
2024-06-03 03:50:47 [INFO]: Epoch 054 - training loss: 0.4972, validation loss: 0.9983
2024-06-03 03:50:48 [INFO]: Epoch 055 - training loss: 0.5059, validation loss: 1.0084
2024-06-03 03:50:49 [INFO]: Epoch 056 - training loss: 0.4916, validation loss: 1.0109
2024-06-03 03:50:50 [INFO]: Epoch 057 - training loss: 0.4885, validation loss: 1.0045
2024-06-03 03:50:51 [INFO]: Epoch 058 - training loss: 0.4881, validation loss: 1.0035
2024-06-03 03:50:51 [INFO]: Epoch 059 - training loss: 0.4955, validation loss: 1.0090
2024-06-03 03:50:52 [INFO]: Epoch 060 - training loss: 0.4872, validation loss: 0.9982
2024-06-03 03:50:53 [INFO]: Epoch 061 - training loss: 0.4824, validation loss: 1.0041
2024-06-03 03:50:54 [INFO]: Epoch 062 - training loss: 0.4906, validation loss: 0.9943
2024-06-03 03:50:55 [INFO]: Epoch 063 - training loss: 0.4892, validation loss: 1.0185
2024-06-03 03:50:56 [INFO]: Epoch 064 - training loss: 0.4944, validation loss: 0.9824
2024-06-03 03:50:57 [INFO]: Epoch 065 - training loss: 0.4931, validation loss: 1.0115
2024-06-03 03:50:58 [INFO]: Epoch 066 - training loss: 0.4890, validation loss: 0.9945
2024-06-03 03:50:59 [INFO]: Epoch 067 - training loss: 0.4896, validation loss: 1.0051
2024-06-03 03:51:00 [INFO]: Epoch 068 - training loss: 0.4855, validation loss: 0.9861
2024-06-03 03:51:01 [INFO]: Epoch 069 - training loss: 0.4832, validation loss: 0.9991
2024-06-03 03:51:02 [INFO]: Epoch 070 - training loss: 0.4878, validation loss: 0.9895
2024-06-03 03:51:03 [INFO]: Epoch 071 - training loss: 0.4788, validation loss: 0.9937
2024-06-03 03:51:05 [INFO]: Epoch 072 - training loss: 0.4722, validation loss: 0.9941
2024-06-03 03:51:06 [INFO]: Epoch 073 - training loss: 0.4888, validation loss: 0.9944
2024-06-03 03:51:06 [INFO]: Epoch 074 - training loss: 0.4669, validation loss: 0.9920
2024-06-03 03:51:06 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:51:06 [INFO]: Finished training. The best model is from epoch#64.
2024-06-03 03:51:07 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_1/20240603_T034941/MICN.pypots
2024-06-03 03:51:07 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_1/imputation.pkl
2024-06-03 03:51:07 [INFO]: Round1 - MICN on ETT_h1: MAE=0.8388, MSE=1.3132, MRE=0.9448
2024-06-03 03:51:07 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 03:51:07 [INFO]: Using the given device: cuda:0
2024-06-03 03:51:07 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_2/20240603_T035107
2024-06-03 03:51:07 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_2/20240603_T035107/tensorboard
2024-06-03 03:51:07 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 03:51:08 [INFO]: Epoch 001 - training loss: 1.1743, validation loss: 1.0590
2024-06-03 03:51:09 [INFO]: Epoch 002 - training loss: 0.6508, validation loss: 1.0669
2024-06-03 03:51:10 [INFO]: Epoch 003 - training loss: 0.6135, validation loss: 1.0668
2024-06-03 03:51:11 [INFO]: Epoch 004 - training loss: 0.5899, validation loss: 1.0560
2024-06-03 03:51:12 [INFO]: Epoch 005 - training loss: 0.5849, validation loss: 1.0540
2024-06-03 03:51:13 [INFO]: Epoch 006 - training loss: 0.5714, validation loss: 1.0591
2024-06-03 03:51:14 [INFO]: Epoch 007 - training loss: 0.5689, validation loss: 1.0481
2024-06-03 03:51:15 [INFO]: Epoch 008 - training loss: 0.5721, validation loss: 1.0592
2024-06-03 03:51:16 [INFO]: Epoch 009 - training loss: 0.5659, validation loss: 1.0503
2024-06-03 03:51:17 [INFO]: Epoch 010 - training loss: 0.5612, validation loss: 1.0442
2024-06-03 03:51:19 [INFO]: Epoch 011 - training loss: 0.5563, validation loss: 1.0476
2024-06-03 03:51:19 [INFO]: Epoch 012 - training loss: 0.5575, validation loss: 1.0452
2024-06-03 03:51:20 [INFO]: Epoch 013 - training loss: 0.5586, validation loss: 1.0510
2024-06-03 03:51:21 [INFO]: Epoch 014 - training loss: 0.5586, validation loss: 1.0405
2024-06-03 03:51:22 [INFO]: Epoch 015 - training loss: 0.5385, validation loss: 1.0517
2024-06-03 03:51:23 [INFO]: Epoch 016 - training loss: 0.5363, validation loss: 1.0430
2024-06-03 03:51:24 [INFO]: Epoch 017 - training loss: 0.5470, validation loss: 1.0449
2024-06-03 03:51:25 [INFO]: Epoch 018 - training loss: 0.5487, validation loss: 1.0446
2024-06-03 03:51:26 [INFO]: Epoch 019 - training loss: 0.5392, validation loss: 1.0411
2024-06-03 03:51:27 [INFO]: Epoch 020 - training loss: 0.5459, validation loss: 1.0339
2024-06-03 03:51:28 [INFO]: Epoch 021 - training loss: 0.5406, validation loss: 1.0418
2024-06-03 03:51:29 [INFO]: Epoch 022 - training loss: 0.5333, validation loss: 1.0429
2024-06-03 03:51:30 [INFO]: Epoch 023 - training loss: 0.5412, validation loss: 1.0431
2024-06-03 03:51:31 [INFO]: Epoch 024 - training loss: 0.5280, validation loss: 1.0446
2024-06-03 03:51:31 [INFO]: Epoch 025 - training loss: 0.5352, validation loss: 1.0410
2024-06-03 03:51:32 [INFO]: Epoch 026 - training loss: 0.5373, validation loss: 1.0367
2024-06-03 03:51:33 [INFO]: Epoch 027 - training loss: 0.5299, validation loss: 1.0389
2024-06-03 03:51:34 [INFO]: Epoch 028 - training loss: 0.5339, validation loss: 1.0356
2024-06-03 03:51:35 [INFO]: Epoch 029 - training loss: 0.5281, validation loss: 1.0412
2024-06-03 03:51:36 [INFO]: Epoch 030 - training loss: 0.5243, validation loss: 1.0345
2024-06-03 03:51:36 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:51:36 [INFO]: Finished training. The best model is from epoch#20.
2024-06-03 03:51:36 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_2/20240603_T035107/MICN.pypots
2024-06-03 03:51:36 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_2/imputation.pkl
2024-06-03 03:51:36 [INFO]: Round2 - MICN on ETT_h1: MAE=0.8628, MSE=1.3748, MRE=0.9718
2024-06-03 03:51:36 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 03:51:36 [INFO]: Using the given device: cuda:0
2024-06-03 03:51:36 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_3/20240603_T035136
2024-06-03 03:51:36 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_3/20240603_T035136/tensorboard
2024-06-03 03:51:37 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 03:51:38 [INFO]: Epoch 001 - training loss: 1.1343, validation loss: 1.0547
2024-06-03 03:51:38 [INFO]: Epoch 002 - training loss: 0.6516, validation loss: 1.0816
2024-06-03 03:51:40 [INFO]: Epoch 003 - training loss: 0.5986, validation loss: 1.0524
2024-06-03 03:51:40 [INFO]: Epoch 004 - training loss: 0.5853, validation loss: 1.0608
2024-06-03 03:51:41 [INFO]: Epoch 005 - training loss: 0.5869, validation loss: 1.0605
2024-06-03 03:51:42 [INFO]: Epoch 006 - training loss: 0.5737, validation loss: 1.0571
2024-06-03 03:51:43 [INFO]: Epoch 007 - training loss: 0.5636, validation loss: 1.0568
2024-06-03 03:51:44 [INFO]: Epoch 008 - training loss: 0.5470, validation loss: 1.0457
2024-06-03 03:51:45 [INFO]: Epoch 009 - training loss: 0.5596, validation loss: 1.0661
2024-06-03 03:51:46 [INFO]: Epoch 010 - training loss: 0.5636, validation loss: 1.0514
2024-06-03 03:51:47 [INFO]: Epoch 011 - training loss: 0.5582, validation loss: 1.0482
2024-06-03 03:51:47 [INFO]: Epoch 012 - training loss: 0.5513, validation loss: 1.0569
2024-06-03 03:51:48 [INFO]: Epoch 013 - training loss: 0.5477, validation loss: 1.0477
2024-06-03 03:51:49 [INFO]: Epoch 014 - training loss: 0.5535, validation loss: 1.0528
2024-06-03 03:51:49 [INFO]: Epoch 015 - training loss: 0.5384, validation loss: 1.0460
2024-06-03 03:51:50 [INFO]: Epoch 016 - training loss: 0.5444, validation loss: 1.0545
2024-06-03 03:51:51 [INFO]: Epoch 017 - training loss: 0.5457, validation loss: 1.0427
2024-06-03 03:51:52 [INFO]: Epoch 018 - training loss: 0.5350, validation loss: 1.0489
2024-06-03 03:51:53 [INFO]: Epoch 019 - training loss: 0.5451, validation loss: 1.0447
2024-06-03 03:51:54 [INFO]: Epoch 020 - training loss: 0.5380, validation loss: 1.0388
2024-06-03 03:51:55 [INFO]: Epoch 021 - training loss: 0.5473, validation loss: 1.0544
2024-06-03 03:51:56 [INFO]: Epoch 022 - training loss: 0.5401, validation loss: 1.0386
2024-06-03 03:51:57 [INFO]: Epoch 023 - training loss: 0.5345, validation loss: 1.0392
2024-06-03 03:51:57 [INFO]: Epoch 024 - training loss: 0.5364, validation loss: 1.0448
2024-06-03 03:51:58 [INFO]: Epoch 025 - training loss: 0.5261, validation loss: 1.0346
2024-06-03 03:51:59 [INFO]: Epoch 026 - training loss: 0.5316, validation loss: 1.0471
2024-06-03 03:52:00 [INFO]: Epoch 027 - training loss: 0.5323, validation loss: 1.0366
2024-06-03 03:52:01 [INFO]: Epoch 028 - training loss: 0.5298, validation loss: 1.0431
2024-06-03 03:52:02 [INFO]: Epoch 029 - training loss: 0.5313, validation loss: 1.0309
2024-06-03 03:52:03 [INFO]: Epoch 030 - training loss: 0.5267, validation loss: 1.0425
2024-06-03 03:52:04 [INFO]: Epoch 031 - training loss: 0.5337, validation loss: 1.0393
2024-06-03 03:52:05 [INFO]: Epoch 032 - training loss: 0.5328, validation loss: 1.0286
2024-06-03 03:52:06 [INFO]: Epoch 033 - training loss: 0.5324, validation loss: 1.0457
2024-06-03 03:52:07 [INFO]: Epoch 034 - training loss: 0.5189, validation loss: 1.0260
2024-06-03 03:52:08 [INFO]: Epoch 035 - training loss: 0.5195, validation loss: 1.0328
2024-06-03 03:52:09 [INFO]: Epoch 036 - training loss: 0.5231, validation loss: 1.0360
2024-06-03 03:52:10 [INFO]: Epoch 037 - training loss: 0.5211, validation loss: 1.0337
2024-06-03 03:52:11 [INFO]: Epoch 038 - training loss: 0.5285, validation loss: 1.0280
2024-06-03 03:52:11 [INFO]: Epoch 039 - training loss: 0.5199, validation loss: 1.0320
2024-06-03 03:52:12 [INFO]: Epoch 040 - training loss: 0.5164, validation loss: 1.0320
2024-06-03 03:52:13 [INFO]: Epoch 041 - training loss: 0.5295, validation loss: 1.0213
2024-06-03 03:52:14 [INFO]: Epoch 042 - training loss: 0.5107, validation loss: 1.0349
2024-06-03 03:52:15 [INFO]: Epoch 043 - training loss: 0.5194, validation loss: 1.0212
2024-06-03 03:52:16 [INFO]: Epoch 044 - training loss: 0.5122, validation loss: 1.0281
2024-06-03 03:52:17 [INFO]: Epoch 045 - training loss: 0.5167, validation loss: 1.0226
2024-06-03 03:52:18 [INFO]: Epoch 046 - training loss: 0.5038, validation loss: 1.0175
2024-06-03 03:52:19 [INFO]: Epoch 047 - training loss: 0.5082, validation loss: 1.0283
2024-06-03 03:52:20 [INFO]: Epoch 048 - training loss: 0.5061, validation loss: 1.0253
2024-06-03 03:52:21 [INFO]: Epoch 049 - training loss: 0.5001, validation loss: 1.0258
2024-06-03 03:52:22 [INFO]: Epoch 050 - training loss: 0.4924, validation loss: 1.0194
2024-06-03 03:52:23 [INFO]: Epoch 051 - training loss: 0.5001, validation loss: 1.0260
2024-06-03 03:52:24 [INFO]: Epoch 052 - training loss: 0.5133, validation loss: 1.0124
2024-06-03 03:52:25 [INFO]: Epoch 053 - training loss: 0.5037, validation loss: 1.0207
2024-06-03 03:52:26 [INFO]: Epoch 054 - training loss: 0.5031, validation loss: 1.0271
2024-06-03 03:52:27 [INFO]: Epoch 055 - training loss: 0.4981, validation loss: 1.0223
2024-06-03 03:52:28 [INFO]: Epoch 056 - training loss: 0.5041, validation loss: 1.0181
2024-06-03 03:52:29 [INFO]: Epoch 057 - training loss: 0.4935, validation loss: 1.0195
2024-06-03 03:52:30 [INFO]: Epoch 058 - training loss: 0.4985, validation loss: 1.0188
2024-06-03 03:52:31 [INFO]: Epoch 059 - training loss: 0.5018, validation loss: 1.0234
2024-06-03 03:52:32 [INFO]: Epoch 060 - training loss: 0.4873, validation loss: 1.0136
2024-06-03 03:52:32 [INFO]: Epoch 061 - training loss: 0.4918, validation loss: 1.0166
2024-06-03 03:52:33 [INFO]: Epoch 062 - training loss: 0.4798, validation loss: 1.0075
2024-06-03 03:52:34 [INFO]: Epoch 063 - training loss: 0.4901, validation loss: 0.9970
2024-06-03 03:52:35 [INFO]: Epoch 064 - training loss: 0.4824, validation loss: 1.0073
2024-06-03 03:52:36 [INFO]: Epoch 065 - training loss: 0.4699, validation loss: 1.0212
2024-06-03 03:52:37 [INFO]: Epoch 066 - training loss: 0.4841, validation loss: 1.0043
2024-06-03 03:52:38 [INFO]: Epoch 067 - training loss: 0.4902, validation loss: 1.0272
2024-06-03 03:52:38 [INFO]: Epoch 068 - training loss: 0.4696, validation loss: 1.0026
2024-06-03 03:52:39 [INFO]: Epoch 069 - training loss: 0.4799, validation loss: 1.0167
2024-06-03 03:52:40 [INFO]: Epoch 070 - training loss: 0.4637, validation loss: 0.9872
2024-06-03 03:52:40 [INFO]: Epoch 071 - training loss: 0.4607, validation loss: 1.0006
2024-06-03 03:52:41 [INFO]: Epoch 072 - training loss: 0.4771, validation loss: 1.0214
2024-06-03 03:52:42 [INFO]: Epoch 073 - training loss: 0.4724, validation loss: 0.9932
2024-06-03 03:52:42 [INFO]: Epoch 074 - training loss: 0.4781, validation loss: 0.9984
2024-06-03 03:52:43 [INFO]: Epoch 075 - training loss: 0.4631, validation loss: 1.0097
2024-06-03 03:52:44 [INFO]: Epoch 076 - training loss: 0.4672, validation loss: 1.0002
2024-06-03 03:52:44 [INFO]: Epoch 077 - training loss: 0.4611, validation loss: 1.0052
2024-06-03 03:52:45 [INFO]: Epoch 078 - training loss: 0.4640, validation loss: 0.9953
2024-06-03 03:52:45 [INFO]: Epoch 079 - training loss: 0.4644, validation loss: 1.0003
2024-06-03 03:52:46 [INFO]: Epoch 080 - training loss: 0.4610, validation loss: 1.0189
2024-06-03 03:52:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:52:46 [INFO]: Finished training. The best model is from epoch#70.
2024-06-03 03:52:46 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_3/20240603_T035136/MICN.pypots
2024-06-03 03:52:46 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_3/imputation.pkl
2024-06-03 03:52:46 [INFO]: Round3 - MICN on ETT_h1: MAE=0.8535, MSE=1.3339, MRE=0.9613
2024-06-03 03:52:46 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 03:52:46 [INFO]: Using the given device: cuda:0
2024-06-03 03:52:46 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_4/20240603_T035246
2024-06-03 03:52:46 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_4/20240603_T035246/tensorboard
2024-06-03 03:52:46 [INFO]: MICN initialized with the given hyperparameters, the number of trainable parameters: 3,153,163
2024-06-03 03:52:47 [INFO]: Epoch 001 - training loss: 1.0720, validation loss: 1.1179
2024-06-03 03:52:48 [INFO]: Epoch 002 - training loss: 0.6573, validation loss: 1.0736
2024-06-03 03:52:48 [INFO]: Epoch 003 - training loss: 0.6048, validation loss: 1.0561
2024-06-03 03:52:49 [INFO]: Epoch 004 - training loss: 0.5836, validation loss: 1.0601
2024-06-03 03:52:50 [INFO]: Epoch 005 - training loss: 0.5789, validation loss: 1.0580
2024-06-03 03:52:50 [INFO]: Epoch 006 - training loss: 0.5740, validation loss: 1.0470
2024-06-03 03:52:51 [INFO]: Epoch 007 - training loss: 0.5597, validation loss: 1.0550
2024-06-03 03:52:51 [INFO]: Epoch 008 - training loss: 0.5682, validation loss: 1.0578
2024-06-03 03:52:52 [INFO]: Epoch 009 - training loss: 0.5627, validation loss: 1.0430
2024-06-03 03:52:53 [INFO]: Epoch 010 - training loss: 0.5680, validation loss: 1.0547
2024-06-03 03:52:53 [INFO]: Epoch 011 - training loss: 0.5621, validation loss: 1.0474
2024-06-03 03:52:54 [INFO]: Epoch 012 - training loss: 0.5635, validation loss: 1.0506
2024-06-03 03:52:54 [INFO]: Epoch 013 - training loss: 0.5541, validation loss: 1.0468
2024-06-03 03:52:55 [INFO]: Epoch 014 - training loss: 0.5436, validation loss: 1.0441
2024-06-03 03:52:56 [INFO]: Epoch 015 - training loss: 0.5456, validation loss: 1.0526
2024-06-03 03:52:56 [INFO]: Epoch 016 - training loss: 0.5353, validation loss: 1.0449
2024-06-03 03:52:57 [INFO]: Epoch 017 - training loss: 0.5421, validation loss: 1.0459
2024-06-03 03:52:58 [INFO]: Epoch 018 - training loss: 0.5373, validation loss: 1.0454
2024-06-03 03:52:58 [INFO]: Epoch 019 - training loss: 0.5417, validation loss: 1.0394
2024-06-03 03:52:59 [INFO]: Epoch 020 - training loss: 0.5367, validation loss: 1.0417
2024-06-03 03:52:59 [INFO]: Epoch 021 - training loss: 0.5288, validation loss: 1.0447
2024-06-03 03:53:00 [INFO]: Epoch 022 - training loss: 0.5389, validation loss: 1.0360
2024-06-03 03:53:01 [INFO]: Epoch 023 - training loss: 0.5325, validation loss: 1.0513
2024-06-03 03:53:01 [INFO]: Epoch 024 - training loss: 0.5459, validation loss: 1.0367
2024-06-03 03:53:02 [INFO]: Epoch 025 - training loss: 0.5360, validation loss: 1.0419
2024-06-03 03:53:03 [INFO]: Epoch 026 - training loss: 0.5382, validation loss: 1.0376
2024-06-03 03:53:03 [INFO]: Epoch 027 - training loss: 0.5250, validation loss: 1.0407
2024-06-03 03:53:04 [INFO]: Epoch 028 - training loss: 0.5328, validation loss: 1.0359
2024-06-03 03:53:05 [INFO]: Epoch 029 - training loss: 0.5268, validation loss: 1.0386
2024-06-03 03:53:05 [INFO]: Epoch 030 - training loss: 0.5331, validation loss: 1.0331
2024-06-03 03:53:06 [INFO]: Epoch 031 - training loss: 0.5285, validation loss: 1.0383
2024-06-03 03:53:06 [INFO]: Epoch 032 - training loss: 0.5313, validation loss: 1.0373
2024-06-03 03:53:07 [INFO]: Epoch 033 - training loss: 0.5277, validation loss: 1.0280
2024-06-03 03:53:08 [INFO]: Epoch 034 - training loss: 0.5245, validation loss: 1.0291
2024-06-03 03:53:08 [INFO]: Epoch 035 - training loss: 0.5241, validation loss: 1.0252
2024-06-03 03:53:09 [INFO]: Epoch 036 - training loss: 0.5306, validation loss: 1.0296
2024-06-03 03:53:09 [INFO]: Epoch 037 - training loss: 0.5093, validation loss: 1.0223
2024-06-03 03:53:10 [INFO]: Epoch 038 - training loss: 0.5158, validation loss: 1.0206
2024-06-03 03:53:11 [INFO]: Epoch 039 - training loss: 0.5276, validation loss: 1.0221
2024-06-03 03:53:11 [INFO]: Epoch 040 - training loss: 0.5194, validation loss: 1.0260
2024-06-03 03:53:12 [INFO]: Epoch 041 - training loss: 0.5231, validation loss: 1.0210
2024-06-03 03:53:12 [INFO]: Epoch 042 - training loss: 0.5272, validation loss: 1.0193
2024-06-03 03:53:13 [INFO]: Epoch 043 - training loss: 0.5140, validation loss: 1.0227
2024-06-03 03:53:14 [INFO]: Epoch 044 - training loss: 0.5107, validation loss: 1.0192
2024-06-03 03:53:14 [INFO]: Epoch 045 - training loss: 0.5196, validation loss: 1.0161
2024-06-03 03:53:15 [INFO]: Epoch 046 - training loss: 0.5291, validation loss: 1.0164
2024-06-03 03:53:15 [INFO]: Epoch 047 - training loss: 0.5159, validation loss: 1.0163
2024-06-03 03:53:16 [INFO]: Epoch 048 - training loss: 0.5166, validation loss: 1.0164
2024-06-03 03:53:17 [INFO]: Epoch 049 - training loss: 0.5101, validation loss: 1.0209
2024-06-03 03:53:17 [INFO]: Epoch 050 - training loss: 0.5212, validation loss: 1.0118
2024-06-03 03:53:18 [INFO]: Epoch 051 - training loss: 0.5098, validation loss: 1.0231
2024-06-03 03:53:19 [INFO]: Epoch 052 - training loss: 0.5030, validation loss: 1.0193
2024-06-03 03:53:19 [INFO]: Epoch 053 - training loss: 0.5105, validation loss: 1.0057
2024-06-03 03:53:20 [INFO]: Epoch 054 - training loss: 0.5070, validation loss: 1.0137
2024-06-03 03:53:21 [INFO]: Epoch 055 - training loss: 0.5060, validation loss: 1.0044
2024-06-03 03:53:21 [INFO]: Epoch 056 - training loss: 0.5052, validation loss: 1.0117
2024-06-03 03:53:22 [INFO]: Epoch 057 - training loss: 0.5146, validation loss: 0.9986
2024-06-03 03:53:22 [INFO]: Epoch 058 - training loss: 0.5092, validation loss: 1.0030
2024-06-03 03:53:23 [INFO]: Epoch 059 - training loss: 0.5044, validation loss: 1.0138
2024-06-03 03:53:24 [INFO]: Epoch 060 - training loss: 0.4972, validation loss: 1.0132
2024-06-03 03:53:24 [INFO]: Epoch 061 - training loss: 0.5069, validation loss: 1.0168
2024-06-03 03:53:25 [INFO]: Epoch 062 - training loss: 0.4990, validation loss: 0.9947
2024-06-03 03:53:25 [INFO]: Epoch 063 - training loss: 0.5033, validation loss: 1.0015
2024-06-03 03:53:26 [INFO]: Epoch 064 - training loss: 0.5047, validation loss: 1.0076
2024-06-03 03:53:26 [INFO]: Epoch 065 - training loss: 0.4839, validation loss: 0.9983
2024-06-03 03:53:27 [INFO]: Epoch 066 - training loss: 0.4899, validation loss: 1.0055
2024-06-03 03:53:27 [INFO]: Epoch 067 - training loss: 0.4905, validation loss: 0.9985
2024-06-03 03:53:28 [INFO]: Epoch 068 - training loss: 0.4843, validation loss: 1.0153
2024-06-03 03:53:28 [INFO]: Epoch 069 - training loss: 0.4855, validation loss: 1.0043
2024-06-03 03:53:29 [INFO]: Epoch 070 - training loss: 0.4964, validation loss: 1.0003
2024-06-03 03:53:29 [INFO]: Epoch 071 - training loss: 0.4882, validation loss: 1.0109
2024-06-03 03:53:29 [INFO]: Epoch 072 - training loss: 0.4945, validation loss: 1.0096
2024-06-03 03:53:29 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:53:29 [INFO]: Finished training. The best model is from epoch#62.
2024-06-03 03:53:30 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_4/20240603_T035246/MICN.pypots
2024-06-03 03:53:30 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MICN_ETT_h1/round_4/imputation.pkl
2024-06-03 03:53:30 [INFO]: Round4 - MICN on ETT_h1: MAE=0.8451, MSE=1.3317, MRE=0.9519
2024-06-03 03:53:30 [INFO]: Done! Final results:
Averaged MICN (3,153,163 params) on ETT_h1: MAE=0.8513 ± 0.00841483522894422, MSE=1.3419 ± 0.021273376323073738, MRE=0.9589 ± 0.009478117589423301, average inference time=0.11
