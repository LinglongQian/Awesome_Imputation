2024-06-03 03:45:33 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 03:45:33 [INFO]: Using the given device: cuda:0
2024-06-03 03:45:34 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_0/20240603_T034534
2024-06-03 03:45:34 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_0/20240603_T034534/tensorboard
2024-06-03 03:45:35 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 2,259
2024-06-03 03:45:41 [INFO]: Epoch 001 - training loss: 1.8126, validation loss: 1.4782
2024-06-03 03:45:42 [INFO]: Epoch 002 - training loss: 1.5881, validation loss: 1.4487
2024-06-03 03:45:42 [INFO]: Epoch 003 - training loss: 1.3803, validation loss: 1.4173
2024-06-03 03:45:43 [INFO]: Epoch 004 - training loss: 1.1623, validation loss: 1.3872
2024-06-03 03:45:44 [INFO]: Epoch 005 - training loss: 1.0741, validation loss: 1.3562
2024-06-03 03:45:44 [INFO]: Epoch 006 - training loss: 0.9938, validation loss: 1.3282
2024-06-03 03:45:45 [INFO]: Epoch 007 - training loss: 0.9357, validation loss: 1.3046
2024-06-03 03:45:45 [INFO]: Epoch 008 - training loss: 0.9028, validation loss: 1.2823
2024-06-03 03:45:46 [INFO]: Epoch 009 - training loss: 0.8692, validation loss: 1.2650
2024-06-03 03:45:46 [INFO]: Epoch 010 - training loss: 0.8542, validation loss: 1.2506
2024-06-03 03:45:46 [INFO]: Epoch 011 - training loss: 0.8310, validation loss: 1.2363
2024-06-03 03:45:47 [INFO]: Epoch 012 - training loss: 0.8146, validation loss: 1.2254
2024-06-03 03:45:48 [INFO]: Epoch 013 - training loss: 0.7980, validation loss: 1.2179
2024-06-03 03:45:48 [INFO]: Epoch 014 - training loss: 0.7875, validation loss: 1.2096
2024-06-03 03:45:49 [INFO]: Epoch 015 - training loss: 0.7790, validation loss: 1.2032
2024-06-03 03:45:49 [INFO]: Epoch 016 - training loss: 0.7684, validation loss: 1.1978
2024-06-03 03:45:50 [INFO]: Epoch 017 - training loss: 0.7619, validation loss: 1.1919
2024-06-03 03:45:50 [INFO]: Epoch 018 - training loss: 0.7510, validation loss: 1.1907
2024-06-03 03:45:51 [INFO]: Epoch 019 - training loss: 0.7502, validation loss: 1.1888
2024-06-03 03:45:52 [INFO]: Epoch 020 - training loss: 0.7400, validation loss: 1.1882
2024-06-03 03:45:52 [INFO]: Epoch 021 - training loss: 0.7315, validation loss: 1.1832
2024-06-03 03:45:53 [INFO]: Epoch 022 - training loss: 0.7279, validation loss: 1.1813
2024-06-03 03:45:53 [INFO]: Epoch 023 - training loss: 0.7198, validation loss: 1.1766
2024-06-03 03:45:54 [INFO]: Epoch 024 - training loss: 0.7146, validation loss: 1.1742
2024-06-03 03:45:54 [INFO]: Epoch 025 - training loss: 0.7059, validation loss: 1.1760
2024-06-03 03:45:55 [INFO]: Epoch 026 - training loss: 0.7053, validation loss: 1.1735
2024-06-03 03:45:55 [INFO]: Epoch 027 - training loss: 0.6982, validation loss: 1.1712
2024-06-03 03:45:56 [INFO]: Epoch 028 - training loss: 0.6963, validation loss: 1.1702
2024-06-03 03:45:56 [INFO]: Epoch 029 - training loss: 0.6941, validation loss: 1.1696
2024-06-03 03:45:57 [INFO]: Epoch 030 - training loss: 0.6932, validation loss: 1.1673
2024-06-03 03:45:58 [INFO]: Epoch 031 - training loss: 0.6881, validation loss: 1.1693
2024-06-03 03:45:58 [INFO]: Epoch 032 - training loss: 0.6837, validation loss: 1.1696
2024-06-03 03:45:59 [INFO]: Epoch 033 - training loss: 0.6832, validation loss: 1.1685
2024-06-03 03:45:59 [INFO]: Epoch 034 - training loss: 0.6805, validation loss: 1.1666
2024-06-03 03:46:00 [INFO]: Epoch 035 - training loss: 0.6822, validation loss: 1.1716
2024-06-03 03:46:00 [INFO]: Epoch 036 - training loss: 0.6761, validation loss: 1.1683
2024-06-03 03:46:01 [INFO]: Epoch 037 - training loss: 0.6834, validation loss: 1.1673
2024-06-03 03:46:01 [INFO]: Epoch 038 - training loss: 0.6708, validation loss: 1.1635
2024-06-03 03:46:02 [INFO]: Epoch 039 - training loss: 0.6699, validation loss: 1.1686
2024-06-03 03:46:03 [INFO]: Epoch 040 - training loss: 0.6773, validation loss: 1.1727
2024-06-03 03:46:03 [INFO]: Epoch 041 - training loss: 0.6767, validation loss: 1.1725
2024-06-03 03:46:04 [INFO]: Epoch 042 - training loss: 0.6716, validation loss: 1.1691
2024-06-03 03:46:04 [INFO]: Epoch 043 - training loss: 0.6655, validation loss: 1.1695
2024-06-03 03:46:05 [INFO]: Epoch 044 - training loss: 0.6630, validation loss: 1.1721
2024-06-03 03:46:05 [INFO]: Epoch 045 - training loss: 0.6618, validation loss: 1.1692
2024-06-03 03:46:06 [INFO]: Epoch 046 - training loss: 0.6700, validation loss: 1.1650
2024-06-03 03:46:06 [INFO]: Epoch 047 - training loss: 0.6673, validation loss: 1.1693
2024-06-03 03:46:07 [INFO]: Epoch 048 - training loss: 0.6653, validation loss: 1.1714
2024-06-03 03:46:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:46:07 [INFO]: Finished training. The best model is from epoch#38.
2024-06-03 03:46:07 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_0/20240603_T034534/MRNN.pypots
2024-06-03 03:46:10 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_0/imputation.pkl
2024-06-03 03:46:10 [INFO]: Round0 - MRNN on ETT_h1: MAE=0.8858, MSE=1.4958, MRE=0.9977
2024-06-03 03:46:10 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 03:46:10 [INFO]: Using the given device: cuda:0
2024-06-03 03:46:10 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_1/20240603_T034610
2024-06-03 03:46:10 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_1/20240603_T034610/tensorboard
2024-06-03 03:46:10 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 2,259
2024-06-03 03:46:13 [INFO]: Epoch 001 - training loss: 1.7700, validation loss: 1.5313
2024-06-03 03:46:14 [INFO]: Epoch 002 - training loss: 1.5540, validation loss: 1.4968
2024-06-03 03:46:15 [INFO]: Epoch 003 - training loss: 1.3485, validation loss: 1.4626
2024-06-03 03:46:15 [INFO]: Epoch 004 - training loss: 1.1404, validation loss: 1.4287
2024-06-03 03:46:16 [INFO]: Epoch 005 - training loss: 1.0538, validation loss: 1.3957
2024-06-03 03:46:16 [INFO]: Epoch 006 - training loss: 0.9877, validation loss: 1.3639
2024-06-03 03:46:17 [INFO]: Epoch 007 - training loss: 0.9449, validation loss: 1.3358
2024-06-03 03:46:17 [INFO]: Epoch 008 - training loss: 0.9096, validation loss: 1.3123
2024-06-03 03:46:18 [INFO]: Epoch 009 - training loss: 0.8808, validation loss: 1.2894
2024-06-03 03:46:18 [INFO]: Epoch 010 - training loss: 0.8547, validation loss: 1.2719
2024-06-03 03:46:19 [INFO]: Epoch 011 - training loss: 0.8310, validation loss: 1.2577
2024-06-03 03:46:19 [INFO]: Epoch 012 - training loss: 0.8123, validation loss: 1.2443
2024-06-03 03:46:20 [INFO]: Epoch 013 - training loss: 0.7988, validation loss: 1.2344
2024-06-03 03:46:20 [INFO]: Epoch 014 - training loss: 0.7888, validation loss: 1.2271
2024-06-03 03:46:21 [INFO]: Epoch 015 - training loss: 0.7801, validation loss: 1.2180
2024-06-03 03:46:21 [INFO]: Epoch 016 - training loss: 0.7677, validation loss: 1.2096
2024-06-03 03:46:22 [INFO]: Epoch 017 - training loss: 0.7543, validation loss: 1.2055
2024-06-03 03:46:22 [INFO]: Epoch 018 - training loss: 0.7499, validation loss: 1.1998
2024-06-03 03:46:23 [INFO]: Epoch 019 - training loss: 0.7437, validation loss: 1.2010
2024-06-03 03:46:23 [INFO]: Epoch 020 - training loss: 0.7344, validation loss: 1.1967
2024-06-03 03:46:23 [INFO]: Epoch 021 - training loss: 0.7269, validation loss: 1.1922
2024-06-03 03:46:24 [INFO]: Epoch 022 - training loss: 0.7178, validation loss: 1.1902
2024-06-03 03:46:24 [INFO]: Epoch 023 - training loss: 0.7152, validation loss: 1.1910
2024-06-03 03:46:25 [INFO]: Epoch 024 - training loss: 0.7153, validation loss: 1.1888
2024-06-03 03:46:25 [INFO]: Epoch 025 - training loss: 0.7055, validation loss: 1.1847
2024-06-03 03:46:26 [INFO]: Epoch 026 - training loss: 0.7001, validation loss: 1.1839
2024-06-03 03:46:26 [INFO]: Epoch 027 - training loss: 0.6950, validation loss: 1.1802
2024-06-03 03:46:26 [INFO]: Epoch 028 - training loss: 0.6962, validation loss: 1.1814
2024-06-03 03:46:27 [INFO]: Epoch 029 - training loss: 0.6978, validation loss: 1.1863
2024-06-03 03:46:27 [INFO]: Epoch 030 - training loss: 0.6945, validation loss: 1.1773
2024-06-03 03:46:28 [INFO]: Epoch 031 - training loss: 0.6873, validation loss: 1.1806
2024-06-03 03:46:28 [INFO]: Epoch 032 - training loss: 0.6764, validation loss: 1.1772
2024-06-03 03:46:28 [INFO]: Epoch 033 - training loss: 0.6774, validation loss: 1.1765
2024-06-03 03:46:29 [INFO]: Epoch 034 - training loss: 0.6772, validation loss: 1.1743
2024-06-03 03:46:29 [INFO]: Epoch 035 - training loss: 0.6700, validation loss: 1.1760
2024-06-03 03:46:30 [INFO]: Epoch 036 - training loss: 0.6688, validation loss: 1.1758
2024-06-03 03:46:30 [INFO]: Epoch 037 - training loss: 0.6701, validation loss: 1.1750
2024-06-03 03:46:30 [INFO]: Epoch 038 - training loss: 0.6686, validation loss: 1.1739
2024-06-03 03:46:31 [INFO]: Epoch 039 - training loss: 0.6715, validation loss: 1.1706
2024-06-03 03:46:31 [INFO]: Epoch 040 - training loss: 0.6664, validation loss: 1.1735
2024-06-03 03:46:32 [INFO]: Epoch 041 - training loss: 0.6626, validation loss: 1.1709
2024-06-03 03:46:32 [INFO]: Epoch 042 - training loss: 0.6594, validation loss: 1.1673
2024-06-03 03:46:32 [INFO]: Epoch 043 - training loss: 0.6672, validation loss: 1.1700
2024-06-03 03:46:33 [INFO]: Epoch 044 - training loss: 0.6596, validation loss: 1.1692
2024-06-03 03:46:33 [INFO]: Epoch 045 - training loss: 0.6643, validation loss: 1.1691
2024-06-03 03:46:33 [INFO]: Epoch 046 - training loss: 0.6602, validation loss: 1.1662
2024-06-03 03:46:34 [INFO]: Epoch 047 - training loss: 0.6652, validation loss: 1.1678
2024-06-03 03:46:34 [INFO]: Epoch 048 - training loss: 0.6542, validation loss: 1.1660
2024-06-03 03:46:35 [INFO]: Epoch 049 - training loss: 0.6516, validation loss: 1.1629
2024-06-03 03:46:35 [INFO]: Epoch 050 - training loss: 0.6508, validation loss: 1.1682
2024-06-03 03:46:36 [INFO]: Epoch 051 - training loss: 0.6547, validation loss: 1.1609
2024-06-03 03:46:36 [INFO]: Epoch 052 - training loss: 0.6536, validation loss: 1.1630
2024-06-03 03:46:36 [INFO]: Epoch 053 - training loss: 0.6504, validation loss: 1.1683
2024-06-03 03:46:37 [INFO]: Epoch 054 - training loss: 0.6508, validation loss: 1.1620
2024-06-03 03:46:37 [INFO]: Epoch 055 - training loss: 0.6497, validation loss: 1.1639
2024-06-03 03:46:38 [INFO]: Epoch 056 - training loss: 0.6502, validation loss: 1.1603
2024-06-03 03:46:38 [INFO]: Epoch 057 - training loss: 0.6434, validation loss: 1.1615
2024-06-03 03:46:38 [INFO]: Epoch 058 - training loss: 0.6476, validation loss: 1.1626
2024-06-03 03:46:39 [INFO]: Epoch 059 - training loss: 0.6471, validation loss: 1.1605
2024-06-03 03:46:39 [INFO]: Epoch 060 - training loss: 0.6440, validation loss: 1.1564
2024-06-03 03:46:40 [INFO]: Epoch 061 - training loss: 0.6489, validation loss: 1.1577
2024-06-03 03:46:40 [INFO]: Epoch 062 - training loss: 0.6444, validation loss: 1.1605
2024-06-03 03:46:40 [INFO]: Epoch 063 - training loss: 0.6443, validation loss: 1.1654
2024-06-03 03:46:41 [INFO]: Epoch 064 - training loss: 0.6552, validation loss: 1.1538
2024-06-03 03:46:41 [INFO]: Epoch 065 - training loss: 0.6474, validation loss: 1.1536
2024-06-03 03:46:41 [INFO]: Epoch 066 - training loss: 0.6472, validation loss: 1.1608
2024-06-03 03:46:42 [INFO]: Epoch 067 - training loss: 0.6462, validation loss: 1.1582
2024-06-03 03:46:42 [INFO]: Epoch 068 - training loss: 0.6361, validation loss: 1.1553
2024-06-03 03:46:43 [INFO]: Epoch 069 - training loss: 0.6373, validation loss: 1.1526
2024-06-03 03:46:43 [INFO]: Epoch 070 - training loss: 0.6450, validation loss: 1.1501
2024-06-03 03:46:43 [INFO]: Epoch 071 - training loss: 0.6389, validation loss: 1.1560
2024-06-03 03:46:44 [INFO]: Epoch 072 - training loss: 0.6397, validation loss: 1.1575
2024-06-03 03:46:44 [INFO]: Epoch 073 - training loss: 0.6373, validation loss: 1.1512
2024-06-03 03:46:45 [INFO]: Epoch 074 - training loss: 0.6354, validation loss: 1.1487
2024-06-03 03:46:45 [INFO]: Epoch 075 - training loss: 0.6371, validation loss: 1.1555
2024-06-03 03:46:46 [INFO]: Epoch 076 - training loss: 0.6352, validation loss: 1.1558
2024-06-03 03:46:46 [INFO]: Epoch 077 - training loss: 0.6348, validation loss: 1.1489
2024-06-03 03:46:46 [INFO]: Epoch 078 - training loss: 0.6306, validation loss: 1.1507
2024-06-03 03:46:47 [INFO]: Epoch 079 - training loss: 0.6356, validation loss: 1.1562
2024-06-03 03:46:47 [INFO]: Epoch 080 - training loss: 0.6285, validation loss: 1.1539
2024-06-03 03:46:48 [INFO]: Epoch 081 - training loss: 0.6310, validation loss: 1.1496
2024-06-03 03:46:48 [INFO]: Epoch 082 - training loss: 0.6273, validation loss: 1.1497
2024-06-03 03:46:48 [INFO]: Epoch 083 - training loss: 0.6273, validation loss: 1.1557
2024-06-03 03:46:49 [INFO]: Epoch 084 - training loss: 0.6305, validation loss: 1.1557
2024-06-03 03:46:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:46:49 [INFO]: Finished training. The best model is from epoch#74.
2024-06-03 03:46:49 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_1/20240603_T034610/MRNN.pypots
2024-06-03 03:46:51 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_1/imputation.pkl
2024-06-03 03:46:51 [INFO]: Round1 - MRNN on ETT_h1: MAE=0.8786, MSE=1.4805, MRE=0.9896
2024-06-03 03:46:51 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 03:46:51 [INFO]: Using the given device: cuda:0
2024-06-03 03:46:51 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_2/20240603_T034651
2024-06-03 03:46:51 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_2/20240603_T034651/tensorboard
2024-06-03 03:46:51 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 2,259
2024-06-03 03:46:53 [INFO]: Epoch 001 - training loss: 1.9657, validation loss: 1.4126
2024-06-03 03:46:54 [INFO]: Epoch 002 - training loss: 1.7625, validation loss: 1.3882
2024-06-03 03:46:54 [INFO]: Epoch 003 - training loss: 1.5809, validation loss: 1.3623
2024-06-03 03:46:55 [INFO]: Epoch 004 - training loss: 1.4073, validation loss: 1.3364
2024-06-03 03:46:55 [INFO]: Epoch 005 - training loss: 1.1966, validation loss: 1.3105
2024-06-03 03:46:55 [INFO]: Epoch 006 - training loss: 1.0774, validation loss: 1.2871
2024-06-03 03:46:56 [INFO]: Epoch 007 - training loss: 1.0060, validation loss: 1.2668
2024-06-03 03:46:56 [INFO]: Epoch 008 - training loss: 0.9471, validation loss: 1.2458
2024-06-03 03:46:56 [INFO]: Epoch 009 - training loss: 0.9045, validation loss: 1.2288
2024-06-03 03:46:57 [INFO]: Epoch 010 - training loss: 0.8753, validation loss: 1.2157
2024-06-03 03:46:57 [INFO]: Epoch 011 - training loss: 0.8427, validation loss: 1.2054
2024-06-03 03:46:57 [INFO]: Epoch 012 - training loss: 0.8215, validation loss: 1.1983
2024-06-03 03:46:58 [INFO]: Epoch 013 - training loss: 0.8018, validation loss: 1.1926
2024-06-03 03:46:58 [INFO]: Epoch 014 - training loss: 0.7956, validation loss: 1.1870
2024-06-03 03:46:58 [INFO]: Epoch 015 - training loss: 0.7822, validation loss: 1.1828
2024-06-03 03:46:59 [INFO]: Epoch 016 - training loss: 0.7689, validation loss: 1.1807
2024-06-03 03:46:59 [INFO]: Epoch 017 - training loss: 0.7592, validation loss: 1.1820
2024-06-03 03:46:59 [INFO]: Epoch 018 - training loss: 0.7520, validation loss: 1.1759
2024-06-03 03:47:00 [INFO]: Epoch 019 - training loss: 0.7470, validation loss: 1.1769
2024-06-03 03:47:00 [INFO]: Epoch 020 - training loss: 0.7293, validation loss: 1.1736
2024-06-03 03:47:00 [INFO]: Epoch 021 - training loss: 0.7224, validation loss: 1.1712
2024-06-03 03:47:01 [INFO]: Epoch 022 - training loss: 0.7173, validation loss: 1.1714
2024-06-03 03:47:01 [INFO]: Epoch 023 - training loss: 0.7180, validation loss: 1.1667
2024-06-03 03:47:01 [INFO]: Epoch 024 - training loss: 0.7101, validation loss: 1.1684
2024-06-03 03:47:02 [INFO]: Epoch 025 - training loss: 0.7025, validation loss: 1.1704
2024-06-03 03:47:02 [INFO]: Epoch 026 - training loss: 0.6973, validation loss: 1.1702
2024-06-03 03:47:02 [INFO]: Epoch 027 - training loss: 0.6915, validation loss: 1.1696
2024-06-03 03:47:03 [INFO]: Epoch 028 - training loss: 0.6904, validation loss: 1.1679
2024-06-03 03:47:03 [INFO]: Epoch 029 - training loss: 0.6879, validation loss: 1.1702
2024-06-03 03:47:03 [INFO]: Epoch 030 - training loss: 0.6860, validation loss: 1.1681
2024-06-03 03:47:04 [INFO]: Epoch 031 - training loss: 0.6863, validation loss: 1.1726
2024-06-03 03:47:04 [INFO]: Epoch 032 - training loss: 0.6807, validation loss: 1.1703
2024-06-03 03:47:04 [INFO]: Epoch 033 - training loss: 0.6798, validation loss: 1.1689
2024-06-03 03:47:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:47:04 [INFO]: Finished training. The best model is from epoch#23.
2024-06-03 03:47:04 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_2/20240603_T034651/MRNN.pypots
2024-06-03 03:47:07 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_2/imputation.pkl
2024-06-03 03:47:07 [INFO]: Round2 - MRNN on ETT_h1: MAE=0.8894, MSE=1.4857, MRE=1.0018
2024-06-03 03:47:07 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 03:47:07 [INFO]: Using the given device: cuda:0
2024-06-03 03:47:07 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_3/20240603_T034707
2024-06-03 03:47:07 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_3/20240603_T034707/tensorboard
2024-06-03 03:47:07 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 2,259
2024-06-03 03:47:08 [INFO]: Epoch 001 - training loss: 1.9190, validation loss: 1.5844
2024-06-03 03:47:09 [INFO]: Epoch 002 - training loss: 1.7238, validation loss: 1.5471
2024-06-03 03:47:09 [INFO]: Epoch 003 - training loss: 1.5767, validation loss: 1.5135
2024-06-03 03:47:09 [INFO]: Epoch 004 - training loss: 1.3958, validation loss: 1.4798
2024-06-03 03:47:09 [INFO]: Epoch 005 - training loss: 1.1913, validation loss: 1.4460
2024-06-03 03:47:10 [INFO]: Epoch 006 - training loss: 1.0885, validation loss: 1.4159
2024-06-03 03:47:10 [INFO]: Epoch 007 - training loss: 1.0243, validation loss: 1.3858
2024-06-03 03:47:10 [INFO]: Epoch 008 - training loss: 0.9759, validation loss: 1.3574
2024-06-03 03:47:11 [INFO]: Epoch 009 - training loss: 0.9256, validation loss: 1.3312
2024-06-03 03:47:11 [INFO]: Epoch 010 - training loss: 0.8947, validation loss: 1.3053
2024-06-03 03:47:11 [INFO]: Epoch 011 - training loss: 0.8672, validation loss: 1.2831
2024-06-03 03:47:11 [INFO]: Epoch 012 - training loss: 0.8425, validation loss: 1.2639
2024-06-03 03:47:12 [INFO]: Epoch 013 - training loss: 0.8222, validation loss: 1.2485
2024-06-03 03:47:12 [INFO]: Epoch 014 - training loss: 0.8065, validation loss: 1.2350
2024-06-03 03:47:12 [INFO]: Epoch 015 - training loss: 0.7994, validation loss: 1.2224
2024-06-03 03:47:13 [INFO]: Epoch 016 - training loss: 0.7876, validation loss: 1.2122
2024-06-03 03:47:13 [INFO]: Epoch 017 - training loss: 0.7663, validation loss: 1.2065
2024-06-03 03:47:13 [INFO]: Epoch 018 - training loss: 0.7524, validation loss: 1.1986
2024-06-03 03:47:14 [INFO]: Epoch 019 - training loss: 0.7462, validation loss: 1.1932
2024-06-03 03:47:14 [INFO]: Epoch 020 - training loss: 0.7365, validation loss: 1.1877
2024-06-03 03:47:14 [INFO]: Epoch 021 - training loss: 0.7280, validation loss: 1.1835
2024-06-03 03:47:14 [INFO]: Epoch 022 - training loss: 0.7193, validation loss: 1.1779
2024-06-03 03:47:15 [INFO]: Epoch 023 - training loss: 0.7088, validation loss: 1.1746
2024-06-03 03:47:15 [INFO]: Epoch 024 - training loss: 0.7099, validation loss: 1.1740
2024-06-03 03:47:15 [INFO]: Epoch 025 - training loss: 0.6987, validation loss: 1.1737
2024-06-03 03:47:16 [INFO]: Epoch 026 - training loss: 0.6972, validation loss: 1.1678
2024-06-03 03:47:16 [INFO]: Epoch 027 - training loss: 0.6875, validation loss: 1.1666
2024-06-03 03:47:16 [INFO]: Epoch 028 - training loss: 0.6870, validation loss: 1.1663
2024-06-03 03:47:16 [INFO]: Epoch 029 - training loss: 0.6897, validation loss: 1.1630
2024-06-03 03:47:17 [INFO]: Epoch 030 - training loss: 0.6855, validation loss: 1.1592
2024-06-03 03:47:17 [INFO]: Epoch 031 - training loss: 0.6805, validation loss: 1.1606
2024-06-03 03:47:17 [INFO]: Epoch 032 - training loss: 0.6806, validation loss: 1.1614
2024-06-03 03:47:17 [INFO]: Epoch 033 - training loss: 0.6802, validation loss: 1.1555
2024-06-03 03:47:18 [INFO]: Epoch 034 - training loss: 0.6817, validation loss: 1.1527
2024-06-03 03:47:18 [INFO]: Epoch 035 - training loss: 0.6808, validation loss: 1.1523
2024-06-03 03:47:18 [INFO]: Epoch 036 - training loss: 0.6828, validation loss: 1.1524
2024-06-03 03:47:19 [INFO]: Epoch 037 - training loss: 0.6864, validation loss: 1.1547
2024-06-03 03:47:19 [INFO]: Epoch 038 - training loss: 0.6812, validation loss: 1.1520
2024-06-03 03:47:19 [INFO]: Epoch 039 - training loss: 0.6745, validation loss: 1.1560
2024-06-03 03:47:20 [INFO]: Epoch 040 - training loss: 0.6785, validation loss: 1.1481
2024-06-03 03:47:20 [INFO]: Epoch 041 - training loss: 0.6753, validation loss: 1.1546
2024-06-03 03:47:20 [INFO]: Epoch 042 - training loss: 0.6689, validation loss: 1.1490
2024-06-03 03:47:20 [INFO]: Epoch 043 - training loss: 0.6636, validation loss: 1.1529
2024-06-03 03:47:21 [INFO]: Epoch 044 - training loss: 0.6656, validation loss: 1.1515
2024-06-03 03:47:21 [INFO]: Epoch 045 - training loss: 0.6583, validation loss: 1.1509
2024-06-03 03:47:21 [INFO]: Epoch 046 - training loss: 0.6621, validation loss: 1.1498
2024-06-03 03:47:22 [INFO]: Epoch 047 - training loss: 0.6580, validation loss: 1.1468
2024-06-03 03:47:22 [INFO]: Epoch 048 - training loss: 0.6657, validation loss: 1.1512
2024-06-03 03:47:22 [INFO]: Epoch 049 - training loss: 0.6559, validation loss: 1.1478
2024-06-03 03:47:23 [INFO]: Epoch 050 - training loss: 0.6553, validation loss: 1.1441
2024-06-03 03:47:23 [INFO]: Epoch 051 - training loss: 0.6566, validation loss: 1.1428
2024-06-03 03:47:23 [INFO]: Epoch 052 - training loss: 0.6547, validation loss: 1.1416
2024-06-03 03:47:23 [INFO]: Epoch 053 - training loss: 0.6523, validation loss: 1.1457
2024-06-03 03:47:24 [INFO]: Epoch 054 - training loss: 0.6541, validation loss: 1.1409
2024-06-03 03:47:24 [INFO]: Epoch 055 - training loss: 0.6513, validation loss: 1.1399
2024-06-03 03:47:24 [INFO]: Epoch 056 - training loss: 0.6517, validation loss: 1.1427
2024-06-03 03:47:24 [INFO]: Epoch 057 - training loss: 0.6548, validation loss: 1.1458
2024-06-03 03:47:25 [INFO]: Epoch 058 - training loss: 0.6476, validation loss: 1.1400
2024-06-03 03:47:25 [INFO]: Epoch 059 - training loss: 0.6483, validation loss: 1.1381
2024-06-03 03:47:25 [INFO]: Epoch 060 - training loss: 0.6473, validation loss: 1.1398
2024-06-03 03:47:25 [INFO]: Epoch 061 - training loss: 0.6442, validation loss: 1.1398
2024-06-03 03:47:26 [INFO]: Epoch 062 - training loss: 0.6453, validation loss: 1.1378
2024-06-03 03:47:26 [INFO]: Epoch 063 - training loss: 0.6411, validation loss: 1.1373
2024-06-03 03:47:26 [INFO]: Epoch 064 - training loss: 0.6406, validation loss: 1.1389
2024-06-03 03:47:26 [INFO]: Epoch 065 - training loss: 0.6438, validation loss: 1.1393
2024-06-03 03:47:27 [INFO]: Epoch 066 - training loss: 0.6425, validation loss: 1.1386
2024-06-03 03:47:27 [INFO]: Epoch 067 - training loss: 0.6382, validation loss: 1.1397
2024-06-03 03:47:27 [INFO]: Epoch 068 - training loss: 0.6382, validation loss: 1.1396
2024-06-03 03:47:28 [INFO]: Epoch 069 - training loss: 0.6384, validation loss: 1.1379
2024-06-03 03:47:28 [INFO]: Epoch 070 - training loss: 0.6404, validation loss: 1.1422
2024-06-03 03:47:28 [INFO]: Epoch 071 - training loss: 0.6424, validation loss: 1.1329
2024-06-03 03:47:28 [INFO]: Epoch 072 - training loss: 0.6386, validation loss: 1.1387
2024-06-03 03:47:29 [INFO]: Epoch 073 - training loss: 0.6399, validation loss: 1.1407
2024-06-03 03:47:29 [INFO]: Epoch 074 - training loss: 0.6388, validation loss: 1.1379
2024-06-03 03:47:29 [INFO]: Epoch 075 - training loss: 0.6290, validation loss: 1.1407
2024-06-03 03:47:29 [INFO]: Epoch 076 - training loss: 0.6301, validation loss: 1.1367
2024-06-03 03:47:30 [INFO]: Epoch 077 - training loss: 0.6255, validation loss: 1.1332
2024-06-03 03:47:30 [INFO]: Epoch 078 - training loss: 0.6242, validation loss: 1.1370
2024-06-03 03:47:30 [INFO]: Epoch 079 - training loss: 0.6308, validation loss: 1.1358
2024-06-03 03:47:31 [INFO]: Epoch 080 - training loss: 0.6261, validation loss: 1.1335
2024-06-03 03:47:31 [INFO]: Epoch 081 - training loss: 0.6279, validation loss: 1.1372
2024-06-03 03:47:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:47:31 [INFO]: Finished training. The best model is from epoch#71.
2024-06-03 03:47:31 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_3/20240603_T034707/MRNN.pypots
2024-06-03 03:47:33 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_3/imputation.pkl
2024-06-03 03:47:33 [INFO]: Round3 - MRNN on ETT_h1: MAE=0.8723, MSE=1.4552, MRE=0.9826
2024-06-03 03:47:33 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 03:47:33 [INFO]: Using the given device: cuda:0
2024-06-03 03:47:33 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_4/20240603_T034733
2024-06-03 03:47:33 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_4/20240603_T034733/tensorboard
2024-06-03 03:47:33 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 2,259
2024-06-03 03:47:34 [INFO]: Epoch 001 - training loss: 1.7906, validation loss: 1.4500
2024-06-03 03:47:35 [INFO]: Epoch 002 - training loss: 1.6219, validation loss: 1.4155
2024-06-03 03:47:35 [INFO]: Epoch 003 - training loss: 1.4259, validation loss: 1.3824
2024-06-03 03:47:35 [INFO]: Epoch 004 - training loss: 1.1923, validation loss: 1.3506
2024-06-03 03:47:35 [INFO]: Epoch 005 - training loss: 1.0568, validation loss: 1.3205
2024-06-03 03:47:35 [INFO]: Epoch 006 - training loss: 0.9726, validation loss: 1.2968
2024-06-03 03:47:36 [INFO]: Epoch 007 - training loss: 0.9230, validation loss: 1.2744
2024-06-03 03:47:36 [INFO]: Epoch 008 - training loss: 0.8895, validation loss: 1.2554
2024-06-03 03:47:36 [INFO]: Epoch 009 - training loss: 0.8715, validation loss: 1.2398
2024-06-03 03:47:36 [INFO]: Epoch 010 - training loss: 0.8409, validation loss: 1.2284
2024-06-03 03:47:36 [INFO]: Epoch 011 - training loss: 0.8183, validation loss: 1.2166
2024-06-03 03:47:37 [INFO]: Epoch 012 - training loss: 0.7978, validation loss: 1.2063
2024-06-03 03:47:37 [INFO]: Epoch 013 - training loss: 0.7850, validation loss: 1.2004
2024-06-03 03:47:37 [INFO]: Epoch 014 - training loss: 0.7721, validation loss: 1.1941
2024-06-03 03:47:37 [INFO]: Epoch 015 - training loss: 0.7572, validation loss: 1.1888
2024-06-03 03:47:37 [INFO]: Epoch 016 - training loss: 0.7484, validation loss: 1.1862
2024-06-03 03:47:38 [INFO]: Epoch 017 - training loss: 0.7353, validation loss: 1.1851
2024-06-03 03:47:38 [INFO]: Epoch 018 - training loss: 0.7256, validation loss: 1.1781
2024-06-03 03:47:38 [INFO]: Epoch 019 - training loss: 0.7191, validation loss: 1.1802
2024-06-03 03:47:38 [INFO]: Epoch 020 - training loss: 0.7118, validation loss: 1.1783
2024-06-03 03:47:38 [INFO]: Epoch 021 - training loss: 0.7135, validation loss: 1.1728
2024-06-03 03:47:39 [INFO]: Epoch 022 - training loss: 0.7086, validation loss: 1.1763
2024-06-03 03:47:39 [INFO]: Epoch 023 - training loss: 0.7003, validation loss: 1.1781
2024-06-03 03:47:39 [INFO]: Epoch 024 - training loss: 0.7097, validation loss: 1.1639
2024-06-03 03:47:39 [INFO]: Epoch 025 - training loss: 0.7085, validation loss: 1.1745
2024-06-03 03:47:39 [INFO]: Epoch 026 - training loss: 0.7011, validation loss: 1.1618
2024-06-03 03:47:39 [INFO]: Epoch 027 - training loss: 0.6988, validation loss: 1.1692
2024-06-03 03:47:39 [INFO]: Epoch 028 - training loss: 0.6933, validation loss: 1.1655
2024-06-03 03:47:40 [INFO]: Epoch 029 - training loss: 0.6929, validation loss: 1.1630
2024-06-03 03:47:40 [INFO]: Epoch 030 - training loss: 0.6799, validation loss: 1.1629
2024-06-03 03:47:40 [INFO]: Epoch 031 - training loss: 0.6785, validation loss: 1.1648
2024-06-03 03:47:40 [INFO]: Epoch 032 - training loss: 0.6854, validation loss: 1.1644
2024-06-03 03:47:40 [INFO]: Epoch 033 - training loss: 0.6784, validation loss: 1.1628
2024-06-03 03:47:41 [INFO]: Epoch 034 - training loss: 0.6769, validation loss: 1.1623
2024-06-03 03:47:41 [INFO]: Epoch 035 - training loss: 0.6749, validation loss: 1.1616
2024-06-03 03:47:41 [INFO]: Epoch 036 - training loss: 0.6672, validation loss: 1.1600
2024-06-03 03:47:41 [INFO]: Epoch 037 - training loss: 0.6699, validation loss: 1.1611
2024-06-03 03:47:41 [INFO]: Epoch 038 - training loss: 0.6717, validation loss: 1.1568
2024-06-03 03:47:41 [INFO]: Epoch 039 - training loss: 0.6685, validation loss: 1.1534
2024-06-03 03:47:41 [INFO]: Epoch 040 - training loss: 0.6753, validation loss: 1.1639
2024-06-03 03:47:42 [INFO]: Epoch 041 - training loss: 0.6730, validation loss: 1.1600
2024-06-03 03:47:42 [INFO]: Epoch 042 - training loss: 0.6620, validation loss: 1.1581
2024-06-03 03:47:42 [INFO]: Epoch 043 - training loss: 0.6591, validation loss: 1.1618
2024-06-03 03:47:42 [INFO]: Epoch 044 - training loss: 0.6720, validation loss: 1.1518
2024-06-03 03:47:42 [INFO]: Epoch 045 - training loss: 0.6706, validation loss: 1.1492
2024-06-03 03:47:42 [INFO]: Epoch 046 - training loss: 0.6649, validation loss: 1.1543
2024-06-03 03:47:43 [INFO]: Epoch 047 - training loss: 0.6617, validation loss: 1.1541
2024-06-03 03:47:43 [INFO]: Epoch 048 - training loss: 0.6584, validation loss: 1.1556
2024-06-03 03:47:43 [INFO]: Epoch 049 - training loss: 0.6636, validation loss: 1.1468
2024-06-03 03:47:43 [INFO]: Epoch 050 - training loss: 0.6593, validation loss: 1.1478
2024-06-03 03:47:43 [INFO]: Epoch 051 - training loss: 0.6533, validation loss: 1.1461
2024-06-03 03:47:43 [INFO]: Epoch 052 - training loss: 0.6556, validation loss: 1.1474
2024-06-03 03:47:43 [INFO]: Epoch 053 - training loss: 0.6583, validation loss: 1.1465
2024-06-03 03:47:43 [INFO]: Epoch 054 - training loss: 0.6484, validation loss: 1.1495
2024-06-03 03:47:44 [INFO]: Epoch 055 - training loss: 0.6568, validation loss: 1.1501
2024-06-03 03:47:44 [INFO]: Epoch 056 - training loss: 0.6440, validation loss: 1.1485
2024-06-03 03:47:44 [INFO]: Epoch 057 - training loss: 0.6487, validation loss: 1.1471
2024-06-03 03:47:44 [INFO]: Epoch 058 - training loss: 0.6487, validation loss: 1.1455
2024-06-03 03:47:44 [INFO]: Epoch 059 - training loss: 0.6423, validation loss: 1.1451
2024-06-03 03:47:44 [INFO]: Epoch 060 - training loss: 0.6454, validation loss: 1.1458
2024-06-03 03:47:44 [INFO]: Epoch 061 - training loss: 0.6421, validation loss: 1.1440
2024-06-03 03:47:45 [INFO]: Epoch 062 - training loss: 0.6426, validation loss: 1.1451
2024-06-03 03:47:45 [INFO]: Epoch 063 - training loss: 0.6408, validation loss: 1.1395
2024-06-03 03:47:45 [INFO]: Epoch 064 - training loss: 0.6496, validation loss: 1.1491
2024-06-03 03:47:45 [INFO]: Epoch 065 - training loss: 0.6420, validation loss: 1.1466
2024-06-03 03:47:45 [INFO]: Epoch 066 - training loss: 0.6418, validation loss: 1.1411
2024-06-03 03:47:45 [INFO]: Epoch 067 - training loss: 0.6483, validation loss: 1.1483
2024-06-03 03:47:45 [INFO]: Epoch 068 - training loss: 0.6504, validation loss: 1.1377
2024-06-03 03:47:45 [INFO]: Epoch 069 - training loss: 0.6446, validation loss: 1.1361
2024-06-03 03:47:46 [INFO]: Epoch 070 - training loss: 0.6475, validation loss: 1.1434
2024-06-03 03:47:46 [INFO]: Epoch 071 - training loss: 0.6366, validation loss: 1.1418
2024-06-03 03:47:46 [INFO]: Epoch 072 - training loss: 0.6382, validation loss: 1.1442
2024-06-03 03:47:46 [INFO]: Epoch 073 - training loss: 0.6306, validation loss: 1.1396
2024-06-03 03:47:46 [INFO]: Epoch 074 - training loss: 0.6298, validation loss: 1.1430
2024-06-03 03:47:46 [INFO]: Epoch 075 - training loss: 0.6335, validation loss: 1.1389
2024-06-03 03:47:46 [INFO]: Epoch 076 - training loss: 0.6304, validation loss: 1.1368
2024-06-03 03:47:47 [INFO]: Epoch 077 - training loss: 0.6365, validation loss: 1.1458
2024-06-03 03:47:47 [INFO]: Epoch 078 - training loss: 0.6341, validation loss: 1.1450
2024-06-03 03:47:47 [INFO]: Epoch 079 - training loss: 0.6323, validation loss: 1.1376
2024-06-03 03:47:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:47:47 [INFO]: Finished training. The best model is from epoch#69.
2024-06-03 03:47:47 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_4/20240603_T034733/MRNN.pypots
2024-06-03 03:47:48 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/MRNN_ETT_h1/round_4/imputation.pkl
2024-06-03 03:47:48 [INFO]: Round4 - MRNN on ETT_h1: MAE=0.8689, MSE=1.4438, MRE=0.9787
2024-06-03 03:47:48 [INFO]: Done! Final results:
Averaged MRNN (2,259 params) on ETT_h1: MAE=0.8790 ± 0.007737862069702671, MSE=1.4722 ± 0.01950607486989119, MRE=0.9901 ± 0.008715603406601918, average inference time=0.45
