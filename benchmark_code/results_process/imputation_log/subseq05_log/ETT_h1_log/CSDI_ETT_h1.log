2024-06-03 03:32:36 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 03:32:36 [INFO]: Using the given device: cuda:0
2024-06-03 03:32:37 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_0/20240603_T033237
2024-06-03 03:32:37 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_0/20240603_T033237/tensorboard
2024-06-03 03:32:37 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 03:32:50 [INFO]: Epoch 001 - training loss: 0.7648, validation loss: 0.4867
2024-06-03 03:32:56 [INFO]: Epoch 002 - training loss: 0.4455, validation loss: 0.4209
2024-06-03 03:33:02 [INFO]: Epoch 003 - training loss: 0.4503, validation loss: 0.4047
2024-06-03 03:33:08 [INFO]: Epoch 004 - training loss: 0.4149, validation loss: 0.4170
2024-06-03 03:33:14 [INFO]: Epoch 005 - training loss: 0.3544, validation loss: 0.3909
2024-06-03 03:33:20 [INFO]: Epoch 006 - training loss: 0.3605, validation loss: 0.3684
2024-06-03 03:33:25 [INFO]: Epoch 007 - training loss: 0.3115, validation loss: 0.3747
2024-06-03 03:33:31 [INFO]: Epoch 008 - training loss: 0.3251, validation loss: 0.3547
2024-06-03 03:33:36 [INFO]: Epoch 009 - training loss: 0.3161, validation loss: 0.3415
2024-06-03 03:33:40 [INFO]: Epoch 010 - training loss: 0.3624, validation loss: 0.3324
2024-06-03 03:33:44 [INFO]: Epoch 011 - training loss: 0.3416, validation loss: 0.3270
2024-06-03 03:33:48 [INFO]: Epoch 012 - training loss: 0.3507, validation loss: 0.3133
2024-06-03 03:33:52 [INFO]: Epoch 013 - training loss: 0.2672, validation loss: 0.3113
2024-06-03 03:33:56 [INFO]: Epoch 014 - training loss: 0.3029, validation loss: 0.3138
2024-06-03 03:34:00 [INFO]: Epoch 015 - training loss: 0.3475, validation loss: 0.2983
2024-06-03 03:34:03 [INFO]: Epoch 016 - training loss: 0.3242, validation loss: 0.2960
2024-06-03 03:34:06 [INFO]: Epoch 017 - training loss: 0.3038, validation loss: 0.2882
2024-06-03 03:34:08 [INFO]: Epoch 018 - training loss: 0.3124, validation loss: 0.2834
2024-06-03 03:34:10 [INFO]: Epoch 019 - training loss: 0.2493, validation loss: 0.2766
2024-06-03 03:34:12 [INFO]: Epoch 020 - training loss: 0.2939, validation loss: 0.2671
2024-06-03 03:34:14 [INFO]: Epoch 021 - training loss: 0.2434, validation loss: 0.2687
2024-06-03 03:34:16 [INFO]: Epoch 022 - training loss: 0.3069, validation loss: 0.2688
2024-06-03 03:34:18 [INFO]: Epoch 023 - training loss: 0.2702, validation loss: 0.2572
2024-06-03 03:34:20 [INFO]: Epoch 024 - training loss: 0.2680, validation loss: 0.2593
2024-06-03 03:34:22 [INFO]: Epoch 025 - training loss: 0.2624, validation loss: 0.2590
2024-06-03 03:34:23 [INFO]: Epoch 026 - training loss: 0.2168, validation loss: 0.2576
2024-06-03 03:34:24 [INFO]: Epoch 027 - training loss: 0.3121, validation loss: 0.2437
2024-06-03 03:34:26 [INFO]: Epoch 028 - training loss: 0.2092, validation loss: 0.2673
2024-06-03 03:34:27 [INFO]: Epoch 029 - training loss: 0.2430, validation loss: 0.2412
2024-06-03 03:34:28 [INFO]: Epoch 030 - training loss: 0.2039, validation loss: 0.2477
2024-06-03 03:34:30 [INFO]: Epoch 031 - training loss: 0.2252, validation loss: 0.2406
2024-06-03 03:34:31 [INFO]: Epoch 032 - training loss: 0.2183, validation loss: 0.2469
2024-06-03 03:34:32 [INFO]: Epoch 033 - training loss: 0.1932, validation loss: 0.2391
2024-06-03 03:34:34 [INFO]: Epoch 034 - training loss: 0.2423, validation loss: 0.2371
2024-06-03 03:34:35 [INFO]: Epoch 035 - training loss: 0.2440, validation loss: 0.2337
2024-06-03 03:34:36 [INFO]: Epoch 036 - training loss: 0.2265, validation loss: 0.2365
2024-06-03 03:34:37 [INFO]: Epoch 037 - training loss: 0.2133, validation loss: 0.2474
2024-06-03 03:34:39 [INFO]: Epoch 038 - training loss: 0.2004, validation loss: 0.2370
2024-06-03 03:34:40 [INFO]: Epoch 039 - training loss: 0.1951, validation loss: 0.2280
2024-06-03 03:34:41 [INFO]: Epoch 040 - training loss: 0.1898, validation loss: 0.2377
2024-06-03 03:34:43 [INFO]: Epoch 041 - training loss: 0.2516, validation loss: 0.2304
2024-06-03 03:34:44 [INFO]: Epoch 042 - training loss: 0.2651, validation loss: 0.2404
2024-06-03 03:34:45 [INFO]: Epoch 043 - training loss: 0.2167, validation loss: 0.2399
2024-06-03 03:34:46 [INFO]: Epoch 044 - training loss: 0.2377, validation loss: 0.2327
2024-06-03 03:34:48 [INFO]: Epoch 045 - training loss: 0.2431, validation loss: 0.2287
2024-06-03 03:34:49 [INFO]: Epoch 046 - training loss: 0.2194, validation loss: 0.2267
2024-06-03 03:34:50 [INFO]: Epoch 047 - training loss: 0.2151, validation loss: 0.2268
2024-06-03 03:34:52 [INFO]: Epoch 048 - training loss: 0.2336, validation loss: 0.2275
2024-06-03 03:34:53 [INFO]: Epoch 049 - training loss: 0.2525, validation loss: 0.2316
2024-06-03 03:34:54 [INFO]: Epoch 050 - training loss: 0.2114, validation loss: 0.2313
2024-06-03 03:34:56 [INFO]: Epoch 051 - training loss: 0.2096, validation loss: 0.2363
2024-06-03 03:34:57 [INFO]: Epoch 052 - training loss: 0.2781, validation loss: 0.2193
2024-06-03 03:34:58 [INFO]: Epoch 053 - training loss: 0.2175, validation loss: 0.2174
2024-06-03 03:34:59 [INFO]: Epoch 054 - training loss: 0.2051, validation loss: 0.2164
2024-06-03 03:35:01 [INFO]: Epoch 055 - training loss: 0.1535, validation loss: 0.2199
2024-06-03 03:35:02 [INFO]: Epoch 056 - training loss: 0.2162, validation loss: 0.2214
2024-06-03 03:35:03 [INFO]: Epoch 057 - training loss: 0.2448, validation loss: 0.2287
2024-06-03 03:35:05 [INFO]: Epoch 058 - training loss: 0.2091, validation loss: 0.2411
2024-06-03 03:35:06 [INFO]: Epoch 059 - training loss: 0.2269, validation loss: 0.2205
2024-06-03 03:35:07 [INFO]: Epoch 060 - training loss: 0.2224, validation loss: 0.2209
2024-06-03 03:35:08 [INFO]: Epoch 061 - training loss: 0.2195, validation loss: 0.2208
2024-06-03 03:35:10 [INFO]: Epoch 062 - training loss: 0.2029, validation loss: 0.2165
2024-06-03 03:35:11 [INFO]: Epoch 063 - training loss: 0.1889, validation loss: 0.2197
2024-06-03 03:35:12 [INFO]: Epoch 064 - training loss: 0.2083, validation loss: 0.2228
2024-06-03 03:35:12 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:35:12 [INFO]: Finished training. The best model is from epoch#54.
2024-06-03 03:35:12 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_0/20240603_T033237/CSDI.pypots
2024-06-03 03:35:56 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_0/imputation.pkl
2024-06-03 03:35:56 [INFO]: Round0 - CSDI on ETT_h1: MAE=0.5536, MSE=0.7571, MRE=0.6235
2024-06-03 03:35:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 03:35:56 [INFO]: Using the given device: cuda:0
2024-06-03 03:35:56 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_1/20240603_T033556
2024-06-03 03:35:56 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_1/20240603_T033556/tensorboard
2024-06-03 03:35:56 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 03:35:57 [INFO]: Epoch 001 - training loss: 0.7496, validation loss: 0.4926
2024-06-03 03:35:59 [INFO]: Epoch 002 - training loss: 0.4092, validation loss: 0.4149
2024-06-03 03:36:00 [INFO]: Epoch 003 - training loss: 0.4156, validation loss: 0.4070
2024-06-03 03:36:01 [INFO]: Epoch 004 - training loss: 0.3989, validation loss: 0.3901
2024-06-03 03:36:03 [INFO]: Epoch 005 - training loss: 0.3768, validation loss: 0.3599
2024-06-03 03:36:04 [INFO]: Epoch 006 - training loss: 0.3019, validation loss: 0.3541
2024-06-03 03:36:05 [INFO]: Epoch 007 - training loss: 0.3767, validation loss: 0.3456
2024-06-03 03:36:06 [INFO]: Epoch 008 - training loss: 0.3607, validation loss: 0.3518
2024-06-03 03:36:08 [INFO]: Epoch 009 - training loss: 0.3239, validation loss: 0.3270
2024-06-03 03:36:09 [INFO]: Epoch 010 - training loss: 0.2937, validation loss: 0.3279
2024-06-03 03:36:10 [INFO]: Epoch 011 - training loss: 0.3054, validation loss: 0.3175
2024-06-03 03:36:12 [INFO]: Epoch 012 - training loss: 0.2922, validation loss: 0.3148
2024-06-03 03:36:13 [INFO]: Epoch 013 - training loss: 0.3422, validation loss: 0.3038
2024-06-03 03:36:14 [INFO]: Epoch 014 - training loss: 0.2772, validation loss: 0.3125
2024-06-03 03:36:16 [INFO]: Epoch 015 - training loss: 0.2837, validation loss: 0.3039
2024-06-03 03:36:17 [INFO]: Epoch 016 - training loss: 0.3279, validation loss: 0.2983
2024-06-03 03:36:18 [INFO]: Epoch 017 - training loss: 0.3221, validation loss: 0.2981
2024-06-03 03:36:20 [INFO]: Epoch 018 - training loss: 0.2683, validation loss: 0.2900
2024-06-03 03:36:21 [INFO]: Epoch 019 - training loss: 0.2622, validation loss: 0.2941
2024-06-03 03:36:22 [INFO]: Epoch 020 - training loss: 0.2821, validation loss: 0.2919
2024-06-03 03:36:23 [INFO]: Epoch 021 - training loss: 0.3390, validation loss: 0.2861
2024-06-03 03:36:25 [INFO]: Epoch 022 - training loss: 0.2743, validation loss: 0.2932
2024-06-03 03:36:26 [INFO]: Epoch 023 - training loss: 0.2914, validation loss: 0.2968
2024-06-03 03:36:27 [INFO]: Epoch 024 - training loss: 0.3069, validation loss: 0.2802
2024-06-03 03:36:28 [INFO]: Epoch 025 - training loss: 0.2556, validation loss: 0.2789
2024-06-03 03:36:30 [INFO]: Epoch 026 - training loss: 0.3116, validation loss: 0.2789
2024-06-03 03:36:31 [INFO]: Epoch 027 - training loss: 0.2518, validation loss: 0.2770
2024-06-03 03:36:32 [INFO]: Epoch 028 - training loss: 0.2421, validation loss: 0.2639
2024-06-03 03:36:34 [INFO]: Epoch 029 - training loss: 0.2624, validation loss: 0.2660
2024-06-03 03:36:35 [INFO]: Epoch 030 - training loss: 0.2380, validation loss: 0.2745
2024-06-03 03:36:36 [INFO]: Epoch 031 - training loss: 0.2267, validation loss: 0.2614
2024-06-03 03:36:37 [INFO]: Epoch 032 - training loss: 0.2463, validation loss: 0.2539
2024-06-03 03:36:39 [INFO]: Epoch 033 - training loss: 0.2399, validation loss: 0.2641
2024-06-03 03:36:40 [INFO]: Epoch 034 - training loss: 0.2332, validation loss: 0.2500
2024-06-03 03:36:41 [INFO]: Epoch 035 - training loss: 0.2395, validation loss: 0.2642
2024-06-03 03:36:43 [INFO]: Epoch 036 - training loss: 0.2509, validation loss: 0.2375
2024-06-03 03:36:44 [INFO]: Epoch 037 - training loss: 0.1833, validation loss: 0.2490
2024-06-03 03:36:45 [INFO]: Epoch 038 - training loss: 0.2531, validation loss: 0.2452
2024-06-03 03:36:46 [INFO]: Epoch 039 - training loss: 0.2150, validation loss: 0.2465
2024-06-03 03:36:48 [INFO]: Epoch 040 - training loss: 0.2523, validation loss: 0.2382
2024-06-03 03:36:49 [INFO]: Epoch 041 - training loss: 0.2254, validation loss: 0.2401
2024-06-03 03:36:50 [INFO]: Epoch 042 - training loss: 0.2155, validation loss: 0.2527
2024-06-03 03:36:52 [INFO]: Epoch 043 - training loss: 0.2192, validation loss: 0.2388
2024-06-03 03:36:53 [INFO]: Epoch 044 - training loss: 0.2483, validation loss: 0.2458
2024-06-03 03:36:54 [INFO]: Epoch 045 - training loss: 0.2772, validation loss: 0.2428
2024-06-03 03:36:55 [INFO]: Epoch 046 - training loss: 0.2310, validation loss: 0.2486
2024-06-03 03:36:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:36:55 [INFO]: Finished training. The best model is from epoch#36.
2024-06-03 03:36:56 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_1/20240603_T033556/CSDI.pypots
2024-06-03 03:37:39 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_1/imputation.pkl
2024-06-03 03:37:39 [INFO]: Round1 - CSDI on ETT_h1: MAE=0.6211, MSE=0.9945, MRE=0.6996
2024-06-03 03:37:39 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 03:37:39 [INFO]: Using the given device: cuda:0
2024-06-03 03:37:39 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_2/20240603_T033739
2024-06-03 03:37:39 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_2/20240603_T033739/tensorboard
2024-06-03 03:37:39 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 03:37:40 [INFO]: Epoch 001 - training loss: 0.7292, validation loss: 0.5212
2024-06-03 03:37:41 [INFO]: Epoch 002 - training loss: 0.5077, validation loss: 0.4489
2024-06-03 03:37:43 [INFO]: Epoch 003 - training loss: 0.4004, validation loss: 0.4018
2024-06-03 03:37:44 [INFO]: Epoch 004 - training loss: 0.3926, validation loss: 0.4002
2024-06-03 03:37:45 [INFO]: Epoch 005 - training loss: 0.3567, validation loss: 0.3765
2024-06-03 03:37:46 [INFO]: Epoch 006 - training loss: 0.3759, validation loss: 0.3557
2024-06-03 03:37:48 [INFO]: Epoch 007 - training loss: 0.3347, validation loss: 0.3524
2024-06-03 03:37:49 [INFO]: Epoch 008 - training loss: 0.3370, validation loss: 0.3388
2024-06-03 03:37:50 [INFO]: Epoch 009 - training loss: 0.3142, validation loss: 0.3400
2024-06-03 03:37:52 [INFO]: Epoch 010 - training loss: 0.2792, validation loss: 0.3313
2024-06-03 03:37:53 [INFO]: Epoch 011 - training loss: 0.3556, validation loss: 0.3309
2024-06-03 03:37:54 [INFO]: Epoch 012 - training loss: 0.3295, validation loss: 0.3168
2024-06-03 03:37:56 [INFO]: Epoch 013 - training loss: 0.3427, validation loss: 0.3255
2024-06-03 03:37:57 [INFO]: Epoch 014 - training loss: 0.2421, validation loss: 0.3124
2024-06-03 03:37:58 [INFO]: Epoch 015 - training loss: 0.3063, validation loss: 0.3005
2024-06-03 03:37:59 [INFO]: Epoch 016 - training loss: 0.2794, validation loss: 0.2954
2024-06-03 03:38:01 [INFO]: Epoch 017 - training loss: 0.2551, validation loss: 0.3065
2024-06-03 03:38:02 [INFO]: Epoch 018 - training loss: 0.3147, validation loss: 0.2863
2024-06-03 03:38:03 [INFO]: Epoch 019 - training loss: 0.3147, validation loss: 0.2840
2024-06-03 03:38:05 [INFO]: Epoch 020 - training loss: 0.2674, validation loss: 0.2818
2024-06-03 03:38:06 [INFO]: Epoch 021 - training loss: 0.3019, validation loss: 0.2712
2024-06-03 03:38:07 [INFO]: Epoch 022 - training loss: 0.2507, validation loss: 0.2681
2024-06-03 03:38:08 [INFO]: Epoch 023 - training loss: 0.3125, validation loss: 0.2675
2024-06-03 03:38:10 [INFO]: Epoch 024 - training loss: 0.2167, validation loss: 0.2624
2024-06-03 03:38:11 [INFO]: Epoch 025 - training loss: 0.2163, validation loss: 0.2524
2024-06-03 03:38:12 [INFO]: Epoch 026 - training loss: 0.2261, validation loss: 0.2593
2024-06-03 03:38:14 [INFO]: Epoch 027 - training loss: 0.1936, validation loss: 0.2560
2024-06-03 03:38:15 [INFO]: Epoch 028 - training loss: 0.2667, validation loss: 0.2511
2024-06-03 03:38:16 [INFO]: Epoch 029 - training loss: 0.2754, validation loss: 0.2534
2024-06-03 03:38:18 [INFO]: Epoch 030 - training loss: 0.2066, validation loss: 0.2479
2024-06-03 03:38:19 [INFO]: Epoch 031 - training loss: 0.2596, validation loss: 0.2399
2024-06-03 03:38:20 [INFO]: Epoch 032 - training loss: 0.2377, validation loss: 0.2486
2024-06-03 03:38:21 [INFO]: Epoch 033 - training loss: 0.2577, validation loss: 0.2389
2024-06-03 03:38:23 [INFO]: Epoch 034 - training loss: 0.2207, validation loss: 0.2395
2024-06-03 03:38:24 [INFO]: Epoch 035 - training loss: 0.2137, validation loss: 0.2363
2024-06-03 03:38:25 [INFO]: Epoch 036 - training loss: 0.2524, validation loss: 0.2371
2024-06-03 03:38:27 [INFO]: Epoch 037 - training loss: 0.2405, validation loss: 0.2387
2024-06-03 03:38:28 [INFO]: Epoch 038 - training loss: 0.2551, validation loss: 0.2334
2024-06-03 03:38:29 [INFO]: Epoch 039 - training loss: 0.2506, validation loss: 0.2332
2024-06-03 03:38:30 [INFO]: Epoch 040 - training loss: 0.2188, validation loss: 0.2286
2024-06-03 03:38:32 [INFO]: Epoch 041 - training loss: 0.1908, validation loss: 0.2222
2024-06-03 03:38:33 [INFO]: Epoch 042 - training loss: 0.1760, validation loss: 0.2237
2024-06-03 03:38:34 [INFO]: Epoch 043 - training loss: 0.2324, validation loss: 0.2240
2024-06-03 03:38:36 [INFO]: Epoch 044 - training loss: 0.2233, validation loss: 0.2305
2024-06-03 03:38:37 [INFO]: Epoch 045 - training loss: 0.2380, validation loss: 0.2242
2024-06-03 03:38:38 [INFO]: Epoch 046 - training loss: 0.2752, validation loss: 0.2257
2024-06-03 03:38:39 [INFO]: Epoch 047 - training loss: 0.2024, validation loss: 0.2219
2024-06-03 03:38:41 [INFO]: Epoch 048 - training loss: 0.1752, validation loss: 0.2280
2024-06-03 03:38:42 [INFO]: Epoch 049 - training loss: 0.2109, validation loss: 0.2280
2024-06-03 03:38:43 [INFO]: Epoch 050 - training loss: 0.1978, validation loss: 0.2240
2024-06-03 03:38:45 [INFO]: Epoch 051 - training loss: 0.1873, validation loss: 0.2206
2024-06-03 03:38:46 [INFO]: Epoch 052 - training loss: 0.2343, validation loss: 0.2238
2024-06-03 03:38:47 [INFO]: Epoch 053 - training loss: 0.2092, validation loss: 0.2292
2024-06-03 03:38:49 [INFO]: Epoch 054 - training loss: 0.2496, validation loss: 0.2178
2024-06-03 03:38:50 [INFO]: Epoch 055 - training loss: 0.1644, validation loss: 0.2313
2024-06-03 03:38:51 [INFO]: Epoch 056 - training loss: 0.2180, validation loss: 0.2260
2024-06-03 03:38:52 [INFO]: Epoch 057 - training loss: 0.2027, validation loss: 0.2219
2024-06-03 03:38:54 [INFO]: Epoch 058 - training loss: 0.2106, validation loss: 0.2184
2024-06-03 03:38:55 [INFO]: Epoch 059 - training loss: 0.1940, validation loss: 0.2189
2024-06-03 03:38:56 [INFO]: Epoch 060 - training loss: 0.2105, validation loss: 0.2159
2024-06-03 03:38:58 [INFO]: Epoch 061 - training loss: 0.1894, validation loss: 0.2174
2024-06-03 03:38:59 [INFO]: Epoch 062 - training loss: 0.2191, validation loss: 0.2183
2024-06-03 03:39:00 [INFO]: Epoch 063 - training loss: 0.2077, validation loss: 0.2171
2024-06-03 03:39:01 [INFO]: Epoch 064 - training loss: 0.2072, validation loss: 0.2159
2024-06-03 03:39:03 [INFO]: Epoch 065 - training loss: 0.1867, validation loss: 0.2169
2024-06-03 03:39:04 [INFO]: Epoch 066 - training loss: 0.1783, validation loss: 0.2158
2024-06-03 03:39:05 [INFO]: Epoch 067 - training loss: 0.2461, validation loss: 0.2112
2024-06-03 03:39:07 [INFO]: Epoch 068 - training loss: 0.2229, validation loss: 0.2083
2024-06-03 03:39:08 [INFO]: Epoch 069 - training loss: 0.2327, validation loss: 0.2217
2024-06-03 03:39:09 [INFO]: Epoch 070 - training loss: 0.2300, validation loss: 0.2140
2024-06-03 03:39:10 [INFO]: Epoch 071 - training loss: 0.2303, validation loss: 0.2131
2024-06-03 03:39:12 [INFO]: Epoch 072 - training loss: 0.1767, validation loss: 0.2153
2024-06-03 03:39:13 [INFO]: Epoch 073 - training loss: 0.1778, validation loss: 0.2134
2024-06-03 03:39:14 [INFO]: Epoch 074 - training loss: 0.2147, validation loss: 0.2176
2024-06-03 03:39:16 [INFO]: Epoch 075 - training loss: 0.2030, validation loss: 0.2110
2024-06-03 03:39:17 [INFO]: Epoch 076 - training loss: 0.2004, validation loss: 0.2134
2024-06-03 03:39:18 [INFO]: Epoch 077 - training loss: 0.2061, validation loss: 0.2128
2024-06-03 03:39:19 [INFO]: Epoch 078 - training loss: 0.1369, validation loss: 0.2074
2024-06-03 03:39:21 [INFO]: Epoch 079 - training loss: 0.2481, validation loss: 0.2068
2024-06-03 03:39:22 [INFO]: Epoch 080 - training loss: 0.2089, validation loss: 0.2075
2024-06-03 03:39:23 [INFO]: Epoch 081 - training loss: 0.2373, validation loss: 0.2121
2024-06-03 03:39:25 [INFO]: Epoch 082 - training loss: 0.2100, validation loss: 0.2110
2024-06-03 03:39:26 [INFO]: Epoch 083 - training loss: 0.2057, validation loss: 0.2076
2024-06-03 03:39:27 [INFO]: Epoch 084 - training loss: 0.1965, validation loss: 0.2128
2024-06-03 03:39:29 [INFO]: Epoch 085 - training loss: 0.2177, validation loss: 0.2091
2024-06-03 03:39:30 [INFO]: Epoch 086 - training loss: 0.2412, validation loss: 0.2119
2024-06-03 03:39:31 [INFO]: Epoch 087 - training loss: 0.2476, validation loss: 0.2132
2024-06-03 03:39:33 [INFO]: Epoch 088 - training loss: 0.1791, validation loss: 0.2023
2024-06-03 03:39:34 [INFO]: Epoch 089 - training loss: 0.2134, validation loss: 0.2160
2024-06-03 03:39:35 [INFO]: Epoch 090 - training loss: 0.1909, validation loss: 0.2157
2024-06-03 03:39:36 [INFO]: Epoch 091 - training loss: 0.2215, validation loss: 0.2159
2024-06-03 03:39:38 [INFO]: Epoch 092 - training loss: 0.2405, validation loss: 0.2051
2024-06-03 03:39:39 [INFO]: Epoch 093 - training loss: 0.1912, validation loss: 0.2034
2024-06-03 03:39:40 [INFO]: Epoch 094 - training loss: 0.1647, validation loss: 0.2025
2024-06-03 03:39:42 [INFO]: Epoch 095 - training loss: 0.2080, validation loss: 0.2028
2024-06-03 03:39:43 [INFO]: Epoch 096 - training loss: 0.1836, validation loss: 0.2042
2024-06-03 03:39:44 [INFO]: Epoch 097 - training loss: 0.1822, validation loss: 0.2072
2024-06-03 03:39:45 [INFO]: Epoch 098 - training loss: 0.2042, validation loss: 0.2051
2024-06-03 03:39:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:39:45 [INFO]: Finished training. The best model is from epoch#88.
2024-06-03 03:39:45 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_2/20240603_T033739/CSDI.pypots
2024-06-03 03:40:28 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_2/imputation.pkl
2024-06-03 03:40:28 [INFO]: Round2 - CSDI on ETT_h1: MAE=0.4772, MSE=0.6204, MRE=0.5375
2024-06-03 03:40:28 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 03:40:28 [INFO]: Using the given device: cuda:0
2024-06-03 03:40:28 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_3/20240603_T034028
2024-06-03 03:40:28 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_3/20240603_T034028/tensorboard
2024-06-03 03:40:28 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 03:40:29 [INFO]: Epoch 001 - training loss: 0.7690, validation loss: 0.5202
2024-06-03 03:40:31 [INFO]: Epoch 002 - training loss: 0.5552, validation loss: 0.4225
2024-06-03 03:40:32 [INFO]: Epoch 003 - training loss: 0.4126, validation loss: 0.4018
2024-06-03 03:40:33 [INFO]: Epoch 004 - training loss: 0.3313, validation loss: 0.3932
2024-06-03 03:40:35 [INFO]: Epoch 005 - training loss: 0.3263, validation loss: 0.3618
2024-06-03 03:40:36 [INFO]: Epoch 006 - training loss: 0.3478, validation loss: 0.3542
2024-06-03 03:40:37 [INFO]: Epoch 007 - training loss: 0.3360, validation loss: 0.3575
2024-06-03 03:40:38 [INFO]: Epoch 008 - training loss: 0.3758, validation loss: 0.3429
2024-06-03 03:40:40 [INFO]: Epoch 009 - training loss: 0.3263, validation loss: 0.3414
2024-06-03 03:40:41 [INFO]: Epoch 010 - training loss: 0.3335, validation loss: 0.3166
2024-06-03 03:40:42 [INFO]: Epoch 011 - training loss: 0.3826, validation loss: 0.3065
2024-06-03 03:40:44 [INFO]: Epoch 012 - training loss: 0.2921, validation loss: 0.3005
2024-06-03 03:40:45 [INFO]: Epoch 013 - training loss: 0.3017, validation loss: 0.3012
2024-06-03 03:40:46 [INFO]: Epoch 014 - training loss: 0.3173, validation loss: 0.3009
2024-06-03 03:40:47 [INFO]: Epoch 015 - training loss: 0.3055, validation loss: 0.3379
2024-06-03 03:40:49 [INFO]: Epoch 016 - training loss: 0.3160, validation loss: 0.2976
2024-06-03 03:40:50 [INFO]: Epoch 017 - training loss: 0.3103, validation loss: 0.2832
2024-06-03 03:40:51 [INFO]: Epoch 018 - training loss: 0.2832, validation loss: 0.2907
2024-06-03 03:40:53 [INFO]: Epoch 019 - training loss: 0.3047, validation loss: 0.2781
2024-06-03 03:40:54 [INFO]: Epoch 020 - training loss: 0.2829, validation loss: 0.2701
2024-06-03 03:40:55 [INFO]: Epoch 021 - training loss: 0.2260, validation loss: 0.2739
2024-06-03 03:40:57 [INFO]: Epoch 022 - training loss: 0.2881, validation loss: 0.2684
2024-06-03 03:40:58 [INFO]: Epoch 023 - training loss: 0.2133, validation loss: 0.2743
2024-06-03 03:40:59 [INFO]: Epoch 024 - training loss: 0.2367, validation loss: 0.2581
2024-06-03 03:41:00 [INFO]: Epoch 025 - training loss: 0.2250, validation loss: 0.2524
2024-06-03 03:41:02 [INFO]: Epoch 026 - training loss: 0.2108, validation loss: 0.2437
2024-06-03 03:41:03 [INFO]: Epoch 027 - training loss: 0.2289, validation loss: 0.2622
2024-06-03 03:41:04 [INFO]: Epoch 028 - training loss: 0.2884, validation loss: 0.2451
2024-06-03 03:41:06 [INFO]: Epoch 029 - training loss: 0.1983, validation loss: 0.2561
2024-06-03 03:41:07 [INFO]: Epoch 030 - training loss: 0.2340, validation loss: 0.2404
2024-06-03 03:41:08 [INFO]: Epoch 031 - training loss: 0.2276, validation loss: 0.2572
2024-06-03 03:41:09 [INFO]: Epoch 032 - training loss: 0.2998, validation loss: 0.2536
2024-06-03 03:41:11 [INFO]: Epoch 033 - training loss: 0.2395, validation loss: 0.2524
2024-06-03 03:41:12 [INFO]: Epoch 034 - training loss: 0.2294, validation loss: 0.2482
2024-06-03 03:41:13 [INFO]: Epoch 035 - training loss: 0.2181, validation loss: 0.2402
2024-06-03 03:41:15 [INFO]: Epoch 036 - training loss: 0.2883, validation loss: 0.2311
2024-06-03 03:41:16 [INFO]: Epoch 037 - training loss: 0.2576, validation loss: 0.2290
2024-06-03 03:41:17 [INFO]: Epoch 038 - training loss: 0.1795, validation loss: 0.2307
2024-06-03 03:41:18 [INFO]: Epoch 039 - training loss: 0.1670, validation loss: 0.2276
2024-06-03 03:41:20 [INFO]: Epoch 040 - training loss: 0.2328, validation loss: 0.2273
2024-06-03 03:41:21 [INFO]: Epoch 041 - training loss: 0.1763, validation loss: 0.2310
2024-06-03 03:41:22 [INFO]: Epoch 042 - training loss: 0.1998, validation loss: 0.2316
2024-06-03 03:41:24 [INFO]: Epoch 043 - training loss: 0.1622, validation loss: 0.2352
2024-06-03 03:41:25 [INFO]: Epoch 044 - training loss: 0.1971, validation loss: 0.2301
2024-06-03 03:41:26 [INFO]: Epoch 045 - training loss: 0.2014, validation loss: 0.2199
2024-06-03 03:41:27 [INFO]: Epoch 046 - training loss: 0.2219, validation loss: 0.2300
2024-06-03 03:41:29 [INFO]: Epoch 047 - training loss: 0.2301, validation loss: 0.2443
2024-06-03 03:41:30 [INFO]: Epoch 048 - training loss: 0.2138, validation loss: 0.2305
2024-06-03 03:41:31 [INFO]: Epoch 049 - training loss: 0.2194, validation loss: 0.2280
2024-06-03 03:41:33 [INFO]: Epoch 050 - training loss: 0.1970, validation loss: 0.2223
2024-06-03 03:41:34 [INFO]: Epoch 051 - training loss: 0.2467, validation loss: 0.2222
2024-06-03 03:41:35 [INFO]: Epoch 052 - training loss: 0.2254, validation loss: 0.2226
2024-06-03 03:41:37 [INFO]: Epoch 053 - training loss: 0.2381, validation loss: 0.2269
2024-06-03 03:41:38 [INFO]: Epoch 054 - training loss: 0.2555, validation loss: 0.2265
2024-06-03 03:41:39 [INFO]: Epoch 055 - training loss: 0.1952, validation loss: 0.2228
2024-06-03 03:41:39 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:41:39 [INFO]: Finished training. The best model is from epoch#45.
2024-06-03 03:41:39 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_3/20240603_T034028/CSDI.pypots
2024-06-03 03:42:23 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_3/imputation.pkl
2024-06-03 03:42:23 [INFO]: Round3 - CSDI on ETT_h1: MAE=0.5470, MSE=0.7070, MRE=0.6161
2024-06-03 03:42:23 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 03:42:23 [INFO]: Using the given device: cuda:0
2024-06-03 03:42:23 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_4/20240603_T034223
2024-06-03 03:42:23 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_4/20240603_T034223/tensorboard
2024-06-03 03:42:23 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,194,993
2024-06-03 03:42:24 [INFO]: Epoch 001 - training loss: 0.7613, validation loss: 0.4944
2024-06-03 03:42:25 [INFO]: Epoch 002 - training loss: 0.4493, validation loss: 0.4245
2024-06-03 03:42:27 [INFO]: Epoch 003 - training loss: 0.4370, validation loss: 0.4018
2024-06-03 03:42:28 [INFO]: Epoch 004 - training loss: 0.3769, validation loss: 0.3739
2024-06-03 03:42:29 [INFO]: Epoch 005 - training loss: 0.3678, validation loss: 0.3613
2024-06-03 03:42:31 [INFO]: Epoch 006 - training loss: 0.3156, validation loss: 0.3512
2024-06-03 03:42:32 [INFO]: Epoch 007 - training loss: 0.2945, validation loss: 0.3505
2024-06-03 03:42:33 [INFO]: Epoch 008 - training loss: 0.3187, validation loss: 0.3365
2024-06-03 03:42:34 [INFO]: Epoch 009 - training loss: 0.3451, validation loss: 0.3331
2024-06-03 03:42:36 [INFO]: Epoch 010 - training loss: 0.3330, validation loss: 0.3251
2024-06-03 03:42:37 [INFO]: Epoch 011 - training loss: 0.3347, validation loss: 0.3221
2024-06-03 03:42:38 [INFO]: Epoch 012 - training loss: 0.3192, validation loss: 0.3113
2024-06-03 03:42:40 [INFO]: Epoch 013 - training loss: 0.3327, validation loss: 0.3079
2024-06-03 03:42:41 [INFO]: Epoch 014 - training loss: 0.2671, validation loss: 0.2992
2024-06-03 03:42:42 [INFO]: Epoch 015 - training loss: 0.3036, validation loss: 0.2911
2024-06-03 03:42:43 [INFO]: Epoch 016 - training loss: 0.3283, validation loss: 0.2849
2024-06-03 03:42:45 [INFO]: Epoch 017 - training loss: 0.2792, validation loss: 0.2866
2024-06-03 03:42:46 [INFO]: Epoch 018 - training loss: 0.2951, validation loss: 0.2857
2024-06-03 03:42:47 [INFO]: Epoch 019 - training loss: 0.2225, validation loss: 0.2778
2024-06-03 03:42:49 [INFO]: Epoch 020 - training loss: 0.3074, validation loss: 0.2736
2024-06-03 03:42:50 [INFO]: Epoch 021 - training loss: 0.2900, validation loss: 0.2765
2024-06-03 03:42:51 [INFO]: Epoch 022 - training loss: 0.2764, validation loss: 0.2761
2024-06-03 03:42:53 [INFO]: Epoch 023 - training loss: 0.2291, validation loss: 0.2610
2024-06-03 03:42:54 [INFO]: Epoch 024 - training loss: 0.2644, validation loss: 0.2516
2024-06-03 03:42:55 [INFO]: Epoch 025 - training loss: 0.2573, validation loss: 0.2493
2024-06-03 03:42:56 [INFO]: Epoch 026 - training loss: 0.1862, validation loss: 0.2472
2024-06-03 03:42:58 [INFO]: Epoch 027 - training loss: 0.3012, validation loss: 0.2478
2024-06-03 03:42:59 [INFO]: Epoch 028 - training loss: 0.2433, validation loss: 0.2461
2024-06-03 03:43:00 [INFO]: Epoch 029 - training loss: 0.2823, validation loss: 0.2515
2024-06-03 03:43:01 [INFO]: Epoch 030 - training loss: 0.2287, validation loss: 0.2470
2024-06-03 03:43:03 [INFO]: Epoch 031 - training loss: 0.2569, validation loss: 0.2494
2024-06-03 03:43:04 [INFO]: Epoch 032 - training loss: 0.2299, validation loss: 0.2396
2024-06-03 03:43:05 [INFO]: Epoch 033 - training loss: 0.2203, validation loss: 0.2373
2024-06-03 03:43:06 [INFO]: Epoch 034 - training loss: 0.2176, validation loss: 0.2377
2024-06-03 03:43:08 [INFO]: Epoch 035 - training loss: 0.2511, validation loss: 0.2310
2024-06-03 03:43:09 [INFO]: Epoch 036 - training loss: 0.2317, validation loss: 0.2252
2024-06-03 03:43:10 [INFO]: Epoch 037 - training loss: 0.2159, validation loss: 0.2314
2024-06-03 03:43:12 [INFO]: Epoch 038 - training loss: 0.2312, validation loss: 0.2307
2024-06-03 03:43:13 [INFO]: Epoch 039 - training loss: 0.2008, validation loss: 0.2292
2024-06-03 03:43:14 [INFO]: Epoch 040 - training loss: 0.2400, validation loss: 0.2378
2024-06-03 03:43:15 [INFO]: Epoch 041 - training loss: 0.2422, validation loss: 0.2349
2024-06-03 03:43:17 [INFO]: Epoch 042 - training loss: 0.3087, validation loss: 0.2302
2024-06-03 03:43:18 [INFO]: Epoch 043 - training loss: 0.1977, validation loss: 0.2310
2024-06-03 03:43:19 [INFO]: Epoch 044 - training loss: 0.2156, validation loss: 0.2314
2024-06-03 03:43:21 [INFO]: Epoch 045 - training loss: 0.2564, validation loss: 0.2273
2024-06-03 03:43:22 [INFO]: Epoch 046 - training loss: 0.2432, validation loss: 0.2226
2024-06-03 03:43:23 [INFO]: Epoch 047 - training loss: 0.2445, validation loss: 0.2231
2024-06-03 03:43:25 [INFO]: Epoch 048 - training loss: 0.2458, validation loss: 0.2304
2024-06-03 03:43:26 [INFO]: Epoch 049 - training loss: 0.2048, validation loss: 0.2229
2024-06-03 03:43:27 [INFO]: Epoch 050 - training loss: 0.2366, validation loss: 0.2231
2024-06-03 03:43:28 [INFO]: Epoch 051 - training loss: 0.2275, validation loss: 0.2138
2024-06-03 03:43:30 [INFO]: Epoch 052 - training loss: 0.1697, validation loss: 0.2142
2024-06-03 03:43:31 [INFO]: Epoch 053 - training loss: 0.2376, validation loss: 0.2329
2024-06-03 03:43:32 [INFO]: Epoch 054 - training loss: 0.2268, validation loss: 0.2290
2024-06-03 03:43:34 [INFO]: Epoch 055 - training loss: 0.1994, validation loss: 0.2230
2024-06-03 03:43:35 [INFO]: Epoch 056 - training loss: 0.1928, validation loss: 0.2261
2024-06-03 03:43:36 [INFO]: Epoch 057 - training loss: 0.2006, validation loss: 0.2215
2024-06-03 03:43:37 [INFO]: Epoch 058 - training loss: 0.1816, validation loss: 0.2275
2024-06-03 03:43:39 [INFO]: Epoch 059 - training loss: 0.2569, validation loss: 0.2185
2024-06-03 03:43:40 [INFO]: Epoch 060 - training loss: 0.2592, validation loss: 0.2174
2024-06-03 03:43:41 [INFO]: Epoch 061 - training loss: 0.2239, validation loss: 0.2196
2024-06-03 03:43:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:43:41 [INFO]: Finished training. The best model is from epoch#51.
2024-06-03 03:43:41 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_4/20240603_T034223/CSDI.pypots
2024-06-03 03:44:25 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/CSDI_ETT_h1/round_4/imputation.pkl
2024-06-03 03:44:25 [INFO]: Round4 - CSDI on ETT_h1: MAE=0.5905, MSE=0.8368, MRE=0.6651
2024-06-03 03:44:25 [INFO]: Done! Final results:
Averaged CSDI (1,194,993 params) on ETT_h1: MAE=0.5579 ± 0.04838403800426334, MSE=0.7832 ± 0.12689368148415206, MRE=0.6284 ± 0.054497751789380114, average inference time=9.83
