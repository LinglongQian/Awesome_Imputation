2024-06-03 03:45:34 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 03:45:34 [INFO]: Using the given device: cuda:0
2024-06-03 03:45:34 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/20240603_T034534
2024-06-03 03:45:34 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/20240603_T034534/tensorboard
2024-06-03 03:45:35 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 03:45:41 [INFO]: Epoch 001 - training loss: 1.0265, validation loss: 0.6716
2024-06-03 03:45:41 [INFO]: Epoch 002 - training loss: 0.7259, validation loss: 0.5652
2024-06-03 03:45:42 [INFO]: Epoch 003 - training loss: 0.6510, validation loss: 0.5615
2024-06-03 03:45:42 [INFO]: Epoch 004 - training loss: 0.6238, validation loss: 0.5576
2024-06-03 03:45:43 [INFO]: Epoch 005 - training loss: 0.6148, validation loss: 0.5266
2024-06-03 03:45:43 [INFO]: Epoch 006 - training loss: 0.6071, validation loss: 0.5171
2024-06-03 03:45:44 [INFO]: Epoch 007 - training loss: 0.5987, validation loss: 0.5165
2024-06-03 03:45:44 [INFO]: Epoch 008 - training loss: 0.5840, validation loss: 0.5089
2024-06-03 03:45:45 [INFO]: Epoch 009 - training loss: 0.5679, validation loss: 0.4991
2024-06-03 03:45:45 [INFO]: Epoch 010 - training loss: 0.5674, validation loss: 0.5038
2024-06-03 03:45:46 [INFO]: Epoch 011 - training loss: 0.5586, validation loss: 0.5037
2024-06-03 03:45:46 [INFO]: Epoch 012 - training loss: 0.5617, validation loss: 0.4880
2024-06-03 03:45:47 [INFO]: Epoch 013 - training loss: 0.5593, validation loss: 0.4965
2024-06-03 03:45:48 [INFO]: Epoch 014 - training loss: 0.5600, validation loss: 0.4878
2024-06-03 03:45:48 [INFO]: Epoch 015 - training loss: 0.5544, validation loss: 0.5035
2024-06-03 03:45:49 [INFO]: Epoch 016 - training loss: 0.5439, validation loss: 0.4902
2024-06-03 03:45:49 [INFO]: Epoch 017 - training loss: 0.5437, validation loss: 0.5033
2024-06-03 03:45:50 [INFO]: Epoch 018 - training loss: 0.5370, validation loss: 0.4891
2024-06-03 03:45:50 [INFO]: Epoch 019 - training loss: 0.5390, validation loss: 0.4978
2024-06-03 03:45:51 [INFO]: Epoch 020 - training loss: 0.5340, validation loss: 0.4986
2024-06-03 03:45:51 [INFO]: Epoch 021 - training loss: 0.5407, validation loss: 0.4979
2024-06-03 03:45:52 [INFO]: Epoch 022 - training loss: 0.5333, validation loss: 0.4918
2024-06-03 03:45:52 [INFO]: Epoch 023 - training loss: 0.5310, validation loss: 0.5051
2024-06-03 03:45:53 [INFO]: Epoch 024 - training loss: 0.5353, validation loss: 0.4930
2024-06-03 03:45:53 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:45:53 [INFO]: Finished training. The best model is from epoch#14.
2024-06-03 03:45:53 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/20240603_T034534/NonstationaryTransformer.pypots
2024-06-03 03:45:53 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_0/imputation.pkl
2024-06-03 03:45:53 [INFO]: Round0 - NonstationaryTransformer on ETT_h1: MAE=0.6084, MSE=0.9200, MRE=0.6853
2024-06-03 03:45:53 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 03:45:53 [INFO]: Using the given device: cuda:0
2024-06-03 03:45:53 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/20240603_T034553
2024-06-03 03:45:53 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/20240603_T034553/tensorboard
2024-06-03 03:45:53 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 03:45:54 [INFO]: Epoch 001 - training loss: 0.9045, validation loss: 0.6863
2024-06-03 03:45:54 [INFO]: Epoch 002 - training loss: 0.7101, validation loss: 0.5831
2024-06-03 03:45:55 [INFO]: Epoch 003 - training loss: 0.6392, validation loss: 0.5409
2024-06-03 03:45:55 [INFO]: Epoch 004 - training loss: 0.6099, validation loss: 0.5303
2024-06-03 03:45:56 [INFO]: Epoch 005 - training loss: 0.6017, validation loss: 0.5299
2024-06-03 03:45:56 [INFO]: Epoch 006 - training loss: 0.5826, validation loss: 0.5109
2024-06-03 03:45:56 [INFO]: Epoch 007 - training loss: 0.5790, validation loss: 0.5143
2024-06-03 03:45:57 [INFO]: Epoch 008 - training loss: 0.5636, validation loss: 0.5076
2024-06-03 03:45:57 [INFO]: Epoch 009 - training loss: 0.5575, validation loss: 0.5001
2024-06-03 03:45:58 [INFO]: Epoch 010 - training loss: 0.5552, validation loss: 0.5076
2024-06-03 03:45:58 [INFO]: Epoch 011 - training loss: 0.5565, validation loss: 0.5071
2024-06-03 03:45:59 [INFO]: Epoch 012 - training loss: 0.5536, validation loss: 0.4981
2024-06-03 03:46:00 [INFO]: Epoch 013 - training loss: 0.5502, validation loss: 0.4995
2024-06-03 03:46:00 [INFO]: Epoch 014 - training loss: 0.5417, validation loss: 0.5005
2024-06-03 03:46:01 [INFO]: Epoch 015 - training loss: 0.5440, validation loss: 0.4858
2024-06-03 03:46:01 [INFO]: Epoch 016 - training loss: 0.5337, validation loss: 0.4933
2024-06-03 03:46:02 [INFO]: Epoch 017 - training loss: 0.5452, validation loss: 0.5082
2024-06-03 03:46:02 [INFO]: Epoch 018 - training loss: 0.5367, validation loss: 0.4934
2024-06-03 03:46:03 [INFO]: Epoch 019 - training loss: 0.5379, validation loss: 0.4932
2024-06-03 03:46:03 [INFO]: Epoch 020 - training loss: 0.5286, validation loss: 0.4924
2024-06-03 03:46:04 [INFO]: Epoch 021 - training loss: 0.5317, validation loss: 0.4911
2024-06-03 03:46:04 [INFO]: Epoch 022 - training loss: 0.5317, validation loss: 0.5029
2024-06-03 03:46:05 [INFO]: Epoch 023 - training loss: 0.5234, validation loss: 0.4930
2024-06-03 03:46:05 [INFO]: Epoch 024 - training loss: 0.5239, validation loss: 0.4930
2024-06-03 03:46:06 [INFO]: Epoch 025 - training loss: 0.5275, validation loss: 0.4956
2024-06-03 03:46:06 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:46:06 [INFO]: Finished training. The best model is from epoch#15.
2024-06-03 03:46:06 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/20240603_T034553/NonstationaryTransformer.pypots
2024-06-03 03:46:06 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_1/imputation.pkl
2024-06-03 03:46:06 [INFO]: Round1 - NonstationaryTransformer on ETT_h1: MAE=0.6126, MSE=0.9162, MRE=0.6900
2024-06-03 03:46:06 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 03:46:06 [INFO]: Using the given device: cuda:0
2024-06-03 03:46:06 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/20240603_T034606
2024-06-03 03:46:06 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/20240603_T034606/tensorboard
2024-06-03 03:46:06 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 03:46:07 [INFO]: Epoch 001 - training loss: 1.1007, validation loss: 0.6663
2024-06-03 03:46:07 [INFO]: Epoch 002 - training loss: 0.7436, validation loss: 0.6171
2024-06-03 03:46:08 [INFO]: Epoch 003 - training loss: 0.6785, validation loss: 0.5734
2024-06-03 03:46:08 [INFO]: Epoch 004 - training loss: 0.6251, validation loss: 0.5293
2024-06-03 03:46:09 [INFO]: Epoch 005 - training loss: 0.5979, validation loss: 0.5056
2024-06-03 03:46:09 [INFO]: Epoch 006 - training loss: 0.5889, validation loss: 0.5063
2024-06-03 03:46:10 [INFO]: Epoch 007 - training loss: 0.5758, validation loss: 0.4940
2024-06-03 03:46:10 [INFO]: Epoch 008 - training loss: 0.5679, validation loss: 0.5049
2024-06-03 03:46:11 [INFO]: Epoch 009 - training loss: 0.5689, validation loss: 0.5000
2024-06-03 03:46:11 [INFO]: Epoch 010 - training loss: 0.5571, validation loss: 0.4922
2024-06-03 03:46:12 [INFO]: Epoch 011 - training loss: 0.5613, validation loss: 0.5062
2024-06-03 03:46:12 [INFO]: Epoch 012 - training loss: 0.5556, validation loss: 0.4987
2024-06-03 03:46:13 [INFO]: Epoch 013 - training loss: 0.5488, validation loss: 0.4910
2024-06-03 03:46:13 [INFO]: Epoch 014 - training loss: 0.5442, validation loss: 0.4970
2024-06-03 03:46:14 [INFO]: Epoch 015 - training loss: 0.5494, validation loss: 0.4962
2024-06-03 03:46:14 [INFO]: Epoch 016 - training loss: 0.5383, validation loss: 0.5045
2024-06-03 03:46:15 [INFO]: Epoch 017 - training loss: 0.5361, validation loss: 0.4915
2024-06-03 03:46:15 [INFO]: Epoch 018 - training loss: 0.5358, validation loss: 0.4969
2024-06-03 03:46:16 [INFO]: Epoch 019 - training loss: 0.5297, validation loss: 0.4978
2024-06-03 03:46:16 [INFO]: Epoch 020 - training loss: 0.5378, validation loss: 0.4871
2024-06-03 03:46:17 [INFO]: Epoch 021 - training loss: 0.5336, validation loss: 0.4930
2024-06-03 03:46:17 [INFO]: Epoch 022 - training loss: 0.5245, validation loss: 0.4839
2024-06-03 03:46:18 [INFO]: Epoch 023 - training loss: 0.5158, validation loss: 0.4880
2024-06-03 03:46:18 [INFO]: Epoch 024 - training loss: 0.5186, validation loss: 0.4928
2024-06-03 03:46:19 [INFO]: Epoch 025 - training loss: 0.5237, validation loss: 0.4920
2024-06-03 03:46:19 [INFO]: Epoch 026 - training loss: 0.5138, validation loss: 0.5062
2024-06-03 03:46:20 [INFO]: Epoch 027 - training loss: 0.5268, validation loss: 0.4990
2024-06-03 03:46:20 [INFO]: Epoch 028 - training loss: 0.5178, validation loss: 0.4902
2024-06-03 03:46:21 [INFO]: Epoch 029 - training loss: 0.5212, validation loss: 0.4869
2024-06-03 03:46:21 [INFO]: Epoch 030 - training loss: 0.5254, validation loss: 0.5008
2024-06-03 03:46:21 [INFO]: Epoch 031 - training loss: 0.5167, validation loss: 0.4859
2024-06-03 03:46:22 [INFO]: Epoch 032 - training loss: 0.5184, validation loss: 0.4907
2024-06-03 03:46:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:46:22 [INFO]: Finished training. The best model is from epoch#22.
2024-06-03 03:46:22 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/20240603_T034606/NonstationaryTransformer.pypots
2024-06-03 03:46:22 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_2/imputation.pkl
2024-06-03 03:46:22 [INFO]: Round2 - NonstationaryTransformer on ETT_h1: MAE=0.6123, MSE=0.9148, MRE=0.6897
2024-06-03 03:46:22 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 03:46:22 [INFO]: Using the given device: cuda:0
2024-06-03 03:46:22 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/20240603_T034622
2024-06-03 03:46:22 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/20240603_T034622/tensorboard
2024-06-03 03:46:22 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 03:46:23 [INFO]: Epoch 001 - training loss: 1.0768, validation loss: 0.6694
2024-06-03 03:46:23 [INFO]: Epoch 002 - training loss: 0.7344, validation loss: 0.6239
2024-06-03 03:46:23 [INFO]: Epoch 003 - training loss: 0.6739, validation loss: 0.5647
2024-06-03 03:46:24 [INFO]: Epoch 004 - training loss: 0.6442, validation loss: 0.5634
2024-06-03 03:46:24 [INFO]: Epoch 005 - training loss: 0.6127, validation loss: 0.5264
2024-06-03 03:46:25 [INFO]: Epoch 006 - training loss: 0.5869, validation loss: 0.5187
2024-06-03 03:46:25 [INFO]: Epoch 007 - training loss: 0.5822, validation loss: 0.5151
2024-06-03 03:46:26 [INFO]: Epoch 008 - training loss: 0.5737, validation loss: 0.5104
2024-06-03 03:46:26 [INFO]: Epoch 009 - training loss: 0.5653, validation loss: 0.4997
2024-06-03 03:46:26 [INFO]: Epoch 010 - training loss: 0.5723, validation loss: 0.5021
2024-06-03 03:46:27 [INFO]: Epoch 011 - training loss: 0.5670, validation loss: 0.5192
2024-06-03 03:46:27 [INFO]: Epoch 012 - training loss: 0.5724, validation loss: 0.4928
2024-06-03 03:46:27 [INFO]: Epoch 013 - training loss: 0.5620, validation loss: 0.5149
2024-06-03 03:46:28 [INFO]: Epoch 014 - training loss: 0.5517, validation loss: 0.4878
2024-06-03 03:46:28 [INFO]: Epoch 015 - training loss: 0.5490, validation loss: 0.4946
2024-06-03 03:46:29 [INFO]: Epoch 016 - training loss: 0.5410, validation loss: 0.4895
2024-06-03 03:46:29 [INFO]: Epoch 017 - training loss: 0.5504, validation loss: 0.4908
2024-06-03 03:46:29 [INFO]: Epoch 018 - training loss: 0.5417, validation loss: 0.4835
2024-06-03 03:46:30 [INFO]: Epoch 019 - training loss: 0.5373, validation loss: 0.4846
2024-06-03 03:46:30 [INFO]: Epoch 020 - training loss: 0.5311, validation loss: 0.4941
2024-06-03 03:46:31 [INFO]: Epoch 021 - training loss: 0.5338, validation loss: 0.4831
2024-06-03 03:46:31 [INFO]: Epoch 022 - training loss: 0.5399, validation loss: 0.4952
2024-06-03 03:46:31 [INFO]: Epoch 023 - training loss: 0.5303, validation loss: 0.4895
2024-06-03 03:46:32 [INFO]: Epoch 024 - training loss: 0.5188, validation loss: 0.4920
2024-06-03 03:46:32 [INFO]: Epoch 025 - training loss: 0.5187, validation loss: 0.4798
2024-06-03 03:46:32 [INFO]: Epoch 026 - training loss: 0.5239, validation loss: 0.4908
2024-06-03 03:46:33 [INFO]: Epoch 027 - training loss: 0.5253, validation loss: 0.4818
2024-06-03 03:46:33 [INFO]: Epoch 028 - training loss: 0.5191, validation loss: 0.4815
2024-06-03 03:46:33 [INFO]: Epoch 029 - training loss: 0.5212, validation loss: 0.4770
2024-06-03 03:46:33 [INFO]: Epoch 030 - training loss: 0.5234, validation loss: 0.4884
2024-06-03 03:46:34 [INFO]: Epoch 031 - training loss: 0.5185, validation loss: 0.4877
2024-06-03 03:46:34 [INFO]: Epoch 032 - training loss: 0.5111, validation loss: 0.4889
2024-06-03 03:46:35 [INFO]: Epoch 033 - training loss: 0.5144, validation loss: 0.4907
2024-06-03 03:46:35 [INFO]: Epoch 034 - training loss: 0.5147, validation loss: 0.4828
2024-06-03 03:46:35 [INFO]: Epoch 035 - training loss: 0.5146, validation loss: 0.4924
2024-06-03 03:46:36 [INFO]: Epoch 036 - training loss: 0.5102, validation loss: 0.4865
2024-06-03 03:46:36 [INFO]: Epoch 037 - training loss: 0.5185, validation loss: 0.4996
2024-06-03 03:46:36 [INFO]: Epoch 038 - training loss: 0.5071, validation loss: 0.4916
2024-06-03 03:46:37 [INFO]: Epoch 039 - training loss: 0.5065, validation loss: 0.4915
2024-06-03 03:46:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:46:37 [INFO]: Finished training. The best model is from epoch#29.
2024-06-03 03:46:37 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/20240603_T034622/NonstationaryTransformer.pypots
2024-06-03 03:46:37 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_3/imputation.pkl
2024-06-03 03:46:37 [INFO]: Round3 - NonstationaryTransformer on ETT_h1: MAE=0.6126, MSE=0.9214, MRE=0.6900
2024-06-03 03:46:37 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 03:46:37 [INFO]: Using the given device: cuda:0
2024-06-03 03:46:37 [INFO]: Model files will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/20240603_T034637
2024-06-03 03:46:37 [INFO]: Tensorboard file will be saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/20240603_T034637/tensorboard
2024-06-03 03:46:37 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 589,927
2024-06-03 03:46:38 [INFO]: Epoch 001 - training loss: 0.9196, validation loss: 0.6057
2024-06-03 03:46:38 [INFO]: Epoch 002 - training loss: 0.6952, validation loss: 0.5738
2024-06-03 03:46:38 [INFO]: Epoch 003 - training loss: 0.6408, validation loss: 0.5355
2024-06-03 03:46:39 [INFO]: Epoch 004 - training loss: 0.6077, validation loss: 0.5152
2024-06-03 03:46:39 [INFO]: Epoch 005 - training loss: 0.5863, validation loss: 0.5142
2024-06-03 03:46:39 [INFO]: Epoch 006 - training loss: 0.5836, validation loss: 0.5018
2024-06-03 03:46:40 [INFO]: Epoch 007 - training loss: 0.5681, validation loss: 0.5060
2024-06-03 03:46:40 [INFO]: Epoch 008 - training loss: 0.5655, validation loss: 0.4935
2024-06-03 03:46:40 [INFO]: Epoch 009 - training loss: 0.5610, validation loss: 0.5073
2024-06-03 03:46:41 [INFO]: Epoch 010 - training loss: 0.5600, validation loss: 0.5044
2024-06-03 03:46:41 [INFO]: Epoch 011 - training loss: 0.5519, validation loss: 0.4919
2024-06-03 03:46:42 [INFO]: Epoch 012 - training loss: 0.5476, validation loss: 0.4990
2024-06-03 03:46:42 [INFO]: Epoch 013 - training loss: 0.5498, validation loss: 0.4943
2024-06-03 03:46:43 [INFO]: Epoch 014 - training loss: 0.5455, validation loss: 0.5008
2024-06-03 03:46:43 [INFO]: Epoch 015 - training loss: 0.5405, validation loss: 0.4901
2024-06-03 03:46:43 [INFO]: Epoch 016 - training loss: 0.5351, validation loss: 0.4888
2024-06-03 03:46:44 [INFO]: Epoch 017 - training loss: 0.5384, validation loss: 0.5046
2024-06-03 03:46:44 [INFO]: Epoch 018 - training loss: 0.5361, validation loss: 0.4897
2024-06-03 03:46:44 [INFO]: Epoch 019 - training loss: 0.5322, validation loss: 0.4920
2024-06-03 03:46:45 [INFO]: Epoch 020 - training loss: 0.5353, validation loss: 0.4899
2024-06-03 03:46:45 [INFO]: Epoch 021 - training loss: 0.5253, validation loss: 0.4862
2024-06-03 03:46:46 [INFO]: Epoch 022 - training loss: 0.5220, validation loss: 0.4902
2024-06-03 03:46:46 [INFO]: Epoch 023 - training loss: 0.5219, validation loss: 0.4803
2024-06-03 03:46:47 [INFO]: Epoch 024 - training loss: 0.5134, validation loss: 0.4944
2024-06-03 03:46:47 [INFO]: Epoch 025 - training loss: 0.5183, validation loss: 0.4980
2024-06-03 03:46:47 [INFO]: Epoch 026 - training loss: 0.5179, validation loss: 0.4975
2024-06-03 03:46:48 [INFO]: Epoch 027 - training loss: 0.5130, validation loss: 0.5106
2024-06-03 03:46:48 [INFO]: Epoch 028 - training loss: 0.5109, validation loss: 0.4883
2024-06-03 03:46:48 [INFO]: Epoch 029 - training loss: 0.5162, validation loss: 0.4987
2024-06-03 03:46:49 [INFO]: Epoch 030 - training loss: 0.5126, validation loss: 0.4842
2024-06-03 03:46:49 [INFO]: Epoch 031 - training loss: 0.5150, validation loss: 0.4981
2024-06-03 03:46:49 [INFO]: Epoch 032 - training loss: 0.5099, validation loss: 0.4829
2024-06-03 03:46:50 [INFO]: Epoch 033 - training loss: 0.4987, validation loss: 0.4825
2024-06-03 03:46:50 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 03:46:50 [INFO]: Finished training. The best model is from epoch#23.
2024-06-03 03:46:50 [INFO]: Saved the model to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/20240603_T034637/NonstationaryTransformer.pypots
2024-06-03 03:46:50 [INFO]: Successfully saved to results_subseq_rate05/ETT_h1/NonstationaryTransformer_ETT_h1/round_4/imputation.pkl
2024-06-03 03:46:50 [INFO]: Round4 - NonstationaryTransformer on ETT_h1: MAE=0.6146, MSE=0.9122, MRE=0.6923
2024-06-03 03:46:50 [INFO]: Done! Final results:
Averaged NonstationaryTransformer (589,927 params) on ETT_h1: MAE=0.6121 ± 0.0020327373586395718, MSE=0.9169 ± 0.0033873771842156896, MRE=0.6895 ± 0.002289590133307584, average inference time=0.05
