2024-06-03 12:03:08 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 12:03:08 [INFO]: Using the given device: cuda:0
2024-06-03 12:03:09 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_0/20240603_T120309
2024-06-03 12:03:09 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_0/20240603_T120309/tensorboard
2024-06-03 12:03:09 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 12:03:09 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 12:03:12 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 12:03:23 [INFO]: Epoch 001 - training loss: 1.0473, validation loss: 0.4087
2024-06-03 12:03:30 [INFO]: Epoch 002 - training loss: 0.6641, validation loss: 0.3291
2024-06-03 12:03:37 [INFO]: Epoch 003 - training loss: 0.5688, validation loss: 0.2916
2024-06-03 12:03:44 [INFO]: Epoch 004 - training loss: 0.5237, validation loss: 0.2823
2024-06-03 12:03:51 [INFO]: Epoch 005 - training loss: 0.4977, validation loss: 0.2659
2024-06-03 12:03:58 [INFO]: Epoch 006 - training loss: 0.4722, validation loss: 0.2598
2024-06-03 12:04:06 [INFO]: Epoch 007 - training loss: 0.4542, validation loss: 0.2564
2024-06-03 12:04:13 [INFO]: Epoch 008 - training loss: 0.4436, validation loss: 0.2479
2024-06-03 12:04:20 [INFO]: Epoch 009 - training loss: 0.4254, validation loss: 0.2456
2024-06-03 12:04:27 [INFO]: Epoch 010 - training loss: 0.4189, validation loss: 0.2468
2024-06-03 12:04:35 [INFO]: Epoch 011 - training loss: 0.4153, validation loss: 0.2450
2024-06-03 12:04:42 [INFO]: Epoch 012 - training loss: 0.4121, validation loss: 0.2395
2024-06-03 12:04:50 [INFO]: Epoch 013 - training loss: 0.4015, validation loss: 0.2390
2024-06-03 12:04:57 [INFO]: Epoch 014 - training loss: 0.3900, validation loss: 0.2371
2024-06-03 12:05:04 [INFO]: Epoch 015 - training loss: 0.3905, validation loss: 0.2351
2024-06-03 12:05:12 [INFO]: Epoch 016 - training loss: 0.3770, validation loss: 0.2395
2024-06-03 12:05:19 [INFO]: Epoch 017 - training loss: 0.3748, validation loss: 0.2408
2024-06-03 12:05:26 [INFO]: Epoch 018 - training loss: 0.3699, validation loss: 0.2317
2024-06-03 12:05:33 [INFO]: Epoch 019 - training loss: 0.3691, validation loss: 0.2395
2024-06-03 12:05:40 [INFO]: Epoch 020 - training loss: 0.3606, validation loss: 0.2348
2024-06-03 12:05:48 [INFO]: Epoch 021 - training loss: 0.3594, validation loss: 0.2389
2024-06-03 12:05:55 [INFO]: Epoch 022 - training loss: 0.3578, validation loss: 0.2368
2024-06-03 12:06:02 [INFO]: Epoch 023 - training loss: 0.3511, validation loss: 0.2366
2024-06-03 12:06:10 [INFO]: Epoch 024 - training loss: 0.3473, validation loss: 0.2293
2024-06-03 12:06:17 [INFO]: Epoch 025 - training loss: 0.3463, validation loss: 0.2415
2024-06-03 12:06:24 [INFO]: Epoch 026 - training loss: 0.3447, validation loss: 0.2336
2024-06-03 12:06:32 [INFO]: Epoch 027 - training loss: 0.3369, validation loss: 0.2292
2024-06-03 12:06:39 [INFO]: Epoch 028 - training loss: 0.3350, validation loss: 0.2332
2024-06-03 12:06:46 [INFO]: Epoch 029 - training loss: 0.3315, validation loss: 0.2340
2024-06-03 12:06:54 [INFO]: Epoch 030 - training loss: 0.3324, validation loss: 0.2327
2024-06-03 12:07:01 [INFO]: Epoch 031 - training loss: 0.3288, validation loss: 0.2303
2024-06-03 12:07:08 [INFO]: Epoch 032 - training loss: 0.3337, validation loss: 0.2331
2024-06-03 12:07:15 [INFO]: Epoch 033 - training loss: 0.3447, validation loss: 0.2292
2024-06-03 12:07:22 [INFO]: Epoch 034 - training loss: 0.3251, validation loss: 0.2295
2024-06-03 12:07:29 [INFO]: Epoch 035 - training loss: 0.3262, validation loss: 0.2265
2024-06-03 12:07:37 [INFO]: Epoch 036 - training loss: 0.3166, validation loss: 0.2226
2024-06-03 12:07:44 [INFO]: Epoch 037 - training loss: 0.3106, validation loss: 0.2235
2024-06-03 12:07:51 [INFO]: Epoch 038 - training loss: 0.3134, validation loss: 0.2239
2024-06-03 12:07:58 [INFO]: Epoch 039 - training loss: 0.3130, validation loss: 0.2269
2024-06-03 12:08:05 [INFO]: Epoch 040 - training loss: 0.3061, validation loss: 0.2234
2024-06-03 12:08:12 [INFO]: Epoch 041 - training loss: 0.3052, validation loss: 0.2317
2024-06-03 12:08:19 [INFO]: Epoch 042 - training loss: 0.3107, validation loss: 0.2242
2024-06-03 12:08:27 [INFO]: Epoch 043 - training loss: 0.3069, validation loss: 0.2162
2024-06-03 12:08:34 [INFO]: Epoch 044 - training loss: 0.3058, validation loss: 0.2225
2024-06-03 12:08:41 [INFO]: Epoch 045 - training loss: 0.3041, validation loss: 0.2208
2024-06-03 12:08:48 [INFO]: Epoch 046 - training loss: 0.2998, validation loss: 0.2196
2024-06-03 12:08:56 [INFO]: Epoch 047 - training loss: 0.2987, validation loss: 0.2245
2024-06-03 12:09:03 [INFO]: Epoch 048 - training loss: 0.2939, validation loss: 0.2213
2024-06-03 12:09:10 [INFO]: Epoch 049 - training loss: 0.2930, validation loss: 0.2197
2024-06-03 12:09:17 [INFO]: Epoch 050 - training loss: 0.2963, validation loss: 0.2152
2024-06-03 12:09:25 [INFO]: Epoch 051 - training loss: 0.2937, validation loss: 0.2192
2024-06-03 12:09:32 [INFO]: Epoch 052 - training loss: 0.2969, validation loss: 0.2167
2024-06-03 12:09:39 [INFO]: Epoch 053 - training loss: 0.2898, validation loss: 0.2149
2024-06-03 12:09:46 [INFO]: Epoch 054 - training loss: 0.2849, validation loss: 0.2156
2024-06-03 12:09:53 [INFO]: Epoch 055 - training loss: 0.2836, validation loss: 0.2127
2024-06-03 12:10:00 [INFO]: Epoch 056 - training loss: 0.2852, validation loss: 0.2169
2024-06-03 12:10:07 [INFO]: Epoch 057 - training loss: 0.2838, validation loss: 0.2148
2024-06-03 12:10:14 [INFO]: Epoch 058 - training loss: 0.2804, validation loss: 0.2133
2024-06-03 12:10:21 [INFO]: Epoch 059 - training loss: 0.2831, validation loss: 0.2151
2024-06-03 12:10:28 [INFO]: Epoch 060 - training loss: 0.2791, validation loss: 0.2095
2024-06-03 12:10:36 [INFO]: Epoch 061 - training loss: 0.2799, validation loss: 0.2147
2024-06-03 12:10:43 [INFO]: Epoch 062 - training loss: 0.2787, validation loss: 0.2135
2024-06-03 12:10:50 [INFO]: Epoch 063 - training loss: 0.2758, validation loss: 0.2109
2024-06-03 12:10:58 [INFO]: Epoch 064 - training loss: 0.2710, validation loss: 0.2200
2024-06-03 12:11:05 [INFO]: Epoch 065 - training loss: 0.2729, validation loss: 0.2113
2024-06-03 12:11:13 [INFO]: Epoch 066 - training loss: 0.2707, validation loss: 0.2074
2024-06-03 12:11:20 [INFO]: Epoch 067 - training loss: 0.2689, validation loss: 0.2143
2024-06-03 12:11:27 [INFO]: Epoch 068 - training loss: 0.2683, validation loss: 0.2067
2024-06-03 12:11:34 [INFO]: Epoch 069 - training loss: 0.2690, validation loss: 0.2067
2024-06-03 12:11:41 [INFO]: Epoch 070 - training loss: 0.2684, validation loss: 0.2120
2024-06-03 12:11:48 [INFO]: Epoch 071 - training loss: 0.2689, validation loss: 0.2083
2024-06-03 12:11:55 [INFO]: Epoch 072 - training loss: 0.2613, validation loss: 0.2071
2024-06-03 12:12:02 [INFO]: Epoch 073 - training loss: 0.2646, validation loss: 0.2135
2024-06-03 12:12:08 [INFO]: Epoch 074 - training loss: 0.2646, validation loss: 0.2081
2024-06-03 12:12:16 [INFO]: Epoch 075 - training loss: 0.2610, validation loss: 0.2053
2024-06-03 12:12:23 [INFO]: Epoch 076 - training loss: 0.2573, validation loss: 0.2065
2024-06-03 12:12:31 [INFO]: Epoch 077 - training loss: 0.2590, validation loss: 0.2066
2024-06-03 12:12:38 [INFO]: Epoch 078 - training loss: 0.2601, validation loss: 0.2054
2024-06-03 12:12:45 [INFO]: Epoch 079 - training loss: 0.2618, validation loss: 0.2081
2024-06-03 12:12:52 [INFO]: Epoch 080 - training loss: 0.2606, validation loss: 0.2088
2024-06-03 12:12:59 [INFO]: Epoch 081 - training loss: 0.2623, validation loss: 0.2068
2024-06-03 12:13:06 [INFO]: Epoch 082 - training loss: 0.2595, validation loss: 0.2044
2024-06-03 12:13:14 [INFO]: Epoch 083 - training loss: 0.2557, validation loss: 0.2077
2024-06-03 12:13:21 [INFO]: Epoch 084 - training loss: 0.2555, validation loss: 0.2054
2024-06-03 12:13:28 [INFO]: Epoch 085 - training loss: 0.2555, validation loss: 0.2063
2024-06-03 12:13:36 [INFO]: Epoch 086 - training loss: 0.2501, validation loss: 0.2030
2024-06-03 12:13:43 [INFO]: Epoch 087 - training loss: 0.2530, validation loss: 0.2007
2024-06-03 12:13:50 [INFO]: Epoch 088 - training loss: 0.2613, validation loss: 0.2014
2024-06-03 12:13:58 [INFO]: Epoch 089 - training loss: 0.2586, validation loss: 0.2004
2024-06-03 12:14:04 [INFO]: Epoch 090 - training loss: 0.2514, validation loss: 0.1994
2024-06-03 12:14:11 [INFO]: Epoch 091 - training loss: 0.2539, validation loss: 0.2082
2024-06-03 12:14:18 [INFO]: Epoch 092 - training loss: 0.2535, validation loss: 0.2060
2024-06-03 12:14:25 [INFO]: Epoch 093 - training loss: 0.2504, validation loss: 0.1981
2024-06-03 12:14:32 [INFO]: Epoch 094 - training loss: 0.2442, validation loss: 0.1999
2024-06-03 12:14:39 [INFO]: Epoch 095 - training loss: 0.2469, validation loss: 0.2002
2024-06-03 12:14:46 [INFO]: Epoch 096 - training loss: 0.2480, validation loss: 0.1979
2024-06-03 12:14:53 [INFO]: Epoch 097 - training loss: 0.2481, validation loss: 0.1963
2024-06-03 12:15:00 [INFO]: Epoch 098 - training loss: 0.2457, validation loss: 0.2017
2024-06-03 12:15:07 [INFO]: Epoch 099 - training loss: 0.2458, validation loss: 0.2056
2024-06-03 12:15:15 [INFO]: Epoch 100 - training loss: 0.2503, validation loss: 0.2002
2024-06-03 12:15:15 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 12:15:20 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_0/20240603_T120309/Transformer.pypots
2024-06-03 12:15:22 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_0/imputation.pkl
2024-06-03 12:15:22 [INFO]: Round0 - Transformer on BeijingAir: MAE=0.2246, MSE=0.2676, MRE=0.3066
2024-06-03 12:15:22 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 12:15:22 [INFO]: Using the given device: cuda:0
2024-06-03 12:15:22 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_1/20240603_T121522
2024-06-03 12:15:22 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_1/20240603_T121522/tensorboard
2024-06-03 12:15:22 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 12:15:22 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 12:15:29 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 12:15:34 [INFO]: Epoch 001 - training loss: 1.0401, validation loss: 0.4079
2024-06-03 12:15:40 [INFO]: Epoch 002 - training loss: 0.6537, validation loss: 0.3223
2024-06-03 12:15:46 [INFO]: Epoch 003 - training loss: 0.5644, validation loss: 0.2943
2024-06-03 12:15:52 [INFO]: Epoch 004 - training loss: 0.5215, validation loss: 0.2837
2024-06-03 12:15:58 [INFO]: Epoch 005 - training loss: 0.4962, validation loss: 0.2697
2024-06-03 12:16:04 [INFO]: Epoch 006 - training loss: 0.4789, validation loss: 0.2658
2024-06-03 12:16:09 [INFO]: Epoch 007 - training loss: 0.4664, validation loss: 0.2564
2024-06-03 12:16:14 [INFO]: Epoch 008 - training loss: 0.4442, validation loss: 0.2539
2024-06-03 12:16:19 [INFO]: Epoch 009 - training loss: 0.4300, validation loss: 0.2462
2024-06-03 12:16:25 [INFO]: Epoch 010 - training loss: 0.4180, validation loss: 0.2468
2024-06-03 12:16:31 [INFO]: Epoch 011 - training loss: 0.4099, validation loss: 0.2444
2024-06-03 12:16:36 [INFO]: Epoch 012 - training loss: 0.4046, validation loss: 0.2379
2024-06-03 12:16:42 [INFO]: Epoch 013 - training loss: 0.4035, validation loss: 0.2360
2024-06-03 12:16:48 [INFO]: Epoch 014 - training loss: 0.3932, validation loss: 0.2370
2024-06-03 12:16:54 [INFO]: Epoch 015 - training loss: 0.3876, validation loss: 0.2371
2024-06-03 12:16:59 [INFO]: Epoch 016 - training loss: 0.3834, validation loss: 0.2357
2024-06-03 12:17:05 [INFO]: Epoch 017 - training loss: 0.3832, validation loss: 0.2368
2024-06-03 12:17:10 [INFO]: Epoch 018 - training loss: 0.3744, validation loss: 0.2379
2024-06-03 12:17:16 [INFO]: Epoch 019 - training loss: 0.3659, validation loss: 0.2389
2024-06-03 12:17:21 [INFO]: Epoch 020 - training loss: 0.3742, validation loss: 0.2420
2024-06-03 12:17:27 [INFO]: Epoch 021 - training loss: 0.3635, validation loss: 0.2377
2024-06-03 12:17:32 [INFO]: Epoch 022 - training loss: 0.3588, validation loss: 0.2397
2024-06-03 12:17:37 [INFO]: Epoch 023 - training loss: 0.3588, validation loss: 0.2499
2024-06-03 12:17:43 [INFO]: Epoch 024 - training loss: 0.3496, validation loss: 0.2295
2024-06-03 12:17:48 [INFO]: Epoch 025 - training loss: 0.3461, validation loss: 0.2450
2024-06-03 12:17:53 [INFO]: Epoch 026 - training loss: 0.3422, validation loss: 0.2349
2024-06-03 12:17:58 [INFO]: Epoch 027 - training loss: 0.3393, validation loss: 0.2338
2024-06-03 12:18:03 [INFO]: Epoch 028 - training loss: 0.3355, validation loss: 0.2358
2024-06-03 12:18:08 [INFO]: Epoch 029 - training loss: 0.3312, validation loss: 0.2300
2024-06-03 12:18:14 [INFO]: Epoch 030 - training loss: 0.3296, validation loss: 0.2296
2024-06-03 12:18:19 [INFO]: Epoch 031 - training loss: 0.3259, validation loss: 0.2360
2024-06-03 12:18:24 [INFO]: Epoch 032 - training loss: 0.3288, validation loss: 0.2335
2024-06-03 12:18:29 [INFO]: Epoch 033 - training loss: 0.3217, validation loss: 0.2339
2024-06-03 12:18:34 [INFO]: Epoch 034 - training loss: 0.3177, validation loss: 0.2356
2024-06-03 12:18:34 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:18:34 [INFO]: Finished training. The best model is from epoch#24.
2024-06-03 12:18:38 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_1/20240603_T121522/Transformer.pypots
2024-06-03 12:18:41 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_1/imputation.pkl
2024-06-03 12:18:41 [INFO]: Round1 - Transformer on BeijingAir: MAE=0.2438, MSE=0.2946, MRE=0.3327
2024-06-03 12:18:41 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 12:18:41 [INFO]: Using the given device: cuda:0
2024-06-03 12:18:41 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_2/20240603_T121841
2024-06-03 12:18:41 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_2/20240603_T121841/tensorboard
2024-06-03 12:18:41 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 12:18:41 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 12:18:46 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 12:18:51 [INFO]: Epoch 001 - training loss: 1.0532, validation loss: 0.3984
2024-06-03 12:18:56 [INFO]: Epoch 002 - training loss: 0.6555, validation loss: 0.3260
2024-06-03 12:19:01 [INFO]: Epoch 003 - training loss: 0.5670, validation loss: 0.2920
2024-06-03 12:19:06 [INFO]: Epoch 004 - training loss: 0.5213, validation loss: 0.2836
2024-06-03 12:19:11 [INFO]: Epoch 005 - training loss: 0.4984, validation loss: 0.2649
2024-06-03 12:19:17 [INFO]: Epoch 006 - training loss: 0.4727, validation loss: 0.2583
2024-06-03 12:19:22 [INFO]: Epoch 007 - training loss: 0.4562, validation loss: 0.2564
2024-06-03 12:19:27 [INFO]: Epoch 008 - training loss: 0.4474, validation loss: 0.2509
2024-06-03 12:19:32 [INFO]: Epoch 009 - training loss: 0.4342, validation loss: 0.2477
2024-06-03 12:19:38 [INFO]: Epoch 010 - training loss: 0.4257, validation loss: 0.2431
2024-06-03 12:19:43 [INFO]: Epoch 011 - training loss: 0.4139, validation loss: 0.2459
2024-06-03 12:19:48 [INFO]: Epoch 012 - training loss: 0.4048, validation loss: 0.2461
2024-06-03 12:19:53 [INFO]: Epoch 013 - training loss: 0.3967, validation loss: 0.2331
2024-06-03 12:19:58 [INFO]: Epoch 014 - training loss: 0.3889, validation loss: 0.2360
2024-06-03 12:20:03 [INFO]: Epoch 015 - training loss: 0.3863, validation loss: 0.2409
2024-06-03 12:20:09 [INFO]: Epoch 016 - training loss: 0.3806, validation loss: 0.2364
2024-06-03 12:20:14 [INFO]: Epoch 017 - training loss: 0.3752, validation loss: 0.2397
2024-06-03 12:20:19 [INFO]: Epoch 018 - training loss: 0.3736, validation loss: 0.2443
2024-06-03 12:20:25 [INFO]: Epoch 019 - training loss: 0.3707, validation loss: 0.2443
2024-06-03 12:20:30 [INFO]: Epoch 020 - training loss: 0.3597, validation loss: 0.2326
2024-06-03 12:20:35 [INFO]: Epoch 021 - training loss: 0.3612, validation loss: 0.2367
2024-06-03 12:20:40 [INFO]: Epoch 022 - training loss: 0.3601, validation loss: 0.2299
2024-06-03 12:20:45 [INFO]: Epoch 023 - training loss: 0.3532, validation loss: 0.2407
2024-06-03 12:20:50 [INFO]: Epoch 024 - training loss: 0.3510, validation loss: 0.2364
2024-06-03 12:20:55 [INFO]: Epoch 025 - training loss: 0.3509, validation loss: 0.2361
2024-06-03 12:21:01 [INFO]: Epoch 026 - training loss: 0.3449, validation loss: 0.2285
2024-06-03 12:21:06 [INFO]: Epoch 027 - training loss: 0.3404, validation loss: 0.2368
2024-06-03 12:21:10 [INFO]: Epoch 028 - training loss: 0.3448, validation loss: 0.2400
2024-06-03 12:21:15 [INFO]: Epoch 029 - training loss: 0.3374, validation loss: 0.2298
2024-06-03 12:21:20 [INFO]: Epoch 030 - training loss: 0.3364, validation loss: 0.2323
2024-06-03 12:21:24 [INFO]: Epoch 031 - training loss: 0.3348, validation loss: 0.2243
2024-06-03 12:21:29 [INFO]: Epoch 032 - training loss: 0.3257, validation loss: 0.2292
2024-06-03 12:21:34 [INFO]: Epoch 033 - training loss: 0.3195, validation loss: 0.2295
2024-06-03 12:21:38 [INFO]: Epoch 034 - training loss: 0.3221, validation loss: 0.2297
2024-06-03 12:21:43 [INFO]: Epoch 035 - training loss: 0.3213, validation loss: 0.2342
2024-06-03 12:21:48 [INFO]: Epoch 036 - training loss: 0.3203, validation loss: 0.2292
2024-06-03 12:21:52 [INFO]: Epoch 037 - training loss: 0.3154, validation loss: 0.2295
2024-06-03 12:21:57 [INFO]: Epoch 038 - training loss: 0.3133, validation loss: 0.2299
2024-06-03 12:22:02 [INFO]: Epoch 039 - training loss: 0.3101, validation loss: 0.2269
2024-06-03 12:22:06 [INFO]: Epoch 040 - training loss: 0.3117, validation loss: 0.2206
2024-06-03 12:22:11 [INFO]: Epoch 041 - training loss: 0.3035, validation loss: 0.2272
2024-06-03 12:22:15 [INFO]: Epoch 042 - training loss: 0.3026, validation loss: 0.2177
2024-06-03 12:22:20 [INFO]: Epoch 043 - training loss: 0.3035, validation loss: 0.2213
2024-06-03 12:22:24 [INFO]: Epoch 044 - training loss: 0.2997, validation loss: 0.2192
2024-06-03 12:22:29 [INFO]: Epoch 045 - training loss: 0.2990, validation loss: 0.2259
2024-06-03 12:22:34 [INFO]: Epoch 046 - training loss: 0.2951, validation loss: 0.2165
2024-06-03 12:22:38 [INFO]: Epoch 047 - training loss: 0.2959, validation loss: 0.2166
2024-06-03 12:22:43 [INFO]: Epoch 048 - training loss: 0.2914, validation loss: 0.2188
2024-06-03 12:22:47 [INFO]: Epoch 049 - training loss: 0.2964, validation loss: 0.2235
2024-06-03 12:22:51 [INFO]: Epoch 050 - training loss: 0.2945, validation loss: 0.2219
2024-06-03 12:22:56 [INFO]: Epoch 051 - training loss: 0.2936, validation loss: 0.2099
2024-06-03 12:23:00 [INFO]: Epoch 052 - training loss: 0.2930, validation loss: 0.2160
2024-06-03 12:23:05 [INFO]: Epoch 053 - training loss: 0.2943, validation loss: 0.2232
2024-06-03 12:23:09 [INFO]: Epoch 054 - training loss: 0.2900, validation loss: 0.2187
2024-06-03 12:23:14 [INFO]: Epoch 055 - training loss: 0.2872, validation loss: 0.2197
2024-06-03 12:23:18 [INFO]: Epoch 056 - training loss: 0.2870, validation loss: 0.2127
2024-06-03 12:23:22 [INFO]: Epoch 057 - training loss: 0.2858, validation loss: 0.2110
2024-06-03 12:23:26 [INFO]: Epoch 058 - training loss: 0.2834, validation loss: 0.2195
2024-06-03 12:23:31 [INFO]: Epoch 059 - training loss: 0.2880, validation loss: 0.2182
2024-06-03 12:23:35 [INFO]: Epoch 060 - training loss: 0.2850, validation loss: 0.2175
2024-06-03 12:23:40 [INFO]: Epoch 061 - training loss: 0.2820, validation loss: 0.2150
2024-06-03 12:23:40 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:23:40 [INFO]: Finished training. The best model is from epoch#51.
2024-06-03 12:23:42 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_2/20240603_T121841/Transformer.pypots
2024-06-03 12:23:44 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_2/imputation.pkl
2024-06-03 12:23:44 [INFO]: Round2 - Transformer on BeijingAir: MAE=0.2350, MSE=0.2824, MRE=0.3207
2024-06-03 12:23:44 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 12:23:44 [INFO]: Using the given device: cuda:0
2024-06-03 12:23:44 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_3/20240603_T122344
2024-06-03 12:23:44 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_3/20240603_T122344/tensorboard
2024-06-03 12:23:44 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 12:23:44 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 12:23:47 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 12:23:52 [INFO]: Epoch 001 - training loss: 1.0300, validation loss: 0.3868
2024-06-03 12:23:56 [INFO]: Epoch 002 - training loss: 0.6527, validation loss: 0.3319
2024-06-03 12:24:00 [INFO]: Epoch 003 - training loss: 0.5628, validation loss: 0.2896
2024-06-03 12:24:04 [INFO]: Epoch 004 - training loss: 0.5187, validation loss: 0.2781
2024-06-03 12:24:08 [INFO]: Epoch 005 - training loss: 0.4907, validation loss: 0.2672
2024-06-03 12:24:13 [INFO]: Epoch 006 - training loss: 0.4732, validation loss: 0.2710
2024-06-03 12:24:17 [INFO]: Epoch 007 - training loss: 0.4617, validation loss: 0.2562
2024-06-03 12:24:21 [INFO]: Epoch 008 - training loss: 0.4472, validation loss: 0.2500
2024-06-03 12:24:25 [INFO]: Epoch 009 - training loss: 0.4273, validation loss: 0.2514
2024-06-03 12:24:29 [INFO]: Epoch 010 - training loss: 0.4218, validation loss: 0.2484
2024-06-03 12:24:33 [INFO]: Epoch 011 - training loss: 0.4127, validation loss: 0.2411
2024-06-03 12:24:38 [INFO]: Epoch 012 - training loss: 0.4059, validation loss: 0.2449
2024-06-03 12:24:42 [INFO]: Epoch 013 - training loss: 0.3997, validation loss: 0.2384
2024-06-03 12:24:46 [INFO]: Epoch 014 - training loss: 0.3927, validation loss: 0.2377
2024-06-03 12:24:50 [INFO]: Epoch 015 - training loss: 0.3895, validation loss: 0.2388
2024-06-03 12:24:53 [INFO]: Epoch 016 - training loss: 0.3770, validation loss: 0.2376
2024-06-03 12:24:56 [INFO]: Epoch 017 - training loss: 0.3750, validation loss: 0.2415
2024-06-03 12:24:59 [INFO]: Epoch 018 - training loss: 0.3761, validation loss: 0.2424
2024-06-03 12:25:02 [INFO]: Epoch 019 - training loss: 0.3643, validation loss: 0.2424
2024-06-03 12:25:04 [INFO]: Epoch 020 - training loss: 0.3624, validation loss: 0.2407
2024-06-03 12:25:07 [INFO]: Epoch 021 - training loss: 0.3589, validation loss: 0.2393
2024-06-03 12:25:10 [INFO]: Epoch 022 - training loss: 0.3573, validation loss: 0.2383
2024-06-03 12:25:12 [INFO]: Epoch 023 - training loss: 0.3501, validation loss: 0.2371
2024-06-03 12:25:15 [INFO]: Epoch 024 - training loss: 0.3414, validation loss: 0.2409
2024-06-03 12:25:18 [INFO]: Epoch 025 - training loss: 0.3408, validation loss: 0.2347
2024-06-03 12:25:21 [INFO]: Epoch 026 - training loss: 0.3452, validation loss: 0.2412
2024-06-03 12:25:23 [INFO]: Epoch 027 - training loss: 0.3458, validation loss: 0.2308
2024-06-03 12:25:26 [INFO]: Epoch 028 - training loss: 0.3419, validation loss: 0.2344
2024-06-03 12:25:29 [INFO]: Epoch 029 - training loss: 0.3359, validation loss: 0.2316
2024-06-03 12:25:31 [INFO]: Epoch 030 - training loss: 0.3251, validation loss: 0.2337
2024-06-03 12:25:34 [INFO]: Epoch 031 - training loss: 0.3256, validation loss: 0.2268
2024-06-03 12:25:37 [INFO]: Epoch 032 - training loss: 0.3323, validation loss: 0.2370
2024-06-03 12:25:40 [INFO]: Epoch 033 - training loss: 0.3275, validation loss: 0.2312
2024-06-03 12:25:42 [INFO]: Epoch 034 - training loss: 0.3209, validation loss: 0.2340
2024-06-03 12:25:45 [INFO]: Epoch 035 - training loss: 0.3253, validation loss: 0.2280
2024-06-03 12:25:48 [INFO]: Epoch 036 - training loss: 0.3203, validation loss: 0.2269
2024-06-03 12:25:50 [INFO]: Epoch 037 - training loss: 0.3160, validation loss: 0.2312
2024-06-03 12:25:53 [INFO]: Epoch 038 - training loss: 0.3177, validation loss: 0.2269
2024-06-03 12:25:56 [INFO]: Epoch 039 - training loss: 0.3126, validation loss: 0.2307
2024-06-03 12:25:59 [INFO]: Epoch 040 - training loss: 0.3085, validation loss: 0.2319
2024-06-03 12:26:01 [INFO]: Epoch 041 - training loss: 0.3085, validation loss: 0.2236
2024-06-03 12:26:04 [INFO]: Epoch 042 - training loss: 0.3114, validation loss: 0.2230
2024-06-03 12:26:07 [INFO]: Epoch 043 - training loss: 0.3070, validation loss: 0.2248
2024-06-03 12:26:09 [INFO]: Epoch 044 - training loss: 0.3038, validation loss: 0.2282
2024-06-03 12:26:12 [INFO]: Epoch 045 - training loss: 0.3022, validation loss: 0.2337
2024-06-03 12:26:15 [INFO]: Epoch 046 - training loss: 0.3070, validation loss: 0.2264
2024-06-03 12:26:17 [INFO]: Epoch 047 - training loss: 0.3012, validation loss: 0.2220
2024-06-03 12:26:20 [INFO]: Epoch 048 - training loss: 0.2949, validation loss: 0.2254
2024-06-03 12:26:23 [INFO]: Epoch 049 - training loss: 0.2922, validation loss: 0.2222
2024-06-03 12:26:26 [INFO]: Epoch 050 - training loss: 0.2990, validation loss: 0.2259
2024-06-03 12:26:28 [INFO]: Epoch 051 - training loss: 0.2938, validation loss: 0.2163
2024-06-03 12:26:31 [INFO]: Epoch 052 - training loss: 0.2863, validation loss: 0.2188
2024-06-03 12:26:34 [INFO]: Epoch 053 - training loss: 0.2870, validation loss: 0.2160
2024-06-03 12:26:37 [INFO]: Epoch 054 - training loss: 0.2830, validation loss: 0.2203
2024-06-03 12:26:39 [INFO]: Epoch 055 - training loss: 0.2887, validation loss: 0.2221
2024-06-03 12:26:42 [INFO]: Epoch 056 - training loss: 0.2827, validation loss: 0.2103
2024-06-03 12:26:45 [INFO]: Epoch 057 - training loss: 0.2867, validation loss: 0.2201
2024-06-03 12:26:47 [INFO]: Epoch 058 - training loss: 0.2834, validation loss: 0.2137
2024-06-03 12:26:50 [INFO]: Epoch 059 - training loss: 0.2797, validation loss: 0.2178
2024-06-03 12:26:53 [INFO]: Epoch 060 - training loss: 0.2815, validation loss: 0.2122
2024-06-03 12:26:56 [INFO]: Epoch 061 - training loss: 0.2795, validation loss: 0.2143
2024-06-03 12:26:58 [INFO]: Epoch 062 - training loss: 0.2755, validation loss: 0.2116
2024-06-03 12:27:01 [INFO]: Epoch 063 - training loss: 0.2796, validation loss: 0.2145
2024-06-03 12:27:04 [INFO]: Epoch 064 - training loss: 0.2740, validation loss: 0.2116
2024-06-03 12:27:06 [INFO]: Epoch 065 - training loss: 0.2716, validation loss: 0.2076
2024-06-03 12:27:09 [INFO]: Epoch 066 - training loss: 0.2735, validation loss: 0.2146
2024-06-03 12:27:12 [INFO]: Epoch 067 - training loss: 0.2754, validation loss: 0.2076
2024-06-03 12:27:15 [INFO]: Epoch 068 - training loss: 0.2714, validation loss: 0.2082
2024-06-03 12:27:17 [INFO]: Epoch 069 - training loss: 0.2662, validation loss: 0.2119
2024-06-03 12:27:20 [INFO]: Epoch 070 - training loss: 0.2667, validation loss: 0.2116
2024-06-03 12:27:23 [INFO]: Epoch 071 - training loss: 0.2649, validation loss: 0.2097
2024-06-03 12:27:25 [INFO]: Epoch 072 - training loss: 0.2610, validation loss: 0.2061
2024-06-03 12:27:28 [INFO]: Epoch 073 - training loss: 0.2598, validation loss: 0.2069
2024-06-03 12:27:31 [INFO]: Epoch 074 - training loss: 0.2617, validation loss: 0.2139
2024-06-03 12:27:34 [INFO]: Epoch 075 - training loss: 0.2642, validation loss: 0.2126
2024-06-03 12:27:36 [INFO]: Epoch 076 - training loss: 0.2660, validation loss: 0.2074
2024-06-03 12:27:39 [INFO]: Epoch 077 - training loss: 0.2620, validation loss: 0.2063
2024-06-03 12:27:42 [INFO]: Epoch 078 - training loss: 0.2618, validation loss: 0.2088
2024-06-03 12:27:44 [INFO]: Epoch 079 - training loss: 0.2595, validation loss: 0.2045
2024-06-03 12:27:47 [INFO]: Epoch 080 - training loss: 0.2571, validation loss: 0.2061
2024-06-03 12:27:50 [INFO]: Epoch 081 - training loss: 0.2576, validation loss: 0.2044
2024-06-03 12:27:53 [INFO]: Epoch 082 - training loss: 0.2532, validation loss: 0.2022
2024-06-03 12:27:55 [INFO]: Epoch 083 - training loss: 0.2534, validation loss: 0.1970
2024-06-03 12:27:58 [INFO]: Epoch 084 - training loss: 0.2543, validation loss: 0.2035
2024-06-03 12:28:01 [INFO]: Epoch 085 - training loss: 0.2535, validation loss: 0.2064
2024-06-03 12:28:03 [INFO]: Epoch 086 - training loss: 0.2581, validation loss: 0.2099
2024-06-03 12:28:06 [INFO]: Epoch 087 - training loss: 0.2627, validation loss: 0.2058
2024-06-03 12:28:09 [INFO]: Epoch 088 - training loss: 0.2591, validation loss: 0.2048
2024-06-03 12:28:12 [INFO]: Epoch 089 - training loss: 0.2519, validation loss: 0.2055
2024-06-03 12:28:14 [INFO]: Epoch 090 - training loss: 0.2494, validation loss: 0.2041
2024-06-03 12:28:17 [INFO]: Epoch 091 - training loss: 0.2515, validation loss: 0.2028
2024-06-03 12:28:20 [INFO]: Epoch 092 - training loss: 0.2488, validation loss: 0.2031
2024-06-03 12:28:22 [INFO]: Epoch 093 - training loss: 0.2444, validation loss: 0.1982
2024-06-03 12:28:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:28:22 [INFO]: Finished training. The best model is from epoch#83.
2024-06-03 12:28:23 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_3/20240603_T122344/Transformer.pypots
2024-06-03 12:28:25 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_3/imputation.pkl
2024-06-03 12:28:25 [INFO]: Round3 - Transformer on BeijingAir: MAE=0.2229, MSE=0.2696, MRE=0.3043
2024-06-03 12:28:25 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 12:28:25 [INFO]: Using the given device: cuda:0
2024-06-03 12:28:25 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_4/20240603_T122825
2024-06-03 12:28:25 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_4/20240603_T122825/tensorboard
2024-06-03 12:28:25 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=128, n_heads=8, d_k=512
2024-06-03 12:28:25 [WARNING]: ⚠️ d_model is reset to 4096 = n_heads (8) * d_k (512)
2024-06-03 12:28:26 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 203,038,852
2024-06-03 12:28:29 [INFO]: Epoch 001 - training loss: 1.0372, validation loss: 0.3906
2024-06-03 12:28:32 [INFO]: Epoch 002 - training loss: 0.6466, validation loss: 0.3210
2024-06-03 12:28:34 [INFO]: Epoch 003 - training loss: 0.5631, validation loss: 0.2928
2024-06-03 12:28:37 [INFO]: Epoch 004 - training loss: 0.5165, validation loss: 0.2797
2024-06-03 12:28:40 [INFO]: Epoch 005 - training loss: 0.4945, validation loss: 0.2651
2024-06-03 12:28:42 [INFO]: Epoch 006 - training loss: 0.4711, validation loss: 0.2606
2024-06-03 12:28:45 [INFO]: Epoch 007 - training loss: 0.4558, validation loss: 0.2594
2024-06-03 12:28:48 [INFO]: Epoch 008 - training loss: 0.4467, validation loss: 0.2480
2024-06-03 12:28:51 [INFO]: Epoch 009 - training loss: 0.4268, validation loss: 0.2476
2024-06-03 12:28:53 [INFO]: Epoch 010 - training loss: 0.4190, validation loss: 0.2447
2024-06-03 12:28:56 [INFO]: Epoch 011 - training loss: 0.4198, validation loss: 0.2428
2024-06-03 12:28:59 [INFO]: Epoch 012 - training loss: 0.4073, validation loss: 0.2392
2024-06-03 12:29:01 [INFO]: Epoch 013 - training loss: 0.4037, validation loss: 0.2455
2024-06-03 12:29:04 [INFO]: Epoch 014 - training loss: 0.3909, validation loss: 0.2375
2024-06-03 12:29:07 [INFO]: Epoch 015 - training loss: 0.3877, validation loss: 0.2497
2024-06-03 12:29:10 [INFO]: Epoch 016 - training loss: 0.3884, validation loss: 0.2408
2024-06-03 12:29:12 [INFO]: Epoch 017 - training loss: 0.3754, validation loss: 0.2398
2024-06-03 12:29:15 [INFO]: Epoch 018 - training loss: 0.3698, validation loss: 0.2432
2024-06-03 12:29:18 [INFO]: Epoch 019 - training loss: 0.3669, validation loss: 0.2441
2024-06-03 12:29:20 [INFO]: Epoch 020 - training loss: 0.3673, validation loss: 0.2320
2024-06-03 12:29:23 [INFO]: Epoch 021 - training loss: 0.3582, validation loss: 0.2367
2024-06-03 12:29:26 [INFO]: Epoch 022 - training loss: 0.3537, validation loss: 0.2407
2024-06-03 12:29:29 [INFO]: Epoch 023 - training loss: 0.3484, validation loss: 0.2327
2024-06-03 12:29:31 [INFO]: Epoch 024 - training loss: 0.3455, validation loss: 0.2320
2024-06-03 12:29:34 [INFO]: Epoch 025 - training loss: 0.3440, validation loss: 0.2359
2024-06-03 12:29:37 [INFO]: Epoch 026 - training loss: 0.3420, validation loss: 0.2380
2024-06-03 12:29:39 [INFO]: Epoch 027 - training loss: 0.3438, validation loss: 0.2375
2024-06-03 12:29:42 [INFO]: Epoch 028 - training loss: 0.3416, validation loss: 0.2406
2024-06-03 12:29:45 [INFO]: Epoch 029 - training loss: 0.3336, validation loss: 0.2316
2024-06-03 12:29:48 [INFO]: Epoch 030 - training loss: 0.3330, validation loss: 0.2304
2024-06-03 12:29:50 [INFO]: Epoch 031 - training loss: 0.3336, validation loss: 0.2283
2024-06-03 12:29:53 [INFO]: Epoch 032 - training loss: 0.3273, validation loss: 0.2279
2024-06-03 12:29:56 [INFO]: Epoch 033 - training loss: 0.3242, validation loss: 0.2282
2024-06-03 12:29:58 [INFO]: Epoch 034 - training loss: 0.3281, validation loss: 0.2334
2024-06-03 12:30:01 [INFO]: Epoch 035 - training loss: 0.3220, validation loss: 0.2240
2024-06-03 12:30:04 [INFO]: Epoch 036 - training loss: 0.3214, validation loss: 0.2281
2024-06-03 12:30:07 [INFO]: Epoch 037 - training loss: 0.3151, validation loss: 0.2277
2024-06-03 12:30:09 [INFO]: Epoch 038 - training loss: 0.3133, validation loss: 0.2304
2024-06-03 12:30:12 [INFO]: Epoch 039 - training loss: 0.3113, validation loss: 0.2289
2024-06-03 12:30:15 [INFO]: Epoch 040 - training loss: 0.3099, validation loss: 0.2264
2024-06-03 12:30:17 [INFO]: Epoch 041 - training loss: 0.3092, validation loss: 0.2247
2024-06-03 12:30:20 [INFO]: Epoch 042 - training loss: 0.3136, validation loss: 0.2284
2024-06-03 12:30:23 [INFO]: Epoch 043 - training loss: 0.3081, validation loss: 0.2223
2024-06-03 12:30:26 [INFO]: Epoch 044 - training loss: 0.3042, validation loss: 0.2277
2024-06-03 12:30:28 [INFO]: Epoch 045 - training loss: 0.3046, validation loss: 0.2232
2024-06-03 12:30:31 [INFO]: Epoch 046 - training loss: 0.3012, validation loss: 0.2297
2024-06-03 12:30:34 [INFO]: Epoch 047 - training loss: 0.2948, validation loss: 0.2292
2024-06-03 12:30:36 [INFO]: Epoch 048 - training loss: 0.2955, validation loss: 0.2218
2024-06-03 12:30:39 [INFO]: Epoch 049 - training loss: 0.2948, validation loss: 0.2217
2024-06-03 12:30:42 [INFO]: Epoch 050 - training loss: 0.2918, validation loss: 0.2169
2024-06-03 12:30:45 [INFO]: Epoch 051 - training loss: 0.2902, validation loss: 0.2216
2024-06-03 12:30:47 [INFO]: Epoch 052 - training loss: 0.2887, validation loss: 0.2166
2024-06-03 12:30:50 [INFO]: Epoch 053 - training loss: 0.2862, validation loss: 0.2205
2024-06-03 12:30:53 [INFO]: Epoch 054 - training loss: 0.2902, validation loss: 0.2238
2024-06-03 12:30:56 [INFO]: Epoch 055 - training loss: 0.2847, validation loss: 0.2211
2024-06-03 12:30:58 [INFO]: Epoch 056 - training loss: 0.2840, validation loss: 0.2180
2024-06-03 12:31:01 [INFO]: Epoch 057 - training loss: 0.2851, validation loss: 0.2198
2024-06-03 12:31:04 [INFO]: Epoch 058 - training loss: 0.2864, validation loss: 0.2146
2024-06-03 12:31:06 [INFO]: Epoch 059 - training loss: 0.2822, validation loss: 0.2124
2024-06-03 12:31:09 [INFO]: Epoch 060 - training loss: 0.2866, validation loss: 0.2189
2024-06-03 12:31:12 [INFO]: Epoch 061 - training loss: 0.2816, validation loss: 0.2152
2024-06-03 12:31:15 [INFO]: Epoch 062 - training loss: 0.2758, validation loss: 0.2120
2024-06-03 12:31:17 [INFO]: Epoch 063 - training loss: 0.2740, validation loss: 0.2148
2024-06-03 12:31:20 [INFO]: Epoch 064 - training loss: 0.2746, validation loss: 0.2110
2024-06-03 12:31:23 [INFO]: Epoch 065 - training loss: 0.2743, validation loss: 0.2127
2024-06-03 12:31:25 [INFO]: Epoch 066 - training loss: 0.2728, validation loss: 0.2108
2024-06-03 12:31:28 [INFO]: Epoch 067 - training loss: 0.2710, validation loss: 0.2184
2024-06-03 12:31:31 [INFO]: Epoch 068 - training loss: 0.2712, validation loss: 0.2066
2024-06-03 12:31:34 [INFO]: Epoch 069 - training loss: 0.2721, validation loss: 0.2119
2024-06-03 12:31:36 [INFO]: Epoch 070 - training loss: 0.2692, validation loss: 0.2166
2024-06-03 12:31:39 [INFO]: Epoch 071 - training loss: 0.2712, validation loss: 0.2100
2024-06-03 12:31:42 [INFO]: Epoch 072 - training loss: 0.2740, validation loss: 0.2079
2024-06-03 12:31:44 [INFO]: Epoch 073 - training loss: 0.2665, validation loss: 0.2140
2024-06-03 12:31:47 [INFO]: Epoch 074 - training loss: 0.2641, validation loss: 0.2098
2024-06-03 12:31:50 [INFO]: Epoch 075 - training loss: 0.2585, validation loss: 0.2109
2024-06-03 12:31:53 [INFO]: Epoch 076 - training loss: 0.2608, validation loss: 0.2105
2024-06-03 12:31:55 [INFO]: Epoch 077 - training loss: 0.2646, validation loss: 0.2064
2024-06-03 12:31:58 [INFO]: Epoch 078 - training loss: 0.2620, validation loss: 0.2113
2024-06-03 12:32:01 [INFO]: Epoch 079 - training loss: 0.2590, validation loss: 0.2116
2024-06-03 12:32:03 [INFO]: Epoch 080 - training loss: 0.2623, validation loss: 0.2086
2024-06-03 12:32:06 [INFO]: Epoch 081 - training loss: 0.2606, validation loss: 0.2052
2024-06-03 12:32:09 [INFO]: Epoch 082 - training loss: 0.2559, validation loss: 0.2091
2024-06-03 12:32:12 [INFO]: Epoch 083 - training loss: 0.2562, validation loss: 0.2015
2024-06-03 12:32:14 [INFO]: Epoch 084 - training loss: 0.2550, validation loss: 0.2052
2024-06-03 12:32:17 [INFO]: Epoch 085 - training loss: 0.2579, validation loss: 0.2059
2024-06-03 12:32:20 [INFO]: Epoch 086 - training loss: 0.2541, validation loss: 0.2027
2024-06-03 12:32:22 [INFO]: Epoch 087 - training loss: 0.2509, validation loss: 0.2077
2024-06-03 12:32:25 [INFO]: Epoch 088 - training loss: 0.2545, validation loss: 0.2015
2024-06-03 12:32:28 [INFO]: Epoch 089 - training loss: 0.2547, validation loss: 0.2070
2024-06-03 12:32:31 [INFO]: Epoch 090 - training loss: 0.2551, validation loss: 0.2071
2024-06-03 12:32:33 [INFO]: Epoch 091 - training loss: 0.2539, validation loss: 0.2093
2024-06-03 12:32:36 [INFO]: Epoch 092 - training loss: 0.2552, validation loss: 0.2012
2024-06-03 12:32:39 [INFO]: Epoch 093 - training loss: 0.2524, validation loss: 0.2041
2024-06-03 12:32:42 [INFO]: Epoch 094 - training loss: 0.2484, validation loss: 0.2032
2024-06-03 12:32:44 [INFO]: Epoch 095 - training loss: 0.2498, validation loss: 0.1984
2024-06-03 12:32:47 [INFO]: Epoch 096 - training loss: 0.2467, validation loss: 0.2025
2024-06-03 12:32:50 [INFO]: Epoch 097 - training loss: 0.2493, validation loss: 0.1956
2024-06-03 12:32:52 [INFO]: Epoch 098 - training loss: 0.2448, validation loss: 0.2006
2024-06-03 12:32:55 [INFO]: Epoch 099 - training loss: 0.2448, validation loss: 0.2063
2024-06-03 12:32:58 [INFO]: Epoch 100 - training loss: 0.2437, validation loss: 0.2061
2024-06-03 12:32:58 [INFO]: Finished training. The best model is from epoch#97.
2024-06-03 12:32:59 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_4/20240603_T122825/Transformer.pypots
2024-06-03 12:33:00 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Transformer_BeijingAir/round_4/imputation.pkl
2024-06-03 12:33:00 [INFO]: Round4 - Transformer on BeijingAir: MAE=0.2252, MSE=0.2721, MRE=0.3073
2024-06-03 12:33:00 [INFO]: Done! Final results:
Averaged Transformer (203,038,852 params) on BeijingAir: MAE=0.2208 ± 0.00866988129001918, MSE=0.2667 ± 0.011473659891415121, MRE=0.2934 ± 0.01152067608669962, average inference time=0.36