2024-06-03 10:37:03 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 10:37:03 [INFO]: Using the given device: cuda:0
2024-06-03 10:37:04 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_0/20240603_T103704
2024-06-03 10:37:04 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_0/20240603_T103704/tensorboard
2024-06-03 10:37:05 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 10:37:17 [INFO]: Epoch 001 - training loss: 1.0076, validation loss: 0.4124
2024-06-03 10:37:26 [INFO]: Epoch 002 - training loss: 0.6621, validation loss: 0.3236
2024-06-03 10:37:35 [INFO]: Epoch 003 - training loss: 0.5622, validation loss: 0.2979
2024-06-03 10:37:43 [INFO]: Epoch 004 - training loss: 0.5186, validation loss: 0.2848
2024-06-03 10:37:52 [INFO]: Epoch 005 - training loss: 0.4978, validation loss: 0.2747
2024-06-03 10:38:01 [INFO]: Epoch 006 - training loss: 0.4859, validation loss: 0.2664
2024-06-03 10:38:09 [INFO]: Epoch 007 - training loss: 0.4684, validation loss: 0.2610
2024-06-03 10:38:18 [INFO]: Epoch 008 - training loss: 0.4513, validation loss: 0.2596
2024-06-03 10:38:27 [INFO]: Epoch 009 - training loss: 0.4388, validation loss: 0.2560
2024-06-03 10:38:35 [INFO]: Epoch 010 - training loss: 0.4263, validation loss: 0.2481
2024-06-03 10:38:43 [INFO]: Epoch 011 - training loss: 0.4174, validation loss: 0.2560
2024-06-03 10:38:52 [INFO]: Epoch 012 - training loss: 0.4107, validation loss: 0.2485
2024-06-03 10:39:01 [INFO]: Epoch 013 - training loss: 0.4073, validation loss: 0.2508
2024-06-03 10:39:09 [INFO]: Epoch 014 - training loss: 0.4028, validation loss: 0.2585
2024-06-03 10:39:18 [INFO]: Epoch 015 - training loss: 0.3964, validation loss: 0.2492
2024-06-03 10:39:26 [INFO]: Epoch 016 - training loss: 0.3955, validation loss: 0.2541
2024-06-03 10:39:34 [INFO]: Epoch 017 - training loss: 0.3832, validation loss: 0.2509
2024-06-03 10:39:43 [INFO]: Epoch 018 - training loss: 0.3804, validation loss: 0.2524
2024-06-03 10:39:52 [INFO]: Epoch 019 - training loss: 0.3769, validation loss: 0.2540
2024-06-03 10:40:00 [INFO]: Epoch 020 - training loss: 0.3717, validation loss: 0.2543
2024-06-03 10:40:00 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:40:00 [INFO]: Finished training. The best model is from epoch#10.
2024-06-03 10:40:00 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_0/20240603_T103704/Informer.pypots
2024-06-03 10:40:06 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_0/imputation.pkl
2024-06-03 10:40:06 [INFO]: Round0 - Informer on BeijingAir: MAE=0.2592, MSE=0.3160, MRE=0.3538
2024-06-03 10:40:06 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 10:40:06 [INFO]: Using the given device: cuda:0
2024-06-03 10:40:06 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_1/20240603_T104006
2024-06-03 10:40:06 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_1/20240603_T104006/tensorboard
2024-06-03 10:40:06 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 10:40:14 [INFO]: Epoch 001 - training loss: 1.0133, validation loss: 0.4456
2024-06-03 10:40:23 [INFO]: Epoch 002 - training loss: 0.6784, validation loss: 0.3426
2024-06-03 10:40:32 [INFO]: Epoch 003 - training loss: 0.5905, validation loss: 0.3080
2024-06-03 10:40:40 [INFO]: Epoch 004 - training loss: 0.5407, validation loss: 0.2908
2024-06-03 10:40:49 [INFO]: Epoch 005 - training loss: 0.5011, validation loss: 0.2753
2024-06-03 10:40:57 [INFO]: Epoch 006 - training loss: 0.4792, validation loss: 0.2665
2024-06-03 10:41:06 [INFO]: Epoch 007 - training loss: 0.4675, validation loss: 0.2627
2024-06-03 10:41:15 [INFO]: Epoch 008 - training loss: 0.4509, validation loss: 0.2641
2024-06-03 10:41:24 [INFO]: Epoch 009 - training loss: 0.4417, validation loss: 0.2572
2024-06-03 10:41:32 [INFO]: Epoch 010 - training loss: 0.4325, validation loss: 0.2491
2024-06-03 10:41:40 [INFO]: Epoch 011 - training loss: 0.4235, validation loss: 0.2591
2024-06-03 10:41:49 [INFO]: Epoch 012 - training loss: 0.4132, validation loss: 0.2573
2024-06-03 10:41:57 [INFO]: Epoch 013 - training loss: 0.4085, validation loss: 0.2540
2024-06-03 10:42:05 [INFO]: Epoch 014 - training loss: 0.4035, validation loss: 0.2482
2024-06-03 10:42:14 [INFO]: Epoch 015 - training loss: 0.3969, validation loss: 0.2538
2024-06-03 10:42:23 [INFO]: Epoch 016 - training loss: 0.3977, validation loss: 0.2516
2024-06-03 10:42:31 [INFO]: Epoch 017 - training loss: 0.3859, validation loss: 0.2482
2024-06-03 10:42:39 [INFO]: Epoch 018 - training loss: 0.3771, validation loss: 0.2468
2024-06-03 10:42:47 [INFO]: Epoch 019 - training loss: 0.3772, validation loss: 0.2426
2024-06-03 10:42:55 [INFO]: Epoch 020 - training loss: 0.3718, validation loss: 0.2433
2024-06-03 10:43:04 [INFO]: Epoch 021 - training loss: 0.3666, validation loss: 0.2377
2024-06-03 10:43:12 [INFO]: Epoch 022 - training loss: 0.3636, validation loss: 0.2469
2024-06-03 10:43:21 [INFO]: Epoch 023 - training loss: 0.3612, validation loss: 0.2422
2024-06-03 10:43:29 [INFO]: Epoch 024 - training loss: 0.3586, validation loss: 0.2499
2024-06-03 10:43:38 [INFO]: Epoch 025 - training loss: 0.3551, validation loss: 0.2467
2024-06-03 10:43:47 [INFO]: Epoch 026 - training loss: 0.3529, validation loss: 0.2427
2024-06-03 10:43:55 [INFO]: Epoch 027 - training loss: 0.3512, validation loss: 0.2412
2024-06-03 10:44:04 [INFO]: Epoch 028 - training loss: 0.3450, validation loss: 0.2406
2024-06-03 10:44:13 [INFO]: Epoch 029 - training loss: 0.3435, validation loss: 0.2424
2024-06-03 10:44:21 [INFO]: Epoch 030 - training loss: 0.3428, validation loss: 0.2386
2024-06-03 10:44:30 [INFO]: Epoch 031 - training loss: 0.3406, validation loss: 0.2390
2024-06-03 10:44:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:44:30 [INFO]: Finished training. The best model is from epoch#21.
2024-06-03 10:44:30 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_1/20240603_T104006/Informer.pypots
2024-06-03 10:44:36 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_1/imputation.pkl
2024-06-03 10:44:36 [INFO]: Round1 - Informer on BeijingAir: MAE=0.2411, MSE=0.3118, MRE=0.3291
2024-06-03 10:44:36 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 10:44:36 [INFO]: Using the given device: cuda:0
2024-06-03 10:44:36 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_2/20240603_T104436
2024-06-03 10:44:36 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_2/20240603_T104436/tensorboard
2024-06-03 10:44:36 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 10:44:45 [INFO]: Epoch 001 - training loss: 0.9736, validation loss: 0.3970
2024-06-03 10:44:54 [INFO]: Epoch 002 - training loss: 0.6421, validation loss: 0.3308
2024-06-03 10:45:03 [INFO]: Epoch 003 - training loss: 0.5607, validation loss: 0.2919
2024-06-03 10:45:11 [INFO]: Epoch 004 - training loss: 0.5178, validation loss: 0.2793
2024-06-03 10:45:19 [INFO]: Epoch 005 - training loss: 0.4934, validation loss: 0.2701
2024-06-03 10:45:28 [INFO]: Epoch 006 - training loss: 0.4716, validation loss: 0.2618
2024-06-03 10:45:37 [INFO]: Epoch 007 - training loss: 0.4549, validation loss: 0.2605
2024-06-03 10:45:45 [INFO]: Epoch 008 - training loss: 0.4428, validation loss: 0.2636
2024-06-03 10:45:55 [INFO]: Epoch 009 - training loss: 0.4373, validation loss: 0.2532
2024-06-03 10:46:03 [INFO]: Epoch 010 - training loss: 0.4246, validation loss: 0.2563
2024-06-03 10:46:12 [INFO]: Epoch 011 - training loss: 0.4181, validation loss: 0.2514
2024-06-03 10:46:19 [INFO]: Epoch 012 - training loss: 0.4164, validation loss: 0.2597
2024-06-03 10:46:27 [INFO]: Epoch 013 - training loss: 0.4098, validation loss: 0.2602
2024-06-03 10:46:35 [INFO]: Epoch 014 - training loss: 0.4090, validation loss: 0.2545
2024-06-03 10:46:43 [INFO]: Epoch 015 - training loss: 0.3957, validation loss: 0.2534
2024-06-03 10:46:50 [INFO]: Epoch 016 - training loss: 0.3900, validation loss: 0.2587
2024-06-03 10:46:58 [INFO]: Epoch 017 - training loss: 0.3938, validation loss: 0.2576
2024-06-03 10:47:05 [INFO]: Epoch 018 - training loss: 0.3931, validation loss: 0.2517
2024-06-03 10:47:13 [INFO]: Epoch 019 - training loss: 0.3771, validation loss: 0.2472
2024-06-03 10:47:20 [INFO]: Epoch 020 - training loss: 0.3736, validation loss: 0.2467
2024-06-03 10:47:28 [INFO]: Epoch 021 - training loss: 0.3669, validation loss: 0.2432
2024-06-03 10:47:35 [INFO]: Epoch 022 - training loss: 0.3627, validation loss: 0.2468
2024-06-03 10:47:43 [INFO]: Epoch 023 - training loss: 0.3640, validation loss: 0.2391
2024-06-03 10:47:50 [INFO]: Epoch 024 - training loss: 0.3640, validation loss: 0.2474
2024-06-03 10:47:57 [INFO]: Epoch 025 - training loss: 0.3526, validation loss: 0.2420
2024-06-03 10:48:05 [INFO]: Epoch 026 - training loss: 0.3546, validation loss: 0.2474
2024-06-03 10:48:12 [INFO]: Epoch 027 - training loss: 0.3529, validation loss: 0.2456
2024-06-03 10:48:20 [INFO]: Epoch 028 - training loss: 0.3544, validation loss: 0.2427
2024-06-03 10:48:28 [INFO]: Epoch 029 - training loss: 0.3419, validation loss: 0.2466
2024-06-03 10:48:35 [INFO]: Epoch 030 - training loss: 0.3419, validation loss: 0.2447
2024-06-03 10:48:43 [INFO]: Epoch 031 - training loss: 0.3368, validation loss: 0.2420
2024-06-03 10:48:51 [INFO]: Epoch 032 - training loss: 0.3360, validation loss: 0.2464
2024-06-03 10:48:58 [INFO]: Epoch 033 - training loss: 0.3408, validation loss: 0.2411
2024-06-03 10:48:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:48:58 [INFO]: Finished training. The best model is from epoch#23.
2024-06-03 10:48:58 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_2/20240603_T104436/Informer.pypots
2024-06-03 10:49:03 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_2/imputation.pkl
2024-06-03 10:49:03 [INFO]: Round2 - Informer on BeijingAir: MAE=0.2475, MSE=0.3098, MRE=0.3378
2024-06-03 10:49:03 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 10:49:03 [INFO]: Using the given device: cuda:0
2024-06-03 10:49:03 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_3/20240603_T104903
2024-06-03 10:49:03 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_3/20240603_T104903/tensorboard
2024-06-03 10:49:03 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 10:49:11 [INFO]: Epoch 001 - training loss: 0.9763, validation loss: 0.4023
2024-06-03 10:49:19 [INFO]: Epoch 002 - training loss: 0.6485, validation loss: 0.3263
2024-06-03 10:49:26 [INFO]: Epoch 003 - training loss: 0.5598, validation loss: 0.2987
2024-06-03 10:49:34 [INFO]: Epoch 004 - training loss: 0.5198, validation loss: 0.2793
2024-06-03 10:49:42 [INFO]: Epoch 005 - training loss: 0.4931, validation loss: 0.2730
2024-06-03 10:49:49 [INFO]: Epoch 006 - training loss: 0.4740, validation loss: 0.2715
2024-06-03 10:49:57 [INFO]: Epoch 007 - training loss: 0.4642, validation loss: 0.2706
2024-06-03 10:50:05 [INFO]: Epoch 008 - training loss: 0.4492, validation loss: 0.2612
2024-06-03 10:50:12 [INFO]: Epoch 009 - training loss: 0.4435, validation loss: 0.2573
2024-06-03 10:50:19 [INFO]: Epoch 010 - training loss: 0.4284, validation loss: 0.2513
2024-06-03 10:50:26 [INFO]: Epoch 011 - training loss: 0.4229, validation loss: 0.2545
2024-06-03 10:50:32 [INFO]: Epoch 012 - training loss: 0.4102, validation loss: 0.2610
2024-06-03 10:50:39 [INFO]: Epoch 013 - training loss: 0.4100, validation loss: 0.2525
2024-06-03 10:50:46 [INFO]: Epoch 014 - training loss: 0.4131, validation loss: 0.2541
2024-06-03 10:50:53 [INFO]: Epoch 015 - training loss: 0.3984, validation loss: 0.2545
2024-06-03 10:51:00 [INFO]: Epoch 016 - training loss: 0.3942, validation loss: 0.2435
2024-06-03 10:51:06 [INFO]: Epoch 017 - training loss: 0.3891, validation loss: 0.2536
2024-06-03 10:51:13 [INFO]: Epoch 018 - training loss: 0.3869, validation loss: 0.2495
2024-06-03 10:51:20 [INFO]: Epoch 019 - training loss: 0.3781, validation loss: 0.2528
2024-06-03 10:51:27 [INFO]: Epoch 020 - training loss: 0.3781, validation loss: 0.2541
2024-06-03 10:51:34 [INFO]: Epoch 021 - training loss: 0.3795, validation loss: 0.2465
2024-06-03 10:51:41 [INFO]: Epoch 022 - training loss: 0.3687, validation loss: 0.2431
2024-06-03 10:51:48 [INFO]: Epoch 023 - training loss: 0.3692, validation loss: 0.2437
2024-06-03 10:51:54 [INFO]: Epoch 024 - training loss: 0.3616, validation loss: 0.2457
2024-06-03 10:52:01 [INFO]: Epoch 025 - training loss: 0.3562, validation loss: 0.2467
2024-06-03 10:52:08 [INFO]: Epoch 026 - training loss: 0.3548, validation loss: 0.2491
2024-06-03 10:52:14 [INFO]: Epoch 027 - training loss: 0.3543, validation loss: 0.2386
2024-06-03 10:52:19 [INFO]: Epoch 028 - training loss: 0.3509, validation loss: 0.2430
2024-06-03 10:52:23 [INFO]: Epoch 029 - training loss: 0.3474, validation loss: 0.2400
2024-06-03 10:52:28 [INFO]: Epoch 030 - training loss: 0.3446, validation loss: 0.2513
2024-06-03 10:52:33 [INFO]: Epoch 031 - training loss: 0.3461, validation loss: 0.2401
2024-06-03 10:52:38 [INFO]: Epoch 032 - training loss: 0.3425, validation loss: 0.2452
2024-06-03 10:52:43 [INFO]: Epoch 033 - training loss: 0.3366, validation loss: 0.2389
2024-06-03 10:52:48 [INFO]: Epoch 034 - training loss: 0.3396, validation loss: 0.2432
2024-06-03 10:52:53 [INFO]: Epoch 035 - training loss: 0.3314, validation loss: 0.2433
2024-06-03 10:52:58 [INFO]: Epoch 036 - training loss: 0.3247, validation loss: 0.2409
2024-06-03 10:53:03 [INFO]: Epoch 037 - training loss: 0.3244, validation loss: 0.2324
2024-06-03 10:53:08 [INFO]: Epoch 038 - training loss: 0.3212, validation loss: 0.2317
2024-06-03 10:53:13 [INFO]: Epoch 039 - training loss: 0.3194, validation loss: 0.2300
2024-06-03 10:53:18 [INFO]: Epoch 040 - training loss: 0.3297, validation loss: 0.2344
2024-06-03 10:53:23 [INFO]: Epoch 041 - training loss: 0.3204, validation loss: 0.2394
2024-06-03 10:53:27 [INFO]: Epoch 042 - training loss: 0.3159, validation loss: 0.2337
2024-06-03 10:53:33 [INFO]: Epoch 043 - training loss: 0.3143, validation loss: 0.2314
2024-06-03 10:53:37 [INFO]: Epoch 044 - training loss: 0.3135, validation loss: 0.2350
2024-06-03 10:53:42 [INFO]: Epoch 045 - training loss: 0.3095, validation loss: 0.2294
2024-06-03 10:53:48 [INFO]: Epoch 046 - training loss: 0.3110, validation loss: 0.2248
2024-06-03 10:53:52 [INFO]: Epoch 047 - training loss: 0.3126, validation loss: 0.2246
2024-06-03 10:53:57 [INFO]: Epoch 048 - training loss: 0.3148, validation loss: 0.2356
2024-06-03 10:54:02 [INFO]: Epoch 049 - training loss: 0.3090, validation loss: 0.2335
2024-06-03 10:54:07 [INFO]: Epoch 050 - training loss: 0.3044, validation loss: 0.2283
2024-06-03 10:54:12 [INFO]: Epoch 051 - training loss: 0.3024, validation loss: 0.2252
2024-06-03 10:54:17 [INFO]: Epoch 052 - training loss: 0.3053, validation loss: 0.2316
2024-06-03 10:54:22 [INFO]: Epoch 053 - training loss: 0.3079, validation loss: 0.2239
2024-06-03 10:54:27 [INFO]: Epoch 054 - training loss: 0.3020, validation loss: 0.2325
2024-06-03 10:54:32 [INFO]: Epoch 055 - training loss: 0.2988, validation loss: 0.2274
2024-06-03 10:54:37 [INFO]: Epoch 056 - training loss: 0.3012, validation loss: 0.2244
2024-06-03 10:54:42 [INFO]: Epoch 057 - training loss: 0.3007, validation loss: 0.2240
2024-06-03 10:54:47 [INFO]: Epoch 058 - training loss: 0.2990, validation loss: 0.2243
2024-06-03 10:54:52 [INFO]: Epoch 059 - training loss: 0.2964, validation loss: 0.2195
2024-06-03 10:54:56 [INFO]: Epoch 060 - training loss: 0.2921, validation loss: 0.2227
2024-06-03 10:55:01 [INFO]: Epoch 061 - training loss: 0.2919, validation loss: 0.2217
2024-06-03 10:55:05 [INFO]: Epoch 062 - training loss: 0.2907, validation loss: 0.2269
2024-06-03 10:55:10 [INFO]: Epoch 063 - training loss: 0.2888, validation loss: 0.2203
2024-06-03 10:55:15 [INFO]: Epoch 064 - training loss: 0.2940, validation loss: 0.2249
2024-06-03 10:55:20 [INFO]: Epoch 065 - training loss: 0.2944, validation loss: 0.2208
2024-06-03 10:55:25 [INFO]: Epoch 066 - training loss: 0.2936, validation loss: 0.2181
2024-06-03 10:55:30 [INFO]: Epoch 067 - training loss: 0.2915, validation loss: 0.2236
2024-06-03 10:55:34 [INFO]: Epoch 068 - training loss: 0.2837, validation loss: 0.2241
2024-06-03 10:55:39 [INFO]: Epoch 069 - training loss: 0.2871, validation loss: 0.2242
2024-06-03 10:55:43 [INFO]: Epoch 070 - training loss: 0.2848, validation loss: 0.2204
2024-06-03 10:55:47 [INFO]: Epoch 071 - training loss: 0.2863, validation loss: 0.2159
2024-06-03 10:55:51 [INFO]: Epoch 072 - training loss: 0.2819, validation loss: 0.2203
2024-06-03 10:55:55 [INFO]: Epoch 073 - training loss: 0.2811, validation loss: 0.2177
2024-06-03 10:55:59 [INFO]: Epoch 074 - training loss: 0.2808, validation loss: 0.2191
2024-06-03 10:56:04 [INFO]: Epoch 075 - training loss: 0.2832, validation loss: 0.2220
2024-06-03 10:56:08 [INFO]: Epoch 076 - training loss: 0.2857, validation loss: 0.2227
2024-06-03 10:56:12 [INFO]: Epoch 077 - training loss: 0.2876, validation loss: 0.2168
2024-06-03 10:56:16 [INFO]: Epoch 078 - training loss: 0.2831, validation loss: 0.2221
2024-06-03 10:56:21 [INFO]: Epoch 079 - training loss: 0.2821, validation loss: 0.2175
2024-06-03 10:56:25 [INFO]: Epoch 080 - training loss: 0.2771, validation loss: 0.2170
2024-06-03 10:56:29 [INFO]: Epoch 081 - training loss: 0.2789, validation loss: 0.2190
2024-06-03 10:56:29 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 10:56:29 [INFO]: Finished training. The best model is from epoch#71.
2024-06-03 10:56:29 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_3/20240603_T104903/Informer.pypots
2024-06-03 10:56:32 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_3/imputation.pkl
2024-06-03 10:56:32 [INFO]: Round3 - Informer on BeijingAir: MAE=0.2276, MSE=0.2876, MRE=0.3107
2024-06-03 10:56:32 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 10:56:32 [INFO]: Using the given device: cuda:0
2024-06-03 10:56:32 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_4/20240603_T105632
2024-06-03 10:56:32 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_4/20240603_T105632/tensorboard
2024-06-03 10:56:32 [INFO]: Informer initialized with the given hyperparameters, the number of trainable parameters: 6,706,308
2024-06-03 10:56:36 [INFO]: Epoch 001 - training loss: 1.0271, validation loss: 0.4260
2024-06-03 10:56:40 [INFO]: Epoch 002 - training loss: 0.6889, validation loss: 0.3440
2024-06-03 10:56:45 [INFO]: Epoch 003 - training loss: 0.5926, validation loss: 0.3064
2024-06-03 10:56:49 [INFO]: Epoch 004 - training loss: 0.5350, validation loss: 0.2884
2024-06-03 10:56:53 [INFO]: Epoch 005 - training loss: 0.5000, validation loss: 0.2766
2024-06-03 10:56:57 [INFO]: Epoch 006 - training loss: 0.4755, validation loss: 0.2609
2024-06-03 10:57:01 [INFO]: Epoch 007 - training loss: 0.4586, validation loss: 0.2604
2024-06-03 10:57:05 [INFO]: Epoch 008 - training loss: 0.4472, validation loss: 0.2590
2024-06-03 10:57:10 [INFO]: Epoch 009 - training loss: 0.4373, validation loss: 0.2487
2024-06-03 10:57:14 [INFO]: Epoch 010 - training loss: 0.4324, validation loss: 0.2518
2024-06-03 10:57:18 [INFO]: Epoch 011 - training loss: 0.4211, validation loss: 0.2492
2024-06-03 10:57:22 [INFO]: Epoch 012 - training loss: 0.4160, validation loss: 0.2536
2024-06-03 10:57:26 [INFO]: Epoch 013 - training loss: 0.4073, validation loss: 0.2483
2024-06-03 10:57:30 [INFO]: Epoch 014 - training loss: 0.4025, validation loss: 0.2454
2024-06-03 10:57:34 [INFO]: Epoch 015 - training loss: 0.4008, validation loss: 0.2509
2024-06-03 10:57:38 [INFO]: Epoch 016 - training loss: 0.3899, validation loss: 0.2445
2024-06-03 10:57:41 [INFO]: Epoch 017 - training loss: 0.3848, validation loss: 0.2461
2024-06-03 10:57:44 [INFO]: Epoch 018 - training loss: 0.3838, validation loss: 0.2505
2024-06-03 10:57:48 [INFO]: Epoch 019 - training loss: 0.3819, validation loss: 0.2507
2024-06-03 10:57:51 [INFO]: Epoch 020 - training loss: 0.3817, validation loss: 0.2434
2024-06-03 10:57:54 [INFO]: Epoch 021 - training loss: 0.3744, validation loss: 0.2526
2024-06-03 10:57:57 [INFO]: Epoch 022 - training loss: 0.3710, validation loss: 0.2425
2024-06-03 10:58:00 [INFO]: Epoch 023 - training loss: 0.3673, validation loss: 0.2440
2024-06-03 10:58:03 [INFO]: Epoch 024 - training loss: 0.3602, validation loss: 0.2388
2024-06-03 10:58:06 [INFO]: Epoch 025 - training loss: 0.3569, validation loss: 0.2457
2024-06-03 10:58:09 [INFO]: Epoch 026 - training loss: 0.3525, validation loss: 0.2393
2024-06-03 10:58:13 [INFO]: Epoch 027 - training loss: 0.3474, validation loss: 0.2426
2024-06-03 10:58:16 [INFO]: Epoch 028 - training loss: 0.3421, validation loss: 0.2356
2024-06-03 10:58:19 [INFO]: Epoch 029 - training loss: 0.3391, validation loss: 0.2367
2024-06-03 10:58:22 [INFO]: Epoch 030 - training loss: 0.3386, validation loss: 0.2367
2024-06-03 10:58:25 [INFO]: Epoch 031 - training loss: 0.3372, validation loss: 0.2421
2024-06-03 10:58:28 [INFO]: Epoch 032 - training loss: 0.3369, validation loss: 0.2383
2024-06-03 10:58:31 [INFO]: Epoch 033 - training loss: 0.3327, validation loss: 0.2352
2024-06-03 10:58:34 [INFO]: Epoch 034 - training loss: 0.3354, validation loss: 0.2409
2024-06-03 10:58:37 [INFO]: Epoch 035 - training loss: 0.3360, validation loss: 0.2358
2024-06-03 10:58:40 [INFO]: Epoch 036 - training loss: 0.3338, validation loss: 0.2367
2024-06-03 10:58:44 [INFO]: Epoch 037 - training loss: 0.3256, validation loss: 0.2329
2024-06-03 10:58:47 [INFO]: Epoch 038 - training loss: 0.3218, validation loss: 0.2331
2024-06-03 10:58:50 [INFO]: Epoch 039 - training loss: 0.3195, validation loss: 0.2298
2024-06-03 10:58:53 [INFO]: Epoch 040 - training loss: 0.3208, validation loss: 0.2272
2024-06-03 10:58:56 [INFO]: Epoch 041 - training loss: 0.3191, validation loss: 0.2337
2024-06-03 10:58:59 [INFO]: Epoch 042 - training loss: 0.3161, validation loss: 0.2277
2024-06-03 10:59:02 [INFO]: Epoch 043 - training loss: 0.3176, validation loss: 0.2284
2024-06-03 10:59:05 [INFO]: Epoch 044 - training loss: 0.3162, validation loss: 0.2307
2024-06-03 10:59:08 [INFO]: Epoch 045 - training loss: 0.3163, validation loss: 0.2282
2024-06-03 10:59:12 [INFO]: Epoch 046 - training loss: 0.3131, validation loss: 0.2297
2024-06-03 10:59:15 [INFO]: Epoch 047 - training loss: 0.3112, validation loss: 0.2277
2024-06-03 10:59:18 [INFO]: Epoch 048 - training loss: 0.3077, validation loss: 0.2288
2024-06-03 10:59:21 [INFO]: Epoch 049 - training loss: 0.3054, validation loss: 0.2317
2024-06-03 10:59:24 [INFO]: Epoch 050 - training loss: 0.3034, validation loss: 0.2259
2024-06-03 10:59:27 [INFO]: Epoch 051 - training loss: 0.3019, validation loss: 0.2228
2024-06-03 10:59:30 [INFO]: Epoch 052 - training loss: 0.3025, validation loss: 0.2227
2024-06-03 10:59:33 [INFO]: Epoch 053 - training loss: 0.3010, validation loss: 0.2300
2024-06-03 10:59:36 [INFO]: Epoch 054 - training loss: 0.2998, validation loss: 0.2187
2024-06-03 10:59:39 [INFO]: Epoch 055 - training loss: 0.3030, validation loss: 0.2208
2024-06-03 10:59:42 [INFO]: Epoch 056 - training loss: 0.2976, validation loss: 0.2194
2024-06-03 10:59:45 [INFO]: Epoch 057 - training loss: 0.2959, validation loss: 0.2215
2024-06-03 10:59:48 [INFO]: Epoch 058 - training loss: 0.2929, validation loss: 0.2196
2024-06-03 10:59:51 [INFO]: Epoch 059 - training loss: 0.2908, validation loss: 0.2152
2024-06-03 10:59:53 [INFO]: Epoch 060 - training loss: 0.2965, validation loss: 0.2206
2024-06-03 10:59:56 [INFO]: Epoch 061 - training loss: 0.2942, validation loss: 0.2230
2024-06-03 10:59:59 [INFO]: Epoch 062 - training loss: 0.2937, validation loss: 0.2178
2024-06-03 11:00:02 [INFO]: Epoch 063 - training loss: 0.2922, validation loss: 0.2252
2024-06-03 11:00:05 [INFO]: Epoch 064 - training loss: 0.2904, validation loss: 0.2230
2024-06-03 11:00:09 [INFO]: Epoch 065 - training loss: 0.2883, validation loss: 0.2209
2024-06-03 11:00:12 [INFO]: Epoch 066 - training loss: 0.2875, validation loss: 0.2255
2024-06-03 11:00:15 [INFO]: Epoch 067 - training loss: 0.2856, validation loss: 0.2166
2024-06-03 11:00:18 [INFO]: Epoch 068 - training loss: 0.2892, validation loss: 0.2216
2024-06-03 11:00:21 [INFO]: Epoch 069 - training loss: 0.2852, validation loss: 0.2152
2024-06-03 11:00:25 [INFO]: Epoch 070 - training loss: 0.2849, validation loss: 0.2222
2024-06-03 11:00:28 [INFO]: Epoch 071 - training loss: 0.2848, validation loss: 0.2191
2024-06-03 11:00:31 [INFO]: Epoch 072 - training loss: 0.2821, validation loss: 0.2158
2024-06-03 11:00:34 [INFO]: Epoch 073 - training loss: 0.2806, validation loss: 0.2241
2024-06-03 11:00:38 [INFO]: Epoch 074 - training loss: 0.2814, validation loss: 0.2143
2024-06-03 11:00:41 [INFO]: Epoch 075 - training loss: 0.2798, validation loss: 0.2171
2024-06-03 11:00:44 [INFO]: Epoch 076 - training loss: 0.2858, validation loss: 0.2240
2024-06-03 11:00:47 [INFO]: Epoch 077 - training loss: 0.2847, validation loss: 0.2174
2024-06-03 11:00:50 [INFO]: Epoch 078 - training loss: 0.2823, validation loss: 0.2203
2024-06-03 11:00:53 [INFO]: Epoch 079 - training loss: 0.2799, validation loss: 0.2234
2024-06-03 11:00:56 [INFO]: Epoch 080 - training loss: 0.2788, validation loss: 0.2186
2024-06-03 11:00:59 [INFO]: Epoch 081 - training loss: 0.2765, validation loss: 0.2176
2024-06-03 11:01:02 [INFO]: Epoch 082 - training loss: 0.2787, validation loss: 0.2135
2024-06-03 11:01:06 [INFO]: Epoch 083 - training loss: 0.2779, validation loss: 0.2202
2024-06-03 11:01:09 [INFO]: Epoch 084 - training loss: 0.2767, validation loss: 0.2131
2024-06-03 11:01:12 [INFO]: Epoch 085 - training loss: 0.2774, validation loss: 0.2133
2024-06-03 11:01:15 [INFO]: Epoch 086 - training loss: 0.2793, validation loss: 0.2157
2024-06-03 11:01:19 [INFO]: Epoch 087 - training loss: 0.2776, validation loss: 0.2173
2024-06-03 11:01:22 [INFO]: Epoch 088 - training loss: 0.2741, validation loss: 0.2160
2024-06-03 11:01:24 [INFO]: Epoch 089 - training loss: 0.2734, validation loss: 0.2156
2024-06-03 11:01:27 [INFO]: Epoch 090 - training loss: 0.2718, validation loss: 0.2127
2024-06-03 11:01:30 [INFO]: Epoch 091 - training loss: 0.2710, validation loss: 0.2205
2024-06-03 11:01:34 [INFO]: Epoch 092 - training loss: 0.2710, validation loss: 0.2173
2024-06-03 11:01:37 [INFO]: Epoch 093 - training loss: 0.2711, validation loss: 0.2113
2024-06-03 11:01:40 [INFO]: Epoch 094 - training loss: 0.2680, validation loss: 0.2167
2024-06-03 11:01:43 [INFO]: Epoch 095 - training loss: 0.2686, validation loss: 0.2116
2024-06-03 11:01:46 [INFO]: Epoch 096 - training loss: 0.2688, validation loss: 0.2128
2024-06-03 11:01:49 [INFO]: Epoch 097 - training loss: 0.2697, validation loss: 0.2128
2024-06-03 11:01:52 [INFO]: Epoch 098 - training loss: 0.2699, validation loss: 0.2127
2024-06-03 11:01:56 [INFO]: Epoch 099 - training loss: 0.2683, validation loss: 0.2115
2024-06-03 11:01:58 [INFO]: Epoch 100 - training loss: 0.2633, validation loss: 0.2109
2024-06-03 11:01:58 [INFO]: Finished training. The best model is from epoch#100.
2024-06-03 11:01:59 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_4/20240603_T105632/Informer.pypots
2024-06-03 11:02:00 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/Informer_BeijingAir/round_4/imputation.pkl
2024-06-03 11:02:00 [INFO]: Round4 - Informer on BeijingAir: MAE=0.2253, MSE=0.2906, MRE=0.3075
2024-06-03 11:02:00 [INFO]: Done! Final results:
Averaged Informer (6,706,308 params) on BeijingAir: MAE=0.2315 ± 0.013405558083580205, MSE=0.2945 ± 0.012911053883810803, MRE=0.3076 ± 0.017813518694905196, average inference time=0.89