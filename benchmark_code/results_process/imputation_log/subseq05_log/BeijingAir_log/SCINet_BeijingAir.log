2024-06-03 12:03:08 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-06-03 12:03:08 [INFO]: Using the given device: cuda:0
2024-06-03 12:03:09 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_0/20240603_T120309
2024-06-03 12:03:09 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_0/20240603_T120309/tensorboard
2024-06-03 12:03:10 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 12:03:22 [INFO]: Epoch 001 - training loss: 1.4004, validation loss: 0.9027
2024-06-03 12:03:28 [INFO]: Epoch 002 - training loss: 1.0311, validation loss: 0.5441
2024-06-03 12:03:33 [INFO]: Epoch 003 - training loss: 0.8275, validation loss: 0.4305
2024-06-03 12:03:38 [INFO]: Epoch 004 - training loss: 0.6991, validation loss: 0.3735
2024-06-03 12:03:44 [INFO]: Epoch 005 - training loss: 0.6248, validation loss: 0.3523
2024-06-03 12:03:49 [INFO]: Epoch 006 - training loss: 0.5785, validation loss: 0.3350
2024-06-03 12:03:54 [INFO]: Epoch 007 - training loss: 0.5483, validation loss: 0.3273
2024-06-03 12:03:59 [INFO]: Epoch 008 - training loss: 0.5178, validation loss: 0.3206
2024-06-03 12:04:04 [INFO]: Epoch 009 - training loss: 0.5009, validation loss: 0.3142
2024-06-03 12:04:10 [INFO]: Epoch 010 - training loss: 0.4908, validation loss: 0.3113
2024-06-03 12:04:15 [INFO]: Epoch 011 - training loss: 0.4718, validation loss: 0.3049
2024-06-03 12:04:20 [INFO]: Epoch 012 - training loss: 0.4575, validation loss: 0.3054
2024-06-03 12:04:25 [INFO]: Epoch 013 - training loss: 0.4485, validation loss: 0.3016
2024-06-03 12:04:31 [INFO]: Epoch 014 - training loss: 0.4385, validation loss: 0.2982
2024-06-03 12:04:36 [INFO]: Epoch 015 - training loss: 0.4294, validation loss: 0.2995
2024-06-03 12:04:41 [INFO]: Epoch 016 - training loss: 0.4232, validation loss: 0.2923
2024-06-03 12:04:47 [INFO]: Epoch 017 - training loss: 0.4174, validation loss: 0.3023
2024-06-03 12:04:53 [INFO]: Epoch 018 - training loss: 0.4140, validation loss: 0.2996
2024-06-03 12:04:58 [INFO]: Epoch 019 - training loss: 0.4065, validation loss: 0.2999
2024-06-03 12:05:03 [INFO]: Epoch 020 - training loss: 0.4025, validation loss: 0.2943
2024-06-03 12:05:09 [INFO]: Epoch 021 - training loss: 0.4000, validation loss: 0.2965
2024-06-03 12:05:14 [INFO]: Epoch 022 - training loss: 0.3944, validation loss: 0.2946
2024-06-03 12:05:19 [INFO]: Epoch 023 - training loss: 0.3963, validation loss: 0.2942
2024-06-03 12:05:25 [INFO]: Epoch 024 - training loss: 0.3931, validation loss: 0.2912
2024-06-03 12:05:30 [INFO]: Epoch 025 - training loss: 0.3876, validation loss: 0.3050
2024-06-03 12:05:35 [INFO]: Epoch 026 - training loss: 0.3838, validation loss: 0.2946
2024-06-03 12:05:41 [INFO]: Epoch 027 - training loss: 0.3820, validation loss: 0.2963
2024-06-03 12:05:46 [INFO]: Epoch 028 - training loss: 0.3797, validation loss: 0.2998
2024-06-03 12:05:51 [INFO]: Epoch 029 - training loss: 0.3768, validation loss: 0.2995
2024-06-03 12:05:57 [INFO]: Epoch 030 - training loss: 0.3785, validation loss: 0.3003
2024-06-03 12:06:02 [INFO]: Epoch 031 - training loss: 0.3772, validation loss: 0.2903
2024-06-03 12:06:07 [INFO]: Epoch 032 - training loss: 0.3714, validation loss: 0.2935
2024-06-03 12:06:13 [INFO]: Epoch 033 - training loss: 0.3714, validation loss: 0.2935
2024-06-03 12:06:18 [INFO]: Epoch 034 - training loss: 0.3658, validation loss: 0.2969
2024-06-03 12:06:24 [INFO]: Epoch 035 - training loss: 0.3649, validation loss: 0.2942
2024-06-03 12:06:29 [INFO]: Epoch 036 - training loss: 0.3681, validation loss: 0.2965
2024-06-03 12:06:35 [INFO]: Epoch 037 - training loss: 0.3681, validation loss: 0.2925
2024-06-03 12:06:40 [INFO]: Epoch 038 - training loss: 0.3661, validation loss: 0.2918
2024-06-03 12:06:46 [INFO]: Epoch 039 - training loss: 0.3663, validation loss: 0.2898
2024-06-03 12:06:51 [INFO]: Epoch 040 - training loss: 0.3627, validation loss: 0.2946
2024-06-03 12:06:56 [INFO]: Epoch 041 - training loss: 0.3615, validation loss: 0.2906
2024-06-03 12:07:01 [INFO]: Epoch 042 - training loss: 0.3587, validation loss: 0.2936
2024-06-03 12:07:06 [INFO]: Epoch 043 - training loss: 0.3631, validation loss: 0.2956
2024-06-03 12:07:12 [INFO]: Epoch 044 - training loss: 0.3585, validation loss: 0.2901
2024-06-03 12:07:17 [INFO]: Epoch 045 - training loss: 0.3563, validation loss: 0.2974
2024-06-03 12:07:22 [INFO]: Epoch 046 - training loss: 0.3550, validation loss: 0.2873
2024-06-03 12:07:27 [INFO]: Epoch 047 - training loss: 0.3551, validation loss: 0.2871
2024-06-03 12:07:32 [INFO]: Epoch 048 - training loss: 0.3557, validation loss: 0.2926
2024-06-03 12:07:37 [INFO]: Epoch 049 - training loss: 0.3533, validation loss: 0.2899
2024-06-03 12:07:43 [INFO]: Epoch 050 - training loss: 0.3549, validation loss: 0.2886
2024-06-03 12:07:48 [INFO]: Epoch 051 - training loss: 0.3510, validation loss: 0.2910
2024-06-03 12:07:53 [INFO]: Epoch 052 - training loss: 0.3506, validation loss: 0.2864
2024-06-03 12:07:58 [INFO]: Epoch 053 - training loss: 0.3508, validation loss: 0.2830
2024-06-03 12:08:04 [INFO]: Epoch 054 - training loss: 0.3516, validation loss: 0.2836
2024-06-03 12:08:09 [INFO]: Epoch 055 - training loss: 0.3492, validation loss: 0.2879
2024-06-03 12:08:15 [INFO]: Epoch 056 - training loss: 0.3494, validation loss: 0.2855
2024-06-03 12:08:20 [INFO]: Epoch 057 - training loss: 0.3499, validation loss: 0.2838
2024-06-03 12:08:25 [INFO]: Epoch 058 - training loss: 0.3479, validation loss: 0.2917
2024-06-03 12:08:30 [INFO]: Epoch 059 - training loss: 0.3468, validation loss: 0.2875
2024-06-03 12:08:35 [INFO]: Epoch 060 - training loss: 0.3452, validation loss: 0.2895
2024-06-03 12:08:41 [INFO]: Epoch 061 - training loss: 0.3444, validation loss: 0.2823
2024-06-03 12:08:46 [INFO]: Epoch 062 - training loss: 0.3431, validation loss: 0.2883
2024-06-03 12:08:51 [INFO]: Epoch 063 - training loss: 0.3451, validation loss: 0.2888
2024-06-03 12:08:56 [INFO]: Epoch 064 - training loss: 0.3438, validation loss: 0.2856
2024-06-03 12:09:01 [INFO]: Epoch 065 - training loss: 0.3436, validation loss: 0.2844
2024-06-03 12:09:07 [INFO]: Epoch 066 - training loss: 0.3459, validation loss: 0.2808
2024-06-03 12:09:12 [INFO]: Epoch 067 - training loss: 0.3429, validation loss: 0.2824
2024-06-03 12:09:17 [INFO]: Epoch 068 - training loss: 0.3400, validation loss: 0.2870
2024-06-03 12:09:22 [INFO]: Epoch 069 - training loss: 0.3396, validation loss: 0.2774
2024-06-03 12:09:28 [INFO]: Epoch 070 - training loss: 0.3425, validation loss: 0.2812
2024-06-03 12:09:33 [INFO]: Epoch 071 - training loss: 0.3413, validation loss: 0.2878
2024-06-03 12:09:39 [INFO]: Epoch 072 - training loss: 0.3404, validation loss: 0.2858
2024-06-03 12:09:44 [INFO]: Epoch 073 - training loss: 0.3369, validation loss: 0.2887
2024-06-03 12:09:50 [INFO]: Epoch 074 - training loss: 0.3368, validation loss: 0.2818
2024-06-03 12:09:56 [INFO]: Epoch 075 - training loss: 0.3375, validation loss: 0.2803
2024-06-03 12:10:01 [INFO]: Epoch 076 - training loss: 0.3342, validation loss: 0.2772
2024-06-03 12:10:06 [INFO]: Epoch 077 - training loss: 0.3362, validation loss: 0.2789
2024-06-03 12:10:11 [INFO]: Epoch 078 - training loss: 0.3382, validation loss: 0.2852
2024-06-03 12:10:16 [INFO]: Epoch 079 - training loss: 0.3377, validation loss: 0.2795
2024-06-03 12:10:22 [INFO]: Epoch 080 - training loss: 0.3363, validation loss: 0.2784
2024-06-03 12:10:27 [INFO]: Epoch 081 - training loss: 0.3346, validation loss: 0.2754
2024-06-03 12:10:32 [INFO]: Epoch 082 - training loss: 0.3356, validation loss: 0.2851
2024-06-03 12:10:38 [INFO]: Epoch 083 - training loss: 0.3346, validation loss: 0.2801
2024-06-03 12:10:43 [INFO]: Epoch 084 - training loss: 0.3344, validation loss: 0.2773
2024-06-03 12:10:48 [INFO]: Epoch 085 - training loss: 0.3342, validation loss: 0.2794
2024-06-03 12:10:54 [INFO]: Epoch 086 - training loss: 0.3335, validation loss: 0.2772
2024-06-03 12:10:59 [INFO]: Epoch 087 - training loss: 0.3315, validation loss: 0.2797
2024-06-03 12:11:05 [INFO]: Epoch 088 - training loss: 0.3302, validation loss: 0.2763
2024-06-03 12:11:10 [INFO]: Epoch 089 - training loss: 0.3338, validation loss: 0.2806
2024-06-03 12:11:15 [INFO]: Epoch 090 - training loss: 0.3369, validation loss: 0.2759
2024-06-03 12:11:21 [INFO]: Epoch 091 - training loss: 0.3306, validation loss: 0.2741
2024-06-03 12:11:26 [INFO]: Epoch 092 - training loss: 0.3298, validation loss: 0.2782
2024-06-03 12:11:31 [INFO]: Epoch 093 - training loss: 0.3307, validation loss: 0.2789
2024-06-03 12:11:36 [INFO]: Epoch 094 - training loss: 0.3285, validation loss: 0.2805
2024-06-03 12:11:41 [INFO]: Epoch 095 - training loss: 0.3363, validation loss: 0.2731
2024-06-03 12:11:47 [INFO]: Epoch 096 - training loss: 0.3388, validation loss: 0.2731
2024-06-03 12:11:52 [INFO]: Epoch 097 - training loss: 0.3299, validation loss: 0.2754
2024-06-03 12:11:57 [INFO]: Epoch 098 - training loss: 0.3278, validation loss: 0.2749
2024-06-03 12:12:02 [INFO]: Epoch 099 - training loss: 0.3309, validation loss: 0.2749
2024-06-03 12:12:08 [INFO]: Epoch 100 - training loss: 0.3260, validation loss: 0.2773
2024-06-03 12:12:08 [INFO]: Finished training. The best model is from epoch#95.
2024-06-03 12:12:08 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_0/20240603_T120309/SCINet.pypots
2024-06-03 12:12:10 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_0/imputation.pkl
2024-06-03 12:12:10 [INFO]: Round0 - SCINet on BeijingAir: MAE=0.2656, MSE=0.3360, MRE=0.3625
2024-06-03 12:12:10 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-06-03 12:12:10 [INFO]: Using the given device: cuda:0
2024-06-03 12:12:10 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_1/20240603_T121210
2024-06-03 12:12:10 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_1/20240603_T121210/tensorboard
2024-06-03 12:12:11 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 12:12:16 [INFO]: Epoch 001 - training loss: 1.4029, validation loss: 0.8829
2024-06-03 12:12:21 [INFO]: Epoch 002 - training loss: 0.9974, validation loss: 0.5150
2024-06-03 12:12:26 [INFO]: Epoch 003 - training loss: 0.7793, validation loss: 0.4068
2024-06-03 12:12:31 [INFO]: Epoch 004 - training loss: 0.6760, validation loss: 0.3697
2024-06-03 12:12:37 [INFO]: Epoch 005 - training loss: 0.6178, validation loss: 0.3443
2024-06-03 12:12:42 [INFO]: Epoch 006 - training loss: 0.5695, validation loss: 0.3298
2024-06-03 12:12:47 [INFO]: Epoch 007 - training loss: 0.5385, validation loss: 0.3175
2024-06-03 12:12:53 [INFO]: Epoch 008 - training loss: 0.5080, validation loss: 0.3086
2024-06-03 12:12:58 [INFO]: Epoch 009 - training loss: 0.4945, validation loss: 0.3127
2024-06-03 12:13:03 [INFO]: Epoch 010 - training loss: 0.4778, validation loss: 0.3010
2024-06-03 12:13:09 [INFO]: Epoch 011 - training loss: 0.4621, validation loss: 0.3002
2024-06-03 12:13:14 [INFO]: Epoch 012 - training loss: 0.4497, validation loss: 0.2989
2024-06-03 12:13:19 [INFO]: Epoch 013 - training loss: 0.4438, validation loss: 0.2985
2024-06-03 12:13:24 [INFO]: Epoch 014 - training loss: 0.4356, validation loss: 0.2917
2024-06-03 12:13:29 [INFO]: Epoch 015 - training loss: 0.4311, validation loss: 0.2974
2024-06-03 12:13:34 [INFO]: Epoch 016 - training loss: 0.4235, validation loss: 0.2946
2024-06-03 12:13:40 [INFO]: Epoch 017 - training loss: 0.4210, validation loss: 0.2979
2024-06-03 12:13:45 [INFO]: Epoch 018 - training loss: 0.4146, validation loss: 0.2944
2024-06-03 12:13:50 [INFO]: Epoch 019 - training loss: 0.4089, validation loss: 0.2961
2024-06-03 12:13:55 [INFO]: Epoch 020 - training loss: 0.4048, validation loss: 0.2958
2024-06-03 12:14:00 [INFO]: Epoch 021 - training loss: 0.4029, validation loss: 0.2981
2024-06-03 12:14:05 [INFO]: Epoch 022 - training loss: 0.3979, validation loss: 0.2978
2024-06-03 12:14:10 [INFO]: Epoch 023 - training loss: 0.3938, validation loss: 0.2944
2024-06-03 12:14:15 [INFO]: Epoch 024 - training loss: 0.3927, validation loss: 0.2938
2024-06-03 12:14:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:14:15 [INFO]: Finished training. The best model is from epoch#14.
2024-06-03 12:14:16 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_1/20240603_T121210/SCINet.pypots
2024-06-03 12:14:17 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_1/imputation.pkl
2024-06-03 12:14:17 [INFO]: Round1 - SCINet on BeijingAir: MAE=0.2856, MSE=0.3602, MRE=0.3898
2024-06-03 12:14:17 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-06-03 12:14:17 [INFO]: Using the given device: cuda:0
2024-06-03 12:14:18 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_2/20240603_T121417
2024-06-03 12:14:18 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_2/20240603_T121417/tensorboard
2024-06-03 12:14:18 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 12:14:24 [INFO]: Epoch 001 - training loss: 1.3449, validation loss: 0.7632
2024-06-03 12:14:30 [INFO]: Epoch 002 - training loss: 0.9612, validation loss: 0.5054
2024-06-03 12:14:35 [INFO]: Epoch 003 - training loss: 0.7997, validation loss: 0.4305
2024-06-03 12:14:40 [INFO]: Epoch 004 - training loss: 0.7025, validation loss: 0.3763
2024-06-03 12:14:46 [INFO]: Epoch 005 - training loss: 0.6317, validation loss: 0.3445
2024-06-03 12:14:51 [INFO]: Epoch 006 - training loss: 0.5826, validation loss: 0.3218
2024-06-03 12:14:56 [INFO]: Epoch 007 - training loss: 0.5487, validation loss: 0.3109
2024-06-03 12:15:01 [INFO]: Epoch 008 - training loss: 0.5203, validation loss: 0.3033
2024-06-03 12:15:06 [INFO]: Epoch 009 - training loss: 0.5023, validation loss: 0.2977
2024-06-03 12:15:12 [INFO]: Epoch 010 - training loss: 0.4858, validation loss: 0.2952
2024-06-03 12:15:16 [INFO]: Epoch 011 - training loss: 0.4709, validation loss: 0.2900
2024-06-03 12:15:19 [INFO]: Epoch 012 - training loss: 0.4627, validation loss: 0.2928
2024-06-03 12:15:23 [INFO]: Epoch 013 - training loss: 0.4522, validation loss: 0.2917
2024-06-03 12:15:26 [INFO]: Epoch 014 - training loss: 0.4424, validation loss: 0.2848
2024-06-03 12:15:28 [INFO]: Epoch 015 - training loss: 0.4366, validation loss: 0.2887
2024-06-03 12:15:32 [INFO]: Epoch 016 - training loss: 0.4302, validation loss: 0.2860
2024-06-03 12:15:36 [INFO]: Epoch 017 - training loss: 0.4271, validation loss: 0.2899
2024-06-03 12:15:41 [INFO]: Epoch 018 - training loss: 0.4190, validation loss: 0.2903
2024-06-03 12:15:45 [INFO]: Epoch 019 - training loss: 0.4131, validation loss: 0.2885
2024-06-03 12:15:49 [INFO]: Epoch 020 - training loss: 0.4069, validation loss: 0.2925
2024-06-03 12:15:53 [INFO]: Epoch 021 - training loss: 0.4008, validation loss: 0.2934
2024-06-03 12:15:57 [INFO]: Epoch 022 - training loss: 0.3985, validation loss: 0.2894
2024-06-03 12:16:01 [INFO]: Epoch 023 - training loss: 0.3935, validation loss: 0.2950
2024-06-03 12:16:06 [INFO]: Epoch 024 - training loss: 0.3929, validation loss: 0.3004
2024-06-03 12:16:06 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:16:06 [INFO]: Finished training. The best model is from epoch#14.
2024-06-03 12:16:06 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_2/20240603_T121417/SCINet.pypots
2024-06-03 12:16:08 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_2/imputation.pkl
2024-06-03 12:16:08 [INFO]: Round2 - SCINet on BeijingAir: MAE=0.2859, MSE=0.3666, MRE=0.3903
2024-06-03 12:16:08 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-06-03 12:16:08 [INFO]: Using the given device: cuda:0
2024-06-03 12:16:08 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_3/20240603_T121608
2024-06-03 12:16:08 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_3/20240603_T121608/tensorboard
2024-06-03 12:16:08 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 12:16:12 [INFO]: Epoch 001 - training loss: 1.2891, validation loss: 0.7127
2024-06-03 12:16:16 [INFO]: Epoch 002 - training loss: 0.9493, validation loss: 0.5212
2024-06-03 12:16:21 [INFO]: Epoch 003 - training loss: 0.8136, validation loss: 0.4574
2024-06-03 12:16:25 [INFO]: Epoch 004 - training loss: 0.7255, validation loss: 0.4079
2024-06-03 12:16:30 [INFO]: Epoch 005 - training loss: 0.6644, validation loss: 0.3824
2024-06-03 12:16:34 [INFO]: Epoch 006 - training loss: 0.6192, validation loss: 0.3589
2024-06-03 12:16:38 [INFO]: Epoch 007 - training loss: 0.5830, validation loss: 0.3421
2024-06-03 12:16:42 [INFO]: Epoch 008 - training loss: 0.5547, validation loss: 0.3321
2024-06-03 12:16:46 [INFO]: Epoch 009 - training loss: 0.5292, validation loss: 0.3218
2024-06-03 12:16:50 [INFO]: Epoch 010 - training loss: 0.5084, validation loss: 0.3136
2024-06-03 12:16:55 [INFO]: Epoch 011 - training loss: 0.4915, validation loss: 0.3084
2024-06-03 12:16:59 [INFO]: Epoch 012 - training loss: 0.4769, validation loss: 0.3071
2024-06-03 12:17:03 [INFO]: Epoch 013 - training loss: 0.4645, validation loss: 0.3053
2024-06-03 12:17:07 [INFO]: Epoch 014 - training loss: 0.4611, validation loss: 0.3013
2024-06-03 12:17:12 [INFO]: Epoch 015 - training loss: 0.4536, validation loss: 0.3019
2024-06-03 12:17:16 [INFO]: Epoch 016 - training loss: 0.4465, validation loss: 0.2991
2024-06-03 12:17:20 [INFO]: Epoch 017 - training loss: 0.4354, validation loss: 0.3027
2024-06-03 12:17:24 [INFO]: Epoch 018 - training loss: 0.4312, validation loss: 0.2969
2024-06-03 12:17:29 [INFO]: Epoch 019 - training loss: 0.4239, validation loss: 0.2942
2024-06-03 12:17:33 [INFO]: Epoch 020 - training loss: 0.4181, validation loss: 0.2948
2024-06-03 12:17:36 [INFO]: Epoch 021 - training loss: 0.4147, validation loss: 0.2998
2024-06-03 12:17:40 [INFO]: Epoch 022 - training loss: 0.4098, validation loss: 0.2993
2024-06-03 12:17:44 [INFO]: Epoch 023 - training loss: 0.4054, validation loss: 0.2971
2024-06-03 12:17:48 [INFO]: Epoch 024 - training loss: 0.4021, validation loss: 0.2975
2024-06-03 12:17:51 [INFO]: Epoch 025 - training loss: 0.3983, validation loss: 0.2922
2024-06-03 12:17:55 [INFO]: Epoch 026 - training loss: 0.3961, validation loss: 0.2952
2024-06-03 12:17:59 [INFO]: Epoch 027 - training loss: 0.3962, validation loss: 0.3032
2024-06-03 12:18:03 [INFO]: Epoch 028 - training loss: 0.3923, validation loss: 0.2918
2024-06-03 12:18:07 [INFO]: Epoch 029 - training loss: 0.3919, validation loss: 0.3013
2024-06-03 12:18:10 [INFO]: Epoch 030 - training loss: 0.3890, validation loss: 0.3018
2024-06-03 12:18:14 [INFO]: Epoch 031 - training loss: 0.3874, validation loss: 0.2972
2024-06-03 12:18:18 [INFO]: Epoch 032 - training loss: 0.3859, validation loss: 0.2976
2024-06-03 12:18:22 [INFO]: Epoch 033 - training loss: 0.3791, validation loss: 0.2976
2024-06-03 12:18:25 [INFO]: Epoch 034 - training loss: 0.3801, validation loss: 0.2918
2024-06-03 12:18:29 [INFO]: Epoch 035 - training loss: 0.3773, validation loss: 0.3066
2024-06-03 12:18:33 [INFO]: Epoch 036 - training loss: 0.3776, validation loss: 0.3011
2024-06-03 12:18:36 [INFO]: Epoch 037 - training loss: 0.3752, validation loss: 0.2991
2024-06-03 12:18:39 [INFO]: Epoch 038 - training loss: 0.3728, validation loss: 0.2990
2024-06-03 12:18:42 [INFO]: Epoch 039 - training loss: 0.3707, validation loss: 0.2906
2024-06-03 12:18:45 [INFO]: Epoch 040 - training loss: 0.3705, validation loss: 0.2988
2024-06-03 12:18:48 [INFO]: Epoch 041 - training loss: 0.3728, validation loss: 0.2938
2024-06-03 12:18:52 [INFO]: Epoch 042 - training loss: 0.3711, validation loss: 0.2962
2024-06-03 12:18:56 [INFO]: Epoch 043 - training loss: 0.3668, validation loss: 0.2968
2024-06-03 12:18:59 [INFO]: Epoch 044 - training loss: 0.3658, validation loss: 0.2892
2024-06-03 12:19:03 [INFO]: Epoch 045 - training loss: 0.3636, validation loss: 0.2942
2024-06-03 12:19:07 [INFO]: Epoch 046 - training loss: 0.3650, validation loss: 0.2955
2024-06-03 12:19:11 [INFO]: Epoch 047 - training loss: 0.3624, validation loss: 0.2998
2024-06-03 12:19:15 [INFO]: Epoch 048 - training loss: 0.3625, validation loss: 0.2911
2024-06-03 12:19:18 [INFO]: Epoch 049 - training loss: 0.3622, validation loss: 0.2917
2024-06-03 12:19:22 [INFO]: Epoch 050 - training loss: 0.3598, validation loss: 0.2948
2024-06-03 12:19:26 [INFO]: Epoch 051 - training loss: 0.3565, validation loss: 0.2909
2024-06-03 12:19:30 [INFO]: Epoch 052 - training loss: 0.3554, validation loss: 0.2926
2024-06-03 12:19:34 [INFO]: Epoch 053 - training loss: 0.3580, validation loss: 0.2923
2024-06-03 12:19:37 [INFO]: Epoch 054 - training loss: 0.3589, validation loss: 0.2910
2024-06-03 12:19:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-06-03 12:19:37 [INFO]: Finished training. The best model is from epoch#44.
2024-06-03 12:19:38 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_3/20240603_T121608/SCINet.pypots
2024-06-03 12:19:39 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_3/imputation.pkl
2024-06-03 12:19:39 [INFO]: Round3 - SCINet on BeijingAir: MAE=0.2738, MSE=0.3519, MRE=0.3737
2024-06-03 12:19:39 [INFO]: Have set the random seed as 2028 for numpy and pytorch.
2024-06-03 12:19:39 [INFO]: Using the given device: cuda:0
2024-06-03 12:19:39 [INFO]: Model files will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_4/20240603_T121939
2024-06-03 12:19:39 [INFO]: Tensorboard file will be saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_4/20240603_T121939/tensorboard
2024-06-03 12:19:40 [INFO]: SCINet initialized with the given hyperparameters, the number of trainable parameters: 26,833,140
2024-06-03 12:19:44 [INFO]: Epoch 001 - training loss: 1.2241, validation loss: 0.6031
2024-06-03 12:19:48 [INFO]: Epoch 002 - training loss: 0.8589, validation loss: 0.4589
2024-06-03 12:19:51 [INFO]: Epoch 003 - training loss: 0.7280, validation loss: 0.3978
2024-06-03 12:19:55 [INFO]: Epoch 004 - training loss: 0.6555, validation loss: 0.3676
2024-06-03 12:19:59 [INFO]: Epoch 005 - training loss: 0.6091, validation loss: 0.3462
2024-06-03 12:20:03 [INFO]: Epoch 006 - training loss: 0.5724, validation loss: 0.3337
2024-06-03 12:20:07 [INFO]: Epoch 007 - training loss: 0.5417, validation loss: 0.3282
2024-06-03 12:20:11 [INFO]: Epoch 008 - training loss: 0.5235, validation loss: 0.3228
2024-06-03 12:20:14 [INFO]: Epoch 009 - training loss: 0.5027, validation loss: 0.3095
2024-06-03 12:20:18 [INFO]: Epoch 010 - training loss: 0.4845, validation loss: 0.3062
2024-06-03 12:20:22 [INFO]: Epoch 011 - training loss: 0.4705, validation loss: 0.3024
2024-06-03 12:20:26 [INFO]: Epoch 012 - training loss: 0.4587, validation loss: 0.2992
2024-06-03 12:20:30 [INFO]: Epoch 013 - training loss: 0.4513, validation loss: 0.3005
2024-06-03 12:20:34 [INFO]: Epoch 014 - training loss: 0.4435, validation loss: 0.2911
2024-06-03 12:20:38 [INFO]: Epoch 015 - training loss: 0.4353, validation loss: 0.2996
2024-06-03 12:20:42 [INFO]: Epoch 016 - training loss: 0.4289, validation loss: 0.2967
2024-06-03 12:20:46 [INFO]: Epoch 017 - training loss: 0.4204, validation loss: 0.2922
2024-06-03 12:20:49 [INFO]: Epoch 018 - training loss: 0.4258, validation loss: 0.2928
2024-06-03 12:20:53 [INFO]: Epoch 019 - training loss: 0.4154, validation loss: 0.2880
2024-06-03 12:20:57 [INFO]: Epoch 020 - training loss: 0.4077, validation loss: 0.2984
2024-06-03 12:21:01 [INFO]: Epoch 021 - training loss: 0.4033, validation loss: 0.2995
2024-06-03 12:21:05 [INFO]: Epoch 022 - training loss: 0.4033, validation loss: 0.2968
2024-06-03 12:21:08 [INFO]: Epoch 023 - training loss: 0.3933, validation loss: 0.2924
2024-06-03 12:21:12 [INFO]: Epoch 024 - training loss: 0.3930, validation loss: 0.2869
2024-06-03 12:21:15 [INFO]: Epoch 025 - training loss: 0.3890, validation loss: 0.2914
2024-06-03 12:21:18 [INFO]: Epoch 026 - training loss: 0.3856, validation loss: 0.2914
2024-06-03 12:21:22 [INFO]: Epoch 027 - training loss: 0.3833, validation loss: 0.2895
2024-06-03 12:21:25 [INFO]: Epoch 028 - training loss: 0.3817, validation loss: 0.2920
2024-06-03 12:21:29 [INFO]: Epoch 029 - training loss: 0.3812, validation loss: 0.2881
2024-06-03 12:21:32 [INFO]: Epoch 030 - training loss: 0.3790, validation loss: 0.2925
2024-06-03 12:21:36 [INFO]: Epoch 031 - training loss: 0.3757, validation loss: 0.2876
2024-06-03 12:21:39 [INFO]: Epoch 032 - training loss: 0.3721, validation loss: 0.2914
2024-06-03 12:21:42 [INFO]: Epoch 033 - training loss: 0.3700, validation loss: 0.2915
2024-06-03 12:21:45 [INFO]: Epoch 034 - training loss: 0.3719, validation loss: 0.2852
2024-06-03 12:21:49 [INFO]: Epoch 035 - training loss: 0.3706, validation loss: 0.2867
2024-06-03 12:21:52 [INFO]: Epoch 036 - training loss: 0.3700, validation loss: 0.2941
2024-06-03 12:21:55 [INFO]: Epoch 037 - training loss: 0.3665, validation loss: 0.2877
2024-06-03 12:21:59 [INFO]: Epoch 038 - training loss: 0.3636, validation loss: 0.2900
2024-06-03 12:22:02 [INFO]: Epoch 039 - training loss: 0.3645, validation loss: 0.2864
2024-06-03 12:22:05 [INFO]: Epoch 040 - training loss: 0.3646, validation loss: 0.2810
2024-06-03 12:22:08 [INFO]: Epoch 041 - training loss: 0.3620, validation loss: 0.2910
2024-06-03 12:22:11 [INFO]: Epoch 042 - training loss: 0.3592, validation loss: 0.2866
2024-06-03 12:22:14 [INFO]: Epoch 043 - training loss: 0.3586, validation loss: 0.2870
2024-06-03 12:22:17 [INFO]: Epoch 044 - training loss: 0.3594, validation loss: 0.2830
2024-06-03 12:22:20 [INFO]: Epoch 045 - training loss: 0.3569, validation loss: 0.2945
2024-06-03 12:22:23 [INFO]: Epoch 046 - training loss: 0.3580, validation loss: 0.2873
2024-06-03 12:22:26 [INFO]: Epoch 047 - training loss: 0.3542, validation loss: 0.2924
2024-06-03 12:22:29 [INFO]: Epoch 048 - training loss: 0.3571, validation loss: 0.2801
2024-06-03 12:22:32 [INFO]: Epoch 049 - training loss: 0.3562, validation loss: 0.2890
2024-06-03 12:22:35 [INFO]: Epoch 050 - training loss: 0.3576, validation loss: 0.2850
2024-06-03 12:22:38 [INFO]: Epoch 051 - training loss: 0.3555, validation loss: 0.2814
2024-06-03 12:22:41 [INFO]: Epoch 052 - training loss: 0.3510, validation loss: 0.2804
2024-06-03 12:22:44 [INFO]: Epoch 053 - training loss: 0.3534, validation loss: 0.2805
2024-06-03 12:22:47 [INFO]: Epoch 054 - training loss: 0.3528, validation loss: 0.2827
2024-06-03 12:22:50 [INFO]: Epoch 055 - training loss: 0.3537, validation loss: 0.2846
2024-06-03 12:22:53 [INFO]: Epoch 056 - training loss: 0.3511, validation loss: 0.2815
2024-06-03 12:22:56 [INFO]: Epoch 057 - training loss: 0.3533, validation loss: 0.2866
2024-06-03 12:22:59 [INFO]: Epoch 058 - training loss: 0.3494, validation loss: 0.2799
2024-06-03 12:23:02 [INFO]: Epoch 059 - training loss: 0.3465, validation loss: 0.2819
2024-06-03 12:23:05 [INFO]: Epoch 060 - training loss: 0.3442, validation loss: 0.2823
2024-06-03 12:23:08 [INFO]: Epoch 061 - training loss: 0.3448, validation loss: 0.2824
2024-06-03 12:23:11 [INFO]: Epoch 062 - training loss: 0.3418, validation loss: 0.2844
2024-06-03 12:23:14 [INFO]: Epoch 063 - training loss: 0.3428, validation loss: 0.2835
2024-06-03 12:23:17 [INFO]: Epoch 064 - training loss: 0.3442, validation loss: 0.2792
2024-06-03 12:23:20 [INFO]: Epoch 065 - training loss: 0.3432, validation loss: 0.2799
2024-06-03 12:23:23 [INFO]: Epoch 066 - training loss: 0.3425, validation loss: 0.2879
2024-06-03 12:23:26 [INFO]: Epoch 067 - training loss: 0.3414, validation loss: 0.2783
2024-06-03 12:23:29 [INFO]: Epoch 068 - training loss: 0.3413, validation loss: 0.2796
2024-06-03 12:23:32 [INFO]: Epoch 069 - training loss: 0.3413, validation loss: 0.2811
2024-06-03 12:23:35 [INFO]: Epoch 070 - training loss: 0.3383, validation loss: 0.2801
2024-06-03 12:23:38 [INFO]: Epoch 071 - training loss: 0.3432, validation loss: 0.2804
2024-06-03 12:23:40 [INFO]: Epoch 072 - training loss: 0.3403, validation loss: 0.2795
2024-06-03 12:23:43 [INFO]: Epoch 073 - training loss: 0.3401, validation loss: 0.2745
2024-06-03 12:23:45 [INFO]: Epoch 074 - training loss: 0.3373, validation loss: 0.2793
2024-06-03 12:23:47 [INFO]: Epoch 075 - training loss: 0.3370, validation loss: 0.2850
2024-06-03 12:23:50 [INFO]: Epoch 076 - training loss: 0.3405, validation loss: 0.2743
2024-06-03 12:23:52 [INFO]: Epoch 077 - training loss: 0.3367, validation loss: 0.2754
2024-06-03 12:23:55 [INFO]: Epoch 078 - training loss: 0.3347, validation loss: 0.2761
2024-06-03 12:23:58 [INFO]: Epoch 079 - training loss: 0.3367, validation loss: 0.2736
2024-06-03 12:24:00 [INFO]: Epoch 080 - training loss: 0.3370, validation loss: 0.2760
2024-06-03 12:24:03 [INFO]: Epoch 081 - training loss: 0.3338, validation loss: 0.2813
2024-06-03 12:24:05 [INFO]: Epoch 082 - training loss: 0.3357, validation loss: 0.2744
2024-06-03 12:24:08 [INFO]: Epoch 083 - training loss: 0.3352, validation loss: 0.2789
2024-06-03 12:24:10 [INFO]: Epoch 084 - training loss: 0.3339, validation loss: 0.2721
2024-06-03 12:24:13 [INFO]: Epoch 085 - training loss: 0.3336, validation loss: 0.2785
2024-06-03 12:24:15 [INFO]: Epoch 086 - training loss: 0.3358, validation loss: 0.2770
2024-06-03 12:24:18 [INFO]: Epoch 087 - training loss: 0.3338, validation loss: 0.2759
2024-06-03 12:24:20 [INFO]: Epoch 088 - training loss: 0.3348, validation loss: 0.2780
2024-06-03 12:24:23 [INFO]: Epoch 089 - training loss: 0.3360, validation loss: 0.2734
2024-06-03 12:24:25 [INFO]: Epoch 090 - training loss: 0.3300, validation loss: 0.2752
2024-06-03 12:24:28 [INFO]: Epoch 091 - training loss: 0.3329, validation loss: 0.2750
2024-06-03 12:24:31 [INFO]: Epoch 092 - training loss: 0.3308, validation loss: 0.2765
2024-06-03 12:24:33 [INFO]: Epoch 093 - training loss: 0.3319, validation loss: 0.2754
2024-06-03 12:24:36 [INFO]: Epoch 094 - training loss: 0.3275, validation loss: 0.2720
2024-06-03 12:24:38 [INFO]: Epoch 095 - training loss: 0.3283, validation loss: 0.2716
2024-06-03 12:24:41 [INFO]: Epoch 096 - training loss: 0.3276, validation loss: 0.2718
2024-06-03 12:24:43 [INFO]: Epoch 097 - training loss: 0.3305, validation loss: 0.2787
2024-06-03 12:24:46 [INFO]: Epoch 098 - training loss: 0.3308, validation loss: 0.2745
2024-06-03 12:24:48 [INFO]: Epoch 099 - training loss: 0.3281, validation loss: 0.2798
2024-06-03 12:24:51 [INFO]: Epoch 100 - training loss: 0.3265, validation loss: 0.2732
2024-06-03 12:24:51 [INFO]: Finished training. The best model is from epoch#95.
2024-06-03 12:24:51 [INFO]: Saved the model to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_4/20240603_T121939/SCINet.pypots
2024-06-03 12:24:52 [INFO]: Successfully saved to results_subseq_rate05/BeijingAir/SCINet_BeijingAir/round_4/imputation.pkl
2024-06-03 12:24:52 [INFO]: Round4 - SCINet on BeijingAir: MAE=0.2649, MSE=0.3292, MRE=0.3615
2024-06-03 12:24:52 [INFO]: Done! Final results:
Averaged SCINet (26,833,140 params) on BeijingAir: MAE=0.2678 ± 0.010125605683312545, MSE=0.3441 ± 0.015455967862044106, MRE=0.3558 ± 0.01345506580273264, average inference time=0.30